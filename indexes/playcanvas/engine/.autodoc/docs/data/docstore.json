[["0",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/conf-api.json)\n\nThis code is a configuration file for generating documentation for the PlayCanvas engine project. The file specifies various settings for the documentation generation process, including which plugins to use, the depth to which the documentation should recurse, the source files to include and exclude, the source type (module), and the tags and templates to use.\n\nThe \"plugins\" array specifies two plugins to use: \"markdown\" and \"./node_modules/jsdoc-tsimport-plugin/index.js\". The \"markdown\" plugin allows for the use of Markdown syntax in the documentation comments, while the \"jsdoc-tsimport-plugin\" plugin allows for the import of TypeScript modules in JSDoc comments.\n\nThe \"recurseDepth\" setting specifies the maximum depth to which the documentation should recurse when generating documentation for the source files.\n\nThe \"source\" object specifies the source files to include and exclude. The \"include\" array specifies that all files in the \"src\" directory should be included, while the \"excludePattern\" setting specifies that any files or directories starting with an underscore should be excluded.\n\nThe \"sourceType\" setting specifies that the source files are modules.\n\nThe \"tags\" object specifies various settings related to JSDoc tags, including allowing unknown tags and specifying which dictionaries to use.\n\nThe \"templates\" object specifies settings related to the documentation templates, including whether to use clever links and monospace links.\n\nFinally, the \"opts\" object specifies various options related to the documentation generation process, including the destination directory for the generated documentation, the encoding to use, whether to recurse through subdirectories, and the template to use.\n\nOverall, this configuration file is an important part of the PlayCanvas engine project, as it allows for the generation of comprehensive documentation for the project's source code. By specifying various settings related to the documentation generation process, this file ensures that the generated documentation is accurate, complete, and easy to read.\n## Questions: \n 1. What plugins are being used in this code and what do they do?\n    - The code is using two plugins: \"plugins/markdown\" and \"./node_modules/jsdoc-tsimport-plugin/index.js\". The first plugin allows for the use of markdown syntax in the documentation, while the second plugin allows for the importing of TypeScript definitions into the documentation.\n2. What is the purpose of the \"source\" object in this code?\n    - The \"source\" object specifies which files and directories should be included in the documentation, as well as which file types to include and exclude.\n3. What is the significance of the \"opts\" object in this code?\n    - The \"opts\" object specifies various options for the documentation generation, such as the output destination, encoding, and template to use.","metadata":{"source":".autodoc/docs/markdown/conf-api.md"}}],["1",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/button/red_button_atlas.json)\n\nThe code above is a JSON object that defines the properties of a texture atlas. A texture atlas is a collection of smaller images, or frames, that are combined into a larger image. This technique is commonly used in game development to reduce the number of texture swaps and draw calls, which can improve performance.\n\nThe `minfilter` and `magfilter` properties define the texture filtering mode for the atlas. In this case, the \"nearest\" mode is used, which means that the nearest pixel value is used when scaling the texture up or down. This mode can result in pixelated or blurry textures, but it is often used for retro-style games or when performance is a concern.\n\nThe `frames` property is an object that contains the individual frames of the atlas. Each frame is identified by a numeric key, starting from 0. For each frame, the `rect` property defines the position and size of the frame within the atlas image. The `pivot` property defines the center point of the frame, which is used for rotation and scaling. The `border` property defines the size of the border around the frame, which can be used for 9-slice scaling.\n\nThis code is likely used in the PlayCanvas engine to define the properties of a texture atlas that is used for rendering sprites or other graphical elements in a game. The JSON object can be loaded into the engine and used to create a texture asset, which can then be used by game objects to render their graphics. Here is an example of how this code might be used in the engine:\n\n```javascript\n// Load the texture atlas JSON file\nvar atlasData = {\n  \"minfilter\": \"nearest\",\n  \"magfilter\": \"nearest\",\n  \"frames\": {\n    \"0\": {\n      \"rect\": [0, 147, 190, 49],\n      \"pivot\": [0.5, 0.5],\n      \"border\": [7,11,7,7]\n    },\n    // ...\n  }\n};\nvar atlasTexture = new pc.Texture();\natlasTexture.setSource(atlasData);\n\n// Create a sprite using the atlas texture\nvar sprite = new pc.Sprite();\nsprite.frame = 0; // Use the first frame of the atlas\nsprite.texture = atlasTexture;\nsprite.type = pc.SPRITETYPE_SIMPLE;\nsprite.pivot = new pc.Vec2(0.5, 0.5);\nsprite.width = 100;\nsprite.height = 100;\n\n// Add the sprite to a game object\nvar entity = new pc.Entity();\nentity.addComponent(\"sprite\", {\n  sprite: sprite\n});\n``` \n\nIn this example, the texture atlas JSON data is loaded into a new `pc.Texture` object, which can then be used to create a `pc.Sprite` object. The `frame` property of the sprite is set to 0, which corresponds to the first frame of the atlas. The `texture` property is set to the atlas texture, and the `pivot`, `width`, and `height` properties are set to define the sprite's position, size, and rotation. Finally, the sprite is added to a new `pc.Entity` object, which can be added to the game world and rendered by the engine.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines the texture filtering and frames for a sprite sheet.\n\n2. What do the values in the \"rect\" arrays represent?\n- The values in the \"rect\" arrays represent the position and size of each frame in the sprite sheet.\n\n3. What is the significance of the \"pivot\" and \"border\" values?\n- The \"pivot\" value represents the point around which the sprite will rotate or scale, while the \"border\" value represents the size of the border around the sprite that should not be scaled or distorted.","metadata":{"source":".autodoc/docs/markdown/examples/assets/button/red_button_atlas.md"}}],["2",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/button/red_button_default.json)\n\nThis code is a JSON object that contains properties related to rendering a texture atlas asset in the PlayCanvas engine. \n\nThe `renderMode` property specifies the rendering mode for the texture atlas. A value of 1 indicates that the texture atlas should be rendered using a 2D canvas context. \n\nThe `pixelsPerUnit` property specifies the number of pixels per unit for the texture atlas. This value is used to scale the texture atlas when rendering it. \n\nThe `textureAtlasAsset` property specifies the path to the texture atlas asset that should be rendered. In this case, the path is \"static/assets/button/red_button_atlas.json\". \n\nThe `frameKeys` property is an array of keys that specify which frames from the texture atlas should be rendered. In this case, the array contains a single value of 1, indicating that only the first frame should be rendered. \n\nThis code is likely used in the larger PlayCanvas engine project to specify how a texture atlas asset should be rendered. The JSON object can be passed as an argument to a function that renders the texture atlas. For example, the following code could be used to render the texture atlas specified in the JSON object:\n\n```\nvar textureAtlasSettings = {\n    \"renderMode\": 1,\n    \"pixelsPerUnit\": 1,\n    \"textureAtlasAsset\": \"static/assets/button/red_button_atlas.json\",\n    \"frameKeys\": [1]\n};\n\nvar textureAtlas = new TextureAtlas(textureAtlasSettings);\ntextureAtlas.render();\n```\n\nIn this example, a new `TextureAtlas` object is created using the settings specified in the `textureAtlasSettings` object. The `render()` method is then called on the `TextureAtlas` object to render the texture atlas.\n## Questions: \n 1. **What does the \"renderMode\" property do?** \nThe \"renderMode\" property is likely used to specify the rendering mode for the asset, but without more context it's difficult to determine exactly what this property does.\n\n2. **What is the purpose of the \"pixelsPerUnit\" property?** \nThe \"pixelsPerUnit\" property is likely used to specify the number of pixels per unit for the asset, which could be important for scaling and positioning the asset correctly.\n\n3. **What is the significance of the \"frameKeys\" array?** \nThe \"frameKeys\" array likely contains keys that correspond to specific frames within the texture atlas asset, which could be used to animate the asset or display different variations of the asset.","metadata":{"source":".autodoc/docs/markdown/examples/assets/button/red_button_default.md"}}],["3",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/button/red_button_disabled.json)\n\nThis code is a JSON object that contains properties related to rendering a specific asset in the PlayCanvas engine. \n\nThe `renderMode` property specifies the rendering mode for the asset. In this case, it is set to `1`, which likely corresponds to a specific rendering method within the engine. \n\nThe `pixelsPerUnit` property specifies the number of pixels per unit for the asset. This can be used to control the size of the asset when it is rendered. \n\nThe `textureAtlasAsset` property specifies the location of the texture atlas asset that contains the image data for the asset being rendered. In this case, it is set to `\"static/assets/button/red_button_atlas.json\"`. \n\nThe `frameKeys` property is an array that contains the frame keys for the asset. This likely corresponds to specific frames within the texture atlas that should be used when rendering the asset. In this case, it contains a single value of `3`. \n\nOverall, this code is used to specify rendering properties for a specific asset within the PlayCanvas engine. It can be used to control how the asset is rendered and what image data is used for rendering. \n\nHere is an example of how this code might be used within the larger project:\n\n```javascript\nvar assetProperties = {\n  \"renderMode\": 1,\n  \"pixelsPerUnit\": 1,\n  \"textureAtlasAsset\": \"static/assets/button/red_button_atlas.json\",\n  \"frameKeys\": [3]\n};\n\nvar asset = new PlayCanvas.Asset(\"button\", \"texture\", assetProperties);\n\n// Use the asset to render a button\nvar button = new PlayCanvas.Entity();\nbutton.addComponent(\"element\", {\n  type: \"image\",\n  asset: asset\n});\n``` \n\nIn this example, the `assetProperties` object is used to create a new asset for a button. The `textureAtlasAsset` property specifies the location of the texture atlas asset that contains the image data for the button. The `frameKeys` property specifies which frame within the texture atlas should be used for rendering the button. \n\nThe asset is then used to create a new entity for the button, which is given an `element` component with the `type` set to `\"image\"` and the `asset` set to the newly created asset. This will render the button using the specified rendering properties.\n## Questions: \n 1. What does the \"renderMode\" property do?\n- The \"renderMode\" property is likely used to specify the rendering mode for the asset, but without more context it is difficult to determine exactly what it does.\n\n2. What is the purpose of the \"pixelsPerUnit\" property?\n- The \"pixelsPerUnit\" property is likely used to specify the number of pixels per unit for the asset, which could be useful for scaling the asset appropriately.\n\n3. What is the significance of the \"frameKeys\" array?\n- The \"frameKeys\" array likely contains keys that correspond to specific frames within the texture atlas asset, but without more context it is difficult to determine exactly what it is used for.","metadata":{"source":".autodoc/docs/markdown/examples/assets/button/red_button_disabled.md"}}],["4",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/button/red_button_hover.json)\n\nThis code is a JSON object that contains properties related to rendering a texture atlas asset in the PlayCanvas engine. \n\nThe `renderMode` property specifies the rendering mode for the texture atlas. In this case, it is set to `1`, which likely corresponds to a specific rendering method within the engine. \n\nThe `pixelsPerUnit` property specifies the number of pixels per unit for the texture atlas. This can be used to control the size of the asset when it is rendered in the game. \n\nThe `textureAtlasAsset` property specifies the path to the texture atlas asset that will be rendered. In this case, it is set to `\"static/assets/button/red_button_atlas.json\"`. This likely corresponds to a specific asset within the game that will be rendered as a button. \n\nThe `frameKeys` property is an array of integers that specifies which frames of the texture atlas should be rendered. In this case, it only contains the integer `0`, which likely corresponds to the first frame of the asset. \n\nOverall, this code is used to specify the properties of a texture atlas asset that will be rendered in the PlayCanvas engine. It can be used to control the rendering mode, size, and specific frames of the asset. \n\nExample usage:\n\n```javascript\nvar buttonAsset = {\n  \"renderMode\": 1,\n  \"pixelsPerUnit\": 1,\n  \"textureAtlasAsset\": \"static/assets/button/red_button_atlas.json\",\n  \"frameKeys\": [0]\n};\n\n// Load the button asset into the engine\nvar buttonEntity = new pc.Entity();\nbuttonEntity.addComponent(\"element\", {\n  type: \"image\",\n  spriteAsset: buttonAsset\n});\n``` \n\nIn this example, the `buttonAsset` object is used to specify the properties of a button asset that will be rendered in the game. The `addComponent` method is then used to add the button asset to a new entity in the game.\n## Questions: \n 1. What does the \"renderMode\" property do?\n   - The \"renderMode\" property likely determines how the graphics are rendered, but without more context it's impossible to say for sure.\n2. What is the significance of the \"pixelsPerUnit\" property?\n   - The \"pixelsPerUnit\" property likely determines the scale of the graphics, but without more context it's impossible to say for sure.\n3. What is the purpose of the \"frameKeys\" array?\n   - The \"frameKeys\" array likely contains keys that correspond to specific frames in an animation or sprite sheet, but without more context it's impossible to say for sure.","metadata":{"source":".autodoc/docs/markdown/examples/assets/button/red_button_hover.md"}}],["5",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/button/red_button_pressed.json)\n\nThis code is a JSON object that contains configuration settings for rendering a specific asset in the PlayCanvas engine. \n\nThe `renderMode` property specifies the rendering mode for the asset. In this case, it is set to `1`, which likely corresponds to a specific rendering technique or shader. \n\nThe `pixelsPerUnit` property specifies the number of pixels per unit for the asset. This can be used to ensure that the asset is rendered at the correct size and scale in the game world. \n\nThe `textureAtlasAsset` property specifies the location of the texture atlas file for the asset. A texture atlas is a collection of smaller images that are combined into a larger image, which can improve rendering performance. In this case, the texture atlas file is located at `static/assets/button/red_button_atlas.json`. \n\nThe `frameKeys` property is an array of keys that correspond to specific frames within the texture atlas. In this case, the array contains a single value of `2`, which likely corresponds to a specific frame within the texture atlas that should be used for rendering the asset. \n\nOverall, this code is used to configure the rendering of a specific asset in the PlayCanvas engine. By specifying the rendering mode, pixels per unit, texture atlas location, and frame keys, developers can ensure that the asset is rendered correctly and efficiently within the game world. \n\nExample usage:\n\n```javascript\nvar assetConfig = {\n  \"renderMode\": 1,\n  \"pixelsPerUnit\": 1,\n  \"textureAtlasAsset\": \"static/assets/button/red_button_atlas.json\",\n  \"frameKeys\": [2]\n};\n\n// Use the assetConfig object to render a specific asset in the game world\n```\n## Questions: \n 1. What does the \"renderMode\" property do?\n   - The \"renderMode\" property sets the rendering mode for the asset.\n2. What is the purpose of the \"pixelsPerUnit\" property?\n   - The \"pixelsPerUnit\" property sets the number of pixels per unit for the asset.\n3. What is the significance of the \"frameKeys\" array?\n   - The \"frameKeys\" array specifies the frame keys to use for the asset. In this case, it is using the second frame key.","metadata":{"source":".autodoc/docs/markdown/examples/assets/button/red_button_pressed.md"}}],["6",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/IridescentDishWithOlives.txt)\n\nThis code does not contain any actual code, but rather provides information about a 3D model called \"Iridescent Dish with Olives\". The model is available on GitHub and was created by Wayfair LLC. The model is licensed under the Creative Commons Attribution 4.0 International License, which allows for commercial use as long as the author is credited. \n\nThis information is likely included in the larger PlayCanvas engine project to provide users with access to 3D models that they can use in their projects. By providing information about the source and licensing of the model, users can ensure that they are using the model in a legal and ethical manner. \n\nHere is an example of how this information might be used in the PlayCanvas engine project:\n\n```javascript\n// Load the Iridescent Dish with Olives model\nvar modelAsset = app.assets.find(\"IridescentDishWithOlives\");\n\n// Create an entity to hold the model\nvar modelEntity = new pc.Entity();\nmodelEntity.addComponent(\"model\", {\n    type: \"gltf\",\n    asset: modelAsset\n});\n\n// Add the entity to the scene\napp.root.addChild(modelEntity);\n```\n\nIn this example, the `modelAsset` variable is set to the asset for the Iridescent Dish with Olives model. This asset can then be used to create a new entity that holds the model. Finally, the entity is added to the scene. By using the information provided about the model's source and licensing, the user can ensure that they are using the model in a legal and ethical manner.\n## Questions: \n 1. **What is the purpose of this code?**\n    \n    This code provides information about a 3D model called \"Iridescent Dish with Olives\", including its title, source, and author, as well as its license type and requirements for use.\n    \n2. **What is the format of the model?**\n    \n    The model is in the glTF 2.0 format, as indicated by the source URL pointing to the KhronosGroup/glTF-Sample-Models repository on GitHub.\n    \n3. **What are the restrictions on using this model?**\n    \n    The model is licensed under CC-BY-4.0, which allows for commercial use but requires attribution to the author, Wayfair LLC.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/IridescentDishWithOlives.md"}}],["7",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/MosquitoInAmber.txt)\n\nThis code does not contain any programming logic or functionality. Instead, it provides information about a 3D model called \"Real-time Refraction Demo: Mosquito in Amber\" that is available on the Sketchfab website. The information includes the title of the model, the source where it can be found, and the author who created it. Additionally, it provides details about the license type and requirements for using the model.\n\nThis information is likely included in the PlayCanvas engine project to provide users with a way to easily access and use 3D models in their projects. By providing details about the model's source, author, and license, users can ensure that they are using the model in a legal and ethical manner. \n\nHere is an example of how this information could be used in the PlayCanvas engine project:\n\n```javascript\n// Load the 3D model from Sketchfab\nvar model = app.assets.loadFromUrl('https://sketchfab.com/3d-models/real-time-refraction-demo-mosquito-in-amber-37233d6ed84844fea1ebe88069ea58d1');\n\n// Set the model's position and rotation\nmodel.setPosition(0, 0, 0);\nmodel.setRotation(0, 90, 0);\n\n// Add the model to the scene\napp.root.addChild(model);\n```\n\nIn this example, the code loads the \"Real-time Refraction Demo: Mosquito in Amber\" model from the Sketchfab website using the URL provided in the model information. It then sets the model's position and rotation and adds it to the scene. By using the information provided in the model information, the code ensures that the model is being used in compliance with its license and that proper credit is given to the author.\n## Questions: \n 1. What is the purpose of this code?\n   - This code provides information about a 3D model called \"Real-time Refraction Demo: Mosquito in Amber\" including its title, source, and author.\n\n2. What is the license for this model?\n   - The license for this model is CC-BY-4.0, which allows for commercial use as long as the author is credited.\n\n3. Are there any specific requirements for using this model?\n   - Yes, the only requirement for using this model is to credit the author.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/MosquitoInAmber.md"}}],["8",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/SheenChair.txt)\n\nThis code does not contain any functional code, but rather provides information about a 3D model called \"Sheen Chair\". The information includes the title of the model, its source location on GitHub, and the author of the model, which is Wayfair LLC. Additionally, the license type for the model is specified as CC-BY-4.0, which allows for commercial use as long as the author is credited.\n\nThis information is likely included in the larger PlayCanvas engine project to provide context and attribution for any 3D models used within the project. By including this information, developers using the PlayCanvas engine can ensure that they are properly crediting the authors of any models they use and complying with any licensing requirements.\n\nAn example of how this information might be used in the PlayCanvas engine project is if a developer wanted to use the Sheen Chair model in their game or application. They would be able to access the source location on GitHub to download the model, and would know that they need to credit Wayfair LLC as the author and comply with the CC-BY-4.0 license requirements. This information can help ensure that the PlayCanvas engine project is using 3D models in a legal and ethical manner.\n## Questions: \n 1. **What is the purpose of this code?**\\\nA smart developer might want to know what this code is actually doing within the PlayCanvas engine and how it fits into the overall functionality of the engine.\n\n2. **What is the Sheen Chair model and why is it important?**\\\nA smart developer might want to know more about the Sheen Chair model, its source, and its author to understand its significance within the PlayCanvas engine.\n\n3. **What are the licensing requirements for this code and the Sheen Chair model?**\\\nA smart developer might want to know more about the licensing requirements for both the code and the Sheen Chair model to ensure that they are in compliance with the licensing terms and conditions.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/SheenChair.md"}}],["9",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/StainedGlassLamp.txt)\n\nThis code does not contain any functional code, but rather provides information about a 3D model called \"Stained Glass Lamp\". The information includes the title of the model, its source location on GitHub, and the author of the model, which is Wayfair LLC. Additionally, the code provides information about the license type of the model, which is CC-BY-4.0, and the requirements for using the model, which include crediting the author and allowing commercial use.\n\nThis information is important for developers who may want to use the \"Stained Glass Lamp\" model in their projects. By providing the source location and license information, developers can ensure that they are using the model in compliance with its license and can properly credit the author. \n\nHere is an example of how this information could be used in a larger project:\n\n```javascript\n// Load the \"Stained Glass Lamp\" model\nvar model = app.assets.find(\"Stained Glass Lamp\");\n\n// Check if the model is loaded\nif (model.resource) {\n  // Add the model to the scene\n  var entity = new pc.Entity();\n  entity.addComponent(\"model\", {\n    type: \"asset\",\n    asset: model.id\n  });\n  app.root.addChild(entity);\n} else {\n  // If the model is not loaded, log an error message\n  console.error(\"Failed to load Stained Glass Lamp model\");\n}\n```\n\nIn this example, the code attempts to load the \"Stained Glass Lamp\" model and add it to the scene. If the model is not loaded, an error message is logged. By including the model information provided in the code, developers can ensure that they are using the correct model and that they are complying with its license.\n## Questions: \n 1. **What is the purpose of this code?**\n    \n    This code provides information about a 3D model of a stained glass lamp, including its title, source, and author, as well as its license type and requirements for use.\n    \n2. **What is the significance of the model's license type?**\n    \n    The model's license type is CC-BY-4.0, which means that the author must be credited and commercial use is allowed. This information is important for developers who may want to use the model in their own projects.\n    \n3. **Is this code part of the PlayCanvas engine itself, or is it simply related to it?**\n    \n    It is unclear from this code whether it is part of the PlayCanvas engine or simply related to it. Further context would be needed to determine this.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/StainedGlassLamp.md"}}],["10",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/batmobile-armored.txt)\n\nThis code is not a part of the PlayCanvas engine project, but rather a comment providing information about the source of a 3D model used in the project. The comment provides a link to the Sketchfab website where the batmobile-armoured model can be obtained, as well as information about the Creative Commons license under which it is distributed.\n\nThis information is important for anyone working on the PlayCanvas engine project who may need to use or modify the batmobile-armoured model. By providing the source and licensing information, the comment ensures that the project is in compliance with the terms of the Creative Commons license and that proper attribution is given to the original creator of the model.\n\nWhile this code does not contain any actual programming logic, it is still an important part of the project documentation. It serves as a reminder to developers to always be mindful of licensing and attribution when using third-party assets in their projects. \n\nExample usage:\n\n// Import the batmobile-armoured model into the PlayCanvas engine project\nconst batmobile = app.assets.find(\"batmobile-armoured\");\nconst batmobileEntity = new pc.Entity();\nbatmobileEntity.addComponent(\"model\", {\n    type: \"asset\",\n    asset: batmobile\n});\napp.root.addChild(batmobileEntity);\n## Questions: \n 1. What is the purpose of this code?\n- This code provides information about the source and licensing of the batmobile-armoured model used in the PlayCanvas engine.\n\n2. How is the batmobile-armoured model used in the PlayCanvas engine?\n- This code does not provide information on how the batmobile-armoured model is used in the PlayCanvas engine. Further investigation is needed.\n\n3. Are there any restrictions on the usage of the batmobile-armoured model?\n- No, the batmobile-armoured model is distributed under a CC license, which allows for free usage and distribution as long as proper attribution is given.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/batmobile-armored.md"}}],["11",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/bench_wooden_01.txt)\n\nThis code is not related to the PlayCanvas engine project, but rather provides information about a 3D model called \"bench_wooden_01\". The code includes a link to where the model can be obtained and the license under which it is distributed. This information is important for anyone using the model in their project, as they need to ensure they have the appropriate permissions and follow the terms of the license.\n\nIn terms of how this code may be used in the larger project, it could be included in a README file or other documentation for the project. This would provide transparency and clarity for anyone using the model in their own project, and ensure that they are following the appropriate licensing requirements.\n\nExample usage:\n\n```\n// Obtain the bench_wooden_01 model\nconst benchModel = loadModel('https://sketchfab.com/3d-models/bench-wooden-01-1400c9340d5049589deb43601462ac55');\n\n// Check the license for the model\nconst license = getLicense(benchModel);\nif (license === 'CC BY 4.0') {\n  console.log('This model can be used under the Creative Commons Attribution 4.0 license.');\n} else {\n  console.log('This model cannot be used without obtaining the appropriate permissions.');\n}\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is simply providing information about the source and licensing of the bench_wooden_01 model used in the PlayCanvas engine project.\n\n2. How is the bench_wooden_01 model used in the PlayCanvas engine?\n- This code does not provide any information on how the model is used in the PlayCanvas engine. Further investigation into the project's codebase may be necessary to answer this question.\n\n3. Are there any restrictions on the usage of the bench_wooden_01 model?\n- The bench_wooden_01 model is distributed under a CC license, which allows for its use as long as proper attribution is given.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/bench_wooden_01.md"}}],["12",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/chess-board.txt)\n\nThis code is not a part of the PlayCanvas engine project, but rather provides information about a 3D model called \"Chess Board\". The code includes details about the model's title, source, author, and license type. It also provides requirements for using the model, including the need to credit the author and the allowance for commercial use.\n\nThe purpose of this code is to provide clear and concise information about the 3D model for anyone who wants to use it in their project. By including the model's license type and requirements, users can ensure that they are using the model in a legal and ethical manner. Additionally, the code provides a credit that users can copy and paste when sharing their project to give proper attribution to the model's author.\n\nHere is an example of how this code might be used in a larger project:\n\nSuppose a game developer wants to use the \"Chess Board\" model in their chess game. They would first need to download the model from the source provided in the code. Then, they would need to include the code in their project to provide information about the model's license and requirements. Finally, they would need to copy and paste the credit provided in the code wherever they share their game to give proper attribution to the model's author.\n\nOverall, this code serves an important role in ensuring that 3D models are used in a legal and ethical manner. By providing clear and concise information about the model's license and requirements, users can avoid any legal issues and give proper credit to the model's author.\n## Questions: \n 1. What is the purpose of this code?\n- This code provides information about a 3D model of a chess board, including its title, source, author, and license.\n\n2. What is the license type for this model?\n- The license type for this model is CC-BY-4.0, which allows for commercial use as long as the author is credited.\n\n3. What are the requirements for using this model in a project?\n- The author must be credited and the license must be included. If the model is shared, the credit information must also be included.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/chess-board.md"}}],["13",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/house.txt)\n\nThis code is not a part of the PlayCanvas engine project, but rather a description of modifications made to a 3D model of a house that will be used in the project. The purpose of this code is to provide information about the source of the house model, its licensing, and the modifications that have been made to it to make it suitable for use in the PlayCanvas engine.\n\nThe house model was obtained from Sketchfab, a platform for sharing and discovering 3D models. It is distributed under a Creative Commons license, which allows for the model to be used and modified as long as proper attribution is given. The modifications made to the model include generating a uv1 channel for lightmapping and stripping out the textures. Additionally, the model has been converted to the glb format, which is a binary file format for 3D models that is optimized for web and mobile applications.\n\nThis code is important for the PlayCanvas engine project because it provides information about the source and licensing of the house model, which is necessary for ensuring that the project is in compliance with copyright laws. It also provides information about the modifications made to the model, which is important for understanding how the model will be used in the project. For example, the fact that the textures have been stripped out means that the model will need to be textured using materials in the PlayCanvas engine.\n\nOverall, this code serves as a documentation of the modifications made to the house model and is an important part of the PlayCanvas engine project.\n## Questions: \n 1. **What is the purpose of this code?**\n    \n    This code explains the modifications done to a house model obtained from a specific address, including the generation of a uv1 channel for lightmapping, stripping out textures, and converting it to glb format.\n\n2. **What is the CC license mentioned in the code?**\n    \n    The CC license mentioned in the code is the Creative Commons Attribution 4.0 International License, under which the house model is distributed.\n\n3. **What is the significance of generating a uv1 channel for lightmapping?**\n    \n    Generating a uv1 channel for lightmapping allows for more efficient and accurate lighting in the 3D model, as it provides a separate set of UV coordinates specifically for lightmap textures.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/house.md"}}],["14",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/playbot/26020273/Playbot_head.json)\n\nThis code represents a JSON object that defines the properties of a shader material in the PlayCanvas engine. The shader used is the Blinn shader, which is a popular lighting model used in computer graphics. The purpose of this code is to define the appearance of a 3D object in a scene by specifying various material properties such as color, texture, reflectivity, and transparency.\n\nThe `ambient`, `diffuse`, and `specular` properties define the color of the material under different lighting conditions. The `diffuseMap`, `emissiveMap`, `normalMap`, and `sphereMap` properties specify the textures to be used for the material. The `diffuseMapOffset` and `diffuseMapTiling` properties control the position and scale of the diffuse texture. Similarly, the `emissiveMapOffset`, `emissiveMapTiling`, `normalMapOffset`, and `normalMapTiling` properties control the position and scale of the emissive, normal, and bump textures respectively.\n\nThe `shininess` property controls the shininess of the material, while the `bumpMapFactor` property controls the strength of the bump mapping effect. The `opacity` property controls the transparency of the material, and the `refractionIndex` property controls the index of refraction for the material.\n\nThe `cubeMapProjectionBox` property defines the box that is used to project the environment map onto the material. The `lightMapChannel`, `lightMapUv`, `lightMapTiling`, and `lightMapOffset` properties control the lightmap used for the material. The `depthTest`, `depthWrite`, and `cull` properties control the depth testing, depth writing, and face culling for the material.\n\nOverall, this code is an important part of the PlayCanvas engine as it allows developers to define the appearance of 3D objects in a scene. By specifying various material properties, developers can create realistic and visually appealing scenes. Here is an example of how this code can be used to create a material:\n\n```javascript\nvar material = new pc.StandardMaterial();\nmaterial.diffuseMap = diffuseMapAsset.resource;\nmaterial.normalMap = normalMapAsset.resource;\nmaterial.shininess = 50;\nmaterial.opacity = 0.8;\nentity.model.material = material;\n```\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n- This code defines the properties of a shader material, including textures and lighting settings. It is likely used to render 3D objects in a scene within the PlayCanvas engine.\n\n2. What do the various properties in this code represent and how do they affect the appearance of the rendered object?\n- The properties include settings for ambient and diffuse lighting, textures for different channels (diffuse, specular, emissive, etc.), reflectivity, opacity, and more. Each property affects the appearance of the rendered object in different ways, such as the shininess of the surface or the intensity of the emissive color.\n\n3. Are there any limitations or compatibility issues with using this code in different browsers or devices?\n- Without more context, it is difficult to determine if there are any limitations or compatibility issues with this code. However, it is possible that certain texture formats or shader settings may not be supported on all devices or browsers, which could affect the appearance of the rendered object.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/playbot/26020273/Playbot_head.md"}}],["15",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/playbot/26020274/Playbot_body.json)\n\nThe code provided is a JSON object that defines the properties of a shader in the PlayCanvas engine. A shader is a program that runs on the GPU and is responsible for rendering the appearance of 3D objects in a scene. The properties defined in this code include the ambient and diffuse colors of the material, as well as various texture maps that can be applied to the material to add detail and realism.\n\nThe `diffuseMap` property specifies the path to a texture image file that will be used as the diffuse map for the material. Similarly, the `emissiveMap` property specifies the path to a texture image file that will be used as the emissive map for the material. The `normalMap` property specifies the path to a texture image file that will be used as the normal map for the material, which affects how light interacts with the surface of the material.\n\nOther properties include `shininess`, which controls the size of the specular highlight on the material, and `opacity`, which controls the transparency of the material. The `cubeMapProjectionBox` property specifies the dimensions of a cube map that can be used to create reflections on the material.\n\nOverall, this code defines the properties of a shader that can be used to render a 3D object with realistic lighting and texture mapping. These properties can be adjusted to create a wide variety of materials with different appearances and properties. For example, a metal material might have a high specular highlight and a reflective cube map, while a fabric material might have a low shininess and a diffuse map with a fabric texture.\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n- This code defines the properties of a shader material, including textures and lighting, and is likely used to render 3D objects in the PlayCanvas engine.\n\n2. What are some of the key properties of the shader material defined in this code?\n- Some of the key properties include the diffuse and specular colors, textures for diffuse, emissive, normal, and metalness maps, as well as settings for opacity, reflectivity, and refraction.\n\n3. How does the cubeMapProjectionBox property work and what is its purpose?\n- The cubeMapProjectionBox property defines the dimensions of a cube map projection used for reflections and is defined by a center point and half-extents.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/playbot/26020274/Playbot_body.md"}}],["16",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/playbot/26020283/Playbot_arm.json)\n\nThis code represents a JSON object that defines the properties of a shader material in the PlayCanvas engine. The shader used in this material is the Blinn shader, which is a popular shader model used in computer graphics for simulating the appearance of materials with specular highlights.\n\nThe material properties are defined using a combination of color values, texture maps, and other parameters. The ambient and diffuse properties define the base color of the material, while the specular and shininess properties control the appearance of the specular highlights. The emissive property controls the amount of light emitted by the material, and the opacity property controls the transparency of the material.\n\nTexture maps are used to add detail to the material's appearance. The diffuseMap property specifies the texture map used for the base color, while the normalMap property specifies a texture map that encodes surface normals to add the appearance of surface detail. The emissiveMap property specifies a texture map that controls the amount of light emitted by the material, and the heightMap property specifies a texture map that encodes height information to add the appearance of surface displacement.\n\nOther properties control various aspects of the material's appearance, such as the reflectivity, refraction index, and use of fog and lighting. The cubeMapProjectionBox property specifies a bounding box that is used to project a cube map onto the material, which can be used to simulate reflections or environment mapping.\n\nOverall, this code defines a material that can be used to render 3D objects in the PlayCanvas engine with a specific appearance. By defining the material properties in this way, developers can create a wide range of different materials with different appearances and behaviors, which can be used to create complex and realistic 3D scenes. For example, the following code snippet shows how this material might be applied to a 3D model in PlayCanvas:\n\n```javascript\nvar material = new pc.StandardMaterial();\nmaterial.initializeFromJSON({\n  \"shader\":\"blinn\",\n  \"ambient\":[0.588,0.588,0.588],\n  \"diffuse\":[0.588,0.588,0.588],\n  \"diffuseMap\":\"../26020279/arm_clean.png\",\n  \"diffuseMapOffset\":[0,0],\n  \"diffuseMapTiling\":[1,1],\n  \"specular\":[0.9,0.9,0.9],\n  \"shininess\":90.9091,\n  \"emissive\":[0,0,0],\n  \"emissiveMap\":\"../26020281/arm_E.png\",\n  \"emissiveMapOffset\":[0,0],\n  \"emissiveMapTiling\":[1,1],\n  \"normalMap\":\"../26020289/arm_N_clean.png\",\n  \"normalMapOffset\":[0,0],\n  \"normalMapTiling\":[1,1],\n  \"bumpMapFactor\":0.3,\n  \"opacity\":1,\n  \"sphereMap\":\"../26020278/env_01.png\",\n  \"reflectivity\":0.2,\n  \"aoMapChannel\":\"r\",\n  \"aoMapTiling\":[1,1],\n  \"aoMapOffset\":[0,0],\n  \"occludeSpecular\":1,\n  \"diffuseMapChannel\":\"rgb\",\n  \"specularMapChannel\":\"rgb\",\n  \"specularMapTiling\":[1,1],\n  \"specularMapOffset\":[0,0],\n  \"specularAntialias\":true,\n  \"metalnessMapChannel\":\"r\",\n  \"metalnessMapTiling\":[1,1],\n  \"metalnessMapOffset\":[0,0],\n  \"metalness\":1,\n  \"conserveEnergy\":true,\n  \"glossMapChannel\":\"r\",\n  \"glossMapTiling\":[1,1],\n  \"glossMapOffset\":[0,0],\n  \"emissiveMapChannel\":\"rgb\",\n  \"emissiveIntensity\":1,\n  \"heightMapChannel\":\"r\",\n  \"heightMapTiling\":[1,1],\n  \"heightMapOffset\":[0,0],\n  \"heightMapFactor\":1,\n  \"opacityMapChannel\":\"r\",\n  \"opacityMapTiling\":[1,1],\n  \"opacityMapOffset\":[0,0],\n  \"refractionIndex\":0.6666666666666666,\n  \"cubeMapProjectionBox\":{\"center\":[0,0,0],\"halfExtents\":[0.5,0.5,0.5]},\n  \"lightMapChannel\":\"rgb\",\n  \"lightMapUv\":1,\n  \"lightMapTiling\":[1,1],\n  \"lightMapOffset\":[0,0],\n  \"depthTest\":true,\n  \"depthWrite\":true,\n  \"cull\":1,\n  \"blendType\":3,\n  \"shadowSampleType\":1,\n  \"useFog\":true,\n  \"useLighting\":true,\n  \"useSkybox\":true,\n  \"useGammaTonemap\":true,\n  \"mapping_format\":\"path\"\n});\nentity.model.material = material;\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines the properties of a shader in the PlayCanvas engine, including its textures, lighting, and other visual effects.\n\n2. What are some of the key features of this shader?\n- Some of the key features of this shader include its use of diffuse, specular, and emissive maps, as well as normal and bump maps for added texture. It also includes settings for opacity, reflectivity, and refraction.\n\n3. How does this code relate to other parts of the PlayCanvas engine?\n- This code is just one part of the PlayCanvas engine, which is a full-featured game development platform. Other parts of the engine might include physics, audio, and networking components, as well as tools for building and deploying games.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/playbot/26020283/Playbot_arm.md"}}],["17",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/playbot/26020285/Playbot_leg.json)\n\nThe code provided is a JSON object that defines the properties of a shader in the PlayCanvas engine. A shader is a program that runs on the GPU and is responsible for rendering the appearance of 3D objects in a scene. \n\nThe `shader` property specifies the type of shader to be used, in this case, a Blinn shader. The `ambient`, `diffuse`, `specular`, and `emissive` properties define the color of the material under different lighting conditions. The `diffuseMap`, `emissiveMap`, `normalMap`, `sphereMap`, and `metalnessMap` properties specify the textures to be used for different aspects of the material. The `opacity` property controls the transparency of the material. \n\nOther properties such as `shininess`, `bumpMapFactor`, `reflectivity`, and `refractionIndex` control the shininess, bumpiness, reflectivity, and refraction of the material, respectively. The `cull` property determines whether the back or front faces of the material are rendered. The `blendType` property controls how the material is blended with other materials in the scene. \n\nOverall, this code defines the properties of a material that can be used to render 3D objects in a scene using the PlayCanvas engine. Developers can use this code as a template to create their own materials with custom properties and textures. For example, to create a new material with a different diffuse color and texture, the `diffuse` and `diffuseMap` properties can be modified as follows:\n\n```\n{\n  \"shader\": \"blinn\",\n  \"ambient\": [0.588, 0.588, 0.588],\n  \"diffuse\": [1, 0, 0], // red color\n  \"diffuseMap\": \"../textures/my_texture.png\",\n  \"diffuseMapOffset\": [0, 0],\n  \"diffuseMapTiling\": [1, 1],\n  \"specular\": [0.9, 0.9, 0.9],\n  \"shininess\": 90.9091,\n  \"emissive\": [0, 0, 0],\n  \"emissiveMap\": \"../textures/my_emissive_texture.png\",\n  \"emissiveMapOffset\": [0, 0],\n  \"emissiveMapTiling\": [1, 1],\n  \"normalMap\": \"../textures/my_normal_map.png\",\n  \"normalMapOffset\": [0, 0],\n  \"normalMapTiling\": [1, 1],\n  \"bumpMapFactor\": 0.3,\n  \"opacity\": 1,\n  \"sphereMap\": \"../textures/my_sphere_map.png\",\n  \"reflectivity\": 0.2,\n  \"aoMapChannel\": \"r\",\n  \"aoMapTiling\": [1, 1],\n  \"aoMapOffset\": [0, 0],\n  \"occludeSpecular\": 1,\n  \"diffuseMapChannel\": \"rgb\",\n  \"specularMapChannel\": \"rgb\",\n  \"specularMapTiling\": [1, 1],\n  \"specularMapOffset\": [0, 0],\n  \"specularAntialias\": true,\n  \"metalnessMapChannel\": \"r\",\n  \"metalnessMapTiling\": [1, 1],\n  \"metalnessMapOffset\": [0, 0],\n  \"metalness\": 1,\n  \"conserveEnergy\": true,\n  \"glossMapChannel\": \"r\",\n  \"glossMapTiling\": [1, 1],\n  \"glossMapOffset\": [0, 0],\n  \"emissiveMapChannel\": \"rgb\",\n  \"emissiveIntensity\": 1,\n  \"heightMapChannel\": \"r\",\n  \"heightMapTiling\": [1, 1],\n  \"heightMapOffset\": [0, 0],\n  \"heightMapFactor\": 1,\n  \"opacityMapChannel\": \"r\",\n  \"opacityMapTiling\": [1, 1],\n  \"opacityMapOffset\": [0, 0],\n  \"refractionIndex\": 0.6666666666666666,\n  \"cubeMapProjectionBox\": {\n    \"center\": [0, 0, 0],\n    \"halfExtents\": [0.5, 0.5, 0.5]\n  },\n  \"lightMapChannel\": \"rgb\",\n  \"lightMapUv\": 1,\n  \"lightMapTiling\": [1, 1],\n  \"lightMapOffset\": [0, 0],\n  \"depthTest\": true,\n  \"depthWrite\": true,\n  \"cull\": 1,\n  \"blendType\": 3,\n  \"shadowSampleType\": 1,\n  \"useFog\": true,\n  \"useLighting\": true,\n  \"useSkybox\": true,\n  \"useGammaTonemap\": true,\n  \"mapping_format\": \"path\"\n}\n```\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n- This code defines the properties of a shader material, including textures, lighting, and other visual effects. It is used to render 3D objects in the PlayCanvas engine.\n\n2. What are some of the key properties that can be customized in this shader material?\n- Some of the key properties that can be customized include the diffuse and specular colors, textures, and tiling, as well as the shininess, opacity, and reflectivity of the material. Other properties control lighting, shadows, and other visual effects.\n\n3. How does the PlayCanvas engine handle different types of textures and maps in this shader material?\n- The PlayCanvas engine supports a variety of texture and map types, including diffuse, specular, emissive, normal, bump, height, opacity, and metalness maps. These can be customized with different channels, tiling, and offsets to achieve a wide range of visual effects.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/playbot/26020285/Playbot_leg.md"}}],["18",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/playbot/playbot.mapping.json)\n\nThe code above is a JSON object that contains information about the mapping of different parts of a robot model in the PlayCanvas engine project. The \"mapping\" property is an array of objects that specify the file paths for each part of the robot model, including the head, body, arms, and legs. Each object in the \"mapping\" array has a \"path\" property that contains the file path for the corresponding part of the model.\n\nThis code is used to load and render the robot model in the PlayCanvas engine project. By specifying the file paths for each part of the model, the engine can retrieve the necessary data and render the model in the correct configuration. The \"area\" property in the JSON object is likely used to calculate the total area of the robot model, which may be useful for certain calculations or optimizations in the project.\n\nHere is an example of how this code may be used in the PlayCanvas engine project:\n\n```javascript\n// Load robot model data\nvar robotData = {\"mapping\":[{\"path\":\"26020273/Playbot_head.json\"},{\"path\":\"26020274/Playbot_body.json\"},{\"path\":\"26020283/Playbot_arm.json\"},{\"path\":\"26020285/Playbot_leg.json\"}],\"area\":0};\n\n// Retrieve file paths for each part of the model\nvar headPath = robotData.mapping[0].path;\nvar bodyPath = robotData.mapping[1].path;\nvar armPath = robotData.mapping[2].path;\nvar legPath = robotData.mapping[3].path;\n\n// Load model data using file paths\nvar headData = loadModelData(headPath);\nvar bodyData = loadModelData(bodyPath);\nvar armData = loadModelData(armPath);\nvar legData = loadModelData(legPath);\n\n// Render robot model using loaded data\nrenderRobotModel(headData, bodyData, armData, legData);\n```\n\nOverall, this code is an important part of the PlayCanvas engine project as it allows for the loading and rendering of complex 3D models. By specifying the file paths for each part of the model, the engine can retrieve the necessary data and render the model in the correct configuration.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code is defining a mapping of paths to JSON files for different parts of a Playbot model, as well as specifying the area of the model.\n\n2. **What is the format of the JSON files being mapped?**\\\nWithout further information, it is unclear what the structure and contents of the JSON files being mapped are.\n\n3. **How is this code being used within the PlayCanvas engine?**\\\nIt is unclear from this code snippet alone how this mapping is being utilized within the PlayCanvas engine and what functionality it provides.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/playbot/playbot.mapping.md"}}],["19",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/portal.txt)\n\nThis code is not a part of the PlayCanvas engine project, but rather a comment explaining the origin and modifications made to a 3D model used in the project. The model is a portal frame obtained from Sketchfab under a Creative Commons license. The modifications made to the model include resizing and compressing the texture to minimize its size.\n\nThis comment serves as a reference for anyone working with the PlayCanvas engine project and using this particular 3D model. It ensures that proper credit is given to the original creator and that any modifications made to the model are documented. Additionally, the information about the texture being resized and compressed can be useful for optimizing the performance of the project.\n\nWhile this code does not contain any actual code, it is still an important part of the project's documentation and serves as an example of the importance of proper documentation in software development.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThe purpose of this code is to provide information about the portal model used in the PlayCanvas engine, including its source and any modifications made to it.\n\n2. **What license is the portal model distributed under?**\\\nThe portal model is distributed under the CC (Creative Commons) license, specifically the CC BY 4.0 license.\n\n3. **What modifications were made to the portal model?**\\\nThe texture of the portal model was resized and compressed in order to minimize its size.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/portal.md"}}],["20",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/models/terrain.txt)\n\nThis code is not a part of the PlayCanvas engine project, but rather a note about the source and modifications made to a 3D model used in the project. The model is a low-poly terrain and was obtained from Sketchfab under a Creative Commons license. The model was then converted to the glb format, likely for use in the PlayCanvas engine.\n\nThis note serves as a record of the source and modifications made to the model, which may be important for legal and attribution purposes. It also provides context for the use of the model in the larger project, indicating that it is a terrain model and has been optimized for use in the PlayCanvas engine.\n\nThere is no code example provided as this is not code, but rather a note about a 3D model.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThe purpose of this code is to provide information about the source and modifications made to the house model used in the PlayCanvas engine project.\n\n2. **What is the significance of the CC license mentioned in the code?**\\\nThe CC license mentioned in the code indicates that the house model is distributed under the Creative Commons Attribution 4.0 International License, which allows for the sharing and adaptation of the model as long as proper credit is given.\n\n3. **What format was the house model converted to?**\\\nThe house model was converted to the glb format, as mentioned in the modifications made to it.","metadata":{"source":".autodoc/docs/markdown/examples/assets/models/terrain.md"}}],["21",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/scripts/asset-loader.js)\n\nThe code provided is a function called `loadManifestAssets` that allows for the loading of multiple assets in the PlayCanvas engine. The function takes three parameters: `app`, `manifest`, and `onLoadedAssets`. \n\nThe `app` parameter is an instance of the PlayCanvas application, which is used to manage assets. The `manifest` parameter is an object that contains information about the assets to be loaded. Each key in the `manifest` object represents an asset, and the value is an object that contains the `type`, `url`, and `data` of the asset. The `type` property specifies the type of asset, such as \"texture\" or \"model\". The `url` property is the URL of the asset to be loaded, and the `data` property is optional and contains any additional data needed to load the asset.\n\nThe `onLoadedAssets` parameter is an optional callback function that is called when all assets in the `manifest` have been loaded. \n\nThe function first counts the number of assets to be loaded by iterating through the `manifest` object and incrementing the `count` variable for each key. \n\nNext, the function defines an `onLoadedAsset` function that is called when an asset has been loaded. This function decrements the `count` variable and sets the `asset` property of the corresponding `manifest` object to the loaded asset. If all assets have been loaded (i.e., `count` is 0), the `onLoadedAssets` callback function is called if it exists.\n\nFinally, the function loads each asset in the `manifest` object using the `app.assets` API. If the asset has a `data` property, it is loaded using the `pc.Asset` constructor and added to the `app.assets` registry. If the asset does not have a `data` property, it is loaded using the `app.assets.loadFromUrl` method. In both cases, the `onLoadedAsset` function is called when the asset has been loaded.\n\nOverall, this function provides a convenient way to load multiple assets in the PlayCanvas engine and allows for the use of a callback function when all assets have been loaded. Here is an example usage of the `loadManifestAssets` function:\n\n```\nvar manifest = {\n    texture: {\n        type: \"texture\",\n        url: \"textures/texture.png\"\n    },\n    model: {\n        type: \"model\",\n        url: \"models/model.json\"\n    }\n};\n\nloadManifestAssets(app, manifest, function() {\n    // All assets have been loaded\n    var texture = app.assets.find(\"texture\");\n    var model = app.assets.find(\"model\");\n    // Use the loaded assets\n});\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines a function called `loadManifestAssets` that loads multiple assets from a manifest object and executes a callback function when all assets are loaded.\n\n2. What parameters does the `loadManifestAssets` function take?\n    \n    The `loadManifestAssets` function takes three parameters: `app`, which is an instance of the PlayCanvas application, `manifest`, which is an object containing information about the assets to be loaded, and `onLoadedAssets`, which is an optional callback function to be executed when all assets are loaded.\n\n3. What types of assets can be loaded using this function?\n    \n    This function can load assets of any type supported by the PlayCanvas engine, as specified in the `entry.type` property of each asset entry in the `manifest` object. The code supports loading assets from URLs or from data provided directly in the manifest object.","metadata":{"source":".autodoc/docs/markdown/examples/assets/scripts/asset-loader.md"}}],["22",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/assets/spine/license.txt)\n\nThis code is a license file for a project that contains images and a project file. The purpose of this license is to specify the terms and conditions under which the images and project file can be used and redistributed. \n\nThe license allows for the redistribution of the images as long as they are accompanied by the license file. However, it prohibits the commercial use of the images. This means that the images cannot be used for any commercial purposes, such as advertising or selling products.\n\nThe project file, on the other hand, is released into the public domain. This means that it can be used as the basis for derivative work. This allows other developers to build upon the project file and create their own projects based on it.\n\nOverall, this license file serves as a legal agreement between the owner of the images and project file and anyone who wants to use or redistribute them. It ensures that the owner's rights are protected while still allowing for the images and project file to be used and built upon by others.\n\nExample usage of this license file would be to include it in the root directory of the project containing the images and project file. This would ensure that anyone who wants to use or redistribute the images and project file is aware of the terms and conditions under which they can do so.\n## Questions: \n 1. What is the purpose of this code?\n   \n   This code is a license file for the images in the PlayCanvas engine project, allowing for their redistribution as long as the license file is included and prohibiting commercial use of the images.\n\n2. Who is the author of this code?\n   \n   The author of this code is Esoteric Software, as indicated in the copyright statement.\n\n3. Are there any restrictions on the use of the project file?\n   \n   No, the project file is released into the public domain and may be used as the basis for derivative work without any restrictions.","metadata":{"source":".autodoc/docs/markdown/examples/assets/spine/license.md"}}],["23",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/lib/arrayFromPolyfill.js)\n\nThe code provided is a polyfill for the `Array.from()` method. This method creates a new array instance from an array-like or iterable object. The `Array.from()` method is not supported in some older browsers, so this polyfill provides a way to use the method in those browsers.\n\nThe code checks if the `Array.from()` method is already defined. If it is not defined, the code defines the method using a self-executing function. The function first checks if the `Symbol.iterator` property is supported. If it is supported, the function uses it to create an iterator. If it is not supported, the function creates a string representation of the property.\n\nThe function then defines several helper functions. The `isCallable()` function checks if a given value is a function. The `toInteger()` function converts a given value to an integer. The `toLength()` function converts a given value to a length value. The `setGetItemHandler()` function returns a function that retrieves an item from an array-like or iterable object. The `getArray()` function creates a new array instance from an array-like or iterable object.\n\nThe `Array.from()` method is then defined using the `getArray()` function. The method takes an array-like or iterable object as its first argument. If a second argument is provided, it is used as a mapping function. If a third argument is provided, it is used as the `this` value for the mapping function. The method returns a new array instance.\n\nOverall, this code provides a way to use the `Array.from()` method in older browsers that do not support it. It defines the method using a self-executing function and several helper functions. The `Array.from()` method takes an array-like or iterable object as its first argument and returns a new array instance.\n## Questions: \n 1. What is the purpose of this code?\n- This code is a polyfill for the `Array.from` method, which creates a new array instance from an array-like or iterable object.\n\n2. What is a polyfill?\n- A polyfill is a piece of code that provides modern functionality on older browsers or environments that do not support it natively.\n\n3. What is the purpose of the `Symbol.iterator` property used in this code?\n- The `Symbol.iterator` property is used to check if an object is iterable, and if so, to get its default iterator. This is used in the `Array.from` method to determine if the input object is iterable or not.","metadata":{"source":".autodoc/docs/markdown/examples/lib/arrayFromPolyfill.md"}}],["24",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/lib/objectValuesPolyfill.js)\n\nThe code above is a polyfill for the `Object.values()` method. The `Object.values()` method is used to extract the values of an object and return them as an array. However, this method is not supported in all browsers, hence the need for a polyfill.\n\nThe code checks if the `Object.values()` method is available in the current environment. If it is not available, the code defines a new function that mimics the behavior of the `Object.values()` method. The new function takes an object as an argument and returns an array of its values.\n\nThe `Object.keys()` method is used to extract the keys of the object, which are then mapped to their corresponding values using the `map()` method. The `map()` method creates a new array with the results of calling a provided function on every element in the calling array.\n\nThis polyfill can be used in the larger PlayCanvas engine project to ensure that the `Object.values()` method is available in all browsers. For example, if the project requires the use of the `Object.values()` method to extract the values of an object, the polyfill can be used to ensure that the method works as expected in all browsers.\n\nHere is an example of how the polyfill can be used:\n\n```\nconst obj = { a: 1, b: 2, c: 3 };\nconst values = Object.values(obj);\nconsole.log(values); // [1, 2, 3]\n```\n\nIn this example, the `Object.values()` method is used to extract the values of the `obj` object and store them in the `values` array. The polyfill ensures that the method works as expected in all browsers, even if the method is not natively supported.\n## Questions: \n 1. **What is the purpose of this code?** \nThis code is checking if the `Object.values` method is available and if not, it is defining a new implementation for it.\n\n2. **Why is the `Object.values` method being used?** \nThe `Object.values` method is being used to return an array of the values of the properties of an object.\n\n3. **What is the expected behavior if the `Object.values` method is already defined?** \nIf the `Object.values` method is already defined, this code will not override it and will not execute the defined implementation.","metadata":{"source":".autodoc/docs/markdown/examples/lib/objectValuesPolyfill.md"}}],["25",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/rollup.config.js)\n\nThis code is a configuration file for the Rollup module bundler. It specifies how to build the PlayCanvas engine application, which is a web-based game engine. The configuration file imports several Rollup plugins, including `resolve`, `commonjs`, `replace`, `typescript`, `copy`, `terser`, `string`, and `alias`. \n\nThe `resolve` plugin resolves module imports to their absolute paths, while the `commonjs` plugin converts CommonJS modules to ES6 modules. The `replace` plugin replaces strings in the code with other strings, which is useful for setting environment variables. The `typescript` plugin compiles TypeScript code to JavaScript, and the `copy` plugin copies files from one directory to another. The `terser` plugin minifies the output code. The `string` plugin converts string files to ES6 modules, which is useful for importing `.d.ts` files.\n\nThe `alias` plugin creates aliases for module imports. In this case, it creates aliases for `@playcanvas/pcui/react` and `@playcanvas/pcui/styles`, which are used in the `tsCompilerOptions` object. The `tsCompilerOptions` object specifies the TypeScript compiler options, including the `baseUrl` and `paths` options. The `baseUrl` option specifies the base directory for resolving non-relative module names, while the `paths` option specifies a mapping of module names to their locations.\n\nThe `export` object specifies the input and output files for the Rollup bundler. The `input` file is `src/app/index.tsx`, which is the entry point for the application. The `output` object specifies the output directory and format. The `plugins` array specifies the Rollup plugins to use, including the ones imported at the beginning of the file. \n\nOverall, this configuration file is used to build the PlayCanvas engine application, which is a web-based game engine. It specifies how to compile TypeScript code, copy files, and create aliases for module imports. It also specifies the input and output files for the Rollup bundler.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file is a configuration file for the PlayCanvas engine project that specifies the input and output files, plugins, and other settings for the Rollup bundler.\n\n2. What is the `PCUI_PATH` variable used for?\n- The `PCUI_PATH` variable is used to specify the path to the `@playcanvas/pcui` package, which is a UI library for the PlayCanvas engine. If the `PCUI_PATH` environment variable is not set, it defaults to `'node_modules/@playcanvas/pcui'`.\n\n3. What does the `replace` plugin do?\n- The `replace` plugin is used to replace occurrences of `process.env.NODE_ENV` with the value of `process.env.NODE_ENV` at build time. This is often used to conditionally include code based on the environment (e.g. development vs production).","metadata":{"source":".autodoc/docs/markdown/examples/rollup.config.md"}}],["26",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/scripts/example-data.mjs)\n\nThis code is responsible for generating example data for the PlayCanvas engine project. The example data is used to showcase the capabilities of the engine and provide users with examples of how to use it. The code imports several modules, including fs, BabelParser, Prettier, and Babel, which are used to read and manipulate files, format code, and transpile TypeScript to JavaScript. \n\nThe code starts by defining the __filename and __dirname variables using the fileURLToPath and dirname functions from the path and url modules, respectively. It then defines the MAIN_DIR variable as the path to the parent directory of the current file. \n\nNext, the code creates an empty object called exampleData, which will be populated with data about the examples. It checks if the dist directory exists and creates it if it doesn't. \n\nThe code then reads the names of the directories in the src/examples directory using the readDirectoryNames function, which is imported from another file. For each directory, it creates a new object in the exampleData object with the name of the directory as the key. It then reads the names of the files in the directory and processes each file. \n\nFor each file, the code extracts the TypeScript code and converts it to JavaScript using Babel. It then formats the JavaScript code using Prettier and stores it in the exampleData object along with the name and category of the example. If the TypeScript code contains a static object called FILES, the code evaluates it and stores the result in the exampleData object. \n\nFinally, the code writes the exampleData object to a file called example-data.js in the dist directory. The object is exported as a module so that it can be used by other parts of the PlayCanvas engine project. \n\nOverall, this code is an important part of the PlayCanvas engine project as it generates example data that is used to showcase the engine's capabilities and provide users with examples of how to use it. The code uses several modules to read, manipulate, and format files, and it processes each example file to extract relevant information and store it in the exampleData object. The resulting object is then written to a file and exported as a module for use in other parts of the project.\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code reads example files from a directory, transforms them from TypeScript to JavaScript, formats them with Prettier, and writes the resulting data to a file.\n\n2. What dependencies are required for this code to run?\n    \n    This code requires the `fs`, `@babel/parser`, `prettier/standalone`, `@babel/standalone`, `path`, and `url` modules.\n\n3. What is the output of this code?\n    \n    The output of this code is a JavaScript file containing an object with data about the example files, including their TypeScript and JavaScript code, file names, and categories.","metadata":{"source":".autodoc/docs/markdown/examples/scripts/example-data.md"}}],["27",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/scripts/example-directory/index.mjs)\n\nThe code is responsible for generating HTML files for each example in the PlayCanvas engine project. It does this by reading a directory of example files, creating a list of categories and examples, and then generating HTML files for each example using a Handlebars template.\n\nThe code starts by importing the necessary modules, including the fs module for reading and writing files, Handlebars for generating HTML templates, and path and url modules for working with file paths. It then defines some constants and functions, including a function for loading the HTML template, a list of categories, and a function for reading directory names.\n\nThe loadHtmlTemplate function reads an HTML template file using the fs module, compiles it using Handlebars, and returns the compiled template. The categoriesList array is used to store the list of categories and examples, which is generated by reading the directory of example files using the readDirectoryNames function. For each category, the code creates a directory in the dist folder if it doesn't already exist, and then generates an HTML file for each example in that category using the loadHtmlTemplate function. The generated HTML file is written to a directory in the dist folder named after the category and example.\n\nFinally, the code writes the categoriesList array to a file named examples.json in the dist folder. This file is used by the PlayCanvas engine to generate a list of examples that can be viewed in the browser.\n\nOverall, this code is an important part of the PlayCanvas engine project, as it allows developers to easily generate HTML files for each example in the project. By automating this process, the code saves developers time and effort, and ensures that the examples are consistent and up-to-date.\n## Questions: \n 1. What is the purpose of the `loadHtmlTemplate` function?\n   - The `loadHtmlTemplate` function reads an HTML template file, compiles it using Handlebars, and returns the compiled template with the provided data.\n2. What is the `categoriesList` variable used for?\n   - The `categoriesList` variable is an array that stores objects representing each category of examples in the `src/examples` directory, along with the list of examples in each category.\n3. What is the output of this code?\n   - The output of this code is a JSON file located at `dist/examples.json` that contains an array of objects representing each category of examples in the `src/examples` directory, along with the list of examples in each category. Additionally, for each example, an HTML file is generated in the `dist` directory with the compiled template and example-specific data.","metadata":{"source":".autodoc/docs/markdown/examples/scripts/example-directory/index.md"}}],["28",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/scripts/iframe/index.mjs)\n\nThis code is responsible for building examples for the PlayCanvas engine. It copies prebuilt files used by the iframe, reads HTML templates, and compiles them using Handlebars. It also reads example files, extracts the example class from the file, and generates an HTML file for each example. \n\nThe code starts by importing the necessary modules, including `fs`, `fse`, `Babel`, `Handlebars`, `formatters`, `readDirectoryNames`, `dirname`, and `fileURLToPath`. It then sets the `__filename` and `__dirname` variables using `fileURLToPath` and `dirname`. The `MAIN_DIR` variable is set to the parent directory of the current directory.\n\nThe code then copies prebuilt files used by the iframe from the `build` directory to the `dist/build` directory. It also copies several polyfill files to the `dist/build` directory. \n\nThe `loadHtmlTemplate` function reads an HTML template file and compiles it using Handlebars. It returns the compiled template with the provided data.\n\nThe `buildExample` function reads an example file and extracts the example class from it using `formatters.getExampleClassFromTextFile`. It then determines the engine type from the example class using `formatters.getEngineTypeFromClass` and sets the `enginePath` variable accordingly. It then writes an HTML file for the example using the `loadHtmlTemplate` function and the example class, engine path, and other data.\n\nThe code then checks if the `dist/iframe` directory exists and creates it if it does not. If the `EXAMPLE` and `CATEGORY` environment variables are set, it calls the `buildExample` function with the provided category and example file. Otherwise, it reads all the example categories using `readDirectoryNames` and reads all the example files for each category using `fs.readdirSync`. It then calls the `buildExample` function for each example file that does not include `index.mjs`.\n\nOverall, this code is responsible for building HTML files for the PlayCanvas engine examples. It reads example files, extracts the example class, and generates an HTML file for each example. It also copies prebuilt files and polyfills used by the iframe. This code is an important part of the PlayCanvas engine project as it allows developers to easily build and test examples for the engine.\n## Questions: \n 1. What is the purpose of the `copyFileSync` function calls?\n- The `copyFileSync` function calls are used to copy prebuilt files to the `dist/build/` directory, including polyfills and the PlayCanvas observer.\n\n2. What is the `loadHtmlTemplate` function used for?\n- The `loadHtmlTemplate` function is used to load an HTML template file and compile it using Handlebars, returning the compiled template with the provided data.\n\n3. What is the purpose of the `buildExample` function?\n- The `buildExample` function is used to build an example by reading a text file, getting the example class from the file using Babel, and writing an HTML file to the `dist/iframe/` directory with the compiled template and example data.","metadata":{"source":".autodoc/docs/markdown/examples/scripts/iframe/index.md"}}],["29",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/scripts/thumbnails.mjs)\n\nThe code is responsible for generating screenshots of examples in the PlayCanvas engine project. The screenshots are used to display a preview of the examples on the project's website. \n\nThe code imports several modules including fs, puppeteer, and sharp. The fs module is used to read and write files, puppeteer is used to automate the process of taking screenshots of web pages, and sharp is used to resize the screenshots. \n\nThe code starts by defining the MAIN_DIR constant which is the path to the root directory of the project. It then creates an empty array called exampleList. \n\nThe code then reads the contents of the examples directory and filters out the index.mjs file. It then loops through each category in the examples directory and reads the contents of each category directory. It filters out the index.mjs file and adds each example to the exampleList array. \n\nThe takeScreenshots function is then defined. It loops through each example in the exampleList array and checks if a screenshot for that example already exists. If a screenshot exists, the function skips that example and moves on to the next one. If a screenshot does not exist, the function launches a new instance of the Puppeteer browser, navigates to the example's URL, and waits for 5 seconds to allow the page to fully load. \n\nOnce the page has loaded, the function creates two directories: dist/thumbnails and dist/temp. It then takes a screenshot of the page and saves it to the dist/temp directory. The screenshot is then resized using the sharp module and saved to the dist/thumbnails directory as both a large and small thumbnail. Finally, the function logs a message indicating that a screenshot has been taken for the example and closes the browser. \n\nThe function then removes the dist/temp directory and moves on to the next example in the exampleList array. \n\nOverall, this code is an important part of the PlayCanvas engine project as it generates the screenshots used to showcase the examples on the project's website. It automates the process of taking screenshots and resizing them, saving developers time and effort.\n## Questions: \n 1. What is the purpose of this code?\n- This code takes screenshots of examples in the PlayCanvas engine and saves them as thumbnails.\n\n2. What dependencies are being used in this code?\n- This code uses the `fs`, `puppeteer`, and `sharp` dependencies.\n\n3. What is the expected output of this code?\n- The expected output of this code is a set of thumbnails for each example in the PlayCanvas engine, saved in the `dist/thumbnails` directory.","metadata":{"source":".autodoc/docs/markdown/examples/scripts/thumbnails.md"}}],["30",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/code-editor.tsx)\n\nThe `CodeEditor` component is a React component that provides a code editor interface for the PlayCanvas engine project. It imports the `MonacoEditor` component from the `@monaco-editor/react` package, which is a wrapper around the Monaco editor, a popular web-based code editor. It also imports several UI components from the `@playcanvas/pcui/react` package.\n\nThe component takes several props, including an array of `File` objects, which represent the files being edited, and several callback functions to handle changes to the files and linting errors. It also takes a boolean flag to indicate whether to use TypeScript or JavaScript.\n\nThe component renders a `Panel` component with a header of \"CODE\" and an ID of \"codePane\". The panel contains a menu bar with several buttons, including a play button, a language button, and a button to open the corresponding file on GitHub. The panel also contains a `Container` component with a tab bar that displays the names of the files being edited. The currently selected file is displayed in the `MonacoEditor` component below the tab bar.\n\nThe `CodeEditor` component uses the `useState` hook to keep track of the currently selected file. It also uses the `useEffect` hook to perform several side effects, including setting up event listeners for the panel toggle button and saving the panel's collapsed state to local storage.\n\nThe `CodeEditor` component uses the `MonacoEditor` component to render the code editor. It passes in the language of the currently selected file, the text of the currently selected file, and several callback functions to handle changes to the text and linting errors. It also sets several options for the editor, including the visibility of the horizontal scrollbar and the read-only state.\n\nThe `CodeEditor` component also includes several helper functions, including `beforeMount`, which adds the PlayCanvas type definitions to the TypeScript and JavaScript language services, and `selectFile`, which updates the currently selected file and highlights the corresponding tab.\n\nOverall, the `CodeEditor` component provides a flexible and extensible code editor interface for the PlayCanvas engine project, allowing developers to edit and debug their code in a familiar and powerful environment.\n## Questions: \n 1. What is the purpose of the `CodeEditor` component?\n- The `CodeEditor` component is used to display and edit code files, with support for multiple files and different file types.\n\n2. What is the role of the `beforeMount` function?\n- The `beforeMount` function is called before the Monaco editor is mounted, and it adds the PlayCanvas type definitions to the TypeScript and JavaScript language defaults.\n\n3. What is the purpose of the `onValidate` function?\n- The `onValidate` function is called when the editor's contents are validated, and it checks if there are any linting errors. If there are no linting errors, it sets the `lintErrors` state to `false`, otherwise it sets it to `true`.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/code-editor.md"}}],["31",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/constants.ts)\n\nThis code exports a constant variable called `MIN_DESKTOP_WIDTH` with a value of 601. The purpose of this constant is to define the minimum width required for a desktop screen to display the PlayCanvas engine properly. \n\nIn the larger project, this constant may be used in various parts of the codebase to ensure that the engine's user interface and graphics are optimized for desktop screens. For example, it may be used in a responsive design function that adjusts the layout and size of UI elements based on the screen size. \n\nHere is an example of how this constant may be used in a function that checks the screen size and adjusts the UI accordingly:\n\n```\nfunction adjustUI() {\n  const screenWidth = window.innerWidth;\n  if (screenWidth >= MIN_DESKTOP_WIDTH) {\n    // adjust UI for desktop screens\n  } else {\n    // adjust UI for mobile screens\n  }\n}\n```\n\nOverall, this code serves as a key piece of information for ensuring that the PlayCanvas engine is optimized for desktop screens, and can be used in various parts of the codebase to achieve this goal.\n## Questions: \n 1. **What is the purpose of this constant?**\\\nA smart developer might want to know why this constant is being exported and what it is used for within the PlayCanvas engine. \n\n2. **Why is the value set to 601?**\\\nA smart developer might question why the minimum desktop width is set to 601 specifically, and if there is any significance to this value.\n\n3. **Is this constant used throughout the entire PlayCanvas engine?**\\\nA smart developer might want to know if this constant is used in multiple files or if it is specific to this particular file.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/constants.md"}}],["32",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/control-panel.tsx)\n\nThe code defines a React component called `ControlPanel` that renders a UI element for controlling a PlayCanvas project. The component uses the `useState` and `useEffect` hooks from React to manage its state and lifecycle respectively. It also imports the `MonacoEditor`, `Button`, and `Container` components from external libraries.\n\nThe `ControlPanel` component renders a container element with an ID of `controls-wrapper`. If the `props.controls` property is truthy, the component adds a class of `has-controls` to the container. If the width of the top-level window is less than a constant value called `MIN_DESKTOP_WIDTH`, the component renders a tabbed interface with two buttons labeled \"CODE\" and \"PARAMETERS\". Clicking on the \"CODE\" button shows a read-only code editor that displays the contents of the first file in the `props.files` array, if it exists. Clicking on the \"PARAMETERS\" button shows the `props.controls` element, which is assumed to be a UI element for controlling the PlayCanvas project.\n\nThe `ControlPanel` component uses the `onClickParametersTab` and `onClickCodeTab` functions to handle clicks on the \"PARAMETERS\" and \"CODE\" buttons respectively. These functions update the component's state to show the appropriate UI element and update the selected tab button. The component also uses the `useEffect` hook to hide the `controlPanel-controls` element if the width of the top-level window is less than `MIN_DESKTOP_WIDTH` or if the URL of the top-level window starts with `#/iframe`.\n\nOverall, the `ControlPanel` component provides a flexible UI element for controlling a PlayCanvas project, with support for both desktop and mobile devices. It can be used as a building block for more complex UI elements in a larger PlayCanvas project. For example, it could be used as part of a larger UI panel that includes other controls and displays.\n## Questions: \n 1. What is the purpose of the `ControlPanel` component?\n- The `ControlPanel` component is used to render a UI control panel that can display either code or parameters.\n\n2. What is the significance of the `MIN_DESKTOP_WIDTH` constant?\n- The `MIN_DESKTOP_WIDTH` constant is used to determine whether the control panel should be collapsed or not based on the width of the window.\n\n3. What is the purpose of the `useEffect` hook in this component?\n- The `useEffect` hook is used to hide the control panel and its controls based on certain conditions, such as the width of the window or the URL hash.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/control-panel.md"}}],["33",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/example.tsx)\n\nThis code is a React component that renders an example from the PlayCanvas engine project. The component is responsible for rendering the example's code, controls, and graphics output. It also provides a way for the user to select the graphics device to use for rendering.\n\nThe component imports several modules from the PlayCanvas engine project, including `Container`, `Spinner`, `SelectInput`, and `Panel` from `@playcanvas/pcui/react`, and `SelectInput` from `@playcanvas/pcui`. It also imports `Observer` from `@playcanvas/observer`, which is used to observe changes to the active graphics device.\n\nThe component defines several interfaces and classes, including `ControlLoaderProps`, `ControlLoaderState`, `ExampleProps`, and `ExampleState`. It also defines several constants, including `deviceTypeNames`, which maps device types to their names, and `MIN_DESKTOP_WIDTH`, which is the minimum width of the desktop viewport.\n\nThe component defines a function component called `Controls`, which renders the controls for the example. The controls are obtained from the example's `controls` function, which is called with the `observerData` object. If there are no controls and the viewport is wide enough, the controls are not rendered.\n\nThe component defines a class component called `ControlLoader`, which is responsible for loading the example's controls. The component listens for the `exampleLoading` and `exampleLoad` events, which are emitted by the example when it starts loading and when it finishes loading, respectively. When the example finishes loading, the component updates the active graphics device and emits an `updateActiveDevice` event.\n\nThe component defines a class component called `Example`, which is the main component that renders the example. The component defines several methods, including `onSetActiveGraphicsDevice`, which updates the active graphics device, and `onSetPreferredGraphicsDevice`, which sets the preferred graphics device. The component also defines several properties, including `defaultFiles`, which is the default set of files for the example, and `path`, which is the path to the example.\n\nThe component renders a `Container` component that contains a `Spinner` component and an `iframe` element. The `iframe` element is used to render the example's graphics output. The component also renders a `Panel` component that contains a `SelectInput` component and a `ControlLoader` component. The `SelectInput` component is used to select the active graphics device, and the `ControlLoader` component is used to load the example's controls.\n\nThe component exports the `Example` component wrapped in a `withRouter` higher-order component, which provides the component with the `match` object from the router. This allows the component to render the correct example based on the URL.\n## Questions: \n 1. What is the purpose of the `ControlLoader` component?\n- The `ControlLoader` component is responsible for loading and rendering the controls for a specific example.\n\n2. What is the significance of the `deviceTypeSelectInput` property?\n- The `deviceTypeSelectInput` property is a reference to the `SelectInput` component used to select the active graphics device, and is used to set and disable options based on the preferred and active graphics devices.\n\n3. What is the role of the `controlsObserver` object?\n- The `controlsObserver` object is used to emit and listen for events related to the active graphics device, and is used to update the controls when the active device changes.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/example.md"}}],["34",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/helpers/example-data.mjs)\n\nThe code is responsible for generating documentation for the PlayCanvas engine project. It imports example data from a file and example classes from another file. It then processes the data and classes to create an object with two properties: `categories` and `paths`. \n\nThe `categories` property is an object that contains information about each category of examples. Each category has a `name` property and an `examples` property. The `name` property is the name of the category, and the `examples` property is an object that contains information about each example in the category. \n\nThe `paths` property is an object that contains information about each example. Each example has a `path` property, an `example` property, and a `files` property. The `path` property is the URL path to the example. The `example` property is the class that implements the example. The `files` property is an array of objects that contain information about the files that make up the example. \n\nThe code uses the `exampleData` and `exampleClasses` objects to generate the `categories` and `paths` objects. The `exampleData` object contains information about each example, such as the JavaScript and TypeScript code for the example. The `exampleClasses` object contains the classes that implement the examples. \n\nThe code loops through each category in the `exampleData` object and generates a category object in the `categories` object. It then loops through each example in the category and generates an example object in the `examples` property of the category object. \n\nFor each example, the code generates an array of files that make up the example. The array contains two files: one with the JavaScript code and one with the TypeScript code. If there are any additional files for the example, the code adds them to the array. \n\nFinally, the code generates an object for each example in the `paths` object. The object contains the URL path to the example, the class that implements the example, and the array of files that make up the example. \n\nThis code is used to generate the documentation for the PlayCanvas engine project. The `categories` and `paths` objects are used to generate the documentation pages for the examples in the project. The `exampleData` and `exampleClasses` objects are updated as new examples are added to the project, and the code is run again to generate the updated documentation. \n\nExample usage:\n\n```javascript\nimport documentation from './path/to/documentation.js';\n\n// Get the categories object\nconst categories = documentation.categories;\n\n// Get the paths object\nconst paths = documentation.paths;\n\n// Get the name of the first category\nconst firstCategoryName = categories[Object.keys(categories)[0]].name;\n\n// Get the URL path to the first example in the first category\nconst firstExamplePath = paths[Object.keys(paths)[0]].path;\n\n// Get the class that implements the first example in the first category\nconst firstExampleClass = paths[Object.keys(paths)[0]].example;\n```\n## Questions: \n 1. What is the purpose of the `exampleData` and `exampleClasses` imports?\n- `exampleData` is imported from a file called `example-data.js` and is used to generate examples for the PlayCanvas engine. `exampleClasses` is imported from a file called `index.mjs` and contains classes for each example category.\n\n2. What is the purpose of the `capitalizeFirstLetter` function?\n- The `capitalizeFirstLetter` function takes a string as input and returns the same string with the first letter capitalized. It is used to format category and example names.\n\n3. What is the output of this module?\n- This module exports an object with two properties: `categories` and `paths`. `categories` is an object containing example categories and their corresponding examples. `paths` is an object containing paths to each example and their corresponding files.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/helpers/example-data.md"}}],["35",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/helpers/formatters.mjs)\n\nThe code provided is a module that exports several utility functions used in the PlayCanvas engine project. These functions are used to extract information from text files containing JavaScript or TypeScript code.\n\nThe `findClosingBracketMatchIndex` function takes a string and a position and returns the index of the closing bracket that matches the opening bracket at the given position. This function is used to find the end of a function definition in a text file.\n\nThe `getTypeScriptFunctionFromText` function takes a string of TypeScript code and returns the text of the function that matches a specific signature. This function is used to extract the example function from a TypeScript file.\n\nThe `getInnerFunctionText` function takes a string of JavaScript code and returns the text of the inner function. This function is used to extract the inner function from a JavaScript file.\n\nThe `getExampleClassFromTextFile` function takes a Babel object and a string of JavaScript or TypeScript code and returns the transformed code as a string. This function is used to transform the example class code to be compatible with the PlayCanvas engine.\n\nThe `getEngineTypeFromClass` function takes a string of JavaScript or TypeScript code and returns the engine type used in the example class. This function is used to determine whether the engine is in debug or performance mode.\n\nThe `getWebgpuEnabledFromClass` function takes a string of JavaScript or TypeScript code and returns a boolean indicating whether WebGPU is enabled in the example class. This function is used to determine whether WebGPU is enabled in the example.\n\nThe `classIncludesMiniStats` function takes a string of JavaScript or TypeScript code and returns a boolean indicating whether the example class includes MiniStats. This function is used to determine whether MiniStats is included in the example.\n\nThe `retrieveStaticObject` function takes a string of JavaScript or TypeScript code and a name and returns the static object with the given name. This function is used to retrieve static objects from the example class.\n\nOverall, these functions are used to extract information from example code files and transform them to be compatible with the PlayCanvas engine. They are used to provide examples and documentation for the engine's features and capabilities.\n## Questions: \n 1. What does the `findClosingBracketMatchIndex` function do?\n- This function finds the index of the closing bracket that matches the opening bracket at the given position in the input string.\n\n2. What is the purpose of the `getExampleClassFromTextFile` function?\n- This function transforms the input text using Babel and returns the transformed text, with certain replacements and modifications made to the original text.\n\n3. What does the `retrieveStaticObject` function do?\n- This function retrieves a static object from the input text by finding the start and end indices of the object and returning the substring that represents the object.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/helpers/formatters.md"}}],["36",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/helpers/loader.tsx)\n\nThe code above is a TypeScript module that defines a `ScriptLoader` class and exports it for use in other parts of the PlayCanvas engine project. The `ScriptLoader` class is a React component that can be used to dynamically load and execute JavaScript files at runtime. \n\nThe `ScriptLoader` class takes two props: `name` and `url`. `name` is a string that represents the name of the script to be loaded, and `url` is a string that represents the URL of the script to be loaded. \n\nThe `ScriptLoader` class has a static method called `load` that takes three arguments: `resource`, `app`, and `onLoad`. `resource` is an object that contains the `name` and `url` properties, `app` is an instance of the `pc.Application` class, and `onLoad` is a callback function that is called when the script has finished loading and executing. \n\nThe `load` method uses the `fetch` API to retrieve the script file from the specified URL. Once the script file has been retrieved, it is converted to a string using the `text()` method. The string is then executed as JavaScript code using the `Function` constructor. The resulting module is then assigned to a global variable with the name specified in the `name` prop. Finally, the `onLoad` callback is called to signal that the script has finished loading and executing. \n\nThis `ScriptLoader` class can be used in other parts of the PlayCanvas engine project to dynamically load and execute JavaScript files at runtime. For example, it could be used to load and execute a script that defines a custom component for use in a PlayCanvas scene. \n\nExample usage:\n\n```\nimport { ScriptLoader } from 'path/to/ScriptLoader';\n\nconst myScript = {\n  name: 'MyCustomComponent',\n  url: 'path/to/myScript.js'\n};\n\nScriptLoader.load(myScript, app, () => {\n  console.log('MyCustomComponent loaded and executed!');\n});\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines a `ScriptLoader` class that can be used to load external scripts in a React application using the PlayCanvas engine.\n\n2. What is the `ScriptLoaderProps` interface used for?\n    \n    The `ScriptLoaderProps` interface defines the shape of the props that can be passed to the `ScriptLoader` component, including the name and URL of the script to be loaded.\n\n3. How does the `load` method work?\n    \n    The `load` method uses the `fetch` API to retrieve the script file from the specified URL, then uses the `Function` constructor to execute the script in a new module context and store the resulting exports in a global variable with the specified name. Finally, it calls the `onLoad` callback to signal that the script has been loaded successfully.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/helpers/loader.md"}}],["37",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/helpers/read-dir-names.mjs)\n\nThe code above is a function that reads the names of directories within a specified directory using the Node.js file system module (fs). The function takes in a single parameter, `dir`, which is the path to the directory whose subdirectories' names are to be read.\n\nThe function first calls the `readdirSync` method of the `fs` module on the specified directory, passing in an options object with the `withFileTypes` property set to `true`. This option ensures that the method returns an array of `fs.Dirent` objects instead of just the names of the files and directories in the specified directory.\n\nThe function then filters the array of `fs.Dirent` objects to only include those that represent directories using the `isDirectory()` method of the `fs.Dirent` class. Finally, the function maps the resulting array of `fs.Dirent` objects to an array of their names using the `name` property of each object.\n\nThe resulting array of directory names is then returned by the function.\n\nThis function can be used in the larger PlayCanvas engine project to dynamically load assets or scripts from subdirectories within a specified directory. For example, if the project has a directory structure for different levels of a game, this function can be used to read the names of the level directories and load the appropriate assets and scripts for each level. \n\nExample usage:\n\n```\nimport readDirectoryNames from './readDirectoryNames';\n\nconst levelDir = './assets/levels';\nconst levelNames = readDirectoryNames(levelDir);\n\n// levelNames is now an array of the names of the subdirectories within the 'levels' directory\n```\n## Questions: \n 1. What does this code do?\n   This code exports a function called `readDirectoryNames` that takes a directory path as an argument and returns an array of directory names within that directory.\n\n2. What library or module is being imported and used in this code?\n   This code imports the `fs` module, which is a built-in Node.js module for working with the file system.\n\n3. What is the expected output of the `readDirectoryNames` function?\n   The expected output of the `readDirectoryNames` function is an array of strings, where each string represents the name of a directory within the specified directory path.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/helpers/read-dir-names.md"}}],["38",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/helpers/types.ts)\n\nThis code defines an interface called `File` which has three properties: `name`, `text`, and `type`. The `name` property is a string that represents the name of the file, while the `text` property is a string that represents the contents of the file. The `type` property is an optional string that represents the type of the file, such as \"text/plain\" or \"image/jpeg\".\n\nThis interface is likely used throughout the PlayCanvas engine project to represent files that are used in various parts of the engine. For example, when loading a texture file for a material, the `File` interface could be used to represent the texture file. Here is an example of how this interface could be used:\n\n```typescript\nconst textureFile: File = {\n  name: \"texture.jpg\",\n  text: \"binary data...\",\n  type: \"image/jpeg\"\n};\n\n// Load the texture file into a material\nconst material = new pc.StandardMaterial();\nmaterial.diffuseMap = new pc.Texture(app.graphicsDevice);\nmaterial.diffuseMap.setSource(textureFile.text);\n```\n\nIn this example, a `textureFile` object is created using the `File` interface, with the `name`, `text`, and `type` properties set appropriately. The `text` property contains the binary data of the texture file. This `textureFile` object is then used to create a new `pc.Texture` object, which is set as the `diffuseMap` property of a new `pc.StandardMaterial` object. This demonstrates how the `File` interface can be used to represent files in the PlayCanvas engine project, and how it can be used to load those files into various parts of the engine.\n## Questions: \n 1. **What is the purpose of this `File` interface?** \nThe `File` interface is likely used to represent a file object within the PlayCanvas engine. It includes properties for the file's name, text content, and an optional type.\n\n2. **What is the expected data type for the `name` and `text` properties?** \nBased on the interface definition, the `name` property is expected to be a string, while the `text` property is also expected to be a string.\n\n3. **How is the `type` property used within the PlayCanvas engine?** \nWithout further context, it's unclear how the `type` property is used within the PlayCanvas engine. However, it's likely used to specify the file type (e.g. \"image/png\", \"text/html\") for certain file objects.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/helpers/types.md"}}],["39",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/index.tsx)\n\nThe code is a React component that renders the main layout of a web application built using the PlayCanvas engine. The component imports several other components from the same project, including `SideBar`, `CodeEditor`, `Menu`, and `Example`. \n\nThe `MainLayout` component sets up the routing for the application using `react-router-dom`. It defines two routes: the first route redirects to a default example, and the second route renders the main view of the application. The main view consists of a sidebar, a menu, a code editor, and an example viewer. \n\nThe `useState` hook is used to manage the state of the component. The `files` state is initialized with an empty array that contains an object with a `name` and `text` property. The `lintErrors` state is initialized with a boolean value of `false`. The `useTypeScript` state is initialized with a boolean value that is retrieved from `localStorage`. \n\nThe `updateShowMiniStats` function is used to update the value of a global variable `_showMiniStats`. The `playButtonRef` and `languageButtonRef` are created using the `createRef` hook. The `useEffect` hook is used to bind click events to these buttons. When the `playButtonRef` is clicked, the `exampleIframe` is reloaded. When the `languageButtonRef` is clicked, the value of `useTypeScript` is toggled and stored in `localStorage`. \n\nThe `MainLayout` component returns a JSX element that renders the main view of the application. The `Router` component defines the routes for the application. The `Switch` component renders the first route that matches the current URL. The `Route` component defines the path and the components to render for each route. The `SideBar` component renders the sidebar of the application. The `Menu` component renders the menu of the application. The `CodeEditor` component renders the code editor of the application. The `Example` component renders the example viewer of the application. \n\nFinally, the `ReactDOM.render` method is used to render the `MainLayout` component to the DOM. It is rendered to an element with the ID `app`. \n\nThis code is a part of a larger project that provides a web-based development environment for building applications using the PlayCanvas engine. The `MainLayout` component is the main entry point for the application and provides the overall structure and layout of the application. The other components imported in this file provide specific functionality for the application, such as editing code, viewing examples, and managing the menu.\n## Questions: \n 1. What is the purpose of the `MainLayout` component?\n- The `MainLayout` component is the main layout of the PlayCanvas engine project, which renders a React Router with different routes and components.\n\n2. What is the purpose of the `updateShowMiniStats` function?\n- The `updateShowMiniStats` function updates the value of a global variable `_showMiniStats` to control whether to show mini stats or not.\n\n3. What is the purpose of the `playButtonRef` and `languageButtonRef` references?\n- The `playButtonRef` and `languageButtonRef` references are used to access and manipulate the DOM elements of the play button and language button, respectively, in the `useEffect` hook.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/index.md"}}],["40",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/menu.tsx)\n\nThe code defines a React component called `Menu` that renders a menu bar with several buttons. The component takes in two props: `useTypeScript`, a boolean value, and `setShowMiniStats`, a function that takes in a boolean value. The purpose of the component is to provide a menu bar with buttons that allow the user to interact with the PlayCanvas engine.\n\nThe `Menu` component has a function called `toggleFullscreen` that toggles the fullscreen mode of the PlayCanvas engine. When the `fullscreen-button` is clicked, the function adds or removes the `fullscreen` class to several elements in the DOM, including the `canvas-container` and `appInner` elements. If the `appInner` element has the `fullscreen` class, the function adds an event listener to the `mousemove` event that sets a timeout to remove the `active` class from the `appInner` element after 2 seconds.\n\nThe `Menu` component also has a `useEffect` hook that adds an event listener to the `keydown` event on the `iframe` and `document` elements. If the `escape` key is pressed and the `canvas-container` element has the `fullscreen` class, the `toggleFullscreen` function is called to exit fullscreen mode.\n\nThe `Menu` component renders a `Container` element with the `id` of `menu` that contains several child elements. The `menu-buttons` child element contains four buttons: an image of the PlayCanvas logo that opens the PlayCanvas engine GitHub repository when clicked, a button with an icon that opens a Twitter intent to share the current PlayCanvas engine example, a button with an icon that toggles the `selected` class on the `showMiniStatsButton` element and calls the `setShowMiniStats` function with the `selected` class status, and a button with an icon that toggles fullscreen mode when clicked.\n\nExample usage of the `Menu` component:\n\n```\nimport React, { useState } from 'react';\nimport Menu from './Menu';\n\nconst App = () => {\n  const [showMiniStats, setShowMiniStats] = useState(true);\n\n  return (\n    <div>\n      <Menu useTypeScript={true} setShowMiniStats={setShowMiniStats} />\n      {/* rest of the app */}\n    </div>\n  );\n};\n\nexport default App;\n```\n\nIn this example, the `Menu` component is used in an app and passed the `useTypeScript` prop with a value of `true` and the `setShowMiniStats` prop with a function that updates the `showMiniStats` state. The `Menu` component renders a menu bar with buttons that allow the user to interact with the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `Menu` component?\n   \n   The `Menu` component is a React component that renders a menu with buttons for opening a link, sharing on Twitter, toggling mini stats, and toggling fullscreen mode.\n\n2. What is the `useEffect` hook used for in this code?\n   \n   The `useEffect` hook is used to add event listeners for the escape key and to clean up those listeners when the component unmounts.\n\n3. What is the `MenuProps` interface used for?\n   \n   The `MenuProps` interface is used to define the props that can be passed to the `Menu` component, which include a boolean value for `useTypeScript` and a function for setting the value of `setShowMiniStats`.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/menu.md"}}],["41",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/app/sidebar.tsx)\n\nThe code is a React component that renders a side bar for the PlayCanvas engine. The side bar is used to display a list of examples that users can select to view. The component imports several React components from the PlayCanvas UI library, including `BindingTwoWay`, `BooleanInput`, `Container`, `Label`, `LabelGroup`, and `Panel`. It also imports the `Link` component from React Router, the `Observer` class from the PlayCanvas Observer library, and some example data and constants.\n\nThe `SideBar` component defines several state variables using the `useState` hook. The `defaultCategories` variable is initialized with example data, and the `filteredCategories` variable is used to store a filtered version of the example data based on user input. The `hash` variable is used to store the current URL hash. The component also creates an instance of the `Observer` class.\n\nThe component uses the `useEffect` hook to perform various side effects when the component mounts or updates. These side effects include setting up event listeners for the control panel toggle button and the URL hash change event, toggling the side bar's collapsed state based on the window width, scrolling to a specific example when the component first opens, and setting up an event listener for the `largeThumbnails:set` event emitted by the `Observer` instance.\n\nThe `SideBar` component renders a `Panel` component with a header text of \"EXAMPLES\". The panel contains a `TextInput` component that allows users to filter the example list based on name. The panel also contains a `LabelGroup` component with a `BooleanInput` component that allows users to toggle between large and small thumbnails for the example list. The `Container` component contains a list of `Panel` components, each representing a category of examples. Each category panel contains a list of `Link` components, each representing an example. The `Link` components use the `to` prop to set the URL hash and navigate to the selected example. The `Link` components also toggle the side bar's collapsed state when clicked.\n\nOverall, the `SideBar` component provides a user-friendly interface for browsing and selecting examples in the PlayCanvas engine. It allows users to filter the example list, toggle between large and small thumbnails, and navigate to specific examples with ease.\n## Questions: \n 1. What is the purpose of the `SideBar` component?\n- The `SideBar` component is responsible for rendering a collapsible panel that displays a list of examples and allows the user to filter and navigate through them.\n\n2. What is the role of the `Observer` object?\n- The `Observer` object is used to manage the state of the `largeThumbnails` property, which determines whether the thumbnails of the examples are displayed in a large or small size.\n\n3. What is the significance of the `MIN_DESKTOP_WIDTH` constant?\n- The `MIN_DESKTOP_WIDTH` constant is used to determine whether the `SideBar` panel should be collapsible or not based on the width of the device's screen. If the screen width is less than the value of `MIN_DESKTOP_WIDTH`, the panel is collapsible, otherwise it is always visible.","metadata":{"source":".autodoc/docs/markdown/examples/src/app/sidebar.md"}}],["42",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/animation/blend-trees-1d.tsx)\n\nThe `BlendTrees1DExample` class is a React component that demonstrates how to use blend trees in the PlayCanvas engine. Blend trees are a way to blend between different animations based on a set of parameters. This example shows how to create a 1D blend tree, which blends between two animations based on a single parameter.\n\nThe `controls` method returns a React component that renders a slider input. This slider input is used to control the blend parameter of the blend tree. When the slider value changes, the `data` observer is updated with the new value.\n\nThe `example` method is the main method of the class. It takes three arguments: a canvas element, a device type, and some data. It creates a new PlayCanvas application, loads some assets, and sets up the scene. It then creates an entity from a loaded model, adds an animation component to the entity, and creates a 1D blend tree for the animation component. Finally, it sets up an event listener for the slider input that updates the blend parameter of the blend tree when the slider value changes.\n\nThe `BlendTrees1DExample` class can be used as a starting point for creating more complex blend trees in PlayCanvas. Developers can modify the example to use different animations, add more parameters to the blend tree, or use different types of blend trees. The example also demonstrates how to load assets, set up a PlayCanvas application, and add entities to the scene.\n## Questions: \n 1. What is the purpose of the `BlendTrees1DExample` class?\n- The `BlendTrees1DExample` class is an example class that demonstrates how to use blend trees in PlayCanvas engine for animation.\n\n2. What assets are being loaded in the `example` method?\n- The `example` method loads several assets including a 3D model, two animations, a texture, and a script.\n\n3. What is the purpose of the `data` parameter in the `controls` method?\n- The `data` parameter is an Observer object that is used to bind the value of a slider input to a `blend` parameter used in the animation blend tree.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/animation/blend-trees-1d.md"}}],["43",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/animation/blend-trees-2d-cartesian.tsx)\n\nThe `BlendTrees2DCartesianExample` class is a React component that demonstrates how to use blend trees in PlayCanvas engine to create 2D animations. The component contains two methods: `controls()` and `example()`. \n\nThe `controls()` method is responsible for rendering a canvas element that allows the user to control the position of the model in the 2D blend tree. The method uses the `useEffect()` hook to listen for the `start` event of the PlayCanvas app and then sets up event listeners for mouse and touch events on the canvas. When the user interacts with the canvas, the method updates the position of the model in the blend tree and redraws the canvas to reflect the new position.\n\nThe `example()` method is responsible for setting up the PlayCanvas app and loading the assets required for the example. The method creates a new `pc.AppBase` instance and sets up the graphics device, camera, light, and model entities. It also creates an animation state graph for the model entity and loads the required animations. Finally, the method starts the PlayCanvas app.\n\nOverall, the `BlendTrees2DCartesianExample` class demonstrates how to use blend trees in PlayCanvas engine to create 2D animations and provides an interactive example for users to experiment with.\n## Questions: \n 1. What is the purpose of the `controls()` method?\n- The `controls()` method sets up a canvas element and event listeners for mouse and touch events, which allow the user to control the position of an animated model using a 2D blend tree.\n\n2. What assets are loaded in the `example()` method?\n- The `example()` method loads several assets, including a 3D model, multiple animations, a texture for a skybox, and a post-processing script for bloom effects.\n\n3. What is the purpose of the `BlendTrees2DCartesianExample` class?\n- The `BlendTrees2DCartesianExample` class is an example implementation of a 2D blend tree animation system using the PlayCanvas engine. It includes methods for setting up user controls and loading assets, as well as a static category and name for the example.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/animation/blend-trees-2d-cartesian.md"}}],["44",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/animation/blend-trees-2d-directional.tsx)\n\nThe code defines a class called `BlendTrees2DDirectionalExample` that contains two methods: `controls()` and `example()`. The class is imported along with other modules from the PlayCanvas engine project. \n\nThe `controls()` method is a React component that renders a canvas element with an id of `2d-blend-control`. The method also sets up an event listener for mouse events on the canvas. When the mouse is moved or clicked, the position of the mouse is used to set the `posX` and `posY` parameters of an animation state graph. The method also draws a 2D representation of the animation state graph on the canvas. \n\nThe `example()` method takes two arguments: a canvas element and a device type. The method creates a new PlayCanvas application with the given canvas element and device type. It then loads several assets, including a 3D model, animations, and a texture. The method sets up a camera, a light, and an entity with the loaded 3D model. The entity also has an animation component that is used to control the playback of the loaded animations. The method loads an animation state graph into the animation component and assigns the loaded animations to the states in the state graph. \n\nThe purpose of this code is to demonstrate how to use blend trees in PlayCanvas to control the animation of a 3D model. Blend trees are a way to blend between multiple animations based on a set of parameters. In this example, the `posX` and `posY` parameters are used to control the blending between four animations: idle, walk, jog, and walk backwards. The `controls()` method provides a way to interactively set the values of these parameters using a 2D representation of the blend tree. \n\nThis code can be used as a starting point for building more complex applications that use blend trees to control the animation of 3D models. Developers can modify the code to load their own assets and create their own animation state graphs. They can also use the `controls()` method as a template for creating their own interactive controls for setting the parameters of the blend tree.\n## Questions: \n 1. What is the purpose of the `BlendTrees2DDirectionalExample` class?\n- The `BlendTrees2DDirectionalExample` class is an example class that demonstrates how to use blend trees in a 2D directional animation.\n\n2. What external libraries or dependencies does this code use?\n- This code imports `React` and `pc` from external libraries. It also uses the `useEffect` hook from React.\n\n3. What is the purpose of the `example` and `controls` methods in the `BlendTrees2DDirectionalExample` class?\n- The `example` method sets up the example scene and animation using the PlayCanvas engine. The `controls` method sets up the UI controls for the example, including a canvas element for displaying the animation.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/animation/blend-trees-2d-directional.md"}}],["45",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/animation/component-properties.tsx)\n\nThe code defines a class called `ComponentPropertiesExample` that demonstrates how to use the PlayCanvas engine to create an animation of two spotlights. The class has two methods: `controls` and `example`. \n\nThe `controls` method returns a React component that renders a button with the text \"Flash\". When the button is clicked, it toggles the value of a boolean property called `flash` in the `data` object passed as an argument to the method. \n\nThe `example` method creates a PlayCanvas application that renders a 3D scene with a box, a plane, and two spotlights. The method first creates a `GraphicsDevice` object using the `createGraphicsDevice` method of the `pc` module. It then creates an `AppBase` object and initializes it with the `GraphicsDevice` object and other options such as mouse and touch input handlers. \n\nThe method then loads a texture asset and creates two animation clips: one for static lights and one for flashing lights. Each clip contains keyframe data for the color and rotation of the spotlights. The method creates entities for the camera, box, plane, and spotlights, and adds them to the scene hierarchy. It also adds an `AnimComponent` to the entity that contains the spotlights and assigns the animation clips to the appropriate states. \n\nFinally, the method starts the PlayCanvas application and listens for changes to the `flash` property in the `data` object. When the property changes, the method transitions the animation state of the spotlights between \"Static\" and \"Flash\" using the `transition` method of the `AnimComponent`. \n\nOverall, the `ComponentPropertiesExample` class demonstrates how to use the PlayCanvas engine to create an animated 3D scene with user interaction. The class can be used as a starting point for creating more complex applications that use the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `controls` method in the `ComponentPropertiesExample` class?\n- The `controls` method returns a React component that includes a button with a `Flash` label. When clicked, the button toggles the `flash` property of the `data` object.\n\n2. What is the `anim` component added to the `lightsEntity` entity for?\n- The `anim` component is added to the `lightsEntity` entity to enable animation of the spot lights. It is configured with two animation clips, `Static` and `Flash`, which are assigned to the appropriate states.\n\n3. What is the purpose of the `WEBGPU_ENABLED` static property in the `ComponentPropertiesExample` class?\n- The `WEBGPU_ENABLED` static property is used to indicate whether the example supports the WebGPU graphics API. If set to `true`, the example will use WebGPU if available, otherwise it will use WebGL.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/animation/component-properties.md"}}],["46",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/animation/events.tsx)\n\nThe code is an example of how to use the PlayCanvas engine to create a 3D scene with animation and event handling. The purpose of the code is to demonstrate how to load assets, create entities, add components, and handle events in a PlayCanvas application. \n\nThe `EventsExample` class is a static class that contains a single method called `example`. This method takes two parameters: a canvas element and a device type. The canvas element is used to create a graphics device, and the device type is used to specify the type of device to create. \n\nThe `example` method begins by defining a set of assets that will be used in the scene. These assets include a 3D model, an animation, a texture, and a script. The method then creates a graphics device using the `pc.createGraphicsDevice` function, passing in the canvas element and device type. Once the graphics device is created, the method creates an instance of the `pc.AppBase` class and initializes it with the graphics device and other options. \n\nThe method then loads the assets using the `pc.AssetListLoader` class and starts the application. Once the application is started, the method sets up the scene by creating a skydome, adding a camera entity, creating a floor made up of box models, and adding a loaded model with an animation component. \n\nThe method then adds two animation events to the walk animation, one for each footstep. These events occur just as each foot touches the ground. When an event occurs, the method highlights the box that is under the foot's bone position. \n\nFinally, the method sets up an update loop that iterates over any currently highlighted boxes and reduces their emissive property. It also removes old highlighted boxes from the update loop. The method then sets the camera to follow the model. \n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a 3D scene with animation and event handling. It shows how to load assets, create entities, add components, and handle events in a PlayCanvas application.\n## Questions: \n 1. What is the purpose of the `EventsExample` class?\n- The `EventsExample` class is an example class that demonstrates how to use events in the PlayCanvas engine.\n\n2. What assets are being loaded in the `example` method?\n- The `example` method loads a 3D model, an animation, a texture, and a script.\n\n3. What does the `highlightBox` function do?\n- The `highlightBox` function lights up a box at a given position with a random color using the emissive material property and adds the box to an array of highlighted boxes.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/animation/events.md"}}],["47",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/animation/index.mjs)\n\nThis code exports a set of examples from various modules within the PlayCanvas engine. These examples demonstrate different features and functionalities of the engine, such as blend trees, component properties, events, layer masks, locomotion, and tweening.\n\nThe purpose of this code is to provide developers with a set of examples that showcase how to use different features of the PlayCanvas engine. By importing these examples, developers can learn how to implement these features in their own projects.\n\nFor example, if a developer wants to learn how to use blend trees in their game, they can import the `BlendTrees1DExample` or `BlendTrees2DDirectionalExample` modules and study the code to understand how to implement blend trees in their own game. Similarly, if a developer wants to learn how to use events in their game, they can import the `EventsExample` module and study the code to understand how to use events in their own game.\n\nThis code is an important part of the PlayCanvas engine project because it helps developers learn how to use the engine's features and functionalities. By providing examples, the PlayCanvas team is making it easier for developers to get started with the engine and build high-quality games and applications.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file exports several examples related to different features of the PlayCanvas engine.\n\n2. What are some examples of features demonstrated in this code file?\n- Some examples of features demonstrated in this code file include blend trees, component properties, events, layer masks, locomotion, and tweening.\n\n3. Are there any dependencies required for these examples to work?\n- It is unclear from this code file whether there are any dependencies required for these examples to work.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/animation/index.md"}}],["48",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/animation/layer-masks.tsx)\n\nThe code is an example of how to use the PlayCanvas engine to create a 3D animation with layer masks. The code imports several modules from the PlayCanvas engine and other libraries. The `LayerMasksExample` class is defined, which has two static properties `CATEGORY` and `NAME`, and a static property `WEBGPU_ENABLED`. The `controls` method returns a JSX element that contains several `Panel` components, each with `LabelGroup`, `SelectInput`, `BooleanInput`, and `SliderInput` components. These components are used to control the animation's layer masks, blend types, and other options. The `example` method takes a canvas element, a device type, and some data as arguments. It creates a `pc.GraphicsDevice` object, sets up an `AppBase` object, loads some assets, creates some entities, and assigns animations to them. It also sets up event listeners to respond to changes in the data object made by the control panel. Finally, it starts the app.\n\nThe `example` method creates several entities, including a camera entity, a light entity, and a model entity. The model entity is created from a loaded model using the `instantiateRenderEntity` method. The `anim` component is added to the model entity, and animations are assigned to it. The `baseLayer` is the default layer, and the `assignAnimation` method is used to assign animations to it. The `addLayer` method is used to create a new layer for the upper body, with additive layer blending. A mask is created for the upper body layer, and animations are assigned to it. The `transition` method is used to transition between animations. The `weight` property is used to control the weight of each layer. The `mask` property is used to set the mask for the upper body layer. The `drawSkeleton` function is used to draw the skeleton of the model entity. The `app.on('update')` event listener is used to call the `drawSkeleton` function every frame. The `data.on('*:set')` event listener is used to respond to changes in the data object made by the control panel.\n\nThe `controls` method returns a JSX element that contains several `Panel` components, each with `LabelGroup`, `SelectInput`, `BooleanInput`, and `SliderInput` components. These components are used to control the animation's layer masks, blend types, and other options. For example, the `SelectInput` component is used to select the active state for the full body layer and the upper body layer. The `BooleanInput` component is used to toggle the use of the mask for the upper body layer. The `SliderInput` component is used to control the blend of the base layer and the upper body layer.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a 3D animation with layer masks. The `LayerMasksExample` class defines a method that creates several entities, assigns animations to them, and sets up event listeners to respond to changes in the data object made by the control panel. The `controls` method returns a JSX element that contains several components that are used to control the animation's layer masks, blend types, and other options.\n## Questions: \n 1. What is the purpose of the `controls` method in the `LayerMasksExample` class?\n- The `controls` method returns a JSX element that represents the UI controls for the layer masks example.\n2. What assets are being loaded in the `example` method of the `LayerMasksExample` class?\n- The `example` method loads several assets including a 3D model, animations, a texture, and a script.\n3. What does the `drawSkeleton` function do?\n- The `drawSkeleton` function recursively draws lines between the joints of a 3D model's skeleton, with the color of the lines determined by the weight of the corresponding animation targets.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/animation/layer-masks.md"}}],["49",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/animation/tween.tsx)\n\nThe code is an example of how to use the PlayCanvas engine to create a tween animation. The TweenExample class is a React component that loads the PlayCanvas engine and creates a tween animation using the TWEEN library. The example creates a series of spheres that move along a path using different easing functions. The purpose of this code is to demonstrate how to use the PlayCanvas engine to create animations and how to integrate external libraries like TWEEN.\n\nThe example is loaded by calling the load() method, which returns a ScriptLoader component that loads the TWEEN library from a CDN. The example is then executed by calling the example() method, which takes a canvas element and a device type as parameters. The canvas element is used to create a graphics device, and the device type is used to specify the type of device to create.\n\nThe example creates a series of entities that represent the spheres and sets their position using the tween animation. The easing functions are used to control the speed and acceleration of the spheres as they move along the path. The example also creates a text label for each sphere that displays the name of the easing function used.\n\nThe example also creates a directional light and a camera entity to provide lighting and perspective for the scene. The update event is used to draw lines that represent the path of the spheres.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create animations and how to integrate external libraries like TWEEN. It also shows how to create entities, set their position using tween animations, and add lighting and perspective to a scene.\n## Questions: \n 1. What is the purpose of the `TweenExample` class?\n- The `TweenExample` class is an example of animation using the `tween.js` library and the PlayCanvas engine.\n\n2. What assets are being loaded in the `load()` method?\n- The `load()` method loads a font asset and a script asset.\n\n3. What is the purpose of the `example()` method?\n- The `example()` method creates a PlayCanvas application, loads assets, creates entities with various components, and sets up a tween animation using the `tween.js` library.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/animation/tween.md"}}],["50",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/camera/first-person.tsx)\n\nThe `FirstPersonExample` class is a code example that demonstrates how to create a first-person camera controller in the PlayCanvas engine. The purpose of this code is to provide developers with a starting point for creating their own first-person camera controllers in their projects. \n\nThe `example` method is the main entry point for the code example. It takes two parameters: a `canvas` element and a `deviceType` string. The `canvas` element is used to create a new `pc.Application` instance, which is the main object that manages the PlayCanvas engine. The `deviceType` string is not used in this code example.\n\nThe `example` method creates a new `pc.Application` instance with a set of input devices, including a mouse, touch device, gamepads, and keyboard. It then loads two assets: a 3D model of a statue and a script that defines the first-person camera controller. Once the assets are loaded, the `run` function is called, which starts the application and creates the scene.\n\nThe `run` function creates a physical floor, a model entity, a camera entity, a character controller, and a directional light. The physical floor is created using a `pc.Entity` instance with a `collision` component and a `rigidbody` component. The model entity is created using the `assets.statue.resource.instantiateRenderEntity` method, which instantiates a new entity with the 3D model of the statue. The camera entity is created using a `pc.Entity` instance with a `camera` component. The character controller is created using a `pc.Entity` instance with a `collision` component, a `rigidbody` component, and several script components that define the first-person camera controller. Finally, the directional light is created using a `pc.Entity` instance with a `light` component.\n\nThe `FirstPersonExample` class can be used as a starting point for creating a first-person camera controller in a PlayCanvas project. Developers can modify the code to suit their specific needs, such as changing the 3D model, adjusting the camera settings, or adding additional input devices. Overall, this code example demonstrates how to create a basic first-person camera controller in the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a first-person camera implementation in the PlayCanvas engine, which creates a 3D environment with a physical floor, a model entity, a camera, a character controller, and a directional light.\n\n2. What external dependencies does this code have?\n- This code depends on the PlayCanvas engine, which is imported using the wildcard syntax, as well as on the Ammo physics engine, which is loaded using WebAssembly.\n\n3. What is the expected output of this code?\n- The expected output of this code is a playable 3D scene with a first-person camera that can be controlled using various input devices, such as a keyboard, a mouse, or a gamepad. The scene includes a physical floor, a model entity, a camera, a character controller, and a directional light, all of which are rendered using the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/camera/first-person.md"}}],["51",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/camera/fly.tsx)\n\nThe code defines a class called `FlyExample` that contains a method called `example`. This method creates a 3D scene using the PlayCanvas engine and populates it with boxes, lights, and a camera. The purpose of this code is to demonstrate how to create a fly camera in PlayCanvas.\n\nThe `example` method takes two parameters: a canvas element and a device type. It creates a new PlayCanvas application using the canvas element and sets up a mouse and keyboard input. It then loads a script asset called `fly-camera.js` that contains the fly camera logic. Once the asset is loaded, it sets the ambient light of the scene to a dark gray color and starts the application.\n\nThe method then defines two helper functions: `createMaterial` and `createBox`. `createMaterial` takes a `pc.Color` object and returns a new `pc.StandardMaterial` with the diffuse color set to the input color. `createBox` takes a position, size, and material and creates a new box entity with a model component of type 'box'. It sets the position, scale, and material of the box and adds it to the scene hierarchy.\n\nThe method then uses these helper functions to create a few red boxes and a white floor in the scene. It also creates an omni-directional light and adds it to the scene hierarchy. Finally, it creates a new camera entity with a camera component and a fly camera script component. It sets the position of the camera and adds it to the scene hierarchy.\n\nOverall, this code demonstrates how to create a simple 3D scene with PlayCanvas and how to add a fly camera to it. It can be used as a starting point for more complex 3D applications that require user-controlled cameras.\n## Questions: \n 1. What is the purpose of the `FlyExample` class?\n- The `FlyExample` class is an example of how to create a fly camera in the PlayCanvas engine.\n\n2. What dependencies does the `example` method of the `FlyExample` class have?\n- The `example` method of the `FlyExample` class has a dependency on an HTML canvas element and a string representing the device type.\n\n3. What does the `createBox` function do?\n- The `createBox` function creates a new entity in the PlayCanvas engine with a model component of type 'box', a specified position and size, and a specified material.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/camera/fly.md"}}],["52",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/camera/index.mjs)\n\nThe code above is a module that exports three examples of camera movement for the PlayCanvas engine. The PlayCanvas engine is a WebGL-based game engine that allows developers to create 3D games and applications that run in a web browser. \n\nThe three camera movement examples that are exported from this module are FirstPersonExample, OrbitExample, and FlyExample. These examples demonstrate different ways that a camera can be controlled in a 3D environment. \n\nThe FirstPersonExample allows the user to move the camera in a first-person perspective, as if they were walking around in the game world. The OrbitExample allows the user to orbit around a target object, as if they were controlling a camera on a tripod. The FlyExample allows the user to fly around the game world, as if they were controlling a camera on a drone. \n\nThese examples are useful for developers who are building games or applications with the PlayCanvas engine, as they provide a starting point for implementing camera movement. Developers can use these examples as a reference or modify them to fit their specific needs. \n\nHere is an example of how a developer might use the FirstPersonExample in their code:\n\n```\nimport { FirstPersonExample } from \"playcanvas-engine-examples\";\n\n// Create a new instance of the FirstPersonExample\nconst firstPerson = new FirstPersonExample();\n\n// Add the camera to the scene\nthis.entity.addChild(firstPerson.camera);\n\n// Update the camera movement every frame\nthis.app.on(\"update\", function (deltaTime) {\n    firstPerson.update(deltaTime);\n});\n```\n\nIn this example, the developer imports the FirstPersonExample from the module and creates a new instance of it. They then add the camera to the scene and update its movement every frame. This allows the user to control the camera in a first-person perspective. \n\nOverall, this module provides a useful resource for developers who are building games or applications with the PlayCanvas engine. It demonstrates different ways that a camera can be controlled in a 3D environment and provides a starting point for implementing camera movement.\n## Questions: \n 1. **What is the purpose of this code file?**\\\nA smart developer might wonder what this code file does and what its purpose is within the PlayCanvas engine. This code file exports three examples: FirstPersonExample, OrbitExample, and FlyExample, which are likely used to demonstrate different camera movement options within the engine.\n\n2. **What are the dependencies of this code file?**\\\nA smart developer might want to know what other files or modules this code file depends on in order to function properly. From the import statements, it appears that this code file depends on three other files: \"./first-person\", \"./orbit\", and \"./fly\".\n\n3. **How can these examples be implemented in a project?**\\\nA smart developer might be interested in using these examples in their own project and would want to know how to implement them. They can import the examples from this code file and use them as needed in their own code.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/camera/index.md"}}],["53",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/camera/orbit.tsx)\n\nThe `OrbitExample` class is a part of the PlayCanvas engine project and is responsible for creating an example of an orbit camera. The purpose of this code is to demonstrate how to create an orbit camera in PlayCanvas. An orbit camera is a type of camera that allows the user to orbit around a target object. This is useful for games and applications where the user needs to view an object from different angles.\n\nThe `example` method is the main method of the `OrbitExample` class. It takes two parameters, a canvas element and a device type. It creates a new PlayCanvas application and starts the update loop. It then loads two assets, a 3D model of a statue and a script for the orbit camera. Once the assets are loaded, it creates an entity hierarchy representing the statue and adds it to the root of the application. It also creates a camera entity with an orbit camera script and adds it to the root of the application. Finally, it creates a directional light and adds it to the root of the application.\n\nThe `OrbitExample` class can be used as a starting point for creating an orbit camera in a PlayCanvas project. Developers can use this code as a reference to understand how to create an orbit camera and customize it to fit their specific needs. For example, they can change the target object, adjust the camera settings, or add additional input methods for the camera. Here is an example of how to use the `OrbitExample` class:\n\n```javascript\nimport OrbitExample from 'path/to/OrbitExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'desktop';\n\nconst orbitExample = new OrbitExample();\norbitExample.example(canvas, deviceType);\n```\n\nThis code creates a new instance of the `OrbitExample` class and calls the `example` method with a canvas element and a device type. This will create an orbit camera in the PlayCanvas application and display the statue object. Developers can then modify the code to fit their specific needs.\n## Questions: \n 1. What is the purpose of the `OrbitExample` class?\n- The `OrbitExample` class is an example of how to create a camera with an orbit camera script and a directional light in the PlayCanvas engine.\n\n2. What are the parameters of the `example` method?\n- The `example` method takes in an HTML canvas element and a device type as parameters.\n\n3. What is the purpose of the `assetListLoader` variable?\n- The `assetListLoader` variable is used to load the assets needed for the example, which are a 3D model of a statue and a script for the orbit camera.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/camera/orbit.md"}}],["54",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/area-lights.tsx)\n\nThe `AreaLightsExample` class is a part of the PlayCanvas engine project and is responsible for demonstrating the use of area lights in a 3D scene. The purpose of this code is to create a 3D scene with area lights and a statue model, and update the scene each frame. \n\nThe `example` method is the main function of this class, which takes two parameters: a canvas element and a device type. It creates a graphics device using the `createGraphicsDevice` method and initializes the app using the `init` method. It then sets up the scene by creating a ground plane, a camera, and three area lights with different shapes and colors. The `createPrimitive` and `createAreaLight` helper functions are used to create the primitives and area lights respectively. \n\nThe `createPrimitive` function creates a primitive with a specified shape, position, scale, and color. It also creates a material of the specified color and sets it to the primitive. The primitive is then added to the scene. \n\nThe `createAreaLight` function creates an area light with a specified type, shape, position, scale, color, intensity, shadows, and range. It also creates a visual representation of the light source using a primitive shape and a material of the same color as the light source. The primitive shape is then added to the scene. \n\nThe `app.on(\"update\")` event listener is used to update the scene each frame. It updates the position and rotation of the area lights based on time and camera position. \n\nOverall, this code demonstrates how to create and use area lights in a 3D scene using the PlayCanvas engine. It can be used as a reference for developers who want to implement area lights in their own projects.\n## Questions: \n 1. What is the purpose of the `example` method in the `AreaLightsExample` class?\n- The `example` method is used to run the example code for the area lights feature of the PlayCanvas engine on a given canvas element and device type.\n\n2. What assets are being loaded in the `assets` object?\n- The `assets` object is loading several assets including textures, a 3D model, and a JSON file containing data for area light look-up tables.\n\n3. What is the purpose of the `createAreaLight` function?\n- The `createAreaLight` function is used to create an area light entity with a visual representation in the scene, including a primitive shape and emissive material. It takes parameters such as light type, shape, position, color, intensity, and range.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/area-lights.md"}}],["55",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/area-picker.tsx)\n\nThe `AreaPickerExample` class is an example implementation of the PlayCanvas engine's area picker functionality. The purpose of this code is to demonstrate how to use the area picker to select objects in a 3D scene based on a 2D screen area. \n\nThe `example` method takes an HTML canvas element and a device type as input parameters. It creates a graphics device using the `pc.createGraphicsDevice` method and initializes a new PlayCanvas application using the `pc.AppBase` class. It then sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. \n\nThe example creates a list of assets that are used in the scene, including a bloom post-processing effect and a cubemap texture. It then loads these assets using the `pc.AssetListLoader` class and starts the application using the `app.start` method. \n\nThe example generates a box area with specified size of random primitives and adds them to the scene. It also creates a main camera and adds a bloom post-processing effect to it. \n\nThe `example` method then sets up the area picker by creating an instance of the `pc.Picker` class. It defines an array of areas that it wants to sample, each with a different highlight color. It then processes each area by displaying a 2D rectangle around it and getting a list of mesh instances inside the area from the picker. It then highlights the mesh instances to the appropriate color for the area. \n\nThe `createPrimitive` function is a helper function that creates a primitive with a specified shape type, position, and scale. It creates a material of random color and a primitive with the specified parameters. \n\nThe `drawRectangle` function is a helper function that draws a 2D rectangle in the screen space coordinates. It transforms 4 2D screen points into world space and connects them using white lines. \n\nThe `highlightMaterial` function sets a material's emissive color to a specified color. \n\nOverall, this code demonstrates how to use the area picker to select objects in a 3D scene based on a 2D screen area. It also shows how to create primitives, draw rectangles, and highlight materials. This example can be used as a starting point for implementing area picking in a PlayCanvas project.\n## Questions: \n 1. What is the purpose of the `AreaPickerExample` class?\n- The `AreaPickerExample` class is an example implementation of an area picker feature in the PlayCanvas engine, which allows users to select and highlight objects within a specified area on the canvas.\n\n2. What are the dependencies required for this code to run?\n- This code requires the PlayCanvas engine, which is imported from a relative path, as well as the `pc` module from the engine. It also requires access to certain assets and libraries, such as a bloom posteffect script and a texture for the skybox.\n\n3. How does the `picker` object work and what is its purpose?\n- The `picker` object is an instance of the `pc.Picker` class, which is used to select and retrieve mesh instances within a specified area on the canvas. It works by rendering the scene into a render target and mapping the pixel values to mesh instances, which can then be highlighted or manipulated as desired.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/area-picker.md"}}],["56",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/asset-viewer.tsx)\n\nThe code is a React component that demonstrates the use of the PlayCanvas engine to create a 3D asset viewer. The component imports the necessary modules from the PlayCanvas engine and other libraries, including React, and defines a class called AssetViewerExample. \n\nThe AssetViewerExample class has two static properties: CATEGORY and NAME. These properties are used to categorize and name the example in the larger project. The class also has two methods: controls and example. \n\nThe controls method returns a React component that renders two buttons labeled \"Previous\" and \"Next\". When clicked, these buttons emit events that are handled by the example method. \n\nThe example method creates a graphics device using the createGraphicsDevice method from the PlayCanvas engine. It then creates an AppBase instance and initializes it with the graphics device and other options. The method loads a list of assets, including textures, models, and fonts, and instantiates them as entities in the scene. \n\nThe method also creates a plane entity with a checkerboard material and adds it to the scene. It creates a camera entity with an orbit camera script and adds it to the scene. It creates a directional light entity and adds it to the scene. It sets the environment map, tone mapping, skybox rotation, and skybox intensity of the scene. \n\nThe method also adds event listeners to the canvas element that handle touch events and change the focus entity of the camera when the \"Previous\" or \"Next\" button is clicked. \n\nOverall, the code demonstrates how to use the PlayCanvas engine to create a 3D asset viewer with a camera that can be controlled by the user. It also shows how to load and instantiate assets, create entities, and add them to the scene.\n## Questions: \n 1. What is the purpose of the `controls` method in the `AssetViewerExample` class?\n- The `controls` method returns a React component that contains two buttons for navigating between assets.\n\n2. What is the purpose of the `gfxOptions` object?\n- The `gfxOptions` object contains options for creating a graphics device, including the device types and URLs for glslang and twgsl.\n\n3. What is the purpose of the `jumpToAsset` function?\n- The `jumpToAsset` function changes the camera position and focus entity to the next or previous asset in the `assetList` array, depending on the offset parameter.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/asset-viewer.md"}}],["57",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/batching-dynamic.tsx)\n\nThe code is an example of how to use the PlayCanvas engine to create a dynamic batching system for rendering multiple meshes in a single draw call. The purpose of this code is to demonstrate how to use the engine's batch manager to group meshes with similar materials and render them together, reducing the number of draw calls and improving performance.\n\nThe code imports the PlayCanvas engine and defines a class called BatchingDynamicExample. This class has a static CATEGORY and NAME property that defines the category and name of the example. It also has a static WEBGPU_ENABLED property that is set to true, indicating that the example can be run on WebGPU.\n\nThe example method takes an HTMLCanvasElement and a deviceType string as parameters. It creates a graphics device using the createGraphicsDevice method and sets up an AppBase instance with the graphics device. It then creates two StandardMaterial instances and a BatchGroup instance using the app's batcher. The BatchGroup is set to be dynamic, allowing the batched meshes to be moved every frame.\n\nThe code then creates multiple primitive instances using the two materials and adds them to the BatchGroup. It also creates an Entity for the ground, a camera Entity, and a directional light Entity. The update function of the app is set to move the entities along orbits and orbit the camera around the scene.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a dynamic batching system for rendering multiple meshes efficiently. It shows how to use the engine's batch manager to group meshes with similar materials and render them together, reducing the number of draw calls and improving performance.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of dynamic batching in the PlayCanvas engine, which allows for rendering multiple meshes in a small number of draw calls.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library using the wildcard syntax, as well as two external libraries for GLSL and TWGSL.\n\n3. What is the expected output of this code?\n- The expected output is a 3D scene with 500 randomly placed and shaped meshes, a ground plane, a camera, and a directional light, all rendered using dynamic batching. The camera orbits around the scene and the meshes move along orbits.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/batching-dynamic.md"}}],["58",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/clustered-area-lights.tsx)\n\nThe code defines a class called `AreaLightsExample` that provides an example of clustered area lights in the PlayCanvas engine. The class has two static properties, `CATEGORY` and `NAME`, which are used to categorize and name the example respectively. The `WEBGPU_ENABLED` property is a boolean that indicates whether the example is enabled for WebGPU.\n\nThe `controls` method returns a React component that renders a panel with two sliders for adjusting the gloss and metalness properties of a material. The `example` method takes a canvas element, a device type, and some data as arguments. It creates a graphics device using the `createGraphicsDevice` method of the PlayCanvas engine, and initializes an app with the device. It then loads some assets, including textures and scripts, and creates a scene with a ground plane, a camera, and a grid of area lights of different shapes and colors.\n\nThe `createPrimitive` function is a helper function that creates a primitive shape with a given type, position, scale, and material. The `createAreaLight` function is another helper function that creates an area light with a given type, shape, position, scale, color, intensity, and range. It also creates a visual representation of the light source using a primitive shape and a material.\n\nThe `example` method sets up some general scene rendering properties, such as tone mapping and clustered lighting. It also sets some parameters for the clustered lighting, such as the number of cells and the maximum number of lights per cell. It enables area lights and disables shadows. It creates some materials, including a black material for the back side of light objects and a ground material with gloss and metalness properties. It creates a camera with an orbit camera script and adds bloom postprocessing to it. It generates a grid of area lights of different shapes and colors. Finally, it handles changes to the gloss and metalness properties of the ground material by updating the material properties.\n\nOverall, this code provides an example of how to create and manipulate area lights in the PlayCanvas engine. It demonstrates how to use the engine's graphics device, assets, and scene to create a 3D scene with area lights and postprocessing effects. It also shows how to handle user input and update material properties in response to changes.\n## Questions: \n 1. What is the purpose of the `controls` method in the `AreaLightsExample` class?\n- The `controls` method returns a React component that contains UI controls for adjusting the gloss and metalness properties of a material.\n\n2. What is the purpose of the `example` method in the `AreaLightsExample` class?\n- The `example` method sets up a PlayCanvas app that demonstrates clustered area lights with different shapes and properties, and allows for adjusting the material properties through the UI controls returned by the `controls` method.\n\n3. What is the significance of the `WEBGPU_ENABLED` static property in the `AreaLightsExample` class?\n- The `WEBGPU_ENABLED` static property indicates that the example supports the WebGPU API for hardware-accelerated graphics rendering, which is a new standard that is currently being developed and implemented in browsers.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/clustered-area-lights.md"}}],["59",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/clustered-lighting.tsx)\n\nThe `ClusteredLightingExample` class is a code example that demonstrates how to use clustered lighting in the PlayCanvas engine. Clustered lighting is a technique used to improve the performance of rendering many lights in a scene. \n\nThe `example` method of the `ClusteredLightingExample` class takes an HTML canvas element and a device type as input parameters. It creates a graphics device using the `pc.createGraphicsDevice` method and initializes a new PlayCanvas application using the `pc.AppBase` constructor. \n\nThe method then sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. It also creates a ground plane and a high polycount cylinder using the `pc.Entity` constructor and adds them to the scene. \n\nThe method then creates many omni and spot lights using the `pc.Entity` constructor and adds them to the scene. It also creates a single directional light which casts shadows. The lights are moved and rotated using the `on` method of the app's `update` event. \n\nFinally, the method sets an update function on the app's update event that moves the lights around the cylinder and rotates the directional light. \n\nThis code example can be used as a starting point for developers who want to implement clustered lighting in their PlayCanvas projects. Developers can modify the code to create their own scenes and add their own lights. They can also experiment with different values for the `lighting.cells` and `lighting.maxLightsPerCell` parameters to optimize the performance of their scenes. \n\nExample usage:\n\n```\nimport ClusteredLightingExample from 'path/to/ClusteredLightingExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2';\n\nconst example = new ClusteredLightingExample();\nexample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of the `ClusteredLightingExample` class?\n- The `ClusteredLightingExample` class is an example of how to use clustered lighting in the PlayCanvas engine, and it includes code for creating and manipulating various types of lights.\n\n2. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property indicates whether the example is compatible with the WebGPU API, which is a new graphics API that is designed to provide better performance and more efficient use of hardware resources.\n\n3. What is the purpose of the `gfxOptions` object?\n- The `gfxOptions` object is used to specify various options for creating a graphics device, including the type of device to create (e.g. WebGL or WebGPU), and the URLs of various libraries that are required for graphics rendering.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/clustered-lighting.md"}}],["60",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/clustered-omni-shadows.tsx)\n\nThe code is an example of how to use the PlayCanvas engine to create a 3D scene with clustered omni shadows. The purpose of the code is to demonstrate how to use the PlayCanvas engine to create a 3D scene with clustered omni shadows, which is a technique used to optimize the rendering of many lights in a scene. The code imports several modules from the PlayCanvas engine, as well as some modules from the @playcanvas/pcui/react and @playcanvas/observer libraries. \n\nThe ClusteredOmniShadowsExample class contains two methods: controls and example. The controls method returns a React component that renders a panel with several input fields that allow the user to adjust the settings of the scene, such as the shadow resolution, the shadow filter type, and whether shadows and cookies are enabled. The example method takes a canvas element, a device type, and some data as arguments, and creates a 3D scene with clustered omni shadows. \n\nThe example method first creates an object that contains several assets, such as textures and scripts, that are used in the scene. It then creates a graphics device using the createGraphicsDevice method of the PlayCanvas engine, and initializes an app using the AppBase class. The app is configured to use the graphics device, and several component systems and resource handlers are added to it. \n\nThe example method then sets up the scene by creating several primitives, such as boxes and spheres, and adding them to the scene. It also creates several omni lights, which are lights that emit light in all directions, and adds them to the scene. The lights are configured to cast shadows and use a cookie texture, which is a texture that is projected onto the scene to create a pattern of light and shadow. \n\nThe example method also sets up the clustered lighting system by enabling clustered lighting and adjusting the default clustered lighting parameters to handle many lights. It also enables clustered shadows and cookies, and sets the resolution of the shadow and cookie atlases. \n\nFinally, the example method sets up an update function that is called every frame, and updates the position of the omni lights. It also provides some debug features that allow the user to display the shadow and cookie textures. \n\nOverall, the code demonstrates how to use the PlayCanvas engine to create a 3D scene with clustered omni shadows, and provides an example of how to use the engine's API to create and configure lights, shadows, and cookies.\n## Questions: \n 1. What is the purpose of the `controls` method in the `ClusteredOmniShadowsExample` class?\n- The `controls` method returns a JSX element that contains UI controls for adjusting settings related to shadows and cookies in the example.\n2. What type of graphics device is created in the `example` method?\n- The `example` method creates a WebGL graphics device using the `pc.createGraphicsDevice` function.\n3. What is the purpose of the `Observer` class imported from `@playcanvas/observer`?\n- The `Observer` class is used to create an observable data object that can be used to track changes to the settings in the example and update the scene accordingly.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/clustered-omni-shadows.md"}}],["61",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/grab-pass.tsx)\n\nThe `GrabPassExample` class is a part of the PlayCanvas engine project and is responsible for rendering a scene with a refraction shader that distorts the view behind it. The class contains a static `example` method that takes a canvas element, a device type, and two shader files as input. The method creates a graphics device using the `createGraphicsDevice` method and initializes a new PlayCanvas application using the `AppBase` class. It then loads a list of assets, including a normal map, a roughness map, and a cubemap texture, and sets up the scene by creating a camera, a skydome, and several primitive shapes. \n\nThe `example` method also creates a box primitive that uses the refraction shader to distort the view behind it. The shader is defined in the `shader.vert` and `shader.frag` files, which are passed as input to the method. The `shader.vert` file defines the vertex shader, which projects the position of the primitive and sets the texture coordinates. The `shader.frag` file defines the fragment shader, which samples the offset texture to add distortion to the sampled background, gets the normalized UV coordinates for the canvas, and calculates the mipmap level based on the roughness of the material. It then samples the background pixel color with distorted offset, tints the material based on the mipmap level, and brightens the refracted texture. \n\nThe `example` method updates the scene each frame by rotating the primitives, orbiting the camera, and updating the refraction material. The method uses the `rotate` method to rotate the primitives and the `setLocalPosition` and `lookAt` methods to orbit the camera. The method also sets the `castShadows` and `receiveShadows` properties of the glass primitive to `false` to prevent it from casting or receiving shadows. \n\nOverall, the `GrabPassExample` class demonstrates how to use the PlayCanvas engine to create a scene with a refraction shader that distorts the view behind it. The class can be used as a starting point for creating more complex scenes with custom shaders and materials.\n## Questions: \n 1. What does this code do?\n- This code is an example implementation of a refraction shader using the PlayCanvas engine. It creates a scene with primitives and a glass object that distorts the view behind it.\n\n2. What are the dependencies of this code?\n- This code depends on the PlayCanvas engine and requires the following files: `shader.vert`, `shader.frag`, `normal-map.png`, `pc-gray.png`, and `helipad-env-atlas.png`.\n\n3. What is the purpose of the `GrabPassExample` class?\n- The `GrabPassExample` class is a container for the example implementation of the refraction shader. It defines the category, name, and files required for the example, and contains the `example` method that sets up the scene and runs the shader.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/grab-pass.md"}}],["62",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/ground-fog.tsx)\n\nThe code defines a class called `GroundFogExample` that demonstrates how to create a custom fog shader in the PlayCanvas engine. The class has a static `CATEGORY` and `NAME` properties that define the category and name of the example in the PlayCanvas editor. It also has a static `WEBGPU_ENABLED` property that indicates whether the example is supported on WebGPU.\n\nThe class has a `controls` method that returns a React component that renders a panel with a boolean input control for toggling the softness of the fog. The control is bound to an Observer object that is passed as an argument to the method.\n\nThe class has an `example` method that takes a canvas element, a device type, a files object, and a data object as arguments. The method creates a PlayCanvas application with the specified graphics device and component systems. It loads several assets, including a terrain model, a texture, and a custom shader. It creates a camera entity with an orbit camera script and a directional light entity with cascaded shadows. It creates a subdivided plane mesh and a ground entity with a custom material that uses the custom shader and the texture asset. The method updates the time and the softness of the fog in the material parameters based on the data object. It also renders the depth texture in the corner of the canvas for debugging purposes.\n\nThe purpose of the code is to demonstrate how to create a custom fog shader in the PlayCanvas engine and how to use it in a PlayCanvas application. The code can be used as a starting point for creating more complex fog effects or for integrating custom shaders into PlayCanvas applications. The code can also be used as a reference for using the PlayCanvas API to create and manipulate entities, assets, materials, and shaders.\n## Questions: \n 1. What is the purpose of the `controls` method in this code?\n- The `controls` method returns a React component that contains UI controls for the example, specifically a toggle for adjusting the softness of the fog.\n\n2. What assets are loaded and used in this example?\n- The example loads four assets: a script, a container, a texture, and a cubemap. These assets are used to create a terrain, a skydome, a camera, and a directional light with cascaded shadows.\n\n3. What does the `example` method do?\n- The `example` method sets up a PlayCanvas application, loads the necessary assets, creates entities and components for the scene, sets up a custom fog shader, and updates the scene on each frame. It also includes event listeners for resizing the canvas and updating the softness of the fog.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/ground-fog.md"}}],["63",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/hardware-instancing.tsx)\n\nThe `HardwareInstancingExample` class is a part of the PlayCanvas engine project and is responsible for demonstrating the use of hardware instancing in 3D graphics rendering. Hardware instancing is a technique used to render multiple instances of the same object with a single draw call, which can significantly improve performance in certain scenarios.\n\nThe `example` method of the `HardwareInstancingExample` class takes an HTML canvas element and a device type as input parameters. It creates a graphics device using the `pc.createGraphicsDevice` method and initializes a PlayCanvas application using the `pc.AppBase` class. It then loads a texture asset and sets up the scene by configuring the skydome, lighting, and camera.\n\nThe method then creates a `StandardMaterial` object and enables instancing on it by setting the `useInstancing` property to `true`. It creates a cylinder-shaped entity with a render component that uses the instancing material and adds it to the scene hierarchy. If the graphics device supports instancing, the method generates a set of random positions, scales, and rotations for a specified number of instances and stores them in a `Float32Array`. It creates a static vertex buffer containing the matrices and initializes instancing using the vertex buffer on the mesh instance of the cylinder entity.\n\nFinally, the method sets an update function on the app's update event that orbits the camera around the scene.\n\nThis code can be used as a reference for implementing hardware instancing in PlayCanvas applications. Developers can modify the code to suit their specific needs, such as using different objects, materials, or instance counts. They can also use the `pc.VertexBuffer` and `pc.MeshInstance` classes to create and manipulate vertex buffers and mesh instances, respectively.\n\nExample usage:\n\n```javascript\nimport HardwareInstancingExample from 'path/to/HardwareInstancingExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2';\n\nconst example = new HardwareInstancingExample();\nexample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of hardware instancing using the PlayCanvas engine. It creates a scene with a camera and a cylinder entity, and renders multiple instances of the cylinder using instancing.\n\n2. What dependencies does this code have?\n- This code imports the PlayCanvas engine using the wildcard import syntax, and also imports the pc namespace from a relative path. It also requires an HTML canvas element and a device type string to be passed as arguments to the example() method.\n\n3. What is the significance of the WEBGPU_ENABLED property?\n- The WEBGPU_ENABLED property is a static property of the HardwareInstancingExample class, and is set to true. This indicates that the example supports the WebGPU API, which is a new graphics API that provides lower-level access to hardware acceleration than WebGL.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/hardware-instancing.md"}}],["64",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/hierarchy.tsx)\n\nThe `HierarchyExample` class is a code example that demonstrates how to create a hierarchy of entities in the PlayCanvas engine. The purpose of this code is to show how to use the PlayCanvas engine to create a complex scene with multiple entities that are organized in a hierarchical structure. The code creates a root entity and then recursively creates child entities with different shapes, positions, and scales. The child entities are added to the parent entity, forming a tree-like structure. The code also creates a camera and a light entity to provide illumination and a viewpoint for the scene.\n\nThe `example` method is the main entry point for the code. It takes two parameters: a canvas element and a device type. The canvas element is used to create a graphics device, which is required to render the scene. The device type specifies the type of graphics device to create, such as \"webgl\" or \"webgpu\". The method creates a graphics device using the `pc.createGraphicsDevice` function and initializes a new PlayCanvas application using the `pc.AppBase` class. It then sets the canvas fill mode and resolution and starts the application.\n\nThe `createPrimitive` function is a helper function that creates a new entity with a render component and a material of random color. The `createChildren` function is a recursive function that creates a new layer of child entities for a specified parent entity. It takes several parameters, including the parent entity, the size of the grid, the scale of the entities, the spacing between the entities, and the number of levels to create. The function creates child entities with random shapes, positions, and scales and adds them to the parent entity. It then calls itself recursively to create the next layer of child entities.\n\nThe code also creates a camera entity and a light entity using the `camera` and `light` components, respectively. The camera is positioned at an angle to provide a viewpoint for the scene, and the light provides illumination for the entities.\n\nFinally, the code updates the rotation of each entity each frame using the `update` event. It creates a new quaternion with a rotation that changes with time and applies it to each entity's local rotation.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a complex scene with multiple entities organized in a hierarchical structure. It shows how to create child entities recursively and how to update their properties each frame. This code can be used as a starting point for creating more complex scenes in the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `HierarchyExample` class?\n- The `HierarchyExample` class is an example of how to create a hierarchy of entities in the PlayCanvas engine.\n\n2. What graphics device options are being set in the `example` method?\n- The `example` method sets the device type, glslang URL, and twgsl URL for the graphics device.\n\n3. What is the purpose of the `createChildren` function?\n- The `createChildren` function is a recursive helper function that creates a hierarchy of child entities for a specified parent entity, based on a specified grid size, scale, spacing, and number of levels.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/hierarchy.md"}}],["65",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/index.mjs)\n\nThis code exports a list of examples from various modules in the PlayCanvas engine. Each example is a separate module that demonstrates a specific feature or technique in the engine. These examples can be used as a reference for developers who want to learn how to use the engine or as a starting point for building their own projects.\n\nFor example, the \"AreaLightsExample\" module demonstrates how to use area lights in the engine, while the \"MeshGenerationExample\" module shows how to generate meshes programmatically. Each module is named after the feature it demonstrates, making it easy to find the relevant example.\n\nDevelopers can import these examples into their own projects and modify them to suit their needs. For instance, they could use the \"MaterialPhysicalExample\" module as a starting point for creating their own physically-based materials.\n\nOverall, this code serves as a valuable resource for developers who want to learn how to use the PlayCanvas engine and take advantage of its features.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file exports a list of examples for various features in the PlayCanvas engine.\n\n2. How are these examples used in the PlayCanvas engine?\n- These examples can be used as reference code for developers to learn how to use and implement various features in the PlayCanvas engine.\n\n3. Are there any dependencies or requirements for using these examples?\n- It is unclear from this code file if there are any dependencies or requirements for using these examples. Developers may need to refer to additional documentation or code files to determine any necessary dependencies.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/index.md"}}],["66",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/layers.tsx)\n\nThe `LayersExample` class is a code example that demonstrates how to use layers in the PlayCanvas engine. Layers are used to control the rendering order of entities in a scene. This code example creates a new layer and inserts it after the \"World\" layer. It then creates two entities, a red box, and a blue box. The red box is rendered first in the \"World\" layer, and the blue box is rendered in the new layer. The blue box is visible even though it should be inside the red box because it does not test for depth and is in a later layer.\n\nThe `example` method is the entry point for the code example. It takes an HTML canvas element and a device type as parameters. It creates a new graphics device using the `pc.createGraphicsDevice` method and initializes a new PlayCanvas application using the `pc.AppBase` class. It sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. It then creates a new layer and inserts it after the \"World\" layer. It creates two entities, a camera, and a light, and adds them to the scene. Finally, it creates two materials, a red material, and a blue material, and uses them to render the red and blue boxes.\n\nThe `LayersExample` class is a useful code example for developers who want to learn how to use layers in the PlayCanvas engine. It demonstrates how to create a new layer, insert it into the scene, and use it to control the rendering order of entities. Developers can use this code example as a starting point for their own projects that require complex rendering order control.\n## Questions: \n 1. What is the purpose of the LayersExample class?\n- The LayersExample class is an example of how to use layers in the PlayCanvas engine to render entities in different layers.\n\n2. What is the significance of the WEBGPU_ENABLED property?\n- The WEBGPU_ENABLED property indicates whether the example is compatible with the WebGPU API, which is a new graphics API for the web.\n\n3. What is the purpose of the gfxOptions object?\n- The gfxOptions object is used to specify options for creating a graphics device, including the device type and the URLs for the glslang and twgsl libraries.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/layers.md"}}],["67",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/lights-baked.tsx)\n\nThe `LightsBakedExample` class is a code example that demonstrates how to use the PlayCanvas engine to create a scene with baked lighting. The purpose of this code is to show how to create entities with different types of render components, add lights to the scene, and bake lightmaps to improve performance.\n\nThe `example` method takes an HTML canvas element and a device type as parameters. It creates a graphics device using the `pc.createGraphicsDevice` method and initializes a new PlayCanvas application using the `pc.AppBase` constructor. It then sets the canvas to fill the window and automatically change resolution to be the same as the canvas size.\n\nThe code creates a material used on the geometry and an array of all render component primitive shape types. It then creates 40 entities with random shapes and positions, each with a render component that is set up to be lightmapped with baked direct lighting. It also creates a ground entity with a plane render component and adds a directional light and an omni light to the scene, both configured as baked lights.\n\nThe code sets the lightmap baking properties, including the lightmap mode, maximum resolution, and size multiplier. It then bakes the lightmaps using the `app.lightmapper.bake` method. Finally, it sets an update function on the app's update event to orbit the camera around the scene.\n\nThis code example can be used as a starting point for creating a PlayCanvas application with baked lighting. Developers can modify the code to add their own entities, lights, and materials, and adjust the lightmap baking properties to optimize performance. The `LightsBakedExample` class can be imported and used in other PlayCanvas projects to quickly set up a scene with baked lighting.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use the PlayCanvas engine to create a scene with baked lighting.\n\n2. What graphics device options are being set?\n- The graphics device options being set include the device type, glslang URL, and twgsl URL.\n\n3. What types of entities are being created in the scene?\n- The scene includes entities with render components of various primitive shapes, a ground plane, a directional light, an omni light, and a camera.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/lights-baked.md"}}],["68",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/lights.tsx)\n\nThe code is a part of the PlayCanvas engine project and is written in React. It is an example of how to use the PlayCanvas engine to create a 3D scene with different types of lights. The code creates a LightsExample class that has two methods: controls and example. The controls method returns a React component that contains UI controls for the lights in the scene. The example method creates a 3D scene with a statue, a ground plane, and three types of lights: omni, spot, and directional. The method also sets up the UI controls to manipulate the lights in the scene.\n\nThe example method first creates a graphics device using the PlayCanvas engine. It then loads the assets required for the scene, including the statue model, textures, and cubemaps. After loading the assets, it creates the entities for the statue, camera, ground plane, and lights. The method also sets up the lighting and the controls for the lights. Finally, it starts the update loop that rotates the lights.\n\nThe controls method returns a React component that contains UI controls for the lights in the scene. The controls are organized into three panels, one for each type of light. Each panel contains controls for enabling/disabling the light, adjusting its intensity, shadow intensity, and cookie intensity. The controls are implemented using the PlayCanvas engine's Observer class, which allows the UI controls to be linked to the light properties in the scene.\n\nOverall, the code demonstrates how to use the PlayCanvas engine to create a 3D scene with different types of lights and how to manipulate the lights using UI controls. The code can be used as a starting point for creating more complex 3D scenes with the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the LightsExample class?\n- The LightsExample class is an example of how to use lights in a PlayCanvas project. It includes controls for adjusting the properties of different types of lights and demonstrates how to create and manipulate entities with lights.\n\n2. What dependencies does this code have?\n- This code imports React and several modules from the PlayCanvas engine. It also imports components from the @playcanvas/pcui/react and @playcanvas/observer packages.\n\n3. What is the example() method responsible for?\n- The example() method sets up a PlayCanvas application with a canvas element and initializes various components and assets. It then creates entities with different types of lights and sets up event listeners to allow the user to toggle the lights on and off. Finally, it includes an update loop that rotates the lights and updates their properties based on user input.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/lights.md"}}],["69",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/lines.tsx)\n\nThe `LinesExample` class is a part of the PlayCanvas engine project and is responsible for rendering a 3D scene with lines and meshes. The purpose of this code is to demonstrate how to use the PlayCanvas engine to create a 3D scene with different types of lines and meshes. The code imports the necessary modules from the PlayCanvas engine and defines a class called `LinesExample`. \n\nThe `LinesExample` class has a static property called `CATEGORY` which is set to 'Graphics' and a static property called `NAME` which is set to 'Lines'. These properties are used to categorize and name the example in the PlayCanvas editor. The `WEBGPU_ENABLED` property is set to true, which indicates that the example can be run on WebGPU-enabled devices.\n\nThe `example` method of the `LinesExample` class takes two parameters: a canvas element and a device type. The method creates a graphics device using the `pc.createGraphicsDevice` method and initializes a new PlayCanvas application using the `pc.AppBase` constructor. The method then sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. \n\nThe method creates an entity with a camera component and a directional light. It also creates a circle of meshes and generates a grid of lines. The `groundElevation` function generates the elevation of a point with [x, y] coordinates, and the `groundColor` function generates a color for a 3D point by lerping between green and red color based on its y coordinate. \n\nThe `update` event of the PlayCanvas application is set to an anonymous function that generates the arrays of lines and colors for rendering. The method also moves the meshes equally spaced out around in the circle, rotates the meshes, and draws a single magenta line from one mesh to the next mesh. Finally, the method renders all gray lines.\n\nThis code can be used as a starting point for creating 3D scenes with lines and meshes in the PlayCanvas engine. Developers can modify the code to create their own 3D scenes with different types of lines and meshes.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use the PlayCanvas engine to generate a 3D scene with lines and meshes.\n\n2. What external dependencies does this code have?\n- This code imports the PlayCanvas engine from a relative path and uses two external libraries, glslang and twgsl, which are loaded from static URLs.\n\n3. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean that indicates whether the example is compatible with the WebGPU API, which is a new graphics API that is designed to replace WebGL.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/lines.md"}}],["70",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/material-anisotropic.tsx)\n\nThe `LightsExample` class is a part of the PlayCanvas engine project and is responsible for rendering a scene with a set of spheres and a directional light. The purpose of this code is to demonstrate the use of the PlayCanvas engine to create a 3D scene with lighting and materials.\n\nThe `example` method of the `LightsExample` class takes two parameters: a canvas element and a device type. It creates a graphics device using the `pc.createGraphicsDevice` method and initializes a new PlayCanvas application using the `pc.AppBase` constructor. It then sets the canvas to fill the window and automatically change resolution to be the same as the canvas size.\n\nThe `example` method also creates an asset list loader to load the required assets for the scene. These assets include a cubemap texture and a font. Once the assets are loaded, the method creates an entity with a camera component and an entity with a directional light component. It then creates a set of spheres with different materials and adds them to the scene.\n\nThe `createSphere` function creates a sphere entity with a `StandardMaterial` component. The material properties are set based on the position of the sphere in the grid. The `createText` function creates a text entity with a font asset and adds it to the scene.\n\nOverall, this code demonstrates the use of the PlayCanvas engine to create a 3D scene with lighting and materials. It shows how to create entities with different components and how to set their properties. This code can be used as a starting point for creating more complex 3D scenes using the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `LightsExample` class?\n- The `LightsExample` class is an example of how to use the PlayCanvas engine to create a scene with spheres and directional lighting.\n\n2. What assets are being loaded in this code and how are they being used?\n- The `helipad` texture and `font` asset are being loaded and used to create the environment map and text elements in the scene, respectively.\n\n3. What is the significance of `WEBGPU_ENABLED` being set to `true`?\n- `WEBGPU_ENABLED` being set to `true` indicates that the code is using the WebGPU API for rendering, which is a newer and more performant graphics API than WebGL.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/material-anisotropic.md"}}],["71",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/material-basic.tsx)\n\nThe `MaterialBasicExample` class is a code example that demonstrates how to use the PlayCanvas engine to create a basic material with alpha blending and alpha testing. The purpose of this code is to show how to create a simple scene with multiple boxes that have different alpha blending modes and alpha test values. The code also shows how to create a text element-based entity and how to rotate the boxes in the scene.\n\nThe `MaterialBasicExample` class imports the PlayCanvas engine and defines three static properties: `CATEGORY`, `NAME`, and `WEBGPU_ENABLED`. These properties are used to categorize and name the example and to indicate whether the example is compatible with the WebGPU API.\n\nThe `example` method of the `MaterialBasicExample` class takes two parameters: a canvas element and a device type string. The method creates two assets: a font asset and a texture asset. It then creates a graphics device using the `createGraphicsDevice` method of the PlayCanvas engine and initializes an app using the `AppBase` class. The app is set to fill the window and automatically change resolution to be the same as the canvas size.\n\nThe method then creates an entity with a camera component and adds it to the app's root entity. It also creates multiple boxes with different alpha blending modes and alpha test values and adds them to the app's root entity. Finally, it creates two text element-based entities and adds them to the app's root entity.\n\nThe `example` method sets an update function on the app's update event that rotates the boxes in the scene. The `MaterialBasicExample` class is exported as the default export of the module.\n\nExample usage:\n\n```javascript\nimport MaterialBasicExample from 'path/to/MaterialBasicExample.js';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2';\n\nconst example = new MaterialBasicExample();\nexample.example(canvas, deviceType);\n```\n\nThis code creates a new instance of the `MaterialBasicExample` class and calls its `example` method with a canvas element and a device type string. The example is then displayed on the canvas.\n## Questions: \n 1. What is the purpose of the `MaterialBasicExample` class?\n- The `MaterialBasicExample` class is an example of how to use basic materials in the PlayCanvas engine to create a scene with boxes that have different alpha blend modes and alpha test values.\n\n2. What assets are being loaded in the `example` method?\n- The `example` method loads two assets: a font and a texture.\n\n3. What is the purpose of the `createText` function?\n- The `createText` function creates a text element-based entity with a specified font, message, position, and rotation, and adds it to the scene. In this example, it is used to create two text elements that label the rows and columns of boxes.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/material-basic.md"}}],["72",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/material-clear-coat.tsx)\n\nThe code is an example of how to use the PlayCanvas engine to create a 3D scene with two spheres, each with a different material. The first sphere has a standard material, while the second sphere has a clear coat material. The example demonstrates how to create entities with different components, such as camera and light, and how to add them to the scene. \n\nThe code starts by importing the PlayCanvas engine and defining a class called MaterialClearCoatExample. The class has a static CATEGORY property that defines the category of the example, a static NAME property that defines the name of the example, and a static WEBGPU_ENABLED property that indicates whether the example is enabled for WebGPU. \n\nThe example method takes two parameters: a canvas element and a device type. The method creates a graphics device using the createGraphicsDevice method of the PlayCanvas engine. It then creates an instance of the AppBase class and initializes it with the graphics device and other options. The example loads several assets, including textures and a cubemap, and sets them as properties of the app.scene object. \n\nThe example then creates two entities: a camera entity and a light entity. The camera entity has a camera component, while the light entity has a light component. The example also defines a function called createSphere that creates a sphere entity with a render component and adds it to the scene. The function takes three parameters: the x, y, and z position of the sphere, and the material to use for the sphere. \n\nThe example creates two materials: a standard material and a clear coat material. Both materials have diffuse, metalness, gloss, and normal maps, as well as other properties such as diffuse color, metalness value, and bumpiness. The clear coat material also has clear coat and clear coat gloss properties. The example calls the createSphere function twice, once for each material, to create two spheres with different materials. \n\nFinally, the example sets up an update event listener that rotates the camera around the spheres. The example starts the app and loads the assets when the assetListLoader finishes loading. \n\nThis code can be used as a starting point for creating 3D scenes with the PlayCanvas engine. Developers can modify the code to add more entities, components, and materials to the scene, and to create more complex interactions between them. For example, developers can add physics components to the entities to create a physics simulation, or add scripting components to the entities to create custom behaviors. \n\nExample usage:\n\n```\nimport MaterialClearCoatExample from './MaterialClearCoatExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2';\n\nconst example = new MaterialClearCoatExample();\nexample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use the Material Clear Coat feature in the PlayCanvas engine to create a 3D scene with two spheres, each with a different material.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library using the wildcard syntax, as well as the HTMLCanvasElement interface from the global namespace. It also relies on several assets (textures) that are loaded asynchronously.\n\n3. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean flag that indicates whether the example is compatible with the WebGPU API. If set to `true`, the example will use WebGPU instead of WebGL to render the scene.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/material-clear-coat.md"}}],["73",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/material-physical.tsx)\n\nThe `MaterialPhysicalExample` class is a code example that demonstrates how to use the PlayCanvas engine to create a 3D scene with physically-based materials. The purpose of this code is to show how to create and manipulate entities, materials, and text elements in a PlayCanvas application.\n\nThe `example` method is the main entry point of the code example. It takes two parameters: a canvas element and a device type string. The canvas element is used to create a graphics device, which is then used to create a PlayCanvas application. The device type string specifies the type of graphics device to create, such as \"webgl2\" or \"webgpu\".\n\nThe `assets` object contains two assets: a texture asset for the environment map and a font asset for the text elements. These assets are loaded asynchronously using the `AssetListLoader` class, which is a utility class provided by the PlayCanvas engine.\n\nOnce the assets are loaded, the PlayCanvas application is initialized and the canvas is set to fill the window. The environment map is set to the loaded texture, and the tone mapping and skybox mip level are set to default values.\n\nThe code then creates a camera entity and adds it to the root of the scene. It also creates a grid of spheres with varying levels of glossiness and metalness, and adds them to the scene. The spheres are created using the `StandardMaterial` class, which is a physically-based material that simulates the behavior of real-world materials.\n\nFinally, the code creates two text elements using the font asset, and adds them to the scene. These text elements display the labels \"Glossiness\" and \"Metalness\" next to the grid of spheres.\n\nThe code also includes a mouse event listener that allows the user to rotate the skybox by dragging the mouse. This is done by updating the skybox rotation quaternion based on the mouse movement.\n\nOverall, this code example demonstrates how to create a simple 3D scene with physically-based materials and text elements using the PlayCanvas engine. It can be used as a starting point for more complex applications that require 3D graphics and interactivity.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a material physical implementation in the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the PlayCanvas engine from a relative path and uses HTMLCanvasElement and pc.GraphicsDevice.\n\n3. What does this code do?\n- This code creates a PlayCanvas app with a scene containing spheres with varying metalness and glossiness, and text labels. It also allows the user to rotate the skybox using mouse input.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/material-physical.md"}}],["74",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/material-translucent-specular.tsx)\n\nThe `MaterialTranslucentSpecularExample` class is an example of how to use the PlayCanvas engine to create a 3D scene with translucent and specular materials. The purpose of this code is to demonstrate how to create a scene with multiple entities, each with a different material, and how to add text elements to the scene. \n\nThe `example` method is the main function that creates the scene. It takes two parameters: a canvas element and a device type. The `assets` object contains two assets: a texture and a font. The `gfxOptions` object contains options for the graphics device, such as the device type and the URLs for the glslang and twgsl libraries. \n\nThe `pc.createGraphicsDevice` method creates a new graphics device and returns a promise that resolves with the device object. The `createOptions` object contains options for the PlayCanvas application, such as the graphics device and the component and resource systems to use. The `app` object is a new instance of the PlayCanvas application, initialized with the options from `createOptions`. \n\nThe `app.setCanvasFillMode` and `app.setCanvasResolution` methods set the canvas to fill the window and automatically change resolution to be the same as the canvas size. The `assetListLoader` object loads the assets in the `assets` object and adds them to the `app.assets` registry. \n\nThe `app.scene` object is the root of the scene graph. The `app.scene.toneMapping` property sets the tone mapping mode to ACES. The `app.scene.envAtlas` property sets the environment map to the `helipad` texture. The `app.scene.skyboxMip` and `app.scene.skyboxIntensity` properties set the skybox mip level and intensity. \n\nThe `createSphere` function creates a new entity with a sphere mesh and a `StandardMaterial` with diffuse, specular, metalness, gloss, blend type, opacity, and alpha write properties. The `createText` function creates a new entity with a text element and a font asset. \n\nThe `for` loops in the `example` method create multiple spheres and text elements and add them to the scene. The `camera` entity is created with a camera component and added to the root of the scene graph. The `light` entities are created with a light component and added to the root of the scene graph. \n\nThis code can be used as a starting point for creating a 3D scene with translucent and specular materials in the PlayCanvas engine. Developers can modify the code to add their own entities, materials, and text elements to the scene.\n## Questions: \n 1. What is the purpose of the `MaterialTranslucentSpecularExample` class?\n- The `MaterialTranslucentSpecularExample` class is an example of how to use translucent specular materials in the PlayCanvas engine.\n\n2. What assets are being loaded in the `example` method?\n- The `example` method is loading a cubemap texture and a font asset.\n\n3. What entities are being created in the `example` method?\n- The `example` method is creating entities with camera and light components, as well as spheres and text elements.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/material-translucent-specular.md"}}],["75",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/mesh-decals.tsx)\n\nThe MeshDecalsExample class is a part of the PlayCanvas engine project and is responsible for creating a demo of mesh decals. Mesh decals are textures that are applied to a mesh to create the illusion of additional detail. This class creates a scene with a ground plane, a bouncing ball, and a set of decals that are applied to the ground plane when the ball bounces. \n\nThe example function takes two parameters, a canvas element and a device type. It creates a graphics device using the createGraphicsDevice function and initializes an app using the AppBase class. It sets the canvas to fill the window and creates a plane primitive, a light, a camera, and a sphere primitive. It then creates a set of decals that are applied to the ground plane when the ball bounces. \n\nThe createDecal function generates a decal with a random size, rotation angle, and color. It creates a quad with four vertices and fills up information for all four vertices. The updateMesh function updates the required vertex streams. It updates the positions and colors when needed and updates the indices and uvs only one time, as they never change. \n\nThe class creates a mesh with a dynamic vertex buffer and a static index buffer. It creates a material with emissive texture only and sets the blend type to additive alpha blend. It creates a mesh instance and an entity with a render component to render the mesh instance. \n\nThe class sets an update function on the app's update event. It bounces the ball around in a circle with changing radius and creates a new decal at the next index when the ball crosses the ground plane. It fades out all vertex colors once a second and updates the mesh with the streams that were updated. \n\nOverall, this class demonstrates how to create a scene with mesh decals using the PlayCanvas engine. It shows how to create a mesh with a dynamic vertex buffer and a static index buffer, how to create a material with emissive texture only, and how to update the required vertex streams. It also shows how to set an update function on the app's update event to create a dynamic scene.\n## Questions: \n 1. What is the purpose of the MeshDecalsExample class?\n- The MeshDecalsExample class is an example of how to use mesh decals in the PlayCanvas engine.\n\n2. What assets are being loaded in this code and how are they being used?\n- The 'spark' texture asset is being loaded and used as the emissive map for the material of the mesh instance that displays the decals.\n\n3. What is the purpose of the updateMesh function and what parameters does it take?\n- The updateMesh function updates the vertex streams of the mesh with new positions and colors. It takes the mesh to update, a boolean for whether to update the positions, a boolean for whether to update the colors, and an optional boolean for whether to initialize all the streams (indices and uvs) or not.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/mesh-decals.md"}}],["76",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/mesh-deformation.tsx)\n\nThe `MeshDeformationExample` class is a part of the PlayCanvas engine project and is responsible for creating an example of mesh deformation. The purpose of this code is to demonstrate how to modify mesh positions on each frame to create a waving effect. \n\nThe `example` method takes two parameters: `canvas` and `deviceType`. The `canvas` parameter is an HTML canvas element that will be used to create a graphics device. The `deviceType` parameter is a string that specifies the type of device to create. \n\nThe `assets` object contains two assets: a 3D model of a statue and a cubemap texture of a helipad environment. These assets are loaded using the `pc.Asset` class. \n\nThe `gfxOptions` object contains options for creating a graphics device. It specifies the device type, as well as the URLs for the glslang and twgsl libraries. \n\nThe `pc.createGraphicsDevice` method is used to create a graphics device using the canvas and options specified. Once the device is created, an `AppOptions` object is created with the graphics device and component systems and resource handlers are added to it. \n\nAn instance of the `pc.AppBase` class is created with the canvas and `AppOptions` object. The canvas is set to fill the window and automatically change resolution to be the same as the canvas size. \n\nThe `assetListLoader` object is created with the assets and the app's assets. Once the assets are loaded, the app is started. \n\nThe `app.scene` object is used to set up the skydome and exposure. A camera entity is created and added to the app's root entity. A hierarchy of entities with render components is created to represent the statue model. The positions from all mesh instances are collected to work on. \n\nOn each frame, the camera is orbited around the statue model. The mesh positions are modified to create a waving effect. The `tempPositions` array is used to avoid per frame allocations. The `srcPositions` array is looped over, and the `tempPositions` array is filled up with a waved version of positions from the `srcPositions` array. The `.x` and `.z` components are modified based on a sine function, which uses the `.y` component. The new positions are set on the mesh, and the mesh is updated. \n\nIn summary, the `MeshDeformationExample` class is an example of how to modify mesh positions on each frame to create a waving effect. It demonstrates how to create a graphics device, load assets, create entities with render components, and modify mesh positions. This code can be used as a reference for creating similar effects in other PlayCanvas projects.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of mesh deformation using the PlayCanvas engine.\n\n2. What external resources does this code depend on?\n- This code depends on a 3D model file and a cubemap texture file, both of which are located in the '/static/assets' directory.\n\n3. What is the expected output of this code?\n- The expected output of this code is a 3D model of a statue with mesh deformation applied to it, which can be viewed and interacted with in a web browser.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/mesh-deformation.md"}}],["77",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/mesh-generation.tsx)\n\nThe `MeshGenerationExample` class is a code example that demonstrates how to generate a 3D grid plane mesh and animate it using lights. The purpose of this code is to show how to create a mesh with dynamic vertex buffer and static index buffer, and how to update the mesh's vertex and index buffers each frame to animate the mesh.\n\nThe `example` method takes an HTML canvas element and a device type as input parameters. It creates a graphics device using the `createGraphicsDevice` method, initializes a new PlayCanvas application, and sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. It then loads a texture asset and creates a list of component systems and resource handlers to be used by the application.\n\nThe method then creates four lights that will move in the scene and deform the mesh as well. It also creates a camera entity and positions it to look at the mesh. It generates positions and UV coordinates for vertices, stores them in Float32Arrays, and generates an array of indices to form a triangle list. It then creates a mesh with dynamic vertex buffer and static index buffer, and creates a material with a diffuse map and other properties.\n\nThe method sets an update function on the app's update event that moves the lights along circles and animates the mesh by updating its vertex buffer each frame. It evaluates the distance of each grid vertex to each light position, and increases the elevation of the vertex if the light is within range. It then stores the elevation in the .y element of the vertex position.\n\nThis code example can be used as a starting point for creating 3D grid plane meshes and animating them using lights. It demonstrates how to create a mesh with dynamic vertex buffer and static index buffer, and how to update the mesh's vertex and index buffers each frame to animate the mesh. It also shows how to create lights and a camera entity, and how to position them in the scene.\n## Questions: \n 1. What is the purpose of the MeshGenerationExample class?\n- The MeshGenerationExample class is an example of how to generate a mesh in PlayCanvas engine and animate it using lights.\n\n2. What graphics device options are being set in this code?\n- The graphics device options being set in this code include the device type and the URLs for glslang and twgsl.\n\n3. What is the role of the updateMesh function?\n- The updateMesh function updates the positions and normals of the mesh each frame, and updates the UVs and indices only once as they do not change each frame. It also lets the mesh update the vertex and index buffer as needed.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/mesh-generation.md"}}],["78",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/mesh-morph-many.tsx)\n\nThe `MeshMorphManyExample` class is a code example that demonstrates how to use the PlayCanvas engine to create a mesh with multiple morph targets. The purpose of this code is to show how to create a mesh with a base shape and then modify it using multiple morph targets. The resulting mesh can be used to create complex animations and deformations.\n\nThe `example` method is the main entry point of the code. It takes two parameters: a canvas element and a device type. The canvas element is used to render the scene, and the device type specifies the type of graphics device to use (e.g., WebGL or WebGPU).\n\nThe code first creates an asset object that contains a texture used for the environment map. It then creates a graphics device using the `createGraphicsDevice` method and initializes the PlayCanvas application using the `AppBase` class. The canvas is set to fill the window, and the resolution is set to be the same as the canvas size.\n\nThe code then creates an entity with a directional light component and an entity with a camera component. The camera is positioned to look at the origin of the scene. \n\nThe `createMorphTarget` function is a helper function that creates a morph target from the original positions, normals, and indices of the base mesh and a plane normal. The function modifies the vertices of the mesh to create a new shape. The `createMorphTarget` function is called multiple times to create a set of morph targets that are used to modify the base mesh.\n\nThe `createCylinder` method is used to create a base mesh in the shape of a cylinder. The `getPosition`, `getNormals`, and `getIndices` methods are used to obtain the vertex and index data of the base mesh. The `createMorphTarget` function is then called multiple times to create a set of morph targets that are used to modify the base mesh.\n\nThe resulting mesh is then rendered using a material and added to the scene. The `on(\"update\")` method is used to modify the weights of the morph targets over time, resulting in a complex animation. The `texturePositions` and `textureNormals` properties of the `morphInstance` object are used to display debug information about the morph targets.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create complex animations and deformations using morph targets. It shows how to create a base mesh and then modify it using multiple morph targets to create a wide range of shapes and animations.\n## Questions: \n 1. What is the purpose of the `MeshMorphManyExample` class?\n- The `MeshMorphManyExample` class is an example of how to use the PlayCanvas engine to create a mesh with multiple morph targets that can be animated.\n\n2. What assets are being loaded in this code and how are they used?\n- The `helipad` texture asset is being loaded and used as the environment atlas for the scene's skybox.\n\n3. What is the purpose of the `createMorphTarget` function?\n- The `createMorphTarget` function creates a morph target from a base mesh by modifying the positions and normals of its vertices based on a specified plane normal and offset.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/mesh-morph-many.md"}}],["79",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/mesh-morph.tsx)\n\nThe `MeshMorphExample` class is a code example that demonstrates how to use the PlayCanvas engine to create and manipulate 3D meshes with morph targets. The purpose of this code is to show how to create a mesh with morph targets, which can be used to animate the mesh by blending between different shapes. \n\nThe `example` method is the main entry point of the code. It takes an HTML canvas element and a device type as input, and initializes the PlayCanvas engine with the specified graphics device. It then creates a scene with a directional light, a camera, and three morph instances. Each morph instance is a mesh with three morph targets, which are created by expanding a part of a sphere along three planes specified by their normal vectors. The morph targets are then blended together by modifying their weights along a sine curve with different frequencies. \n\nThe `createMorphTarget` function is a helper function that takes the original positions, normals, and indices of a mesh, and creates a morph target by modifying the positions and normals based on a specified plane normal. The function first calculates the shortest distance from each vertex to the plane, and then modifies the distance to a displacement amount using a smoothstep function. The function then generates new positions by extruding each vertex along the normal by the displacement amount. The function also generates new normals based on the modified positions and indices, and calculates the delta positions and normals between the base position/normal and the modified position/normal. Finally, the function creates a morph target object with the delta positions and normals. \n\nThe `createMorphInstance` function is another helper function that creates a mesh instance with a morph instance. The function first creates a base mesh, which is a sphere with a high amount of vertices and triangles. The function then obtains the base mesh vertex and index data, and builds three morph targets by calling the `createMorphTarget` function with different plane normals. The function then creates a morph object with the three morph targets, and sets the morph object to the mesh's `morph` property. The function then creates a mesh instance with a standard material, and adds a morph instance to the mesh instance. The function also creates an entity with a render component that uses the mesh instance, and adds the entity to the scene. Finally, the function returns the morph instance. \n\nThe `shortestDistance` function is another helper function that calculates the shortest distance from a point to a plane defined by its normal vector. The function takes the point coordinates and the plane normal as input, and returns the shortest distance. \n\nThe `MeshMorphExample` class also defines some static properties, such as `CATEGORY`, `NAME`, and `WEBGPU_ENABLED`, which are used to categorize and name the example, and to indicate whether it is compatible with the WebGPU API. \n\nOverall, this code demonstrates how to use the PlayCanvas engine to create and manipulate 3D meshes with morph targets, which can be used to create complex animations and visual effects.\n## Questions: \n 1. What is the purpose of the MeshMorphExample class?\n- The MeshMorphExample class is an example of how to use morph targets to create a mesh that can be animated in real-time.\n\n2. What graphics device options are being set in this code?\n- The graphics device options being set in this code include the device type, the URL for the glslang and twgsl libraries, and the canvas element to render to.\n\n3. What is the purpose of the createMorphInstance function?\n- The createMorphInstance function creates a mesh instance with a morph target that can be animated by modifying the weights of the morph targets.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/mesh-morph.md"}}],["80",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/model-asset.tsx)\n\nThe `ModelAssetExample` class is a code example that demonstrates how to load and render a 3D model asset in the PlayCanvas engine. The purpose of this code is to provide developers with a working example of how to use the PlayCanvas engine to create 3D graphics applications.\n\nThe `example` method is the main entry point of the code. It takes two parameters: a `canvas` element and a `deviceType` string. The `canvas` element is used to create a graphics device, which is responsible for rendering the 3D graphics. The `deviceType` string specifies the type of graphics device to create, such as \"webgl2\" or \"webgpu\".\n\nThe `assets` object is a dictionary that contains a single model asset called \"statue\". The `pc.Asset` class is used to create the asset, which is loaded from a GLB file located at `/static/assets/models/statue.glb`. The `gfxOptions` object is used to configure the graphics device. It specifies the device type to use, as well as the URLs for the GLSLang and TWGSL libraries.\n\nThe `pc.createGraphicsDevice` function is used to create a graphics device from the `canvas` element and `gfxOptions` object. Once the device is created, a `pc.AppBase` instance is created with the device as a parameter. The `AppBase` class is the main class of the PlayCanvas engine, and is responsible for managing the application lifecycle, as well as the scene graph and rendering.\n\nThe `createOptions` object is used to configure the `AppBase` instance. It specifies the component systems and resource handlers to use. In this case, the `ModelComponentSystem`, `CameraComponentSystem`, and `LightComponentSystem` are used for rendering, and the `TextureHandler` and `ContainerHandler` are used for loading assets.\n\nThe `app.setCanvasFillMode` and `app.setCanvasResolution` methods are used to configure the canvas to fill the window and automatically change resolution to be the same as the canvas size.\n\nThe `assetListLoader` object is used to load the model asset. Once the asset is loaded, the `app.start` method is called to start the application. The `app.scene.ambientLight` property is set to a gray color to provide some ambient lighting.\n\nThe `assets.statue.resource.instantiateModelEntity` method is used to create an entity with the loaded model asset. The `castShadows` option is set to `true` to enable shadow casting. The entity is added to the root of the scene graph using the `app.root.addChild` method.\n\nA clone of the entity is created using the `entity.clone` method. The clone is scaled down and positioned using the `setLocalScale` and `setLocalPosition` methods, respectively. A camera entity and a light entity are also created and added to the scene graph.\n\nFinally, the `app.on(\"update\")` method is used to rotate the model entity around the y-axis. This method is called every frame, and the `dt` parameter is the time elapsed since the last frame.\n\nOverall, this code demonstrates how to load and render a 3D model asset in the PlayCanvas engine, and how to create entities with various components such as cameras and lights. Developers can use this code as a starting point for creating their own 3D graphics applications using the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `ModelAssetExample` class?\n- The `ModelAssetExample` class is an example class that demonstrates how to use the PlayCanvas engine to load and display a 3D model asset.\n\n2. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property indicates whether the example code is compatible with the WebGPU API, which is a new graphics API for the web that provides lower-level access to the GPU.\n\n3. What is the purpose of the `gfxOptions` object?\n- The `gfxOptions` object is used to specify options for creating a graphics device, including the type of device to create and the URLs of the glslang and twgsl libraries to use.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/model-asset.md"}}],["81",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/model-outline.tsx)\n\nThe `ModelOutlineExample` class is an example of how to use the PlayCanvas engine to create a model outline effect. The purpose of this code is to demonstrate how to create a post-processing effect that outlines 3D models in a scene. The example creates a scene with a ground plane and three primitives (a sphere, a box, and a cone) that are visible in both the world layer and the outline layer. The example also creates two cameras, one for rendering the world layer and one for rendering the outline layer. The outline camera renders the outline layer into a render target, which is then used as a texture for the outline effect.\n\nThe `example` method takes two arguments, a canvas element and a device type. It creates a graphics device using the `createGraphicsDevice` method and initializes a new PlayCanvas application using the `AppBase` class. It then sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. The example loads an asset for the outline effect and creates a list of component systems and resource handlers to be used by the application. It then creates a list of assets to be loaded and loads them using an `AssetListLoader`. Once the assets are loaded, the example starts the application and sets the ambient light of the scene.\n\nThe example defines a helper function `createPrimitive` that creates a primitive with a specified shape type, position, scale, color, and layer. It also defines a function `createRenderTarget` that creates a texture and render target for rendering into, including a depth buffer. The example creates a layer for rendering to texture and adds it to the beginning of layers to render into it first. It then creates the ground plane and three primitives, visible in both layers.\n\nThe example creates two cameras, one for rendering entities in the world layer and one for rendering entities in the outline layer into the render target. It creates an `OutlineEffect` object and adds it to the post-effects of the main camera. The example also creates an entity with an omni light component and adds it to both layers.\n\nThe example handles canvas resize and updates things each frame. It rotates the camera around the objects and updates the position and rotation of the outline camera to match the main camera.\n\nOverall, this code demonstrates how to create a model outline effect using the PlayCanvas engine. It shows how to create a scene with multiple layers, cameras, and post-processing effects. This example can be used as a starting point for creating more complex scenes with custom post-processing effects.\n## Questions: \n 1. What does this code do?\n- This code is an example of how to create a model outline effect using the PlayCanvas engine. It creates a scene with several primitives and two cameras, one for the main view and one for the outline effect, and applies a post-processing effect to the main camera to create the outline effect.\n\n2. What dependencies does this code have?\n- This code depends on the PlayCanvas engine, which is imported at the beginning of the file using the wildcard import syntax. It also depends on several external scripts and libraries, including a glslang compiler and a twgsl library, which are specified in the gfxOptions object.\n\n3. What is the purpose of the createPrimitive and createRenderTarget functions?\n- The createPrimitive function is a helper function that creates a primitive shape (such as a sphere or box) with a specified position, scale, color, and layer, and adds it to the scene. The createRenderTarget function creates a texture and render target for rendering into, including a depth buffer, which is used by the outline camera to render the outline effect.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/model-outline.md"}}],["82",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/model-textured-box.tsx)\n\nThe `ModelTexturedBoxExample` class is a code example that demonstrates how to create a 3D scene with a textured box, a light source, and a camera using the PlayCanvas engine. The purpose of this code is to show how to use the PlayCanvas engine to create a simple 3D scene with basic components.\n\nThe `example` method is the main entry point of the code example. It takes two parameters: a canvas element and a device type. The canvas element is used to render the 3D scene, while the device type specifies the type of graphics device to use (e.g., WebGL, WebGPU).\n\nThe code first creates an asset object that contains a texture asset for the clouds. It then creates a graphics device using the `pc.createGraphicsDevice` method, passing in the canvas element and graphics options. The graphics options specify the device type and the URLs for the glslang and twgsl libraries.\n\nNext, the code creates an `AppOptions` object that specifies the graphics device and the component systems and resource handlers to use. It then creates a new `pc.AppBase` object and initializes it with the `AppOptions`.\n\nThe code sets the canvas fill mode to `pc.FILLMODE_FILL_WINDOW` and the canvas resolution to `pc.RESOLUTION_AUTO`. It then loads the assets using an `AssetListLoader` object and starts the app.\n\nThe code sets the ambient light of the scene to a dark gray color. It then creates a `StandardMaterial` object with the clouds texture and creates a box entity with a render component that uses the material. It also creates a light entity with an omni light component and a sphere model component. The light entity is scaled down to 0.1m. Finally, the code creates a camera entity with a camera component.\n\nThe code adds the box, light, and camera entities to the app's root entity. It then sets an update function on the app's update event that rotates the box and moves the light in a circle.\n\nThis code example demonstrates how to create a simple 3D scene with a textured box, a light source, and a camera using the PlayCanvas engine. It can be used as a starting point for more complex 3D scenes that require additional components and functionality.\n## Questions: \n 1. What is the purpose of the `ModelTexturedBoxExample` class?\n- The `ModelTexturedBoxExample` class is an example of how to create a 3D scene with a textured box, an omni light, and a camera using the PlayCanvas engine.\n\n2. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property indicates whether the example is compatible with the WebGPU API, which is a new graphics API for the web that provides lower-level access to the GPU.\n\n3. What is the purpose of the `gfxOptions` object?\n- The `gfxOptions` object is used to specify options for creating a graphics device, including the type of device to create and the URLs for the glslang and twgsl libraries.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/model-textured-box.md"}}],["83",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/multi-view.tsx)\n\nThe `MultiViewExample` class is a code example that demonstrates how to create a PlayCanvas application with multiple cameras and views. The purpose of this code is to show how to create a scene with multiple cameras that can be used to render different parts of the scene. This is useful for creating split-screen games or for displaying different views of a 3D model.\n\nThe `example` method is the main entry point for the code example. It takes two parameters: a canvas element and a device type. The canvas element is used to create a graphics device, which is used to render the scene. The device type is used to specify which type of graphics device to create (e.g. WebGL or WebGPU).\n\nThe code first sets up and loads the Draco module, which is used to decode compressed GLB files. It then creates a list of assets that are used in the scene, including a chess board model, a cubemap texture, and a camera script. It also creates a list of graphics options that are used to create the graphics device.\n\nThe code then creates a new PlayCanvas application and sets the canvas to fill the window. It loads the assets and starts the application. It then creates three cameras: a left camera, a right orthographic camera, and a top camera. The left camera is a perspective camera that is positioned to the left of the scene. The right camera is an orthographic camera that is positioned above the scene. The top camera is a perspective camera that is positioned above and to the left of the scene. The code also adds an orbit camera script to the top camera, which allows the user to orbit around the scene using the mouse or touch input.\n\nThe code then creates a directional light that casts shadows and sets the skybox to a cubemap texture. Finally, it sets up an update function that is called once per frame. This function updates the position of the left camera and the orthographic height of the right camera.\n\nOverall, this code example demonstrates how to create a PlayCanvas application with multiple cameras and views. It shows how to load assets, create cameras, add scripts, and update the scene. This code can be used as a starting point for creating split-screen games or for displaying different views of a 3D model.\n## Questions: \n 1. What is the purpose of the `MultiViewExample` class?\n- The `MultiViewExample` class is an example of how to use the PlayCanvas engine to create a scene with multiple cameras and a chess board model.\n\n2. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property indicates whether the example is compatible with the WebGPU API, which is a new graphics API for the web that provides lower-level access to the GPU.\n\n3. What is the purpose of the `demo` function?\n- The `demo` function is a callback function that is called after the Draco decoder module is loaded. It initializes the PlayCanvas app and sets up the scene with cameras, lighting, and models.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/multi-view.md"}}],["84",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/paint-mesh.tsx)\n\nThe `PaintMeshExample` class is an example of how to use the PlayCanvas engine to create a custom shader that can be used to paint a texture onto a 3D mesh. The example demonstrates how to create a high-quality sphere mesh, apply a texture to it, and then use a custom shader to paint a decal texture onto the surface of the sphere.\n\nThe `example` method of the `PaintMeshExample` class takes three arguments: a canvas element, a device type, and an object containing two shader files. The method creates a new PlayCanvas application, loads several assets (including the sphere texture, the decal texture, and a cubemap), and sets up two cameras: one for rendering the sphere with the decal texture, and one for rendering the scene with the sphere and the decal camera. \n\nThe method then creates a high-quality sphere mesh using the `createHighQualitySphere` function, which takes a material and a layer as arguments. The function creates a new entity, adds a render component to it with the sphere mesh and the provided material, and returns the entity. The method also creates a new layer for rendering decals, a new camera for rendering the decal texture, and a new directional light entity.\n\nThe custom shader used to paint the decal texture onto the sphere is defined in the `FILES` object, which contains two shader files: `shader.vert` and `shader.frag`. The vertex shader transforms the vertex position to world space and then to decal space, and passes it to the fragment shader to sample the decal texture. The fragment shader calculates the decal space position of each pixel, checks if it is within the bounds of the projection box, and then samples the decal texture at that position.\n\nThe method creates a new `pc.Shader` object from the vertex and fragment shaders, and then creates a new `pc.Material` object with the shader. The method then creates a new entity that is a child of the sphere entity, and adds a render component to it with the sphere mesh and the decal material. The method also updates the decal projection position and direction each frame, and renders the decal texture to a texture every half second.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a custom shader that can be used to paint a decal texture onto a 3D mesh. The example shows how to create a high-quality sphere mesh, apply a texture to it, and then use a custom shader to paint a decal texture onto the surface of the sphere. This technique can be used to add detailed textures and decals to 3D models in PlayCanvas projects.\n## Questions: \n 1. What does this code do?\n- This code is an example of how to use a custom shader to render decals onto a mesh in the PlayCanvas engine. It creates a high-resolution sphere with a texture, and then renders decals onto it using a custom shader.\n\n2. What are the inputs and outputs of the `example` function?\n- The `example` function takes in an HTML canvas element, a device type string, and an object containing two shader files as strings. It does not have any outputs.\n\n3. What is the purpose of the `decalLayer` and `decalCamera` entities?\n- The `decalLayer` entity is a layer used for rendering decals onto the mesh. The `decalCamera` entity is a camera that renders the decals using the `decalLayer`, and renders before the main camera.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/paint-mesh.md"}}],["85",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/painter.tsx)\n\nThe `PainterExample` class is a demonstration of how to use the PlayCanvas engine to create a simple painting application. The `example` method is the main entry point for the demonstration, and it takes two parameters: a canvas element and a device type. The method creates a graphics device using the `pc.createGraphicsDevice` method, which takes the canvas element and a set of graphics options as parameters. The graphics options specify the device type, as well as the URLs for the glslang and twgsl libraries.\n\nOnce the graphics device is created, the method creates an instance of the `pc.AppBase` class, which is the main class for the PlayCanvas engine. The `createOptions` object is used to specify various options for the application, such as the graphics device, mouse, touch, and keyboard input devices, as well as the component systems and resource handlers that the application will use.\n\nThe method then creates a texture and a render target for rendering into, and adds a layer to the scene for rendering to the texture. It also creates a material for the paint brush, which uses emissive color to control its color. The method then creates a pool of brushes, which are reused each frame to render multiple brush imprints to make smooth lines.\n\nThe method creates two cameras: an orthographic camera for rendering brushes in the paint layer, and a main camera for rendering entities in the world layer. It also creates a box entity, which is used to display the rendered texture in the world layer.\n\nThe method then sets up an update loop, which generates a new random brush stroke each frame, and renders it using the pool of brushes. The brush stroke is made up of multiple brush imprints, which are rendered along a line between two random positions. The brush color and width are also randomized for each stroke.\n\nOverall, the `PainterExample` class demonstrates how to use the PlayCanvas engine to create a simple painting application, and how to use various features of the engine, such as graphics devices, cameras, layers, materials, and entities. The code can be used as a starting point for creating more complex applications that use the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `gfxOptions` object and what are its properties used for?\n   \n   The `gfxOptions` object is used to specify options for creating a graphics device. Its properties are used to specify the device types to create, as well as the URLs for the glslang and twgsl libraries.\n\n2. What is the purpose of the `createPrimitive` function and what does it return?\n   \n   The `createPrimitive` function is a helper function used to create a primitive with a specified shape type, position, scale, color, and layer. It returns the created primitive as an `pc.Entity` object.\n\n3. What is the purpose of the `getBrush` function and how is it used?\n   \n   The `getBrush` function is used to get a brush entity for rendering brush imprints. It either creates a new brush entity or reuses an already allocated one from a pool of brushes. The returned brush entity is then used to render brush imprints for a brush stroke.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/painter.md"}}],["86",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/particles-anim-index.tsx)\n\nThe `ParticlesAnimIndexExample` class is an example of how to use the PlayCanvas engine to create a particle system with animated textures. The purpose of this code is to demonstrate how to create a particle system with multiple entities, each with a different animation index, and display the full particle texture on a panel.\n\nThe `example` method takes in a canvas element and a device type as parameters. It creates a new PlayCanvas application and sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. It then creates an entity with a camera component, a directional light entity, a screen entity, and a panel entity. The camera entity is positioned at (0, 0, 20) and the light entity is rotated to (45, 0, 0). The screen entity is set to have a resolution of (640, 480) and a reference resolution of (1280, 720). The panel entity is added as a child of the screen entity.\n\nFour particle entities are created and added as children of the root entity. Each particle entity is positioned at a different location. The `particleSystemConfiguration` object is created with properties for the particle system, such as the number of particles, lifetime, rate, color map, initial velocity, emitter shape, emitter radius, animation loop, animation tiles, animation speed, and scale graph. A `scaleCurve` object is created with a curve that gradually makes sparks bigger.\n\nFor each particle entity, an `options` object is created by merging the `particleSystemConfiguration` object with an object that specifies the animation index and number of frames for that entity. The `options` object is then passed to the `addComponent` method of the particle entity with the `particlesystem` component type.\n\nFinally, the full particle texture is added to the panel entity as an image element. The application is started and the particle system is displayed on the canvas.\n\nThis code can be used as a starting point for creating more complex particle systems with animated textures in the PlayCanvas engine. Developers can modify the properties of the `particleSystemConfiguration` object to create different effects, and add more particle entities with different animation indices to create more complex animations. The `scaleCurve` object can also be modified to create different scaling effects for the particles.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to create and display particle systems using the PlayCanvas engine.\n\n2. What assets are being used in this code?\n- This code is using a texture asset called 'particlesNumbers' located at '/static/assets/textures/particles-numbers.png'.\n\n3. What is the significance of the 'animIndex' property in the particle system configuration?\n- The 'animIndex' property sets the animation index of the particle system, which determines which animation in the sprite sheet to use. In this code, each of the four particle systems has a different 'animIndex' value to display a different animation.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/particles-anim-index.md"}}],["87",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/particles-random-sprites.tsx)\n\nThe `ParticlesRandomSpritesExample` class is a code example that demonstrates how to use the PlayCanvas engine to create particle systems with randomly selected sprites. The purpose of this code is to show how to create a particle system that uses different sprites for each particle, and how to display the full sprite sheet for each particle system.\n\nThe `example` method is the main entry point of the code example. It takes two arguments: a canvas element and a device type. It creates a new PlayCanvas application with a camera, a directional light, and a screen entity. It also creates two particle entities, each with a particle system component that uses a different sprite sheet. The particle system configuration is defined by the `particleSystemConfiguration` function, which takes an asset, the number of animation tiles in the x and y directions, and returns an object with various particle system properties.\n\nThe `scaleCurve` and `alphaCurve` variables define curves that control the size and opacity of the particles over their lifetime. The `panel` and `panel2` entities are used to display the full sprite sheet for each particle system. The `addComponent` method is used to add a particle system component to each particle entity, with the configuration defined by the `particleSystemConfiguration` function. The `randomizeAnimIndex` property is set to true to randomly select a different animation for each particle.\n\nThe `addComponent` method is also used to add an element component to each panel entity, with the texture asset set to the corresponding sprite sheet. The `anchor` and `pivot` properties are used to position the sprite sheet within the panel entity.\n\nFinally, the `start` method is called on the PlayCanvas application to start the rendering loop.\n\nThis code example can be used as a starting point for creating more complex particle systems with different sprite sheets and animations. It demonstrates how to use the PlayCanvas engine to create particle systems that are visually appealing and interactive.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to create and display particle systems with random sprites using the PlayCanvas engine.\n\n2. What assets are being used in this code?\n- Two texture assets are being used: 'particlesCoinsTexture' and 'particlesBonusTexture'.\n\n3. What is the configuration for the particle systems being created?\n- The particle systems have 32 particles, a lifetime of 2 seconds, a rate of 0.2 particles per second, and use a sphere emitter with a radius of 2.0. They also have animations with a specified number of tiles and frames, and use curves to control particle size and alpha values.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/particles-random-sprites.md"}}],["88",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/particles-snow.tsx)\n\nThe code is an example of how to create a snow particle system using the PlayCanvas engine. The purpose of the code is to demonstrate how to create a particle system that simulates snowfall. The code creates an application, sets up a camera and a directional light, and then creates a particle system entity that emits snowflakes. \n\nThe example function takes two parameters, a canvas element and a device type. The canvas element is used to create the application, and the device type is not used in this code. \n\nThe code starts by creating an application object and an assets object. The assets object contains a single texture asset, which is a snowflake image. The assetListLoader is used to load the assets, and when the assets are loaded, the application is started. \n\nThe canvas is set to fill the window, and the resolution is set to be the same as the canvas size. A camera entity and a directional light entity are created and added to the scene hierarchy. \n\nThe particle system is created by creating a new entity and adding a particle system component to it. The particle system component is configured to emit 100 particles, each with a lifetime of 10 seconds. The particles are emitted at a rate of 0.1 particles per second, and they are emitted in all directions. The velocity of the particles is set using two curve sets, which define the velocity in the x, y, and z directions. The rotation speed of the particles is set using two curves, which define the rotation speed in degrees per second. The scale of the particles is set using a single curve, which defines the scale of the particles over their lifetime. Finally, the color of the particles is set to the snowflake texture asset. \n\nOverall, this code demonstrates how to create a snow particle system using the PlayCanvas engine. The code can be used as a starting point for creating more complex particle systems, and it can be customized to create different types of particle effects.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a particle system that generates snowflakes using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library using the wildcard symbol `*`, and it also imports the `HTMLCanvasElement` type from the global namespace.\n\n3. What is the structure of the particle system being created?\n- The particle system has 100 particles, each with a lifetime of 10 seconds and a rate of 0.1 particles per second. The particles are emitted from a point with a circular spread of 5 units in the x-axis, and they have a downward velocity that varies randomly between -0.4 and -0.7 units per second. The particles also have a random rotation speed between -100 and 100 degrees per second, and a constant scale of 0.1. Finally, the particles are textured with a snowflake image.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/particles-snow.md"}}],["89",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/particles-spark.tsx)\n\nThe code defines a class called `ParticlesSparkExample` which is used to create a particle system example in the PlayCanvas engine. The `example` method of this class takes two arguments: a canvas element and a device type. \n\nThe method creates a new PlayCanvas application and starts the update loop. It then loads an image asset called `spark` which is used as a texture for the particle system. Once the asset is loaded, the method sets the canvas to fill the window and creates a camera entity and a directional light entity. These entities are added to the scene hierarchy of the application.\n\nThe method then defines several curves that are used to control the behavior of the particle system. These curves control the position, velocity, gravity, scale, rotation, and color of the particles. The curves are created using the `pc.Curve` and `pc.CurveSet` classes provided by the PlayCanvas engine.\n\nFinally, the method creates a new entity for the particle system and adds a `particlesystem` component to it. The `particlesystem` component is configured with various properties such as the number of particles, lifetime, rate, scale graph, rotation speed graph, color graph, color map, velocity graph, and local velocity graphs. These properties are set using the curves defined earlier.\n\nOverall, this code demonstrates how to create a particle system in the PlayCanvas engine using curves to control the behavior of the particles. This particle system can be used to add visual effects to games or other interactive applications built with the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a particle system using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library and uses the HTMLCanvasElement interface.\n\n3. What does the example() function do?\n- The example() function creates a PlayCanvas application, loads a texture asset, creates entities for a camera and a directional light, and creates an entity for a particle system with various curves and graphs to control particle behavior.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/particles-spark.md"}}],["90",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/point-cloud-simulation.tsx)\n\nThe `PointCloudSimulationExample` class is a demonstration of how to create a point cloud simulation using the PlayCanvas engine. The purpose of this code is to show how to create a particle system that simulates the movement of particles in 3D space. The simulation is created using a mesh with dynamic vertex buffer and a shader that calculates the position and color of each particle.\n\nThe `example` method of the `PointCloudSimulationExample` class takes three parameters: a canvas element, a device type, and an object containing two shader files. It creates a new PlayCanvas application, sets up a camera entity, and creates a mesh with a dynamic vertex buffer. The mesh is then rendered using a shader that calculates the position and color of each particle based on its distance from the camera.\n\nThe `update` function of the PlayCanvas application is used to update the position of each particle using simple Verlet integration. The position of each particle is stored in two buffers, `positions` and `oldPositions`, and is updated each frame based on its velocity. The number of visible particles is also updated each second to create the illusion of particles appearing and disappearing.\n\nOverall, this code demonstrates how to create a point cloud simulation using the PlayCanvas engine. It can be used as a starting point for creating more complex particle systems, such as smoke, fire, or water. The dynamic vertex buffer and shader used in this example can be modified to create different effects, and the Verlet integration algorithm can be replaced with other physics engines to create more realistic particle movement.\n## Questions: \n 1. What does this code do?\n- This code is an example of a point cloud simulation using the PlayCanvas engine. It generates random positions and old positions within a small cube and updates particle positions using simple Verlet integration, and keeps them inside a sphere boundary.\n\n2. What is the purpose of the `shader.vert` and `shader.frag` files?\n- The `shader.vert` and `shader.frag` files contain the vertex and fragment shaders, respectively, used to create the shader for the point cloud simulation.\n\n3. What is the significance of the `visiblePoints` variable?\n- The `visiblePoints` variable determines how many points are visible in the point cloud simulation and is updated once a second to be a random number between 50000 and the maximum number of points.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/point-cloud-simulation.md"}}],["91",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/point-cloud.tsx)\n\nThe `PointCloudExample` class is a part of the PlayCanvas engine project and is responsible for rendering a point cloud model on a canvas element. The `example` method of this class takes three arguments: a canvas element, a device type, and an object containing two shader files. \n\nThe method creates a new PlayCanvas application and loads a 3D model of a statue. It then creates a camera entity and adds it to the scene hierarchy. The method also creates a new entity from the loaded 3D model and adds it to the scene hierarchy. \n\nThe method then creates a new shader definition using the vertex and fragment shader files passed as arguments. It creates a new material using this shader and assigns it to all mesh instances of the 3D model entity. The material is set to render as points, and the point size and color are determined by the vertex shader. \n\nThe method also sets up an update loop that rotates the 3D model entity and updates the time parameter of the vertex shader. \n\nOverall, this code demonstrates how to load a 3D model, apply a custom shader to it, and render it as a point cloud. It can be used as a starting point for creating more complex 3D visualizations and simulations using the PlayCanvas engine. \n\nExample usage:\n\n```\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'web';\nconst shaderFiles = {\n  'shader.vert': /* glsl */`\n    // vertex shader code here\n  `,\n  'shader.frag': /* glsl */`\n    // fragment shader code here\n  `\n};\n\nconst pointCloudExample = new PointCloudExample();\npointCloudExample.example(canvas, deviceType, shaderFiles);\n```\n## Questions: \n 1. What is the purpose of the `PointCloudExample` class?\n- The `PointCloudExample` class is an example of how to create a point cloud using the PlayCanvas engine.\n\n2. What are the input parameters of the `example` method?\n- The `example` method takes in an HTML canvas element, a string representing the device type, and an object containing two strings representing the vertex and fragment shaders.\n\n3. What does the `example` method do?\n- The `example` method creates an application using the PlayCanvas engine, loads a 3D model, creates a camera entity, creates a new entity with a point cloud material, and updates the point cloud material based on time.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/point-cloud.md"}}],["92",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/portal.tsx)\n\nThe `PortalExample` class is an example implementation of a portal in the PlayCanvas engine. The `example` method is the main entry point of the example, which takes an HTML canvas element and a device type as input parameters. The method creates a new PlayCanvas application, loads the required assets, sets up the scene, and creates entities for the portal, the portal visual geometry, and two other entities that are visible inside and outside the portal.\n\nThe example starts by creating a new PlayCanvas application and loading the required assets. The assets include a cubemap texture, three container assets for the portal, statue, and bitmoji models, respectively. The `AssetListLoader` is used to load all the assets, and the `load` method is called with a callback function that initializes the scene and creates the entities.\n\nThe scene is initialized by setting the canvas to fill the window and automatically change resolution to be the same as the canvas size. The skybox is set using the cubemap texture, and the tone mapping, skybox mip, and skybox intensity are set to specific values. A script called `Rotator` is created to rotate the scene, and another script called `Portal` is created to set up rendering the portal itself.\n\nThe `Portal` script initializes the stencil parameters for all materials of the portal geometry. The stencil parameters are used to increment the value in the stencil buffer for stencil geometry. The script sets the stencil and other parameters on all materials, and only writes to the stencil buffer.\n\nThe `PortalGeometry` script sets stencil options for entities inside or outside the portal. The script initializes the stencil parameters based on the value in the stencil buffer, either rendering the geometry when the value is equal or not equal to zero.\n\nThe example creates a new camera entity with a camera component that renders both world and portal layers. A directional light entity is also created, and a root entity for the graphical scene is created. The portal entity is created as a plane that fills the inside of the portal geometry, and the portal visual geometry is created using the `instantiateRenderEntity` method of the `portal` asset. The statue and bitmoji entities are created with the `instantiateRenderEntity` method of their respective assets. The `PortalGeometry` script is added to both entities, with the `inside` attribute set to `true` for the statue entity and `false` for the bitmoji entity.\n\nOverall, the `PortalExample` class demonstrates how to create a portal in the PlayCanvas engine using stencil buffer rendering. The example shows how to set up the scene, create entities, and use scripts to set stencil parameters for entities inside or outside the portal. The example can be used as a starting point for creating more complex scenes with portals in the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `PortalExample` class?\n- The `PortalExample` class is an example implementation of a portal in the PlayCanvas engine, which includes setting up the application, loading assets, creating entities, and adding scripts to them.\n\n2. What is the role of the `PortalGeometry` script?\n- The `PortalGeometry` script sets up stencil parameters for entities inside or outside the portal, based on the value in the stencil buffer. It is used to determine whether to render the geometry when the value is equal or not equal to zero.\n\n3. What is the purpose of the `bitmoji` entity?\n- The `bitmoji` entity is an example of an entity that is visible outside the portal only. It is created using the `bitmoji.glb` asset and a `portalGeometry` script with the `inside` attribute set to `false`.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/portal.md"}}],["93",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/reflection-cubemap.tsx)\n\nThe `ReflectionCubemapExample` class is an example of how to use the PlayCanvas engine to create a 3D scene with a reflective cubemap. The purpose of this code is to demonstrate how to create a shiny ball that reflects the environment around it, and how to project that environment onto different types of textures. \n\nThe `example` method takes an HTML canvas element and a device type as arguments. It creates a graphics device using the `pc.createGraphicsDevice` method, which initializes the WebGL context and sets up the rendering pipeline. It then creates an `AppBase` instance, which is the main entry point for the PlayCanvas engine. \n\nThe `example` method loads a set of assets, including a cubemap texture and a script that handles cubemap rendering. It then sets up the scene by creating a skydome, a green plane, a camera, and a directional light. It also creates a high-quality sphere that will reflect the environment around it. \n\nThe `example` method then sets up the shiny ball by adding a camera component to it and a script component that handles cubemap rendering. It also creates several textures that will be used to project the cubemap onto different shapes. \n\nFinally, the `example` method updates the scene each frame by rotating the primitives around their center and orbiting them around the shiny sphere. It also projects the cubemap onto different textures and displays them on the screen. \n\nOverall, this code demonstrates how to create a 3D scene with a reflective cubemap using the PlayCanvas engine. It shows how to set up the scene, create a shiny ball that reflects the environment, and project the environment onto different textures. This code can be used as a starting point for creating more complex 3D scenes with reflective surfaces.\n## Questions: \n 1. What is the purpose of the `example` function in this code?\n- The `example` function is used to set up and run a graphics example using the PlayCanvas engine.\n\n2. What is the role of the `shinyBall` entity in this code?\n- The `shinyBall` entity is used to render a high-polygon sphere and generate a dynamic cubemap for use in the example.\n\n3. What is the purpose of the `createPrimitive` function in this code?\n- The `createPrimitive` function is used to create a primitive shape with a specified position, scale, color, and layer, and add it to the scene.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/reflection-cubemap.md"}}],["94",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/reflection-planar.tsx)\n\nThe `ReflectionPlanarExample` class is a code example that demonstrates how to create a planar reflection effect in a 3D scene using the PlayCanvas engine. The example creates a scene with a reflective ground plane and several randomly placed 3D primitives. The reflection effect is achieved by rendering the scene from the perspective of a reflection camera and then applying the resulting texture to the ground plane using a custom shader.\n\nThe `example` method of the `ReflectionPlanarExample` class takes three arguments: a canvas element to render the scene, a string representing the type of graphics device to use (e.g. \"webgl2\"), and an object containing the vertex and fragment shader code for the custom shader. The method first creates a `pc.GraphicsDevice` object using the `pc.createGraphicsDevice` method and initializes a new `pc.AppBase` object with the graphics device. It then loads several assets, including a cubemap texture for the skybox, a 3D model of a statue, and a custom script for rendering the reflection texture.\n\nThe method then creates a new `pc.Entity` object for the main camera and sets its field of view and layer mask to render the world, excluded, and skybox layers. It also creates a new `pc.Entity` object for the reflection camera and sets its field of view and layer mask to render the world and skybox layers only. The reflection camera is positioned and oriented to reflect the scene from the perspective of the ground plane. A custom script component is added to the reflection camera to render the reflection texture using the `planarRenderer` script.\n\nThe method then sets up the scene by creating a reflective ground plane using a custom shader, creating several randomly placed 3D primitives, and adding them to the world layer. It also sets up the skybox and other rendering properties of the scene. Finally, the method sets up an update function to animate the scene and update the reflection texture each frame.\n\nOverall, this code example demonstrates how to create a planar reflection effect in a 3D scene using the PlayCanvas engine. It shows how to use custom shaders, cameras, and scripts to achieve the effect and how to set up a scene with multiple layers and assets. The example can be used as a starting point for creating more complex scenes with reflective surfaces.\n## Questions: \n 1. What is the purpose of the `example` method in this code?\n- The `example` method is a function that takes in three parameters (canvas, deviceType, and files) and sets up a scene with various entities and cameras, and updates them each frame.\n\n2. What is the `planarRenderer` script used for?\n- The `planarRenderer` script is used to render the reflection texture for the ground material. It takes in various attributes such as the scene camera entity, scale, mipmaps, depth, plane point, and plane normal.\n\n3. What is the purpose of the `gfxOptions` object?\n- The `gfxOptions` object is used to specify various options for creating the graphics device, such as the device types to use, and the URLs for the glslang and twgsl libraries.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/reflection-planar.md"}}],["95",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/render-asset.tsx)\n\nThe `RenderAssetExample` class is a part of the PlayCanvas engine project and is responsible for rendering 3D assets on a canvas element. The class imports the `pc` module from the PlayCanvas engine and defines three static properties: `CATEGORY`, `NAME`, and `WEBGPU_ENABLED`. The `example` method of the class takes two arguments: a canvas element and a device type. \n\nThe `example` method creates three assets: `helipad`, `statue`, and `cube`. The `helipad` asset is a texture, while the `statue` and `cube` assets are 3D models. The `gfxOptions` object is defined with the device type and URLs for the glslang and twgsl libraries. The `pc.createGraphicsDevice` method is called with the canvas element and `gfxOptions` object to create a graphics device. \n\nThe `createOptions` object is defined with the graphics device, component systems, and resource handlers. The `pc.AppBase` class is instantiated with the canvas element and `createOptions` object. The `app.setCanvasFillMode` and `app.setCanvasResolution` methods are called to set the canvas to fill the window and automatically change resolution to be the same as the canvas size. \n\nThe `assetListLoader` object is created with the assets and `app.assets` as arguments. The `assetListLoader.load` method is called to load the assets. The `app.start` method is called to start the application. \n\nTwo entities are created from the `cube` asset and added to the scene. The `statue` asset is instantiated and added to the scene. A camera entity is created and added to the scene. The `app.scene.envAtlas`, `app.scene.toneMapping`, and `app.scene.skyboxMip` properties are set to configure the skybox. The `app.on` method is called to spin the meshes on the entities. \n\nThe `RenderAssetExample` class can be used to render 3D assets on a canvas element in a PlayCanvas project. The `example` method can be called with a canvas element and device type to create a graphics device and render the assets. The method can be customized to load different assets and configure the scene. \n\nExample usage:\n\n```\nimport RenderAssetExample from 'path/to/RenderAssetExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2';\n\nconst renderAssetExample = new RenderAssetExample();\nrenderAssetExample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of this code and what does it do?\n- This code is an example of how to render assets using the PlayCanvas engine. It creates a graphics device, loads assets such as textures and models, sets up entities with render components, and spins the meshes.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas engine using the wildcard syntax, as well as the HTMLCanvasElement interface. It also relies on external resources such as glslang and twgsl libraries.\n\n3. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean that indicates whether the example code is compatible with the WebGPU API. If it is set to `true`, the example will use WebGPU if available, otherwise it will fall back to WebGL.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/render-asset.md"}}],["96",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/render-to-texture.tsx)\n\nThe `RenderToTextureExample` class is a code example that demonstrates how to render a scene to a texture using the PlayCanvas engine. The purpose of this code is to show how to create a texture and a render target, and how to use two cameras to render objects into the texture and the main camera. \n\nThe code creates a graphics device, initializes an app, and sets the canvas to fill the window. It then loads assets, including a texture and a script, and creates a layer for objects that do not render into the texture. The code also creates a main camera that renders entities in the world, excluded, and skybox layers, and an orbit camera script with mouse and touch support. \n\nThe code creates a texture camera that renders entities in the world and skybox layers into the texture, and an entity with an omni light component that is added to the world layer. The code also creates a plane called `tv` that is used to display the rendered texture, and a skydome that uses the top mipmap level of a cubemap. \n\nThe code updates things each frame, including rotating the texture camera around the objects and switching the texture camera between perspective and orthographic projection every 5 seconds. \n\nThis code can be used as a reference for developers who want to create a similar feature in their PlayCanvas project. For example, a developer could use this code to create a mini-map that shows a top-down view of the game world, or to create a security camera system that displays a live feed from a camera in the game world. \n\nExample usage:\n\n```javascript\nconst canvas = document.getElementById('application-canvas');\nconst deviceType = 'webgl2';\nconst renderToTextureExample = new RenderToTextureExample();\nrenderToTextureExample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of the `RenderToTextureExample` class?\n- The `RenderToTextureExample` class is an example of how to render objects into a texture using the PlayCanvas engine.\n\n2. What are the two cameras used in this example and what are their purposes?\n- The two cameras used in this example are `textureCamera` and `camera`. `textureCamera` renders entities in the `worldLayer` and `skyboxLayer` into the texture, while `camera` renders entities in the `worldLayer`, `excludedLayer`, and `skyboxLayer`.\n\n3. What is the purpose of the `createPrimitive` function?\n- The `createPrimitive` function is a helper function that creates a primitive with a specified shape type, position, scale, color, and layer. It creates a material of the specified color, creates a primitive with the specified type and layer, sets the position and scale, and adds it to the scene.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/render-to-texture.md"}}],["97",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/shader-burn.tsx)\n\nThe `ShaderBurnExample` class is a part of the PlayCanvas engine project and is responsible for rendering a 3D model of a statue with a custom shader. The purpose of this code is to demonstrate how to create a custom shader and apply it to a 3D model in PlayCanvas. \n\nThe `ShaderBurnExample` class has a static `FILES` object that contains two properties, `shader.vert` and `shader.frag`. These properties contain the vertex and fragment shader code, respectively. The vertex shader is responsible for transforming the vertices of the 3D model, while the fragment shader is responsible for coloring the pixels of the model. \n\nThe `example` method of the `ShaderBurnExample` class takes three arguments: a canvas element, a device type, and a files object. The `example` method creates a new `pc.GraphicsDevice` object using the `pc.createGraphicsDevice` method and sets up an instance of the PlayCanvas application using the `pc.AppBase` constructor. \n\nThe `example` method then loads the 3D model and texture assets using the `pc.AssetListLoader` and adds them to the PlayCanvas application. It creates an entity with a camera component and an entity with a light component and adds them to the scene hierarchy. \n\nThe `example` method then creates a custom shader using the `pc.createShaderFromCode` method and sets it as the material for the 3D model. The `uHeightMap` parameter of the shader is set to the `clouds` texture asset. \n\nFinally, the `example` method updates the `uTime` parameter of the shader in the `on(\"update\")` event listener, which causes the shader to \"burn\" away parts of the model based on the height map texture. \n\nOverall, the `ShaderBurnExample` class demonstrates how to create a custom shader and apply it to a 3D model in PlayCanvas. It can be used as a reference for developers who want to create their own custom shaders in PlayCanvas.\n## Questions: \n 1. What does this code do?\n- This code defines a class called `ShaderBurnExample` which contains a static `example` method that creates a 3D scene with a statue model and a shader that changes the color of the model based on a height map texture.\n\n2. What external dependencies does this code have?\n- This code imports the `pc` module from a relative path that is three levels up from the current directory. It also relies on external assets such as a 3D model and a texture.\n\n3. What is the purpose of the `WEBGPU_ENABLED` static property?\n- The `WEBGPU_ENABLED` property is a boolean that is set to `true`. It is likely used to indicate whether the code is compatible with the WebGPU API, which is a new graphics API for the web that is currently in development.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/shader-burn.md"}}],["98",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/shader-compile.tsx)\n\nThe `ShaderCompileExample` class is a part of the PlayCanvas engine project and is responsible for testing the speed of shader compilation. The purpose of this code is to create a test framework for large shader compilation speed tests. The code creates a new PlayCanvas application and loads a set of assets, including textures, JSON files, and a cubemap. It then creates a ground plane, a camera, a grid of spheres, and some omni lights. Each sphere has a unique material and shader generated by the `createPrimitive` function. The function creates a primitive with a specified shape type, position, scale, and color. It also creates a material of the specified color and sets its diffuse, gloss, and metalness properties. The function then sets the diffuse, normal, and gloss maps of the material to the corresponding assets. It also generates a unique shader for each primitive to avoid any shader compilation caching. The function returns the primitive entity.\n\nThe `example` function is called with an HTML canvas element and a device type as arguments. It enables tracking for shader compilation and starts the update loop of the PlayCanvas application. It also sets up some general scene rendering properties, such as area lights, tone mapping, and skydome. It generates a grid of spheres, each with a unique material and shader. It also creates some omni lights and updates their positions each frame. The purpose of this code is to test the speed of shader compilation and to demonstrate the use of PlayCanvas assets, materials, and entities. This code can be used as a starting point for creating PlayCanvas applications that require complex shaders and materials. Developers can modify the `createPrimitive` function to create their own primitives with custom materials and shaders. They can also modify the `example` function to add their own entities and lights to the scene.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a test framework for large shader compilation speed test using the PlayCanvas engine.\n\n2. What assets are being loaded in this code?\n- The code is loading several assets such as textures, a JSON file, and a cubemap.\n\n3. What is the function of the `createPrimitive` function?\n- The `createPrimitive` function is a helper function that creates a primitive with a specified shape type, position, scale, and color. It also creates a material with specified properties and updates it. Finally, it creates an entity with a render component and adds it to the scene.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/shader-compile.md"}}],["99",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/shader-toon.tsx)\n\nThe `ShaderToonExample` class is a code example that demonstrates how to create a toon shader in the PlayCanvas engine. The toon shader is a type of non-photorealistic rendering (NPR) technique that creates a cartoon-like effect by reducing the number of colors and shading gradients in a 3D model. \n\nThe `ShaderToonExample` class contains a static `FILES` object that defines the vertex and fragment shaders for the toon shader. The vertex shader calculates the diffuse intensity of the model's surface and passes it to the fragment shader as a texture coordinate. The fragment shader uses this texture coordinate to look up the color of the resulting fragment. The `example` method of the `ShaderToonExample` class creates a new graphics device and initializes a new PlayCanvas application. It loads a 3D model of a statue and creates a new camera and light entity. It then creates a new shader from the vertex and fragment shaders defined in the `FILES` object and applies it to the statue model. Finally, it rotates the statue entity in the scene.\n\nThis code example can be used as a starting point for creating custom shaders in the PlayCanvas engine. Developers can modify the vertex and fragment shaders to create different types of NPR effects or to achieve specific visual styles. The `ShaderToonExample` class can also be used as a reference for loading 3D models, creating entities, and manipulating materials in a PlayCanvas application. \n\nExample usage:\n\n```\nimport ShaderToonExample from 'path/to/ShaderToonExample';\n\nconst canvas = document.getElementById('application-canvas');\nconst deviceType = 'webgl2';\nconst files = {\n  'shader.vert': /* glsl */`\n    // custom vertex shader code\n  `,\n  'shader.frag': /* glsl */`\n    // custom fragment shader code\n  `\n};\n\nconst shaderToonExample = new ShaderToonExample();\nshaderToonExample.example(canvas, deviceType, files);\n```\n## Questions: \n 1. What does this code do?\n- This code defines a class called `ShaderToonExample` which contains a static `example` method that creates a 3D scene with a statue model and applies a toon shader to it.\n\n2. What dependencies does this code have?\n- This code imports the `pc` module from a relative path that is three levels up from the current directory.\n\n3. What is the purpose of the `example` method?\n- The `example` method creates a 3D scene with a statue model and applies a toon shader to it. It also sets up the camera, light, and material for the scene.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/shader-toon.md"}}],["100",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/shader-wobble.tsx)\n\nThe `ShaderWobbleExample` class is a part of the PlayCanvas engine project and is responsible for creating a wobbling effect on a 3D model using a custom shader. The class contains a static `example` method that takes in a canvas element, device type, and shader files as parameters. \n\nThe method first creates a `pc.Asset` object for the 3D model and a `gfxOptions` object that specifies the device type and URLs for the glslang and twgsl libraries. It then creates a graphics device using the `pc.createGraphicsDevice` method and initializes a new PlayCanvas application using the `pc.AppBase` constructor. \n\nThe method sets the canvas fill mode and resolution, creates a camera and light entity, and adds them to the scene hierarchy. It then creates a custom shader using the `pc.createShaderFromCode` method and a new material using the `pc.Material` constructor. The method instantiates the 3D model and sets the new material on all meshes in the model, using the original texture from the model on the new material. \n\nFinally, the method sets up an update event listener that updates the time parameter for the shader and updates the material accordingly. \n\nThis method can be used to create a wobbling effect on any 3D model in a PlayCanvas application. Developers can customize the shader code to achieve different effects and modify the material properties to change the appearance of the model. \n\nExample usage:\n\n```\nconst canvas = document.getElementById('application-canvas');\nconst deviceType = 'webgl2';\nconst shaderFiles = {\n  'shader.vert': /* glsl */`\n    // vertex shader code\n  `,\n  'shader.frag': /* glsl */`\n    // fragment shader code\n  `\n};\n\nShaderWobbleExample.example(canvas, deviceType, shaderFiles);\n```\n## Questions: \n 1. What does this code do?\n- This code defines a class called `ShaderWobbleExample` that contains a static `example` method which creates a 3D scene with a wobbling shader effect on a 3D model.\n\n2. What external dependencies does this code have?\n- This code imports the `pc` module from a relative path that is three levels up from the current directory. It also relies on external assets such as a 3D model file and two JavaScript files for glslang and twgsl.\n\n3. What is the purpose of the `example` method?\n- The `example` method creates a 3D scene with a camera, an omni light, and a 3D model with a wobbling shader effect. It also sets up the graphics device, loads assets, creates a shader, and updates the shader's time parameter.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/shader-wobble.md"}}],["101",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/shadow-cascades.tsx)\n\nThe code is a React component that demonstrates the use of cascaded shadow maps in the PlayCanvas engine. The component provides a UI for adjusting various settings related to the shadows, such as filtering, count, resolution, distribution, and VSM blur. The component also creates a 3D scene with a terrain, clouds, a tree, a camera, and a directional light that casts cascaded shadows.\n\nThe `ShadowCascadesExample` class has two methods: `controls` and `example`. The `controls` method returns a JSX element that contains the UI controls for adjusting the shadow settings. The controls are implemented using the `pcui` library, which provides React components for building UIs in PlayCanvas. The `example` method takes three arguments: a canvas element, a device type string, and some data. The method creates a PlayCanvas app with the given canvas and device type, loads some assets, and sets up the 3D scene. The method also updates the shadow settings based on the data passed in, and handles changes to the settings made by the user in the UI.\n\nThe `ShadowCascadesExample` class is exported as the default export of the module, which means that it can be imported and used in other parts of the PlayCanvas engine or in other projects that use the engine.\n\nHere is an example of how to use the `ShadowCascadesExample` component in a React app:\n\n```jsx\nimport React, { useRef, useEffect } from 'react';\nimport ShadowCascadesExample from 'path/to/ShadowCascadesExample';\n\nfunction App() {\n  const canvasRef = useRef(null);\n\n  useEffect(() => {\n    const canvas = canvasRef.current;\n    const deviceType = 'webgl2'; // or 'webgl1' or 'webgpu'\n    const data = {}; // initial data for the shadow settings\n    ShadowCascadesExample.example(canvas, deviceType, data);\n  }, []);\n\n  return (\n    <div>\n      <canvas ref={canvasRef} />\n      <ShadowCascadesExample.controls />\n    </div>\n  );\n}\n```\n\nIn this example, the `ShadowCascadesExample` component is used to create a 3D scene with cascaded shadows and a UI for adjusting the shadow settings. The `canvasRef` ref is used to get a reference to the canvas element that will be used as the rendering surface for the 3D scene. The `useEffect` hook is used to initialize the 3D scene when the component mounts. The `ShadowCascadesExample.controls` element is used to render the UI controls for adjusting the shadow settings.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use the Shadow Cascades feature in the PlayCanvas engine to create cascaded shadows for a 3D scene.\n\n2. What dependencies does this code have?\n- This code imports React, as well as several modules from the PlayCanvas engine and the @playcanvas/pcui/react and @playcanvas/observer packages.\n\n3. What settings can be controlled by the user interface generated by this code?\n- The user interface generated by this code allows the user to control the filtering, count, every frame, resolution, distribution, and VSM blur settings for the cascaded shadows.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/shadow-cascades.md"}}],["102",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/shapes.tsx)\n\nThe code defines a class called `ShapesExample` that contains a single method called `example`. The purpose of this method is to create a simple 3D scene that displays various primitive shapes using the PlayCanvas engine. \n\nThe `example` method takes two arguments: a `canvas` element and a `deviceType` string. The `canvas` element is used to create a `GraphicsDevice` object, which is then used to initialize a new `AppBase` object. The `deviceType` string is used to specify the type of graphics device to create (e.g. \"webgl2\" or \"webgpu\").\n\nOnce the `AppBase` object is created and initialized, the method proceeds to create a number of entities that represent different primitive shapes (e.g. box, plane, sphere, etc.). Each entity is given a `render` component that specifies the type of shape to render. The entities are then added to the scene and positioned in a grid layout.\n\nIn addition to the primitive shapes, the method also creates an entity with a `light` component and an entity with a `camera` component. The `light` component is a directional light that is positioned at an angle to illuminate the scene. The `camera` component is used to render the scene from a particular viewpoint.\n\nOverall, the purpose of this code is to demonstrate how to use the PlayCanvas engine to create a simple 3D scene with primitive shapes, lighting, and a camera. This code could be used as a starting point for more complex 3D applications that require similar functionality. For example, a game developer could use this code to create a prototype of a 3D game level that includes basic geometry and lighting. \n\nExample usage:\n\n```javascript\nimport ShapesExample from 'path/to/ShapesExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2';\n\nconst shapesExample = new ShapesExample();\nshapesExample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of the `ShapesExample` class?\n- The `ShapesExample` class is an example class that demonstrates how to create and render primitive shapes using the PlayCanvas engine.\n\n2. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean value that indicates whether the example can be run using the WebGPU API.\n\n3. What are the different component systems and resource handlers being used in the `createOptions` object?\n- The `createOptions` object includes the `componentSystems` array, which contains `RenderComponentSystem`, `CameraComponentSystem`, and `LightComponentSystem`, and the `resourceHandlers` array, which contains `TextureHandler` and `ContainerHandler`. These are all systems and handlers that are used by the PlayCanvas engine to manage different types of components and resources.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/shapes.md"}}],["103",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/texture-basis.tsx)\n\nThe `TextureBasisExample` class is a code example that demonstrates how to use the PlayCanvas engine to load and display textures in the Basis format. The Basis format is a compressed texture format that provides high-quality textures with smaller file sizes. \n\nThe `example` method of the `TextureBasisExample` class takes two parameters: a canvas element and a device type. It initializes the Basis library by calling the `basisInitialize` method with the URLs of the Basis WebAssembly and JavaScript files. It then creates a new PlayCanvas application with the canvas element and loads four assets: a color texture, a gloss texture, a normal map texture, and a cubemap texture. \n\nOnce the assets are loaded, the application starts and sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. It also sets the skybox to a cubemap texture and creates a directional light. \n\nThe code then constructs a material with the loaded textures and creates a torus shape with the material. It also creates an entity with a camera component and adds all the entities to the application hierarchy. Finally, it sets an update function on the application's update event to rotate the torus shape.\n\nThis code example demonstrates how to use the PlayCanvas engine to load and display textures in the Basis format. It shows how to load multiple textures and use them to construct a material for a 3D object. It also demonstrates how to create entities and add them to the application hierarchy. This code can be used as a starting point for building 3D applications that use Basis textures.\n## Questions: \n 1. What is the purpose of the `TextureBasisExample` class?\n- The `TextureBasisExample` class is an example of how to use the PlayCanvas engine to create a 3D scene with a torus shape and various textures.\n\n2. What is the `basisInitialize` function doing?\n- The `basisInitialize` function is initializing the basis library by providing URLs to the necessary files.\n\n3. What textures are being used in this example and how are they being applied?\n- The example is using a color texture, a gloss texture, and a normal map texture, which are being applied to a torus shape using a `StandardMaterial`. The textures have been converted using the `basisu` tool with specific arguments.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/texture-basis.md"}}],["104",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/transform-feedback.tsx)\n\nThe `TransformFeedbackExample` class is a part of the PlayCanvas engine project and is responsible for rendering point sprite particles. The class contains a static `example` method that takes in a canvas element, device type, and files object as parameters. The method creates an instance of the PlayCanvas application and starts the update loop. It sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. It also sets the ambient light of the scene to a dark gray color.\n\nThe method then creates a small 2D texture representing movement direction (wind) and initializes it with random data. It creates a main camera that renders the world and sets up texture transform part on WebGL2 devices only. It generates random data that is used as seeds to generate particles in the vertex shader. It stores these in a vertex buffer of a mesh and creates a new material with a shader and additive alpha blending. It creates the mesh instance and an entity used to render the mesh instance using a render component.\n\nThe method sets up transform feedback, which creates a clone of the vertex buffer and sets up rendering to ping pong between them. It sets up simulation parameters and executes the simulation. The simulation is executed each frame, and the camera is rotated around the particles. \n\nThe `TransformFeedbackExample` class is used to render point sprite particles in the PlayCanvas engine project. It is a high-level class that encapsulates the details of rendering particles and provides a simple interface for developers to use. Developers can use this class to create particle effects in their games or applications. They can customize the particle behavior by modifying the shader code or changing the simulation parameters. \n\nExample usage:\n\n```\nimport TransformFeedbackExample from 'path/to/TransformFeedbackExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2';\nconst files = {\n  'shaderFeedback.vert': '...',\n  'shaderCloud.vert': '...',\n  'shaderCloud.frag': '...'\n};\n\nTransformFeedbackExample.example(canvas, deviceType, files);\n```\n## Questions: \n 1. What is the purpose of the `TransformFeedbackExample` class?\n- The `TransformFeedbackExample` class is an example of a graphics simulation using transform feedback, which involves moving particles based on a texture containing random direction vectors.\n\n2. What are the requirements for the `example` method to work?\n- The `example` method requires an HTML canvas element, a string representing the device type, and an object containing three shader files: `shaderFeedback.vert`, `shaderCloud.vert`, and `shaderCloud.frag`.\n\n3. What is the role of the `tf` and `shader` variables?\n- The `tf` variable is used to set up transform feedback, which creates a clone of a vertex buffer and sets up rendering to ping pong between them. The `shader` variable is used to create a shader from the `shaderFeedback.vert` file, which is used to render point sprites.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/transform-feedback.md"}}],["105",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/graphics/video-texture.tsx)\n\nThe `VideoTextureExample` class is a part of the PlayCanvas engine project and is responsible for rendering a video texture on a 3D object in a 3D scene. This class is used to demonstrate how to use video textures in PlayCanvas. \n\nThe `example` method of the `VideoTextureExample` class takes two parameters: a canvas element and a device type. It creates a new PlayCanvas application and starts the update loop. It then loads a 3D model of a TV and creates an entity to render it. A camera and an omni light are also created and added to the scene. \n\nA texture is created to hold the video frame data. An HTML video element is created and added to the page. The video element is set to autoplay and muted so that it can play automatically without sound. The video is then loaded and when it is ready to play, the video texture is set as the source of the texture. \n\nA material is created that uses the video texture as its emissive map. The material is then set on the TV mesh. The TV object is rotated and the video data is uploaded to the texture every other frame. \n\nThis class can be used as a reference for developers who want to add video textures to their PlayCanvas projects. Developers can use this class to learn how to create a texture to hold video frame data, how to create an HTML video element, how to set the video texture as the source of the texture, and how to create a material that uses the video texture. \n\nExample usage:\n\n```javascript\nimport VideoTextureExample from 'path/to/VideoTextureExample';\n\nconst canvas = document.getElementById('application-canvas');\nconst deviceType = 'web';\n\nconst videoTextureExample = new VideoTextureExample();\nvideoTextureExample.example(canvas, deviceType);\n```\n\nThis will create a new instance of the `VideoTextureExample` class and call its `example` method with the canvas element and device type as parameters. The video texture will be rendered on a 3D object in the scene.\n## Questions: \n 1. What does this code do?\n- This code is an example implementation of a video texture using the PlayCanvas engine. It creates an application, loads a 3D model of a TV, and applies a video texture to it.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas engine using the wildcard syntax, and also uses the HTMLCanvasElement and HTMLVideoElement interfaces.\n\n3. What is the purpose of the 'canplaythrough' event listener?\n- The 'canplaythrough' event listener waits for the video to have enough data to play through without buffering, and then sets the video as the source for the video texture.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/graphics/video-texture.md"}}],["106",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/index.mjs)\n\nThe code above is a module that imports and exports various sub-modules from the PlayCanvas engine project. The purpose of this module is to provide a centralized way of accessing different functionalities of the engine. \n\nThe module imports sub-modules such as Animation, Camera, Graphics, Input, Loaders, Misc, Physics, Sound, UserInterface, and Xr. Each of these sub-modules contains specific functionalities related to their respective areas. For example, the Animation sub-module contains functions and classes related to animating objects in the scene, while the Camera sub-module contains functions and classes related to camera manipulation.\n\nBy exporting these sub-modules, the module allows other parts of the PlayCanvas engine project to easily access and use these functionalities. For example, if a developer wants to use the animation functionality in their project, they can simply import the Animation sub-module from this module and use its functions and classes.\n\nHere is an example of how this module can be used:\n\n```\nimport { Graphics } from 'playcanvas';\n\n// Use the Graphics sub-module to create a new texture\nconst texture = new Graphics.Texture({\n    width: 512,\n    height: 512,\n    format: Graphics.PIXELFORMAT_R8_G8_B8_A8,\n    mipmaps: true\n});\n```\n\nIn the example above, the Graphics sub-module is imported from the PlayCanvas engine module. The sub-module is then used to create a new texture object with specific properties.\n\nOverall, this module serves as a convenient way of organizing and accessing different functionalities of the PlayCanvas engine project.\n## Questions: \n 1. What is the purpose of this code?\n   This code exports various modules from different directories within the PlayCanvas engine project.\n\n2. What are some examples of functionality provided by the exported modules?\n   The exported modules include Animation, Camera, Graphics, Input, Loaders, Misc, Physics, Sound, UserInterface, and Xr, which likely provide functionality related to those areas.\n\n3. Are there any dependencies required for this code to work?\n   It is unclear from this code snippet whether there are any dependencies required for the exported modules to work properly.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/index.md"}}],["107",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/input/gamepad.tsx)\n\nThe code defines a class called `GamepadExample` that demonstrates how to use gamepad input in a PlayCanvas application. The `example` method of the class takes two arguments: a canvas element and a string representing the type of graphics device to use. \n\nThe method creates a new PlayCanvas application and sets up the graphics device using the specified device type. It then loads two assets: a cubemap texture and a 3D model. Once the assets are loaded, the method starts the application and sets the skybox of the scene to the cubemap texture. It also creates an entity with a camera component and an entity with a render component for the 3D model. \n\nThe method then creates a new `pc.GamePads` object and registers an event listener for the `update` event of the application. Inside the event listener, it calls the `update` method of the `pc.GamePads` object to update the state of connected gamepads. It then checks if certain buttons on the first gamepad are pressed or were just pressed, and rotates the 3D model entity accordingly. \n\nThis code can be used as a starting point for implementing gamepad input in a PlayCanvas application. Developers can modify the code to handle different types of gamepads and to perform different actions based on the input. For example, they could use gamepad input to control the movement of a player character or to trigger certain events in the game. \n\nExample usage:\n\n```javascript\nimport GamepadExample from 'path/to/GamepadExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2'; // or 'webgl1' or 'auto'\n\nconst gamepadExample = new GamepadExample();\ngamepadExample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of the `GamepadExample` class?\n- The `GamepadExample` class is an example of how to use gamepads for input in a PlayCanvas application.\n\n2. What assets are being loaded in the `example` method?\n- The `example` method loads a texture asset and a container asset, which contains a 3D model.\n\n3. What input events are being handled in the `example` method?\n- The `example` method handles input events from a gamepad, specifically the left and right directional pads for rotation around the Y-axis, and the up and down directional pads for rotation around the X-axis.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/input/gamepad.md"}}],["108",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/input/index.mjs)\n\nThe code above is a module that exports three classes: `GamepadExample`, `KeyboardExample`, and `MouseExample`. These classes are examples of how to use gamepad, keyboard, and mouse input in the PlayCanvas engine.\n\nThe `import` statements at the beginning of the code import the three classes from separate files located in the same directory as this module. This allows for modularization and separation of concerns in the codebase.\n\nThe `export` statement at the end of the code exports the three classes as named exports. This means that when another module imports this module, they can access these classes by their respective names.\n\nFor example, if another module wanted to use the `KeyboardExample` class, they could import it like this:\n\n```\nimport { KeyboardExample } from \"./inputExamples\";\n```\n\nThen, they could create a new instance of the `KeyboardExample` class and use it to handle keyboard input in their PlayCanvas project.\n\nOverall, this module serves as a helpful resource for developers using the PlayCanvas engine who want to learn how to handle gamepad, keyboard, and mouse input. By providing examples of how to use these input methods, developers can save time and effort in implementing these features in their own projects.\n## Questions: \n 1. **What is the purpose of this code file?**\\\nThis code file is exporting three modules: `GamepadExample`, `KeyboardExample`, and `MouseExample`. It is likely that these modules provide examples or utilities related to gamepad, keyboard, and mouse input handling.\n\n2. **Where are the modules `GamepadExample`, `KeyboardExample`, and `MouseExample` defined?**\\\nThese modules are imported from three separate files located in the same directory as this code file: `gamepad.js`, `keyboard.js`, and `mouse.js`.\n\n3. **How can a developer use these exported modules in their code?**\\\nA developer can import these modules into their code using the `import` statement and the appropriate file path. For example, to use the `KeyboardExample` module, a developer could write `import { KeyboardExample } from \"./path/to/this/file\";`.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/input/index.md"}}],["109",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/input/keyboard.tsx)\n\nThe code defines a class called `KeyboardExample` that demonstrates how to use the PlayCanvas engine to create a 3D scene with a model that can be rotated using keyboard input. \n\nThe `example` method of the `KeyboardExample` class takes two arguments: a canvas element and a string representing the type of graphics device to use. It creates a dictionary of assets that will be used in the scene, including a texture and a 3D model. It then creates a graphics device using the `pc.createGraphicsDevice` method, passing in the canvas element and an object with options for the graphics device. \n\nNext, it creates an instance of the `pc.AppBase` class, passing in the canvas element and an object with options for the app. It sets the canvas fill mode to `pc.FILLMODE_FILL_WINDOW` and the canvas resolution to `pc.RESOLUTION_AUTO`. It then loads the assets using an instance of the `pc.AssetListLoader` class and starts the app.\n\nOnce the app is started, it sets the skybox of the scene to the texture asset, sets the tone mapping to `pc.TONEMAP_ACES`, sets the exposure to 1.6, and sets the skybox mip level to 1. It then creates an entity with a camera component and adds it to the root of the scene. It also creates an instance of the 3D model asset, adds it to the root of the scene, and sets up keyboard input using an instance of the `pc.Keyboard` class. \n\nThe `pc.Keyboard` class takes a DOM element as an argument and provides methods for checking whether a key is currently pressed. The `app.on(\"update\")` method is used to listen for updates to the app and check whether the left or right arrow key is pressed. If the left arrow key is pressed, the model entity is rotated to the left, and if the right arrow key is pressed, the model entity is rotated to the right.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a 3D scene with keyboard input. It shows how to load assets, create a graphics device and an app, set up a camera, add entities to the scene, and handle keyboard input. This code could be used as a starting point for creating more complex 3D scenes with user input.\n## Questions: \n 1. What is the purpose of the `KeyboardExample` class?\n- The `KeyboardExample` class is an example of how to use keyboard input in the PlayCanvas engine.\n\n2. What assets are being loaded in the `example` method?\n- The `example` method loads a texture asset and a container asset, which are used to set the skybox and instantiate a render entity, respectively.\n\n3. What is the purpose of the `gfxOptions` object?\n- The `gfxOptions` object is used to specify the graphics device type and the URLs for the glslang and twgsl libraries.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/input/keyboard.md"}}],["110",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/input/mouse.tsx)\n\nThe code defines a class called `MouseExample` that provides an example of how to use the PlayCanvas engine to create a 3D scene that responds to mouse input. The purpose of the code is to demonstrate how to create a PlayCanvas application that uses the mouse to rotate a 3D object in the scene.\n\nThe `MouseExample` class has a single method called `example` that takes two arguments: a canvas element and a device type. The canvas element is used to create a graphics device, which is then used to create a PlayCanvas application. The device type is used to specify the type of graphics device to create, such as WebGL or WebGPU.\n\nThe `example` method creates two assets: a texture and a 3D model. It then creates a graphics device using the `pc.createGraphicsDevice` method, passing in the canvas element and device type. Once the graphics device is created, it creates a PlayCanvas application using the `pc.AppBase` class and initializes it with the graphics device.\n\nThe method then sets the canvas fill mode and resolution, loads the assets using an `AssetListLoader`, and starts the application. It sets the skybox, creates a camera entity, adds the 3D model to the scene, and creates a `pc.Mouse` object that listens for mouse events.\n\nWhen the mouse moves, the `mousemove` event is fired, and the code checks if the left mouse button is pressed. If it is, it updates the rotation of the 3D model based on the mouse movement.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a 3D scene that responds to mouse input. It shows how to create a graphics device, load assets, create entities, and handle mouse events. This code can be used as a starting point for creating more complex PlayCanvas applications that use mouse input.\n## Questions: \n 1. What is the purpose of the `MouseExample` class?\n- The `MouseExample` class is an example of how to use mouse input in the PlayCanvas engine.\n\n2. What assets are being loaded in the `example` method?\n- The `example` method loads a texture asset and a container asset.\n\n3. What does the `mousemove` event listener do?\n- The `mousemove` event listener updates the rotation of an entity based on the movement of the mouse, but only if the left mouse button is pressed.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/input/mouse.md"}}],["111",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/loaders/draco-glb.tsx)\n\nThe code is an example of how to load and render a 3D model in the PlayCanvas engine using the Draco GLB format. The Draco GLB format is a compressed version of the GLB format, which is a binary format for 3D models. The code imports the PlayCanvas engine and defines a class called DracoGlbExample. The class has a static CATEGORY property that defines the category of the example, a static NAME property that defines the name of the example, and a static WEBGPU_ENABLED property that defines whether the example is enabled for WebGPU.\n\nThe example method of the class takes an HTML canvas element and a device type as arguments. The method loads the Draco decoder module using the pc.WasmModule.setConfig method and the pc.WasmModule.getInstance method. The method then creates a graphics device using the pc.createGraphicsDevice method and sets the canvas fill mode and resolution using the app.setCanvasFillMode and app.setCanvasResolution methods. The method loads a 3D model in the Draco GLB format using the pc.AssetListLoader and adds the model to the scene using the app.root.addChild method. The method also creates a camera entity and a light entity and adds them to the scene using the app.root.addChild method. Finally, the method starts the app and adds an update event listener that rotates the 3D model.\n\nThis code can be used as a starting point for loading and rendering 3D models in the Draco GLB format in the PlayCanvas engine. Developers can modify the code to load different 3D models and add different entities to the scene. For example, developers can change the URL of the 3D model in the assets object to load a different model. Developers can also add different types of entities to the scene, such as a skybox entity or a particle system entity. The code demonstrates how to use the PlayCanvas engine to create interactive 3D applications that run in a web browser.\n## Questions: \n 1. What is the purpose of the `DracoGlbExample` class?\n- The `DracoGlbExample` class is an example class that loads a GLB model using the Draco decoder and displays it on a canvas using the PlayCanvas engine.\n\n2. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean value that indicates whether the example supports the WebGPU API or not.\n\n3. What is the purpose of the `demo` function?\n- The `demo` function is a callback function that is called after the Draco decoder module is loaded. It initializes the PlayCanvas app and starts rendering the GLB model on the canvas.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/loaders/draco-glb.md"}}],["112",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/loaders/glb.tsx)\n\nThe `GlbExample` class is a code example that demonstrates how to load and display a glTF Binary (GLB) file in the PlayCanvas engine. The GLB file contains meshes, lights, and cameras, and the example switches between the cameras every two seconds. \n\nThe `example` method takes two parameters: a `canvas` element and a `deviceType` string. The `canvas` element is used to create a graphics device, and the `deviceType` string specifies the type of device to create. \n\nThe GLB file is loaded using the `pc.Asset` class, which is passed to an `AssetListLoader` instance along with the `app.assets` object. Once the assets are loaded, the GLB file is instantiated as a render entity, and its cameras and lights are enabled. \n\nThe example uses the `pc.CameraComponent` and `pc.LightComponent` classes to manipulate the cameras and lights in the scene. The `pc.CameraComponent` class is used to set the aspect ratio, aperture, shutter, and sensitivity of the cameras, while the `pc.LightComponent` class is used to enable the lights in the scene. \n\nThe example also uses the `pc.AppBase` class to create an instance of the PlayCanvas application, and the `pc.createGraphicsDevice` function to create a graphics device. The `pc.AppOptions` class is used to specify the graphics device and component systems and resource handlers to use. \n\nFinally, the example uses the `app.on` method to listen for the `update` event, which is fired every frame. The `update` event is used to switch between the cameras every two seconds. \n\nOverall, the `GlbExample` class provides a simple example of how to load and display a GLB file in the PlayCanvas engine. It demonstrates how to use the `pc.Asset` class to load assets, the `pc.CameraComponent` and `pc.LightComponent` classes to manipulate cameras and lights, and the `pc.AppBase` class to create a PlayCanvas application.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of loading a glb file that contains meshes, lights, and cameras, and switching between the cameras every 2 seconds.\n\n2. What dependencies are required for this code to run?\n- This code requires the PlayCanvas engine and the glslang and twgsl libraries.\n\n3. What is the expected output of running this code?\n- The expected output is a canvas displaying a 3D scene loaded from a glb file, with multiple cameras and lights, and the cameras switching every 2 seconds.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/loaders/glb.md"}}],["113",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/loaders/gltf-export.tsx)\n\nThe code is a React component that exports a 3D scene in GLTF format using the PlayCanvas engine. The component is called `GltfExportExample` and is part of the PlayCanvas engine project. \n\nThe `GltfExportExample` class has two static properties: `CATEGORY` and `NAME`. These properties are used to categorize and name the example in the PlayCanvas engine's examples browser. The `WEBGPU_ENABLED` property is a boolean that indicates whether the example can be run on WebGPU-enabled devices.\n\nThe `GltfExportExample` class has two methods: `controls` and `example`. The `controls` method returns a React component that renders a button with the text \"Download GLTF\". When the button is clicked, it emits a `download` event using the `Observer` class from the `@playcanvas/observer` package.\n\nThe `example` method is the main method of the `GltfExportExample` class. It takes four arguments: a canvas element, a device type string, a `pcx` object, and a `data` object. The `canvas` element is used to create a graphics device using the `pc.createGraphicsDevice` method. The `deviceType` string specifies the type of device to create (e.g. \"webgl2\" or \"webgpu\"). The `pcx` object is an instance of the PlayCanvas engine's `pcx` namespace, which contains various utility classes and functions. The `data` object is an instance of the `Observer` class and is used to listen for the `download` event.\n\nThe `example` method sets up the PlayCanvas engine by creating an `AppBase` instance and initializing it with a set of options. It then loads a set of assets (a cubemap texture, a 3D model of a bench, a 3D model of a character, and a 3D model of a chess board) using the `pc.AssetListLoader` class. Once the assets are loaded, it creates a 3D scene by instantiating render entities from the loaded assets and adding them to the scene graph. It also creates a camera entity and sets up the skybox and tone mapping.\n\nFinally, the `example` method exports the entire scene to a GLTF file using the `pcx.GltfExporter` class. It creates a link element in the HTML document and sets its `href` attribute to a URL representing the exported GLTF file. When the \"Download GLTF\" button is clicked, it triggers a download of the GLTF file by programmatically clicking the link element.\n\nOverall, the `GltfExportExample` class demonstrates how to use the PlayCanvas engine to create a 3D scene and export it to a GLTF file. It also shows how to use React to create a UI component that triggers the GLTF export.\n## Questions: \n 1. What is the purpose of the `controls` method in the `GltfExportExample` class?\n- The `controls` method returns a React component that includes a button with text \"Download GLTF\" and an `onClick` event listener that emits a \"download\" event.\n\n2. What is the significance of the `WEBGPU_ENABLED` property in the `GltfExportExample` class?\n- The `WEBGPU_ENABLED` property is a boolean value that indicates whether the example supports the WebGPU API.\n\n3. What is the `GltfExporter` class used for in this code?\n- The `GltfExporter` class is used to export the entire scene into a glb format, which is then downloaded when the \"Download GLTF\" button is clicked.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/loaders/gltf-export.md"}}],["114",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/loaders/index.mjs)\n\nThe code above is a module that exports several examples related to 3D model loading and exporting in the PlayCanvas engine project. The module imports six different examples, each located in a separate file, and exports them as named exports.\n\nThe first example, `DracoGlbExample`, is related to loading 3D models in the GLB format using the Draco compression library. This example demonstrates how to use the `DracoLoader` class from the `@loaders.gl/draco` package to decode compressed GLB data.\n\nThe second example, `LoadersGlExample`, shows how to use the `GLTFLoader` class from the `@loaders.gl/gltf` package to load 3D models in the GLTF format. This example also demonstrates how to use the `GLTFEnvironment` class to create a basic environment for the loaded model.\n\nThe third example, `GlbExample`, is similar to the first example but shows how to load uncompressed GLB data using the `GLBLoader` class from the `@loaders.gl/glb` package.\n\nThe fourth example, `GltfExportExample`, demonstrates how to export a 3D model in the GLTF format using the `GLTFExporter` class from the `@loaders.gl/gltf` package.\n\nThe fifth example, `ObjExample`, shows how to load 3D models in the OBJ format using the `OBJLoader` class from the `@loaders.gl/obj` package.\n\nFinally, the sixth example, `UsdzExportExample`, demonstrates how to export a 3D model in the USDZ format using the `USDZExporter` class from the `@loaders.gl/usdz` package.\n\nBy exporting these examples, other developers can easily learn how to use the different loaders and exporters available in the PlayCanvas engine project. For example, a developer who needs to load a 3D model in the GLTF format can refer to the `LoadersGlExample` code to see how it's done. Similarly, a developer who needs to export a 3D model in the USDZ format can refer to the `UsdzExportExample` code.\n## Questions: \n 1. **What is the purpose of this code file?**\\\nThis code file exports several examples related to loading and exporting 3D models in different formats using the PlayCanvas engine.\n\n2. **What are the examples included in this file?**\\\nThis file includes examples for loading and exporting 3D models in Draco GLB, regular GLB, GLTF, OBJ, and USDZ formats.\n\n3. **Where are the actual implementations of these examples located?**\\\nThe actual implementations of these examples are located in separate files imported at the beginning of this code file, such as `draco-glb.js` and `gltf-export.js`.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/loaders/index.md"}}],["115",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/loaders/loaders-gl.tsx)\n\nThe code is a React component that demonstrates how to use the `loaders.gl` library to load and render a 3D model in a web application. The `loaders.gl` library is a collection of loaders and writers for various 3D file formats, including glTF, OBJ, and Draco. The `LoadersGlExample` class has a `load` method that loads the `loaders.gl` library from a CDN and a `example` method that demonstrates how to use the library to load and render a Draco point cloud model.\n\nThe `example` method creates a new `pc.Application` object, which is the main object for the PlayCanvas engine. It then loads a Draco point cloud model using the `CORE.load` method from the `loaders.gl` library. The `loadModel` function converts the loaded colors to an array of RGBA with alpha of 255 and creates a mesh with position and color vertex data. It then creates a shader to render the mesh as circular points with color and a material using the shader. Finally, it adds an entity with a render component to render the mesh.\n\nThe `example` method also creates an entity with a camera component and adds it to the root of the application. It then loads the Draco model and starts the application. The `update` event is used to update the camera position and orientation each frame.\n\nThis code can be used as a starting point for building a web application that loads and renders 3D models using the `loaders.gl` library and the PlayCanvas engine. Developers can modify the `example` method to load and render different 3D models and use the PlayCanvas engine to add interactivity and animations to the application.\n## Questions: \n 1. What is the purpose of the `LoadersGlExample` class?\n- The `LoadersGlExample` class is an example class that demonstrates how to use the `@loaders.gl` library to load and render a 3D model.\n\n2. What are the dependencies of this code?\n- The code depends on the `React` library, the `@loaders.gl/core` library, and the `@loaders.gl/draco` library.\n\n3. What is the expected output of the `example` method?\n- The `example` method is expected to render a 3D model loaded from the specified URL using the `@loaders.gl` library. The model is rendered as circular points with color, and the camera orbits around the model.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/loaders/loaders-gl.md"}}],["116",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/loaders/obj.tsx)\n\nThe `ObjExample` class is a code example that demonstrates how to load and display a 3D model in the PlayCanvas engine using the OBJ file format. The purpose of this code is to provide developers with a working example of how to use the PlayCanvas engine to load and display 3D models in their applications.\n\nThe `example` method is the main entry point for this code example. It takes two parameters: a `canvas` element and a `deviceType` string. The `canvas` element is used to create a new PlayCanvas application, while the `deviceType` string is not used in this example.\n\nThe code first creates a new PlayCanvas application using the `pc.Application` constructor. It then sets the ambient light of the scene to a dark gray color using the `pc.Color` constructor.\n\nThe code then defines the URLs for the OBJ file and the OBJ model parser script. It loads the OBJ model parser script using the `app.assets.loadFromUrl` method and adds the OBJ parser to the model resource handler using the `app.loader.getHandler(\"model\").addParser` method. This enables the PlayCanvas engine to parse OBJ files and load them as 3D models.\n\nThe code then loads the OBJ file using the `app.assets.loadFromUrl` method and creates a new entity to display the model. It adds a `model` component to the entity and sets the `model` property to the loaded model resource. It also generates a random material for each mesh instance in the model and assigns it to the `material` property of the mesh instance.\n\nThe code then creates a new entity with a `camera` component and adds it to the scene. It also creates a new entity with an `omni` light component and adds it to the scene.\n\nFinally, the code sets up an update loop using the `app.on(\"update\")` method. In the update loop, it rotates the model entity around the y-axis if it exists.\n\nThis code example demonstrates how to load and display a 3D model in the PlayCanvas engine using the OBJ file format. Developers can use this code as a starting point for their own applications that require 3D models. They can modify the code to load different models, use different materials, and add more entities to the scene.\n## Questions: \n 1. What is the purpose of the `ObjExample` class?\n- The `ObjExample` class is an example class that demonstrates how to load and display an OBJ model using the PlayCanvas engine.\n\n2. What are the URLs for the OBJ model and the OBJ model parser script?\n- The URLs for the OBJ model and the OBJ model parser script are `/static/assets/models/monkey.obj` and `/static/scripts/parsers/obj-model.js`, respectively.\n\n3. What components are added to the camera and light entities?\n- The camera entity has a `camera` component with a clear color of `new pc.Color(0.4, 0.45, 0.5)`, while the light entity has a `light` component with a type of `\"omni\"`, a color of `new pc.Color(1, 1, 1)`, and a range of `100`.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/loaders/obj.md"}}],["117",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/loaders/usdz-export.tsx)\n\nThe code defines a class called UsdzExportExample that exports a 3D model in the USDZ format. The class is a part of the PlayCanvas engine project and is written in TypeScript. The class has two static properties, CATEGORY and NAME, which are used to categorize and name the example in the PlayCanvas editor. The class also has a static property called WEBGPU_ENABLED, which is a boolean value that determines whether the example can be run on WebGPU-enabled devices.\n\nThe class has two methods, controls and example. The controls method returns a React component that renders a button with the text \"Download USDZ\". The onClick event of the button emits a \"download\" event using the Observer pattern.\n\nThe example method takes four parameters: a canvas element, a device type, a pcx object, and a data object. The method creates a graphics device using the pc.createGraphicsDevice method and initializes a PlayCanvas application using the pc.AppBase constructor. The method then loads two assets, a texture and a 3D model, using the pc.Asset constructor and the pc.AssetListLoader class. The method creates a camera entity, sets the skybox, and spins the 3D model. Finally, the method exports the 3D model in the USDZ format using the pcx.UsdzExporter class and the build method. The exported file is downloaded when the \"Download USDZ\" button is clicked.\n\nThis code can be used as an example of how to export a 3D model in the USDZ format using the PlayCanvas engine. The code can be modified to export different 3D models and textures by changing the URLs in the assets object. The code can also be modified to add more functionality, such as adding more buttons to download different formats or adding more 3D models to the scene.\n## Questions: \n 1. What is the purpose of the `UsdzExportExample` class?\n- The `UsdzExportExample` class is an example of how to export a 3D model to USDZ format using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports React, `pc` from a relative path, `Button` and `Observer` from `@playcanvas/pcui/react`.\n\n3. What does the `example` method do?\n- The `example` method initializes a PlayCanvas app, loads assets, creates entities, sets up a camera, sets the skybox, exports a 3D model to USDZ format, and triggers a download of the resulting file when a button is clicked.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/loaders/usdz-export.md"}}],["118",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/misc/hello-world.tsx)\n\nThe code defines a class called `HelloWorldExample` that contains a single method called `example`. This method takes in two parameters: an HTML canvas element and a string representing the type of graphics device to use. The purpose of this code is to create a simple 3D scene using the PlayCanvas engine and display it on the canvas element.\n\nThe method begins by defining an object called `gfxOptions` that contains options for creating a graphics device. This object specifies the type of device to create and the URLs for two JavaScript files that are needed for shader compilation.\n\nNext, the `pc.createGraphicsDevice` function is called with the canvas element and the `gfxOptions` object. This function returns a promise that resolves to a `pc.GraphicsDevice` object. Once the promise is resolved, the method continues by creating an `pc.AppOptions` object that specifies the graphics device to use and the component systems and resource handlers to load.\n\nAn `pc.AppBase` object is then created with the canvas element and the `createOptions` object. The `init` method is called on the `app` object to initialize it, and the `start` method is called to start the rendering loop.\n\nThe canvas is set to fill the window and automatically change resolution to be the same as the canvas size. A box entity, camera entity, and directional light entity are then created and added to the scene. The box entity is given a render component of type 'box', the camera entity is given a camera component with a clear color of light blue, and the light entity is given a light component.\n\nFinally, the `app` object's `on` method is called to register an event listener for the 'update' event. This event is fired every frame and is used to rotate the box entity based on the delta time since the last frame.\n\nOverall, this code demonstrates how to create a simple 3D scene using the PlayCanvas engine and display it on an HTML canvas element. It can be used as a starting point for more complex projects that require 3D graphics. Here is an example of how to use this code:\n\n```javascript\nimport HelloWorldExample from 'path/to/HelloWorldExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2'; // or 'webgl1' or 'webgpu'\n\nconst example = new HelloWorldExample();\nexample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example implementation of a \"Hello World\" program using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library using the `import * as pc` statement, and also requires an HTML canvas element to be passed as a parameter to the `example` function.\n\n3. What does the `WEBGPU_ENABLED` property do?\n- The `WEBGPU_ENABLED` property is a boolean value that indicates whether the example program is compatible with the WebGPU API.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/misc/hello-world.md"}}],["119",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/misc/index.mjs)\n\nThe code above is a module that exports three examples from the PlayCanvas engine project: HelloWorldExample, MiniStatsExample, and SpineboyExample. \n\nThe purpose of this module is to provide developers with examples of how to use the PlayCanvas engine in their projects. Each of the examples demonstrates a different aspect of the engine's capabilities. \n\nThe HelloWorldExample is a simple example that shows how to create a basic scene with a single entity and a camera. The MiniStatsExample demonstrates how to use the engine's built-in stats module to display performance statistics in the corner of the screen. The SpineboyExample shows how to use the engine's Spine animation system to animate a character. \n\nDevelopers can use these examples as a starting point for their own projects, or as a reference for how to use specific features of the engine. For example, if a developer wants to add Spine animations to their game, they can refer to the SpineboyExample to see how it's done. \n\nTo use these examples in their own projects, developers can import them from this module using the following syntax:\n\n```javascript\nimport { HelloWorldExample, MiniStatsExample, SpineboyExample } from \"playcanvas-engine-examples\";\n```\n\nOverall, this module serves as a valuable resource for developers who are new to the PlayCanvas engine or who want to learn more about its capabilities.\n## Questions: \n 1. **What is the purpose of this code file?**\\\nA smart developer might wonder what this code file is used for and how it fits into the overall PlayCanvas engine project. This code file appears to be exporting three examples: HelloWorldExample, MiniStatsExample, and SpineboyExample.\n\n2. **What are the dependencies of this code file?**\\\nA smart developer might want to know what other files or modules this code file depends on in order to function properly. From the import statements, it appears that this code file depends on three other files: \"./hello-world\", \"./mini-stats\", and \"./spineboy\".\n\n3. **How can these examples be used in a PlayCanvas project?**\\\nA smart developer might be interested in knowing how to use these examples in their own PlayCanvas project. It's not clear from this code file alone how to use these examples, but a developer could look for documentation or examples on how to import and use these modules in a PlayCanvas project.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/misc/index.md"}}],["120",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/misc/mini-stats.tsx)\n\nThe MiniStatsExample class is a demonstration of how to use the MiniStats system in the PlayCanvas engine. The MiniStats system is a tool that provides real-time performance statistics for a PlayCanvas application. The MiniStatsExample class creates a PlayCanvas application, sets up the MiniStats system, and adds some entities to the scene to demonstrate how the MiniStats system works.\n\nThe example method of the MiniStatsExample class takes three parameters: a canvas element, a device type string, and a pcx object. The canvas element is used to create the PlayCanvas application. The device type string is not used in this example. The pcx object is used to access the MiniStats system.\n\nThe example method creates a PlayCanvas application and sets the canvas to fill the window. It also sets up the options for the MiniStats system. The options include the sizes of the MiniStats display, the statistics to display, and the starting size of the MiniStats display. The example method then creates the MiniStats system using the options.\n\nThe example method adds some entities to the scene to demonstrate how the MiniStats system works. The entities are created and destroyed every frame to simulate the allocation and deallocation of resources. The MiniStats system displays statistics such as frame update time, total number of draw calls, number of triangles, number of materials used, frame time for frustum culling, VRAM usage, frames per second, and delta time. The statistics are updated in real-time as the entities are created and destroyed.\n\nThe MiniStatsExample class is a useful tool for developers who want to optimize the performance of their PlayCanvas applications. By using the MiniStats system, developers can identify performance bottlenecks and optimize their code accordingly. The MiniStatsExample class can be used as a starting point for developers who want to integrate the MiniStats system into their own PlayCanvas applications.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use the MiniStats system in the PlayCanvas engine to display performance statistics in a web application.\n\n2. What are the requirements for the MiniStats system to report values for certain counters?\n- For most of the counters to report values, either a debug or profiling engine build needs to be used.\n\n3. What does the `createPrimitive` function do?\n- The `createPrimitive` function creates a primitive shape with a random color material, given a shape type, position, and scale.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/misc/mini-stats.md"}}],["121",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/misc/spineboy.tsx)\n\nThe `SpineboyExample` class is a code example that demonstrates how to use the Spine animation library with the PlayCanvas engine. The purpose of this code is to show how to load and play Spine animations in a PlayCanvas application. \n\nThe `example` method is the main entry point of the code example. It takes two parameters: a `canvas` element and a `deviceType` string. The `canvas` element is used to create a graphics device, and the `deviceType` string specifies the type of device to create (e.g., \"webgl2\", \"webgpu\", etc.). \n\nThe `assets` object contains a list of assets that are required to load and play the Spine animation. These assets include the Spine skeleton data, the Spine atlas data, the Spine texture data, and the PlayCanvas Spine script. \n\nThe `gfxOptions` object contains options for creating the graphics device. These options include the device type and the URLs for the glslang and twgsl libraries. \n\nThe `pc.createGraphicsDevice` method is used to create the graphics device. This method takes the `canvas` element and the `gfxOptions` object as parameters. Once the graphics device is created, the `pc.AppBase` class is used to create a new PlayCanvas application. \n\nThe `createOptions` object is used to configure the PlayCanvas application. It specifies the graphics device to use, as well as the component systems and resource handlers that are required for the Spine animation. \n\nThe `app.setCanvasFillMode` and `app.setCanvasResolution` methods are used to set the canvas fill mode and resolution. These methods ensure that the canvas fills the window and that the resolution is automatically adjusted to match the canvas size. \n\nThe `assetListLoader` object is used to load the Spine assets. Once the assets are loaded, the `app.start` method is called to start the PlayCanvas application. \n\nThe `createSpineInstance` method is used to create a new Spine entity. This method takes three parameters: the position of the entity, the scale of the entity, and the time scale of the animation. The Spine entity is created using the `pc.Entity` class, and the Spine component is added to the entity using the `addComponent` method. The Spine component is configured with the Spine atlas, skeleton, and texture assets. \n\nFinally, the `createSpineInstance` method is called twice to create two Spine entities. Each entity is positioned and scaled differently, and each entity plays a different Spine animation. \n\nIn summary, the `SpineboyExample` class is a code example that demonstrates how to use the Spine animation library with the PlayCanvas engine. It shows how to load and play Spine animations in a PlayCanvas application, and it provides a starting point for developers who want to integrate Spine animations into their PlayCanvas projects.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example implementation of the Spine animation system using the PlayCanvas engine.\n\n2. What external dependencies does this code have?\n- This code depends on the PlayCanvas engine, as well as the Spine animation system and associated assets.\n\n3. What is the expected output of this code?\n- The expected output of this code is a canvas element displaying two Spine animations, each with different positions, scales, and time scales.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/misc/spineboy.md"}}],["122",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/physics/compound-collision.tsx)\n\nThe `CompoundCollisionExample` class is a part of the PlayCanvas engine project and is responsible for demonstrating the use of compound collision shapes in a physics simulation. The purpose of this code is to create a scene with a chair and a ground, where the chair has a compound collision shape that is made up of several child entities. The scene is rendered using WebGL and the physics simulation is powered by the Ammo.js physics engine.\n\nThe `example` method of the `CompoundCollisionExample` class takes two arguments: a canvas element and a device type. It first sets the configuration for the Ammo.js physics engine by specifying the URLs for the JavaScript and WebAssembly files. It then creates a graphics device using the `createGraphicsDevice` method of the PlayCanvas engine, passing in the canvas element and device type. The graphics device is used to render the scene.\n\nThe `example` method then creates an instance of the Ammo.js physics engine using the `getInstance` method of the `WasmModule` class. It initializes the PlayCanvas application by creating an instance of the `AppBase` class and passing in an instance of the `AppOptions` class. The `AppOptions` class is used to specify the graphics device, keyboard, component systems, and resource handlers for the application.\n\nThe `example` method then defines a scene hierarchy in JSON format, which is loaded and parsed using the `parseScene` function. The scene hierarchy consists of a chair entity with a compound collision shape and several child entities, a ground entity with a box collision shape, a directional light entity, and a camera entity. The `parseEntity` function is used to convert an entity definition in the JSON structure to a `pc.Entity` object.\n\nThe `example` method then sets an update function on the application's update event. The update function is responsible for adding new chair entities to the scene every 250 milliseconds and changing the material of the chair entities based on whether they are active or frozen.\n\nOverall, the `CompoundCollisionExample` class demonstrates how to create a physics simulation with compound collision shapes in the PlayCanvas engine. It shows how to define a scene hierarchy in JSON format, how to convert an entity definition to a `pc.Entity` object, and how to set an update function on the application's update event.\n## Questions: \n 1. What is the purpose of the `example` method in the `CompoundCollisionExample` class?\n- The `example` method is used to run a demo of the compound collision feature of the PlayCanvas engine, using the specified canvas and device type.\n\n2. What is the significance of the `pc.WasmModule` calls in the `demo` function?\n- The `pc.WasmModule` calls are used to configure and initialize the Ammo physics engine, which is used for the compound collision feature.\n\n3. What is the purpose of the `parseEntity` and `parseScene` functions?\n- The `parseEntity` function is used to convert an entity definition in JSON format to a `pc.Entity` object, while the `parseScene` function is used to parse a scene hierarchy in JSON format and add the entities to the scene's root entity.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/physics/compound-collision.md"}}],["123",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/physics/falling-shapes.tsx)\n\nThe `FallingShapesExample` class is a demonstration of how to use the PlayCanvas engine to create a physics simulation of falling shapes. The purpose of this code is to show how to create a scene with a floor, lights, camera, and various shapes that fall from above and collide with the floor. The code uses the PlayCanvas engine to create a 3D graphics application that runs in a web browser.\n\nThe `example` method is the main entry point for the code. It takes two arguments: a canvas element and a device type. The canvas element is the HTML canvas element that the graphics will be rendered to. The device type is a string that specifies the type of graphics device to use, such as \"webgl2\" or \"webgpu\". The method sets up the physics engine and creates the scene with the floor, lights, camera, and falling shapes.\n\nThe `demo` function is called after the physics engine is set up. It loads the assets needed for the scene, creates the graphics device, and initializes the application. The `createOptions` object is used to specify the graphics device, keyboard, and component systems. The `resourceHandlers` array is used to specify the handlers for loading different types of resources, such as textures, scripts, and fonts.\n\nThe `app` object is an instance of the `pc.AppBase` class, which represents the 3D graphics application. The `setCanvasFillMode` and `setCanvasResolution` methods are used to set the canvas to fill the window and automatically change resolution to be the same as the canvas size. The `app.start` method is called to start the application.\n\nThe `createMaterial` function is used to create a material with a given color. The `red` and `gray` materials are created using this function. The `floor` entity is created with a render component, rigidbody component, and collision component. The `light` entity is created with a light component. The `camera` entity is created with a camera component. The `createTemplate` function is used to create templates for the falling shapes. The `boxTemplate`, `sphereTemplate`, `capsuleTemplate`, `cylinderTemplate`, and `meshTemplate` templates are created using this function.\n\nThe `app.on(\"update\")` method is used to set an update function that is called every frame. The update function creates a falling shape every 0.2 seconds and positions it above the floor. The `clone` of the falling shape is created by cloning a random template and enabling it. The `teleport` method is used to position the clone above the floor. The `angularVelocity` property is used to give the clone a random spin. The `isActive` method is used to determine if a rigidbody is active or frozen, and the material of the mesh instance is set accordingly.\n\nIn summary, the `FallingShapesExample` class is a demonstration of how to use the PlayCanvas engine to create a physics simulation of falling shapes. The code creates a scene with a floor, lights, camera, and various shapes that fall from above and collide with the floor. The code uses the PlayCanvas engine to create a 3D graphics application that runs in a web browser.\n## Questions: \n 1. What is the purpose of the `FallingShapesExample` class?\n- The `FallingShapesExample` class is an example of a physics simulation that creates falling shapes and demonstrates the use of the PlayCanvas engine.\n\n2. What is the significance of the `WasmModule` object and its `setConfig` and `getInstance` methods?\n- The `WasmModule` object is used to configure and instantiate the Ammo physics engine, which is used by the PlayCanvas engine. The `setConfig` method sets the configuration options for the Ammo engine, and the `getInstance` method instantiates the engine.\n\n3. What is the purpose of the `createTemplate` function and how is it used?\n- The `createTemplate` function is a helper function that creates a template for a physics collider. It is used to create templates for falling shapes such as boxes, spheres, capsules, cylinders, and meshes. These templates are then cloned and used to create the actual falling shapes in the physics simulation.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/physics/falling-shapes.md"}}],["124",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/physics/index.mjs)\n\nThe code above is a module that exports several examples from the PlayCanvas engine project. The purpose of this code is to provide developers with examples of how to use different features of the engine. \n\nThe `import` statements at the beginning of the code import different example files from the project. These files contain code that demonstrates how to use specific features of the engine. For example, the `CompoundCollisionExample` file demonstrates how to use compound collision shapes, while the `VehicleExample` file demonstrates how to create a vehicle using the engine.\n\nThe `export` statement at the end of the code exports these example files as named exports. This means that other modules in the project can import these examples and use them in their own code. For example, a developer working on a game that uses the PlayCanvas engine could import the `VehicleExample` and use it as a starting point for creating their own vehicle.\n\nHere is an example of how a developer might use the `VehicleExample`:\n\n```\nimport { VehicleExample } from \"playcanvas-examples\";\n\n// Create a new instance of the VehicleExample\nconst vehicle = new VehicleExample();\n\n// Add the vehicle to the scene\napp.root.addChild(vehicle.entity);\n\n// Update the vehicle every frame\napp.on(\"update\", function (deltaTime) {\n    vehicle.update(deltaTime);\n});\n```\n\nIn summary, this code provides developers with examples of how to use different features of the PlayCanvas engine. These examples can be imported and used in other modules in the project.\n## Questions: \n 1. **What is the purpose of this code file?**\\\nThis code file exports several examples from the PlayCanvas engine related to compound collision, offset collision, falling shapes, raycasting, and vehicle simulation.\n\n2. **Where are the actual implementations of these examples located?**\\\nThe code file imports the examples from separate files located in the same directory, such as `compound-collision.js` and `falling-shapes.js`.\n\n3. **How can a developer use these examples in their own project?**\\\nA developer can import the desired example(s) from this code file into their own project using the `import` statement, such as `import { CompoundCollisionExample } from \"playcanvas-engine/examples\";`. They can then use the example code as a reference or starting point for their own implementation.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/physics/index.md"}}],["125",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/physics/offset-collision.tsx)\n\nThe `OffsetCollisionExample` class is a part of the PlayCanvas engine project and is responsible for demonstrating how to use the engine's physics and animation systems to create a simple game. The purpose of this code is to create a 3D environment where a character model can move around and interact with other objects in the scene. \n\nThe `example` method is the main entry point of the class and takes three arguments: a canvas element, a device type, and some data. The canvas element is used to render the scene, the device type specifies the type of graphics device to use, and the data argument is not used in this example. \n\nThe `demo` function is called after the Ammo physics engine is loaded and initializes the scene by creating assets, setting up the graphics device, and creating an instance of the PlayCanvas application. The `createOptions` object is used to specify the component systems and resource handlers that will be used by the application. \n\nThe `app` object is used to manage the scene and its entities. The `setCanvasFillMode` and `setCanvasResolution` methods are used to set the canvas to fill the window and automatically change resolution to be the same as the canvas size. \n\nThe `createMaterial` function is used to create a new material with a specified color. The `red` and `gray` materials are used to create the floor and other objects in the scene. \n\nThe `floor` entity is created with a render component, a rigidbody component, and a collision component. The `modelEntity` entity is created from a loaded model using the render component and an anim component is added to it. The `animStateGraphData` object is used to create an animation state graph that is loaded into the anim component. \n\nThe `cameraEntity` entity is created with a camera component and is positioned to look at the `modelEntity`. The `ball` entity is created as a template that can be cloned in the update loop. \n\nThe `app.on(\"update\")` function is used to create a falling ball every 0.2 seconds and to show active and frozen bodies in red and gray, respectively. The `app.scene.immediate.drawWireSphere` method is used to render the offset collision of the `modelEntity`. \n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a simple 3D game with physics and animation. It shows how to create entities with different components, how to load assets, and how to use the update loop to create dynamic behavior.\n## Questions: \n 1. What is the purpose of the `example` method in this code?\n- The `example` method is a function that sets up a PlayCanvas application and creates a demo scene with physics and collision components.\n\n2. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean flag that indicates whether the demo should use the WebGPU graphics API instead of WebGL.\n\n3. What is the `createMaterial` function used for?\n- The `createMaterial` function is used to create a new `StandardMaterial` with a specified diffuse color, which is used to render objects in the demo scene.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/physics/offset-collision.md"}}],["126",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/physics/raycast.tsx)\n\nThe `RaycastExample` class is a demonstration of how to use the PlayCanvas engine to perform raycasting in a 3D environment. Raycasting is a technique used in 3D graphics to determine the intersection of a ray with an object in a scene. This is useful for a variety of applications, such as detecting collisions between objects or selecting objects with a mouse click.\n\nThe `example` method of the `RaycastExample` class takes two arguments: a canvas element and a device type. It first sets the configuration for the Ammo physics engine, which is used for the raycasting. It then creates a graphics device using the `createGraphicsDevice` method, which initializes the WebGL context and sets up the rendering pipeline. The `AppOptions` object is used to specify various options for the application, such as the graphics device, keyboard input, and component systems. The `AppBase` object is then created and initialized with the options.\n\nThe `createPhysicalShape` function is used to create physical shapes in the scene, such as boxes, capsules, cones, cylinders, and spheres. These shapes are given a material and a rigidbody component, which allows them to interact with other objects in the scene. The `createMaterial` function is used to create a material with a given color.\n\nThe `app.on(\"update\")` event is used to set an update function that is called every frame. This function first resets all shapes to green, then creates a ray from a starting point to an ending point and renders it in white. It then performs a raycast using the `raycastFirst` method, which returns the first object that the ray intersects with. If an object is found, its material is changed to red and a normal vector is rendered in blue. The same process is repeated for the `raycastAll` method, which returns all objects that the ray intersects with.\n\nFinally, the `createText` function is used to create text elements in the scene, which display the names of the raycasting methods. These text elements are added to the root entity of the application.\n\nOverall, the `RaycastExample` class demonstrates how to use the PlayCanvas engine to perform raycasting in a 3D environment. It shows how to create physical shapes, materials, and text elements, and how to use the `raycastFirst` and `raycastAll` methods to detect collisions between objects. This code can be used as a starting point for building more complex 3D applications that require raycasting functionality.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use raycasting with the PlayCanvas engine for physics simulations.\n\n2. What external libraries or dependencies does this code rely on?\n- This code relies on the Ammo library for physics simulation and the glslang and twgsl libraries for graphics rendering.\n\n3. What is the expected output or behavior of this code?\n- The expected behavior of this code is to create a physics simulation with various physical shapes and perform raycasting to detect collisions and render the results. The output should be a visual representation of the simulation with text labels for the different types of raycasting used.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/physics/raycast.md"}}],["127",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/physics/vehicle.tsx)\n\nThe code is an example of a vehicle simulation in the PlayCanvas engine. The purpose of the code is to demonstrate how to create a physical vehicle with wheels, a chassis, and a cab, and how to add it to a scene. The code also shows how to create a ground shape for the vehicle to drive on, how to create a wall of blocks for the vehicle to smash through, and how to add a camera and a directional light source to the scene.\n\nThe code imports the PlayCanvas engine and defines a class called VehicleExample. The class has a static property called CATEGORY, which is set to 'Physics', and a static property called NAME, which is set to 'Vehicle'. The class also has a static property called WEBGPU_ENABLED, which is set to true.\n\nThe class has a method called example, which takes two parameters: a canvas element and a device type. The method sets the configuration for the Ammo physics engine, which is used by the PlayCanvas engine. The method then creates an instance of the Ammo physics engine and loads the assets needed for the scene.\n\nThe method creates a graphics device for the canvas element and initializes the PlayCanvas application with the graphics device. The method sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. The method then creates a ground shape for the vehicle to drive on, creates four wheels for the vehicle, creates a physical vehicle with a chassis and a cab, adds the vehicle to the scene, and builds a wall of blocks for the vehicle to smash through.\n\nThe method also creates a directional light source, a camera to render the scene, and enables rendering and resetting of all rigid bodies in the scene. Finally, the method adds an event listener to the keyboard to reset the scene when the 'R' key is pressed.\n\nThe code can be used as a starting point for creating a vehicle simulation in the PlayCanvas engine. Developers can modify the code to create different types of vehicles, add different types of obstacles, and customize the camera and light sources. The code can also be used to learn how to use the PlayCanvas engine and the Ammo physics engine.\n## Questions: \n 1. What is the purpose of the `VehicleExample` class?\n- The `VehicleExample` class is an example of a physics-based vehicle simulation that can be run on the PlayCanvas engine.\n\n2. What is the significance of the `WasmModule` object and its methods?\n- The `WasmModule` object is used to configure and instantiate the Ammo physics engine, which is used for the vehicle simulation. Its `setConfig` method is used to specify the URLs for the Ammo WebAssembly files, and its `getInstance` method is used to create an instance of the Ammo engine.\n\n3. What components and systems are used to create the vehicle and its environment?\n- The code uses various components and systems, including `rigidbody`, `collision`, `script`, `light`, and `camera` components, as well as `ModelComponentSystem`, `CameraComponentSystem`, `LightComponentSystem`, `ScriptComponentSystem`, `CollisionComponentSystem`, and `RigidBodyComponentSystem`. These are used to create the vehicle, its wheels, the ground, the blocks, the skydome, the lighting, and the camera.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/physics/vehicle.md"}}],["128",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/sound/index.mjs)\n\nThis code exports the `PositionalExample` class from the `positional.js` file located in the same directory. \n\nThe `PositionalExample` class likely contains functionality related to positioning objects within the PlayCanvas engine. By exporting this class, it can be used in other parts of the project to manipulate the position of objects in the game world.\n\nFor example, if we wanted to create a new object and set its position using the `PositionalExample` class, we could do the following:\n\n```\nimport { PositionalExample } from \"playcanvas-engine\";\n\nconst myObject = new PositionalExample();\nmyObject.setPosition(0, 0, 0);\n```\n\nThis code would create a new instance of the `PositionalExample` class and set its position to (0, 0, 0) in the game world.\n\nOverall, this code is a simple example of how modules can be used in the PlayCanvas engine to organize and reuse code across different parts of a project. By exporting classes and functions from one file, they can be easily imported and used in other parts of the project without having to rewrite the same code multiple times.\n## Questions: \n 1. **What is the purpose of the `PositionalExample` module?** \nThe `PositionalExample` module is being imported from a file located at `./positional` and then exported for use elsewhere in the project. It is unclear what functionality this module provides without examining the code in the `positional` file.\n\n2. **Are there any other modules being exported from this file?** \nNo, this file is only exporting the `PositionalExample` module. It is possible that other modules are being exported from other files in the project.\n\n3. **What is the overall purpose of the PlayCanvas engine project?** \nWithout additional context, it is unclear what the PlayCanvas engine project is intended to do. It is possible that it is a game engine or a web development framework, but this cannot be determined from the provided code snippet alone.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/sound/index.md"}}],["129",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/sound/positional.tsx)\n\nThe `PositionalExample` class is a code example that demonstrates how to use the PlayCanvas engine to create a 3D scene with a walking character that emits positional audio. The purpose of this code is to showcase the capabilities of the PlayCanvas engine and provide developers with a starting point for creating similar projects.\n\nThe `example` method is the main entry point of the code and takes two parameters: a `canvas` element and a `deviceType` string. It creates a new `pc.Application` instance with the given `canvas` element and an empty options object. It then creates a list of assets that are required for the scene, including a 3D model of a character, an animation for the character, and an audio file for the footsteps.\n\nThe `AssetListLoader` class is used to load all the assets asynchronously. Once the assets are loaded, the code creates a camera entity, a ground entity, a light entity, and a walking character entity. The camera entity is positioned and rotated to provide a good view of the scene. The ground entity is a simple box with a gray material. The light entity is a directional light that casts shadows. The walking character entity is created with a 3D model, an animation, and a sound component.\n\nThe walking character entity is positioned and rotated in a circular path around the center of the scene. The `update` event is used to update the position and rotation of the character entity on every frame. The `setLocalPosition` and `setLocalEulerAngles` methods are used to set the position and rotation of the character entity relative to its parent entity.\n\nThe `sound` component of the character entity is used to emit positional audio for the footsteps. The `addSlot` method is used to add a new sound slot with the `gravel` audio asset. The `pitch` property is set to 1.7 to make the footsteps sound faster. The `loop` property is set to true to make the sound loop continuously. The `autoPlay` property is set to true to make the sound start playing automatically.\n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a 3D scene with a walking character that emits positional audio. It shows how to load assets, create entities, position and rotate entities, and use the sound component to emit audio. This code can be used as a starting point for creating similar projects with the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `PositionalExample` class?\n- The `PositionalExample` class is an example implementation of a sound feature in the PlayCanvas engine.\n\n2. What assets are being loaded in the `example` method?\n- The `example` method loads a 3D model, an animation, and an audio file.\n\n3. What is the purpose of the `update` event listener?\n- The `update` event listener updates the position and rotation of the walking dude entity in the scene.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/sound/positional.md"}}],["130",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/button-basic.tsx)\n\nThe `ButtonBasicExample` class is a code example that demonstrates how to create a basic button using the PlayCanvas engine. The purpose of this code is to show how to create a button entity, add components to it, and handle user input events. \n\nThe `example` method is the main entry point of the code example. It takes two parameters: a `canvas` element and a `deviceType` string. The `canvas` element is used to create a graphics device, which is required to render the scene. The `deviceType` string specifies the type of graphics device to create, such as \"webgl2\" or \"webgpu\". \n\nThe `assets` object contains a single font asset that is used to render the text on the button. The `gfxOptions` object specifies the options for creating the graphics device, such as the GLSLang and TWGSL URLs. \n\nThe `pc.createGraphicsDevice` method is used to create a graphics device from the `canvas` element and `gfxOptions`. Once the graphics device is created, an `AppOptions` object is created to specify the options for creating the PlayCanvas application. This includes the graphics device, input devices, component systems, and resource handlers. \n\nThe `pc.AppBase` class is used to create a new PlayCanvas application instance. The `app.init` method is called to initialize the application with the `createOptions` object. The `app.setCanvasFillMode` and `app.setCanvasResolution` methods are used to set the canvas fill mode and resolution. \n\nThe `assetListLoader` object is used to load the font asset. Once the asset is loaded, the `app.start` method is called to start the application. \n\nThe `camera` entity is created to represent the camera in the scene. The `screen` entity is created to represent the 2D screen. The `button` entity is created to represent the button. The `label` entity is created to represent the label on the button. \n\nThe `button.addComponent` method is used to add a `button` component to the `button` entity. This component is used to handle user input events, such as clicks. The `button.addComponent` method is also used to add an `element` component to the `button` entity. This component is used to render the button image. \n\nThe `label.addComponent` method is used to add an `element` component to the `label` entity. This component is used to render the text on the button. \n\nThe `button.button.on` method is used to add an event listener to the button component. This listener is called every time the button is clicked. When the button is clicked, the background color of the camera is changed to a random color. \n\nOverall, this code example demonstrates how to create a basic button using the PlayCanvas engine. It shows how to create entities, add components to them, and handle user input events. This code can be used as a starting point for creating more complex user interfaces in PlayCanvas applications.\n## Questions: \n 1. What does this code do?\n- This code defines a class called `ButtonBasicExample` that has a static method called `example` which creates a PlayCanvas app that displays a button and changes the background color of the app when the button is clicked.\n\n2. What external dependencies does this code have?\n- This code imports the entire PlayCanvas engine library from a relative path and uses it to create a PlayCanvas app.\n\n3. What is the purpose of the `gfxOptions` object?\n- The `gfxOptions` object is used to configure the graphics device that is created by the `pc.createGraphicsDevice` function. It specifies the type of device to create and the URLs of the glslang and twgsl libraries that are used by the device.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/button-basic.md"}}],["131",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/button-sprite.tsx)\n\nThe `ButtonSpriteExample` class is a code example that demonstrates how to create a simple button with a label and sprite animation in the PlayCanvas engine. The purpose of this code is to provide developers with a working example of how to create a user interface element in PlayCanvas. \n\nThe `example` method takes two parameters: a canvas element and a device type string. It creates a graphics device using the `pc.createGraphicsDevice` method, which initializes the WebGL context and returns a `pc.GraphicsDevice` object. It then creates an instance of `pc.AppBase` and initializes it with the graphics device and other options. \n\nThe code then loads two assets: a font and a texture atlas for the button sprite. It creates a camera and a 2D screen entity, and adds a button entity as a child of the screen entity. The button entity has a `button` component and an `element` component. The `button` component handles the button's behavior, such as changing the sprite animation and triggering events when clicked or pressed. The `element` component defines the button's position, size, and appearance. \n\nThe code also creates a label entity as a child of the button entity. The label entity has an `element` component that defines the label's position, size, and appearance. The label's text is set to \"CLICK ME\". \n\nThe code then sets up event listeners for the button's `click` and `pressedstart`/`pressedend` events. When the button is clicked, the camera's clear color is set to a random color. When the button is pressed, the label is moved up and down by changing its local position. \n\nFinally, the code creates a `pc.TextureAtlas` object and sets its frames to the sprite animation frames. It creates a sprite asset for each frame using the `pc.Sprite` class, and assigns the sprite assets to the button's `element` and `button` components. \n\nThis code example can be used as a starting point for creating more complex user interface elements in PlayCanvas. Developers can modify the code to customize the appearance and behavior of the button and label entities, and add additional entities and components as needed. \n\nExample usage:\n\n```javascript\nconst canvas = document.getElementById('application-canvas');\nconst deviceType = 'webgl2';\n\nconst example = new ButtonSpriteExample();\nexample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to create a simple button with a label and change the background color every time the button is clicked using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the PlayCanvas engine using the wildcard syntax and uses HTMLCanvasElement, pc, and pc.GraphicsDevice.\n\n3. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean that indicates whether the example can be run using the WebGPU API.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/button-sprite.md"}}],["132",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/custom-shader.tsx)\n\nThe `CustomShaderExample` class is a code example that demonstrates how to create a custom shader for a PlayCanvas engine project. The purpose of this code is to show how to create a custom shader that can be used for simple UI shaders. The code example includes a vertex shader and a fragment shader that are used to create a new material with an additive alpha blending effect. The material is then applied to a UI image element.\n\nThe `CustomShaderExample` class has a static `example` method that takes three parameters: a canvas element, a device type, and an object containing the vertex and fragment shader code. The method creates a new `pc.GraphicsDevice` object using the `pc.createGraphicsDevice` method and initializes a new `pc.AppBase` object with the graphics device. It then creates a camera and a 2D screen entity and adds them to the app's root entity. The method then creates a new shader from the vertex and fragment shader code using the `pc.createShaderFromCode` method and creates a new material with the shader and additive alpha blending. Finally, the method creates a new UI image element with the custom material and adds it to the screen entity.\n\nThe `example` method also updates the material's `amount` parameter to animate the inverse effect. It does this by adding an event listener to the app's `update` event and updating the material's `amount` parameter with a sine wave that varies from 0 to 1.\n\nThis code example can be used as a starting point for creating custom shaders for PlayCanvas engine projects. Developers can modify the vertex and fragment shader code to create custom effects and apply them to UI elements or other entities in their projects.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a custom shader for a user interface in the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library and requires an HTML canvas element, a device type string, and two shader files as input.\n\n3. What does this code do?\n- This code creates a custom shader for a UI image element using a vertex and fragment shader, sets up a camera and screen, and animates the inverse effect of the image using a sine wave.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/custom-shader.md"}}],["133",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/index.mjs)\n\nThis code exports a set of examples for various features of the PlayCanvas engine. The purpose of this code is to provide developers with examples of how to use different features of the engine in their projects. \n\nThe code imports several example modules, each of which demonstrates a different feature of the engine. These features include basic button functionality, custom shaders, particle systems, and text manipulation. \n\nThe `export` statement at the end of the code exports all of the imported modules, making them available for use in other parts of the project. \n\nFor example, a developer working on a project that requires text manipulation could import the `TextAutoFontSizeExample`, `TextEmojisExample`, `TextLocalizationExample`, `TextTypewriterExample`, and `TextExample` modules to see how to implement these features in their own project. \n\nSimilarly, a developer working on a project that requires UI elements could import the `ButtonBasicExample`, `ButtonSpriteExample`, `LayoutGroupExample`, `ScrollViewExample`, and `WorldUiExample` modules to see how to create and manipulate UI elements in their project. \n\nOverall, this code serves as a valuable resource for developers working with the PlayCanvas engine, providing them with examples of how to use various features of the engine in their own projects.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file exports a list of examples for various features in the PlayCanvas engine.\n\n2. How are these examples intended to be used?\n- These examples can be imported and used as reference or starting points for implementing the corresponding features in a PlayCanvas project.\n\n3. Are there any dependencies or requirements for using these examples?\n- It is not clear from this code file whether there are any dependencies or requirements for using these examples. Further documentation or context may be needed to answer this question.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/index.md"}}],["134",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/layout-group.tsx)\n\nThe `LayoutGroupExample` class is a part of the PlayCanvas engine project and is responsible for creating a layout group entity that can be used to organize and position child entities in a specific way. The purpose of this code is to demonstrate how to create a layout group entity and add child entities to it. \n\nThe `example` method takes two parameters, a canvas element and a device type. It creates a graphics device using the `pc.createGraphicsDevice` method and initializes a new PlayCanvas application using the `pc.AppBase` class. It then sets the canvas to fill the window and creates a camera and a 2D screen entity. \n\nThe `LayoutGroupExample` class creates a new entity called `group` and adds it to the screen entity. The `group` entity is a layout group entity that is used to organize and position child entities. It has a `layoutgroup` component that specifies the orientation, spacing, and fitting of the child entities. The `group` entity also has an `element` component that specifies the width and height of the layout group entity. \n\nThe `example` method then creates 15 child entities and adds them to the `group` entity. Each child entity has an `element` component that specifies its color, type, and position. The child entities also have a `layoutchild` component that specifies whether they should be excluded from the layout. \n\nThis code can be used in the larger PlayCanvas project to create user interfaces that require organized and positioned child entities. The `LayoutGroupExample` class can be extended and modified to create different types of layout group entities with different orientations, spacing, and fitting. \n\nExample usage:\n\n```\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'webgl2';\n\nconst layoutGroupExample = new LayoutGroupExample();\nlayoutGroupExample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use the Layout Group component in the PlayCanvas engine to create a group of child elements that can be arranged in a specific layout.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas engine using the wildcard import syntax, and it also relies on external assets such as a font file and external libraries for graphics options.\n\n3. What is the expected output of this code?\n- The expected output of this code is a canvas element with a group of child elements arranged in a specific layout using the Layout Group component. The child elements are randomly colored panels with text labels.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/layout-group.md"}}],["135",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/particle-system.tsx)\n\nThe code defines a class called `ParticleSystemExample` that demonstrates how to create a particle system in the PlayCanvas engine. The `example` method of this class takes two parameters: a canvas element and a device type. It creates a graphics device using the `createGraphicsDevice` method of the PlayCanvas engine, and initializes an app using the `AppBase` class. The app is set to fill the window and automatically change resolution to be the same as the canvas size. \n\nThe method then loads two assets: a font and a texture. It creates a camera, a 2D screen, a panel, and a label for the panel. It also creates an entity for the particle system, and inserts it as a child of the panel. The particle system is set to render in the UI layer, and its properties are defined using curves. The method then adds a particle system component to the entity, and sets its properties. Finally, it sets up an event listener to move the buttons along a circular path.\n\nThis code can be used as a starting point for creating a particle system in a PlayCanvas project. Developers can modify the properties of the particle system to achieve different effects, and add it to their own entities. They can also use the `AppBase` class to create an app that fills the window and automatically changes resolution to be the same as the canvas size. The `createGraphicsDevice` method can be used to create a graphics device for the app, and the `Asset` class can be used to load assets. The `Entity` class can be used to create entities, and the `addComponent` method can be used to add components to entities. The `Curve` and `CurveSet` classes can be used to define curves for particle system properties.\n## Questions: \n 1. What is the purpose of the `example` method in the `ParticleSystemExample` class?\n- The `example` method is used to create a particle system example in a 2D screen using the PlayCanvas engine.\n\n2. What assets are being loaded in the `assets` object and how are they being used?\n- The `assets` object is loading a font and a texture asset, which are being used to create a label and a particle system component respectively.\n\n3. What is the purpose of the `UILayer` variable and how is it being used?\n- The `UILayer` variable is used to specify the layer on which the particle system will be rendered, and it is being added to the `layers` array of the particle system component.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/particle-system.md"}}],["136",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/scroll-view.tsx)\n\nThe code defines a class called `ScrollViewExample` that demonstrates how to use the PlayCanvas engine to create a scroll view control. The `example` method of this class takes an HTML canvas element and a device type as input, and creates a PlayCanvas application that renders the scroll view control on the canvas.\n\nThe `example` method first creates a graphics device using the `pc.createGraphicsDevice` method, which takes the canvas and some graphics options as input. It then initializes a PlayCanvas application using the `pc.AppBase` constructor and some application options. The application is set to fill the window and automatically change resolution to be the same as the canvas size.\n\nThe method then loads some assets, including a font, using the `pc.Asset` constructor and an `pc.AssetListLoader` object. It creates a camera and a 2D screen using `pc.Entity` and `addComponent` methods. It also defines a function called `createScrollbar` that creates a scrollbar entity with a handle and adds it to the scroll view control.\n\nThe method then creates some text content using another `pc.Entity` and `addComponent` methods. It groups the content inside the scroll view's viewport using another `pc.Entity` and `addComponent` methods. It creates a scroll view viewport entity and adds the content group, horizontal scrollbar, and vertical scrollbar entities to it. It then creates a scroll view entity and adds the viewport, horizontal scrollbar, and vertical scrollbar entities to it. Finally, it adds the scroll view entity to the screen entity and adds a `scrollview` component to it.\n\nThe `scrollview` component takes several options as input, including the content entity, horizontal and vertical scrollbar entities, and the viewport entity. It also specifies the scroll mode, friction, and bounce amount of the scroll view control. The `example` method then starts the PlayCanvas application and the scroll view control is rendered on the canvas.\n\nThis code can be used as a starting point for creating a scroll view control in a PlayCanvas project. Developers can modify the code to customize the appearance and behavior of the scroll view control to suit their needs. For example, they can change the font, text content, and scrollbar appearance, or add event listeners to handle user input.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example implementation of a scroll view control using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the PlayCanvas engine from a relative path and uses external libraries for glslang and twgsl.\n\n3. What components and systems are used in this code?\n- This code uses various component systems such as RenderComponentSystem, CameraComponentSystem, ScreenComponentSystem, ButtonComponentSystem, ElementComponentSystem, LayoutGroupComponentSystem, ScrollViewComponentSystem, and ScrollbarComponentSystem. It also uses entities with element and scrollbar components.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/scroll-view.md"}}],["137",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/text-auto-font-size.tsx)\n\nThe code is an example of how to use the PlayCanvas engine to create a UI element with text that automatically adjusts its font size to fit within its container. The example is called \"Text Auto Font Size\" and is located in the \"User Interface\" category. The code imports the PlayCanvas engine and defines a class called \"TextAutoFontSizeExample\". \n\nThe \"example\" method of this class takes an HTML canvas element and a device type as input. It then creates a graphics device using the \"createGraphicsDevice\" method of the PlayCanvas engine, passing in the canvas and device options. It also creates an \"AppOptions\" object that specifies the graphics device, mouse, touch, and element input. \n\nThe code then creates a new PlayCanvas application using the \"AppBase\" constructor and initializes it with the options. It sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. \n\nThe code then loads a font asset and creates a camera and a 2D screen entity. It also creates a container entity with an image component and a text element with auto font size, and places it inside the container. The text element is set up to take the entire parent space and adjust its font size to fit within the container. \n\nFinally, the code updates the container's size to showcase the auto-sizing feature. It does this by adding an event listener for the \"update\" event and changing the container's width and height based on a sine wave over time. \n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a UI element with text that automatically adjusts its font size to fit within its container. It shows how to create a graphics device, initialize a PlayCanvas application, load assets, create entities, and update them over time. This example can be used as a starting point for creating more complex UI elements with the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use the PlayCanvas engine to create a 2D screen with a container entity that has an auto-sizing text element.\n\n2. What external dependencies does this code have?\n- This code imports the PlayCanvas engine from a relative path and uses two external URLs for glslang and twgsl.\n\n3. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean that indicates whether the example is compatible with the WebGPU API, which is a new graphics API for the web that is currently in development.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/text-auto-font-size.md"}}],["138",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/text-emojis.tsx)\n\nThe `TextEmojisExample` class is a code example that demonstrates how to use PlayCanvas engine to render text with emojis using the `CanvasFont` class. The purpose of this code is to show how to create a 2D screen, add text entities to it, and use a canvas font asset to render text with emojis.\n\nThe `example` method takes two parameters: a canvas element and a device type. It creates a graphics device using the `createGraphicsDevice` method, initializes an app using the `AppBase` class, and sets the canvas to fill the window and automatically change resolution to be the same as the canvas size. It then creates a camera and a 2D screen, adds them to the app's root entity, and creates text entities using the `createText` method. The `createText` method takes a y-coordinate and a string of text as parameters, creates a new entity, adds an `element` component to it, sets its font to the `CanvasFont` asset, and adds it to the screen entity.\n\nThe `CanvasFont` asset is created using the `new pc.CanvasFont` constructor, which takes an app instance and an options object as parameters. The options object specifies the font name, font size, and texture size. The `createTextures` method is called to create the texture atlas for the first string of text, and the `updateTextures` method is called to update the texture atlas for the other two strings of text.\n\nThe code also creates a layout group entity and adds it to the screen entity. The layout group entity has a `layoutgroup` component that specifies its orientation, width and height fitting, and wrapping. It creates a child entity for each texture in the `CanvasFont` asset and adds an `element` component and a `layoutchild` component to each child entity. The `element` component specifies the texture and type of the child entity, and the `layoutchild` component specifies whether the child entity should be excluded from the layout.\n\nOverall, this code example demonstrates how to use PlayCanvas engine to render text with emojis using the `CanvasFont` class and how to create a 2D screen and a layout group entity. It can be used as a reference for developers who want to add text with emojis to their PlayCanvas projects.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use PlayCanvas engine to create a canvas font asset that supports emojis and render it on a 2D screen.\n\n2. What external dependencies does this code have?\n- This code imports the entire PlayCanvas engine library from a relative path and uses two external URLs for glslang and twgsl.\n\n3. What is the expected output of this code?\n- The expected output of this code is a 2D screen with three lines of text, each containing different emojis rendered using a canvas font asset. Additionally, there is a debug text and a layout group entity with child elements that display the texture atlases of the canvas font asset.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/text-emojis.md"}}],["139",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/text-localization.tsx)\n\nThe `TextLocalizationExample` class is a code example that demonstrates how to use the PlayCanvas engine to create a localized text user interface. The purpose of this code is to show how to create a basic text element, a camera, and a 2D screen, and how to use the PlayCanvas engine's i18n (internationalization) system to change the text displayed on the screen based on the user's locale.\n\nThe `example` method takes two parameters: a canvas element and a device type. It creates a graphics device using the `pc.createGraphicsDevice` method, which takes the canvas and a set of graphics options as parameters. It then creates an `AppBase` instance using the graphics device and a set of app options. The app options include the component systems and resource handlers that the app will use.\n\nThe `example` method then loads the font asset and starts the app. It adds data to the i18n system that maps the \"HELLO\" key to a message in each of the supported locales. It creates a camera, a 2D screen, and a basic text element, and adds them to the app's root entity. It also creates four buttons, each with a label that corresponds to a supported locale. When a button is clicked, the i18n system's locale is changed to the locale corresponding to the button's label.\n\nThe `createButton` function creates a button entity with an image and a label. The label's text corresponds to the locale that the button represents. When the button is clicked, the i18n system's locale is changed to the locale corresponding to the button's label.\n\nThis code example demonstrates how to use the PlayCanvas engine's i18n system to create a localized text user interface. It can be used as a starting point for creating more complex user interfaces that support multiple languages.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of text localization in the PlayCanvas engine, which allows developers to create multilingual applications.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas engine, as well as the pc module from a relative path.\n\n3. What is the expected output of this code?\n- This code creates a canvas and populates it with a text element and four buttons, each of which changes the locale of the text element when clicked. The text element displays a greeting in the selected language.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/text-localization.md"}}],["140",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/text-typewriter.tsx)\n\nThe `TextTypewriterExample` class is a code example that demonstrates how to create a typewriter effect for text in a PlayCanvas application. The example creates a 2D screen with a text element that wraps text over several lines. The text is initially empty, and the example uses a timer to render a new character every 75ms until the entire text is displayed. \n\nThe example uses the PlayCanvas engine to create a graphics device, initialize an application, and create entities for the camera, screen, and text. The `createGraphicsDevice` method creates a graphics device for the specified canvas element and device type. The `AppBase` class initializes the application with the specified options, including the graphics device, mouse, touch, and element input. The `setCanvasFillMode` and `setCanvasResolution` methods set the canvas to fill the window and automatically change resolution to be the same as the canvas size. \n\nThe `Asset` class is used to load the font asset, which is used by the text element. The `AssetListLoader` class is used to load the font asset and start the application when the asset is loaded. The `Entity` class is used to create entities for the camera, screen, and text. The `addComponent` method is used to add components to the entities, including the camera component, screen component, and element component. The `pc.Color` class is used to set the clear color for the camera component. \n\nThe `TextTypewriterExample` class is exported as the default export of the module, which allows it to be imported and used in other modules. The `CATEGORY`, `NAME`, and `WEBGPU_ENABLED` static properties are used to categorize the example and indicate whether it is compatible with the WebGPU API. \n\nExample usage:\n\n```javascript\nimport TextTypewriterExample from 'path/to/TextTypewriterExample';\n\nconst canvas = document.getElementById('application-canvas');\nconst deviceType = 'webgl2';\n\nconst example = new TextTypewriterExample();\nexample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a text typewriter feature in the PlayCanvas engine, which renders a new character of a given text every 75ms.\n\n2. What dependencies does this code have?\n- This code imports the PlayCanvas engine module and uses HTMLCanvasElement, pc.Asset, pc.GraphicsDevice, pc.Mouse, pc.TouchDevice, pc.ElementInput, pc.AppOptions, pc.AppBase, pc.Entity, pc.Color, and pc.Vec2 classes.\n\n3. What is the significance of the `WEBGPU_ENABLED` property?\n- The `WEBGPU_ENABLED` property is a boolean flag that indicates whether the example supports the WebGPU API, which is a new graphics rendering API that provides better performance and efficiency than WebGL.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/text-typewriter.md"}}],["141",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/text.tsx)\n\nThe `TextExample` class is a code example that demonstrates how to use the PlayCanvas engine to create text elements in a 2D screen. The purpose of this code is to show how to create different types of text elements, such as basic text, markup text with wrap, text with outline, and text with drop shadow. \n\nThe `example` method takes two parameters: a canvas element and a device type. It creates a graphics device using the `createGraphicsDevice` method and initializes a new PlayCanvas application using the `AppBase` class. It then loads the required assets, starts the application, and creates a camera and a 2D screen. Finally, it creates four text entities and adds them to the screen entity.\n\nThe `TextExample` class is useful for developers who want to create text elements in their PlayCanvas projects. The code demonstrates how to use the `element` component to create different types of text elements with various properties, such as font size, color, outline, and shadow. Developers can use this code as a starting point to create their own text elements with custom properties.\n\nHere is an example of how to use the `TextExample` class:\n\n```javascript\nimport TextExample from './TextExample.js';\n\nconst canvas = document.getElementById('application-canvas');\nconst deviceType = 'webgl2'; // or 'webgl1' or 'webgpu'\n\nconst textExample = new TextExample();\ntextExample.example(canvas, deviceType);\n```\n\nThis code creates a new instance of the `TextExample` class and calls the `example` method with a canvas element and a device type. The `example` method creates text elements and adds them to the screen entity, which is displayed on the canvas. Developers can modify the code to create their own text elements with custom properties.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use the PlayCanvas engine to create text elements with different styles and effects.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library, and also requires external font and shader files.\n\n3. What is the expected output of this code?\n- The expected output of this code is a canvas element with four text elements, each with different styles and effects applied.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/text.md"}}],["142",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/world-to-screen.tsx)\n\nThe `WorldToScreenExample` class is a code example that demonstrates how to convert a coordinate in world space into a screen's space using the PlayCanvas engine. The purpose of this code is to show how to create a 3D world with entities that have 2D UI elements attached to them, such as text and health bars, that follow the entities as they move around the world. \n\nThe `example` method takes an HTML canvas element and a device type as input parameters. It creates a graphics device using the `pc.createGraphicsDevice` method and initializes a PlayCanvas application using the `pc.AppBase` class. It then creates several entities, including a camera, a ground plane, a light, and a 2D screen. The `createPlayer` function is called three times to create three player entities with different starting angles, speeds, and radii. Each player entity has a capsule render component and a text element that hovers above its head. \n\nThe `worldToScreenSpace` function is used to convert a world-space coordinate to a screen-space coordinate. It takes a `pc.Vec3` representing the world-space coordinate, a `pc.CameraComponent`, and a `pc.ScreenComponent` as input parameters. It returns a `pc.Vec3` representing the input world position relative to the camera and screen. The `createPlayer` function uses this function to update the position of the player's text element every frame so that it always hovers above the player's head. \n\nThe `WorldToScreenExample` class is a useful code example for developers who want to create 3D worlds with 2D UI elements that follow entities as they move around the world. It demonstrates how to use the PlayCanvas engine to create a graphics device, initialize an application, create entities with different components, and convert world-space coordinates to screen-space coordinates. Developers can use this code as a starting point for their own projects or modify it to suit their specific needs.\n## Questions: \n 1. What does this code do?\n- This code is an example of how to convert a coordinate in world space into a screen's space using the PlayCanvas engine. It creates a 3D scene with a camera, ground, light, and three player entities that move around and display UI elements above their heads.\n\n2. What external dependencies does this code have?\n- This code imports the PlayCanvas engine using a relative path, and it also loads two external assets: a checkboard texture and a Courier font.\n\n3. What is the purpose of the `worldToScreenSpace` function?\n- The `worldToScreenSpace` function takes a 3D coordinate in world space and converts it to a 2D coordinate in screen space, relative to a camera and a screen. It also takes into account the pixel ratio and screen scaling to ensure accurate positioning. This function is used to position the UI elements above the player entities' heads.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/world-to-screen.md"}}],["143",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/user-interface/world-ui.tsx)\n\nThe `WorldUiExample` class is a code example that demonstrates how to create a 3D world screen with user interface (UI) elements using the PlayCanvas engine. The purpose of this code is to show how to create UI elements that are placed in the 3D world and interact with the scene. \n\nThe `example` method is the main entry point of the code. It takes two parameters: a `canvas` element and a `deviceType` string. The `canvas` element is used to create a graphics device, which is then used to create a PlayCanvas application. The `deviceType` string specifies the type of graphics device to create, such as \"webgl2\" or \"webgpu\". \n\nThe `assets` object contains three assets that are used in the example: a texture, a font, and a script. These assets are loaded using the `pc.Asset` class and passed to the `pc.AssetListLoader` class to load them asynchronously. \n\nThe `gfxOptions` object contains options for creating the graphics device, such as the URL of the glslang and twgsl libraries. \n\nThe `createOptions` object contains options for creating the PlayCanvas application, such as the graphics device, mouse and touch input devices, and component and resource systems. \n\nThe `app` object is an instance of the `pc.AppBase` class, which is the main class of the PlayCanvas engine. It is initialized with the `createOptions` object and started with the `start` method. \n\nThe code then creates several entities that are added to the scene. A camera entity is created with a camera component and an orbit camera script that allows the camera to orbit around a target. A ground entity is created with a box render component and a standard material that uses the checkboard texture. A light entity is created with a directional light component that casts shadows. \n\nThe main part of the code creates a 3D world screen entity with a screen component that has `screenSpace` set to false. This means that the screen is placed in the 3D world and not in screen space. A text entity and a button entity are added to the screen entity. The text entity has an element component with text that is displayed on the screen. The button entity has a button component that listens for click events and changes the camera clear color to a random color when clicked. \n\nOverall, this code demonstrates how to create a 3D world screen with UI elements that interact with the scene. It shows how to use the PlayCanvas engine to create a graphics device, load assets, create entities, and add components to them. This code can be used as a starting point for creating more complex 3D world screens with UI elements.\n## Questions: \n 1. What is the purpose of the `WorldUiExample` class?\n- The `WorldUiExample` class is an example of how to create a 3D world screen with UI elements using the PlayCanvas engine.\n\n2. What are the assets being loaded in the `example` method?\n- The assets being loaded are a checkboard texture, a font, and a script for an orbit camera.\n\n3. What happens when the button is clicked?\n- When the button is clicked, the background color of the camera is changed to a random color.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/user-interface/world-ui.md"}}],["144",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/xr/ar-basic.tsx)\n\nThe code defines a class called `ArBasicExample` that provides an example of how to use the PlayCanvas engine to create an augmented reality (AR) experience. The purpose of the code is to demonstrate how to create a simple AR scene with a grid of cubes and a light source, and how to activate the AR session using mouse or touch events, or the keyboard. \n\nThe `example` method of the `ArBasicExample` class takes two arguments: a canvas element and a device type. It creates a new PlayCanvas application with a camera and a light source, and a grid of cubes. If the device supports WebXR, it listens for mouse, touch, and keyboard events to activate and end the AR session, and displays messages to inform the user about the status of the AR session. If the device does not support WebXR, it displays a message to inform the user that WebXR is not supported.\n\nThe `example` method first defines a `message` function that creates a message element and appends it to the document body. The `message` function takes a string argument that is used as the text content of the message element. \n\nThe `example` method then creates a new PlayCanvas application with a canvas element, a mouse device, a touch device, a keyboard, and graphics device options. It sets the maximum pixel ratio of the graphics device to the device pixel ratio of the window. It then starts the application.\n\nThe `example` method creates a new camera entity with a clear color and a far clip distance, and adds it to the root of the application. It also creates a new light entity with a spot light type and a range of 30, and adds it to the root of the application. \n\nThe `example` method defines a `createCube` function that takes three number arguments: x, y, and z. The `createCube` function creates a new cube entity with a box render component, a local scale of 0.5, and a translation based on the x, y, and z arguments. It then adds the cube entity to the root of the application. \n\nThe `example` method then creates a grid of cubes by calling the `createCube` function in a nested loop that iterates over the x and y coordinates of the grid. \n\nIf the device supports WebXR, the `example` method defines an `activate` function that starts the AR session if it is available, and displays an error message if it fails to start. The `activate` function is called when the mouse is clicked or the touch event ends. The `example` method also listens for the keyboard ESC key to end the AR session. It displays messages to inform the user about the status of the AR session, such as when it starts, ends, or becomes available or unavailable. If the AR session is not available, it displays a message to inform the user that it is not available. \n\nIf the device does not support WebXR, the `example` method displays a message to inform the user that WebXR is not supported. \n\nThe `ArBasicExample` class is exported as the default export of the module. It can be imported and used in other modules of the PlayCanvas engine project. For example, it can be used as a starting point for creating more complex AR scenes, or as a reference for how to use the PlayCanvas engine with WebXR.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to create an AR (augmented reality) experience using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library using the wildcard syntax (*), and also imports the HTMLCanvasElement interface from the global namespace.\n\n3. What events trigger the start and end of an AR session?\n- The start of an AR session is triggered by the `mousedown` event on the mouse, or the `touchend` event on a touch device. The end of an AR session is triggered by the `keydown` event on the keyboard, specifically when the `ESC` key is pressed.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/xr/ar-basic.md"}}],["145",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/xr/index.mjs)\n\nThe code above is a module that exports several examples related to augmented reality (AR), virtual reality (VR), and extended reality (XR) using the PlayCanvas engine. The purpose of this module is to provide developers with examples of how to use the PlayCanvas engine to create AR, VR, and XR experiences.\n\nThe module imports several other modules, each of which contains an example related to AR, VR, or XR. These examples include `ArBasicExample`, `ArHitTestExample`, `VrBasicExample`, `VrControllersExample`, `VrHandsExample`, `VrMovementExample`, and `XrPickingExample`. Each of these examples demonstrates a different aspect of AR, VR, or XR development using the PlayCanvas engine.\n\nFor example, `ArBasicExample` provides a basic example of how to create an AR experience using the PlayCanvas engine. `VrControllersExample` demonstrates how to use VR controllers in a PlayCanvas VR experience. `XrPickingExample` shows how to use the PlayCanvas engine to pick objects in an XR experience.\n\nDevelopers can use these examples as a starting point for their own AR, VR, or XR projects. By studying the code and understanding how it works, developers can learn how to use the PlayCanvas engine to create immersive experiences for their users.\n\nTo use one of these examples in a project, a developer would import the desired example from this module and then use it in their code. For example, to use the `ArBasicExample`, a developer would write:\n\n```\nimport { ArBasicExample } from \"playcanvas-ar-vr-examples\";\n\n// Use ArBasicExample in code\n```\n\nOverall, this module provides a valuable resource for developers looking to create AR, VR, or XR experiences using the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file exports several examples related to AR, VR, and XR functionality in the PlayCanvas engine.\n\n2. What are some examples of AR, VR, and XR functionality included in this code file?\n- Some examples included in this code file are ARBasicExample, ArHitTestExample, VrBasicExample, VrControllersExample, VrHandsExample, VrMovementExample, and XrPickingExample.\n\n3. Are there any dependencies required for these examples to work?\n- It is unclear from this code file whether there are any dependencies required for these examples to work.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/xr/index.md"}}],["146",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/xr/vr-basic.tsx)\n\nThe code defines a class called `VrBasicExample` that provides an example of how to use the PlayCanvas engine to create a basic VR scene. The class has a single method called `example` that takes two parameters: a canvas element and a device type. The method creates a PlayCanvas application, sets up a camera and a light, and creates a grid of cubes. It also sets up event listeners for mouse, touch, and keyboard input, and handles the activation and deactivation of VR mode.\n\nThe `example` method first creates a message function that displays a message on the screen. It then creates a new PlayCanvas application using the provided canvas element and sets up input devices for mouse, touch, and keyboard. It sets the fill mode of the canvas to fill the window and the resolution to auto. It also sets the maximum pixel ratio of the graphics device to the device pixel ratio of the window.\n\nNext, it creates a camera entity and a light entity and adds them to the root of the scene. It then defines a function called `createCube` that creates a new cube entity at the specified position and adds it to the root of the scene. It uses this function to create a grid of cubes.\n\nIf VR is supported by the browser, the method sets up event listeners for mouse and touch input to activate and deactivate VR mode. It also sets up an event listener for the keyboard to end the VR session when the ESC key is pressed. It also sets up event listeners for the `start`, `end`, and `available` events of the XR system to display messages on the screen.\n\nIf VR is not supported by the browser, the method displays a message on the screen indicating that WebXR is not supported.\n\nOverall, this code provides a basic example of how to use the PlayCanvas engine to create a VR scene and handle input events. It can be used as a starting point for more complex VR projects.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a basic VR application using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the PlayCanvas engine from a relative path.\n\n3. What is the expected input and output of the `example` function?\n- The `example` function takes in an HTML canvas element and a device type as arguments, and does not have a return value. It sets up a PlayCanvas application with a camera, light, and grid of cubes, and adds event listeners for VR activation and deactivation.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/xr/vr-basic.md"}}],["147",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/xr/vr-controllers.tsx)\n\nThe code is an example of how to use VR controllers in a PlayCanvas engine project. The `VrControllersExample` class is a static class that has a single method called `example`. The method takes two parameters: a canvas element and a device type. The method creates a PlayCanvas application instance and sets up the canvas element as the rendering target. It also loads a 3D model of a VR controller and creates a grid of cubes. \n\nThe method then checks if the browser supports WebXR. If it does, it sets up event listeners for mouse and touch input to activate VR mode. When VR mode is activated, the method creates a camera entity and starts the VR session. It also creates a controller entity for each input source (e.g. a VR controller) that is added to the session. The position and rotation of each controller entity is updated every frame based on the input source's position and rotation. \n\nIf the browser does not support WebXR, the method displays a message indicating that WebXR is not supported. \n\nThe `VrControllersExample` class can be used as a starting point for implementing VR controller support in a PlayCanvas engine project. Developers can modify the code to suit their specific needs, such as changing the 3D model of the VR controller or adding custom logic for handling input from the controllers. \n\nExample usage:\n\n```javascript\nimport VrControllersExample from 'path/to/VrControllersExample';\n\nconst canvas = document.getElementById('canvas');\nconst deviceType = 'vr'; // or 'ar' for augmented reality\n\nVrControllersExample.example(canvas, deviceType);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of how to use VR controllers with the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library and uses HTML and CSS.\n\n3. What is the expected output of this code?\n- The expected output is a PlayCanvas application that allows the user to enter VR mode and see and interact with virtual controllers.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/xr/vr-controllers.md"}}],["148",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/xr/vr-hands.tsx)\n\nThe `VrHandsExample` class is a code example that demonstrates how to use the PlayCanvas engine to create a virtual reality (VR) experience with hand tracking. The purpose of this code is to show how to use the PlayCanvas engine to create a VR application that supports hand tracking and input. \n\nThe `example` method is the main entry point of the code. It takes two parameters: a canvas element and a device type. The method creates a new PlayCanvas application and sets up the canvas element as the rendering target. It also creates a camera and a light source, and adds them to the scene. \n\nThe `createCube` function creates a grid of cubes in the scene. The `createController` function creates a new controller entity for each input source (e.g., hand or grip) and adds it to the scene. The function also handles the removal of the controller entity when the input source is removed. \n\nThe code uses the `app.xr` object to check if the browser supports WebXR. If WebXR is supported, the code sets up event listeners for mouse and touch input to activate the VR experience. The code also sets up event listeners for keyboard input to end the VR session. \n\nThe code uses the `app.xr.input` object to listen for new input sources (e.g., hand or grip) and creates a new controller entity for each input source. The code updates the position and rotation of each controller entity based on the input source data. \n\nThe code also handles hand tracking events, such as when tracking is lost or recovered. When tracking is lost, the code changes the color of the hand joints to red. When tracking is recovered, the code changes the color of the hand joints to white. \n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a VR experience with hand tracking and input. Developers can use this code as a starting point to create their own VR applications using the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of a VR hands implementation using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas library from a relative path.\n\n3. What is the expected output of this code?\n- The expected output of this code is a VR hands demo that can be interacted with using a compatible device.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/xr/vr-hands.md"}}],["149",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/xr/vr-movement.tsx)\n\nThe `VrMovementExample` class is a code example that demonstrates how to implement VR movement in a PlayCanvas engine project. The code creates a scene with a camera, a light, and a grid of cubes. It also creates two controller boxes that are used to move and rotate the camera in the scene. \n\nThe `example` method of the `VrMovementExample` class takes two parameters: a canvas element and a device type. It first creates a message function that displays messages on the screen. It then creates a new PlayCanvas application with a mouse, touch device, and keyboard. The canvas fill mode is set to `FILLMODE_FILL_WINDOW`, and the canvas resolution is set to `RESOLUTION_AUTO`. The application is started, and a camera parent entity is created. \n\nThe camera parent entity is used to move and rotate the camera in the scene. A camera entity is created and added as a child of the camera parent entity. A light entity is also created and added to the scene. \n\nThe `createCube` function is used to create a grid of cubes in the scene. The `createController` function is used to create the two controller boxes. \n\nIf the PlayCanvas engine supports WebXR, the `activate` function is called when the user clicks on the screen or touches it. The `activate` function starts the immersive VR experience. The `movementSpeed`, `rotateSpeed`, `rotateThreshold`, and `rotateResetThreshold` variables are used to control the movement and rotation of the camera. \n\nThe `app.on('update')` function is used to update the position and rotation of the camera based on the input from the controller boxes. The left controller box is used to move the camera, and the right controller box is used to rotate the camera. The `app.drawLine` function is used to render the controller ray, and the `controllers[i].model.enabled` property is used to show or hide the controller box. \n\nIf the PlayCanvas engine does not support WebXR, a message is displayed on the screen. \n\nOverall, the `VrMovementExample` class provides a simple example of how to implement VR movement in a PlayCanvas engine project. It demonstrates how to create a scene, add a camera and light, and use controller boxes to move and rotate the camera.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of VR movement using the PlayCanvas engine.\n\n2. What dependencies does this code have?\n- This code imports the PlayCanvas engine using an import statement.\n\n3. What is the expected output of this code?\n- The expected output of this code is a VR movement example that allows the user to move and rotate in a virtual environment using VR controllers.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/xr/vr-movement.md"}}],["150",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/examples/xr/xr-picking.tsx)\n\nThe `XrPickingExample` class is a code example that demonstrates how to use the PlayCanvas engine to create a virtual reality (VR) experience. The code creates a grid of cubes and allows the user to pick and change the color of a cube by pointing at it with a VR controller. \n\nThe `example` method is the main entry point of the code. It takes two parameters: a canvas element and a device type. The method creates a new PlayCanvas application and sets up the canvas to fill the window. It also creates a camera and a light entity and adds them to the scene. The `createCube` function is used to create a grid of cubes and add them to the scene. \n\nThe code checks if the browser supports WebXR and if so, it sets up event listeners for mouse and touch input to activate VR mode. When the user enters VR mode, the code listens for input events from the VR controller and uses a raycasting technique to determine which cube the user is pointing at. If a cube is selected, its color is randomized. The code also renders a line to represent the input source ray of the VR controller.\n\nIf the browser does not support WebXR, the code displays a message to the user indicating that the feature is not supported.\n\nThis code example can be used as a starting point for developers who want to create VR experiences using the PlayCanvas engine. It demonstrates how to create a scene, add entities to it, and interact with them using VR controllers. Developers can modify the code to create their own VR experiences, such as games or simulations.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of XR Picking using the PlayCanvas engine. It creates a grid of cubes and allows the user to pick and change the color of the closest cube using input source rays.\n\n2. What dependencies does this code have?\n- This code imports the entire PlayCanvas engine using the wildcard import syntax. It also uses HTML and CSS to create a message element.\n\n3. What is the expected output of this code?\n- The expected output of this code is an interactive 3D scene displayed on an HTML canvas. The user can enter VR mode by tapping the screen and pick and change the color of the closest cube using input source rays. A message element is also displayed to provide instructions to the user.","metadata":{"source":".autodoc/docs/markdown/examples/src/examples/xr/xr-picking.md"}}],["151",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/lib/glslang/glslang.js)\n\nThe code provided is a JavaScript module that exports a function that returns an instance of the glslang library. The glslang library is a shader compiler and validator for the OpenGL ES and Vulkan graphics APIs. \n\nThe module uses the Universal Module Definition (UMD) pattern to support different module systems (CommonJS, AMD, and global). The UMD pattern checks the environment and exports the module accordingly. \n\nThe exported function is an asynchronous function that checks if the glslang instance has already been created. If it has, it returns the existing instance. If not, it loads the glslang library from a remote location (https://unpkg.com/@webgpu/glslang@0.0.15/dist/web-devel/glslang.js) using the dynamic import() function. Once the library is loaded, it creates an instance of the glslang library and returns it. \n\nThis module is likely used in a larger project that requires shader compilation and validation. The module provides a convenient way to load the glslang library and create an instance of it. The module can be imported into other JavaScript modules using CommonJS, AMD, or global module systems. \n\nExample usage:\n\n```javascript\nimport getGlslang from 'glslang';\n\nasync function compileShader(shaderSource) {\n  const glslang = await getGlslang();\n  const result = glslang.compileGLSL(shaderSource, 'fragment');\n  if (result.log) {\n    console.error(result.log);\n  }\n  return result.output;\n}\n``` \n\nIn this example, the `getGlslang` function is imported and used to load the glslang library. The `compileShader` function takes a shader source code as input, loads the glslang library using `getGlslang`, and compiles the shader using the `compileGLSL` function provided by the glslang library. If there are any errors during compilation, the function logs them to the console and returns `undefined`. Otherwise, it returns the compiled shader code.\n## Questions: \n 1. What is the purpose of this code?\n- This code is a webpack module definition for the PlayCanvas engine's glslang library.\n\n2. What is the role of the `glslang` variable?\n- The `glslang` variable is used to cache the glslang module so that it is only loaded once.\n\n3. What is the purpose of the `async` function that is exported as the default?\n- The `async` function is used to load the glslang module asynchronously and return it once it is loaded.","metadata":{"source":".autodoc/docs/markdown/examples/src/lib/glslang/glslang.md"}}],["152",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/src/static/index.html)\n\nThis code is an HTML file that serves as the entry point for the PlayCanvas engine project. The purpose of this file is to provide a basic HTML structure that includes the necessary metadata, stylesheets, and scripts required to run the PlayCanvas engine. \n\nThe `<!DOCTYPE html>` declaration at the beginning of the file specifies that this is an HTML5 document. The `<head>` section contains metadata such as the page title, description, and keywords. It also includes a reference to an external stylesheet (`styles.css`) that defines the visual style of the page. \n\nThe `<meta>` tags specify the character encoding and viewport settings for the page. The `viewport` meta tag is particularly important for mobile devices, as it ensures that the page is displayed at the correct scale and size. \n\nThe `<link>` tag references an image file (`playcanvas-logo.png`) that is used as the favicon for the page. \n\nThe two `<script>` tags reference external JavaScript files that are required to run the PlayCanvas engine. The first script tag references the Babel standalone library, which is used to transpile modern JavaScript code to a format that is compatible with older browsers. The second script tag references the `index.js` file, which contains the main code for the PlayCanvas engine. The `defer` attribute on both script tags ensures that the scripts are loaded asynchronously, which improves page load times. \n\nFinally, the `<div>` tag with an `id` of `app` is an empty container that will be populated with the PlayCanvas engine's output. \n\nOverall, this HTML file provides the basic structure and dependencies required to run the PlayCanvas engine. Developers can use this file as a starting point for their own PlayCanvas projects, and can customize it as needed to suit their specific requirements. \n\nExample usage:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>My PlayCanvas Project</title>\n    <meta name=\"description\" content=\"\">\n    <meta name=\"keywords\" content=\"PlayCanvas\">\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0\">\n    <link rel=\"icon\" type=\"image/png\" href=\"./playcanvas-logo.png\" />\n    <link rel=\"stylesheet\" href=\"styles.css\">\n    <script defer src=\"https://unpkg.com/@babel/standalone@7.16.7/babel.min.js\"></script>\n    <script defer src=\"index.js\"></script>\n</head>\n<body>\n    <div id='my-app'></div>\n</body>\n</html>\n```\n\nIn this example, the developer has customized the page title and the ID of the container element to match their own project. They have also left the other metadata and script references unchanged, as they are required for the PlayCanvas engine to function properly.\n## Questions: \n 1. What is the purpose of the PlayCanvas engine?\n- The code provided is for an example page for the PlayCanvas engine, but it does not provide information on the engine's purpose.\n\n2. What is the significance of the \"viewport\" meta tag?\n- The \"viewport\" meta tag is used to set the width of the viewport and the initial zoom level when the page is first loaded on a mobile device.\n\n3. What is the purpose of the Babel script included in the code?\n- The Babel script is used to transpile modern JavaScript code into a version that is compatible with older browsers.","metadata":{"source":".autodoc/docs/markdown/examples/src/static/index.md"}}],["153",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/tsconfig.json)\n\nThis code is a configuration file for the TypeScript compiler. It specifies various options for the compiler to use when compiling TypeScript code into JavaScript. \n\nThe `compilerOptions` object contains several properties that affect the compilation process. \n\n- `outDir` specifies the output directory for compiled JavaScript files. \n- `noImplicitAny` enforces the use of explicit types for all variables and function parameters. \n- `module` specifies the module system to use. In this case, it is set to ES2015, which is a widely supported module system for modern browsers and Node.js. \n- `target` specifies the version of ECMAScript to target. In this case, it is set to ES5, which is a widely supported version of ECMAScript. \n- `allowJs` allows the compiler to compile JavaScript files as well as TypeScript files. \n- `jsx` specifies the syntax for React components. \n- `lib` specifies the libraries to include in the compilation process. In this case, it includes the ES2019 library and the DOM library. \n- `allowSyntheticDefaultImports` allows for default imports from modules with no default export. \n- `esModuleInterop` enables interoperability between CommonJS and ES Modules. \n- `moduleResolution` specifies the module resolution strategy to use. In this case, it is set to \"node\", which is the default strategy for Node.js. \n\nThe `include` property specifies the directories or files to include in the compilation process. In this case, it includes the \"src\" directory. \n\nThe `exclude` property specifies the directories or files to exclude from the compilation process. In this case, it excludes the \"node_modules\" directory. \n\nThis configuration file is an important part of the PlayCanvas engine project because it ensures that all TypeScript code is compiled consistently and correctly. It also allows for the use of modern ECMAScript features and libraries, which can improve the performance and maintainability of the codebase. \n\nExample usage:\n\nTo compile TypeScript code using this configuration file, run the following command in the terminal:\n\n```\ntsc\n```\n\nThis will compile all TypeScript files in the \"src\" directory and output the compiled JavaScript files to the \"dist\" directory, as specified by the `outDir` property in the configuration file.\n## Questions: \n 1. What is the purpose of this code?\n    - This code is a configuration file for the TypeScript compiler, specifying options such as the output directory, target language version, and included/excluded files.\n\n2. What is the significance of the \"module\" option being set to \"ES2015\"?\n    - This option specifies that the code should be compiled as a module using the ES2015 syntax, which allows for features such as import/export statements and default exports.\n\n3. What is the difference between the \"include\" and \"exclude\" options?\n    - The \"include\" option specifies which files should be included in the compilation process, while the \"exclude\" option specifies which files should be excluded. In this case, the \"src\" directory is included and the \"node_modules\" directory is excluded.","metadata":{"source":".autodoc/docs/markdown/examples/tsconfig.md"}}],["154",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/webgpu-temp/four.html)\n\nThe code is an HTML file that creates a canvas element and loads the PlayCanvas engine. The purpose of this code is to demonstrate the use of the PlayCanvas engine to create a 3D scene with a skinned mesh model and animation. \n\nThe code imports various modules from the PlayCanvas engine, including Asset, AppBase, Shader, Texture, RenderTarget, Entity, and Material. It also imports component systems for rendering, models, cameras, lights, and animations, as well as resource handlers for textures and containers. \n\nThe code defines an assets object that contains two assets: a skinned mesh model and an animation for the model. When the assets are loaded, the code creates an entity with a camera component and an entity with a light component. It then instantiates the skinned mesh model as a render entity and adds it to the scene. Finally, it assigns the animation to the model entity. \n\nThe code also sets up tracing for various operations, such as rendering frames, passes, textures, shaders, and bind groups. \n\nThe main function creates a graphics device using the createGraphicsDevice function from the PlayCanvas engine. It then initializes an AppBase instance with the graphics device and component systems and resource handlers. The lighting settings are also configured to disable shadows and cookies. The asset list loader is used to load the assets, and the onLoaded function is called when the assets are loaded. \n\nOverall, this code demonstrates how to use the PlayCanvas engine to create a 3D scene with a skinned mesh model and animation. It also shows how to configure tracing for various operations and how to load assets using the asset list loader.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an HTML file that sets up a canvas element and imports various modules from the PlayCanvas engine to create a 3D scene with a camera, light, and animated model.\n\n2. What is the significance of the warning message in the HTML file?\n- The warning message is displayed if the browser being used does not support WebGPU rendering, which is required for this code to work. It suggests installing a developer build of a browser that supports WebGPU.\n\n3. What are some of the modules being imported in this code?\n- Some of the modules being imported include Asset, AppBase, createGraphicsDevice, Shader, Texture, RenderTarget, Entity, Tracing, Color, Vec3, Quat, StandardMaterial, Material, BasicMaterial, RenderComponentSystem, ModelComponentSystem, CameraComponentSystem, LightComponentSystem, AnimComponentSystem, TextureHandler, and ContainerHandler. These modules are used to create and manipulate various components of the 3D scene.","metadata":{"source":".autodoc/docs/markdown/examples/webgpu-temp/four.md"}}],["155",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/webgpu-temp/three.html)\n\nThis code is an HTML file that creates a canvas element and imports various modules from the PlayCanvas engine. It then uses these modules to create a 3D scene with morphing objects that can be viewed in a web browser. \n\nThe code first imports various modules from the PlayCanvas engine, including Asset, AppBase, Shader, Texture, and RenderTarget. It also imports modules related to creating entities, materials, and meshes. \n\nThe `onLoaded` function is called when the app is loaded and creates a 3D scene with morphing objects. It first creates a directional light and a camera entity and adds them to the scene. It then defines two helper functions: `shortestDistance` and `createMorphTarget`. The `shortestDistance` function calculates the shortest distance between a point and a plane defined by a normal vector. The `createMorphTarget` function creates a morph target from a set of positions, normals, and indices, and a plane normal. \n\nThe `createMorphInstance` function creates a morph instance by creating a sphere mesh and expanding a part of it along three planes. It then creates a morph using these three targets and adds the morph instance to the mesh instance. Finally, it creates an entity, adds a render component with the mesh instance, and adds the entity to the scene. \n\nThe `onLoaded` function creates three morph instances using the `createMorphInstance` function and updates their weights every frame. It also orbits the camera around the scene. \n\nThe `main` function creates a graphics device and initializes the app with the graphics device and various component systems and resource handlers. It then sets up the scene lighting and calls the `onLoaded` function. \n\nOverall, this code creates a 3D scene with morphing objects that can be viewed in a web browser. It demonstrates the use of various modules from the PlayCanvas engine, including entities, materials, meshes, and morph targets.\n## Questions: \n 1. What is the purpose of this code?\n- This code is an example of using the PlayCanvas engine to create a WebGPU test.\n\n2. What libraries or modules are being imported in this code?\n- This code imports various modules from the PlayCanvas engine, including asset, app, graphics, entity, tracing, math, scene, components, and handlers.\n\n3. What does the `createMorphTarget` function do?\n- The `createMorphTarget` function takes in positions, normals, and indices of a mesh, as well as a plane normal, and generates a morph target by modifying the vertices and normals of the mesh based on the distance to the specified plane.","metadata":{"source":".autodoc/docs/markdown/examples/webgpu-temp/three.md"}}],["156",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/examples/webgpu-temp/two.html)\n\nThis code is an HTML file that creates a canvas element and loads the PlayCanvas engine. The purpose of this code is to create a 3D scene using the PlayCanvas engine and render it on the canvas element. \n\nThe code imports various modules from the PlayCanvas engine, including Asset, AppBase, Shader, Texture, RenderTarget, Entity, Tracing, and Material. It also imports component systems for rendering, camera, and light, as well as resource handlers for textures and containers. \n\nThe code defines an assets object that contains a texture asset and a model asset. The model asset is loaded from a GLB file and is used to create a render entity that is added to the scene. The code also creates a camera entity and a directional light entity that are added to the scene. \n\nThe code sets up a render loop that updates the camera position and rotation over time. The camera orbits around the scene, while looking at a fixed point. \n\nThe code uses the WebGPU API to create a graphics device and initialize the PlayCanvas engine. It also sets up tracing for shader allocation. \n\nOverall, this code sets up a basic PlayCanvas scene with a camera, light, and model asset. It demonstrates how to use the PlayCanvas engine to create 3D scenes and render them on a canvas element.\n## Questions: \n 1. What is the purpose of this code?\n- This code is a HTML file that creates a canvas element and imports various modules from the PlayCanvas engine to render a 3D model.\n\n2. What is the significance of the `createGraphicsDevice` function?\n- The `createGraphicsDevice` function creates a graphics device using the WebGPU API and initializes it with the specified options.\n\n3. What is the purpose of the `onLoaded` function?\n- The `onLoaded` function is called after all assets have finished loading and initializes the scene by creating entities, adding components, and setting up the camera and lighting.","metadata":{"source":".autodoc/docs/markdown/examples/webgpu-temp/two.md"}}],["157",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/exporters/core-exporter.js)\n\nThe `CoreExporter` class in the PlayCanvas engine project contains a method called `textureToCanvas` that converts a texture to a canvas element. This method takes in two arguments: the `texture` to be converted and an optional `options` object that can contain a `color` property to tint the texture and a `maxTextureSize` property to set the maximum size of the texture. The method returns the resulting canvas element.\n\nThe method first gets the source image of the texture using the `getSource()` method. It then checks if the source image is an instance of `HTMLImageElement`, `HTMLCanvasElement`, `OffscreenCanvas`, or `ImageBitmap`. If it is, the method proceeds to convert the image to a canvas element.\n\nThe method resizes the canvas if the `maxTextureSize` option is set and the texture is larger than the specified size. It then creates a new canvas element with the specified width and height and gets its 2D context. The source image is drawn onto the canvas using the `drawImage()` method.\n\nIf the `color` option is set, the method tints the texture by modifying the pixel data of the canvas. It gets the pixel data using the `getImageData()` method, multiplies the red, green, and blue values of each pixel by the corresponding values of the `color` property, and puts the modified pixel data back onto the canvas using the `putImageData()` method.\n\nThis method can be used in the larger PlayCanvas engine project to convert textures to canvas elements for various purposes such as rendering, manipulation, and export. The `options` object provides flexibility in modifying the resulting canvas element, such as changing its size and tinting its color. Here is an example usage of the `textureToCanvas` method:\n\n```\nconst exporter = new CoreExporter();\nconst texture = app.assets.find('myTexture').resource;\nconst canvas = exporter.textureToCanvas(texture, { color: new pc.Color(1, 0, 0), maxTextureSize: 512 });\ndocument.body.appendChild(canvas);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `CoreExporter` that has a method `textureToCanvas` which converts a texture to a canvas and optionally applies a tint color.\n\n2. What arguments does the `textureToCanvas` method take?\n- The `textureToCanvas` method takes two arguments: `texture` which is the source texture to be converted, and `options` which is an object for passing optional arguments. The `options` object can have two optional properties: `color` which is the tint color to modify the texture with, and `maxTextureSize` which is the maximum texture size. \n\n3. What does the `textureToCanvas` method return?\n- The `textureToCanvas` method returns a canvas element containing the image if the source texture is an instance of `HTMLImageElement`, `HTMLCanvasElement`, `OffscreenCanvas`, or `ImageBitmap`. If the source texture is not an instance of any of these, the method returns `undefined`.","metadata":{"source":".autodoc/docs/markdown/extras/exporters/core-exporter.md"}}],["158",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/index.js)\n\nThis code exports several modules from the PlayCanvas engine project. The first line exports the `MiniStats` module from the `mini-stats/mini-stats.js` file. This module provides a small statistics panel that can be used to display performance metrics for a PlayCanvas application. \n\nThe next two lines export the `UsdzExporter` and `GltfExporter` modules from their respective files in the `exporters` directory. These modules provide functionality for exporting PlayCanvas scenes to the USDZ and GLTF file formats, respectively. \n\nBy exporting these modules, other parts of the PlayCanvas engine project or external applications can import and use them. For example, a developer building a PlayCanvas application could import the `MiniStats` module to display performance metrics in the application's UI. Or, a developer building a tool for exporting PlayCanvas scenes could import the `UsdzExporter` or `GltfExporter` modules to handle the export process. \n\nHere is an example of how the `MiniStats` module could be imported and used in a PlayCanvas application:\n\n```javascript\nimport { MiniStats } from 'playcanvas';\n\n// create a new MiniStats panel and add it to the application's UI\nconst stats = new MiniStats(app);\nstats.dom.style.position = 'absolute';\nstats.dom.style.left = '0px';\nstats.dom.style.bottom = '0px';\ndocument.body.appendChild(stats.dom);\n```\n\nOverall, this code is a small but important part of the PlayCanvas engine project, providing useful functionality for monitoring performance and exporting scenes.\n## Questions: \n 1. **What is the purpose of the `MiniStats` module?**  \nThe `MiniStats` module is exported from the `mini-stats/mini-stats.js` file, but without seeing the contents of that file, it's unclear what the module does.\n\n2. **What file formats can be exported using the PlayCanvas engine?**  \nThe code exports two modules, `UsdzExporter` and `GltfExporter`, which suggest that the PlayCanvas engine can export to USDZ and GLTF file formats.\n\n3. **Are there any other modules being exported from this file?**  \nNo, there are only three export statements in this file, and they all export modules from other files.","metadata":{"source":".autodoc/docs/markdown/extras/index.md"}}],["159",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/mini-stats/cpu-timer.js)\n\nThe code defines a class called `CpuTimer` that is used to measure the time taken by different parts of the PlayCanvas engine. The class constructor takes an instance of the PlayCanvas application as an argument and sets up event listeners for the `frameupdate`, `framerender`, and `frameend` events. These events are fired by the PlayCanvas engine at different stages of the frame rendering process.\n\nThe `CpuTimer` class has several properties and methods that are used to measure and store timing information. The `_frameIndex` property is used to keep track of the current frame index, while the `_frameTimings`, `_timings`, and `_prevTimings` properties are arrays that store timing information for the current frame, the previous frame, and the frame before that, respectively. The `unitsName` and `decimalPlaces` properties are used to specify the units and precision of the timing information.\n\nThe `begin` method is called at the beginning of each frame and is used to reset the timing information for the current frame. It first checks if the timer is enabled and returns if it is not. It then clears any timing information that was collected for the current frame and swaps the `_timings` and `_frameTimings` arrays. Finally, it calls the `mark` method to mark the beginning of the frame.\n\nThe `mark` method is called at different stages of the frame rendering process and is used to record the time taken by each stage. It first checks if the timer is enabled and returns if it is not. It then records the current timestamp and calculates the time taken by the previous mark or the previous frame, depending on the value of `_frameIndex`. It then updates the `_frameTimings` array with the name and timestamp of the current mark and increments the `_frameIndex`.\n\nThe `timings` getter method is used to retrieve the timing information for the current frame. It first removes the last time point from the `_timings` array, which represents the time spent outside of PlayCanvas. It then maps the remaining time points to an array of time values and returns it.\n\nOverall, the `CpuTimer` class is an important part of the PlayCanvas engine that is used to measure the performance of different parts of the engine. It can be used to identify performance bottlenecks and optimize the engine for better performance. Here is an example of how the `CpuTimer` class can be used:\n\n```\nimport { CpuTimer } from 'playcanvas';\n\nconst app = new pc.Application();\nconst timer = new CpuTimer(app);\n\n// enable the timer\ntimer.enabled = true;\n\n// start the application\napp.start();\n\n// do some rendering\napp.render();\n\n// get the timing information for the current frame\nconst timings = timer.timings;\n\nconsole.log(timings);\n```\n## Questions: \n 1. What is the purpose of the `CpuTimer` class?\n- The `CpuTimer` class is used to measure the time taken by different parts of the PlayCanvas engine during a frame update.\n\n2. What events is the `CpuTimer` class listening to?\n- The `CpuTimer` class is listening to the `frameupdate`, `framerender`, and `frameend` events.\n\n3. What is the purpose of the `timings` getter?\n- The `timings` getter returns an array of timings for each frame update, excluding the time spent outside of PlayCanvas.","metadata":{"source":".autodoc/docs/markdown/extras/mini-stats/cpu-timer.md"}}],["160",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/mini-stats/gpu-timer.js)\n\nThe `GpuTimer` class is a utility class that provides a way to measure the time taken by the GPU to execute certain tasks. It is designed to work with the PlayCanvas engine and is used to profile the performance of the engine itself or any other application built on top of it.\n\nThe class constructor takes an instance of the `app` object, which is a reference to the PlayCanvas application. It initializes the WebGL context and the extension required for timer queries. It also creates a pool of free queries, a list of current frame queries, and a list of previous frame queries. The class also maintains two arrays of timings, one for the current frame and one for the previous frame.\n\nThe `begin` method is called at the beginning of each frame and takes a name parameter. It stores the previous frame's queries, checks if all in-flight queries have been invalidated, and resolves the previous frame timings. It then calls the `mark` method with the given name parameter.\n\nThe `mark` method takes a name parameter and is called to mark the beginning of a new query. It ends the previous query, allocates a new query, and begins it.\n\nThe `end` method is called at the end of each frame and ends the current query and adds the current frame queries to the list of previous frame queries.\n\nThe `_checkDisjoint` method checks if the GPU has been interrupted, thereby invalidating all in-flight queries. If it has, it returns all queries to the free list.\n\nThe `_allocateQuery` method either returns a previously freed query or allocates a new one if there aren't any.\n\nThe `_resolveFrameTimings` method attempts to resolve one frame's worth of timings. It waits for the last query in the frame to be available and then retrieves the timings for each query.\n\nThe `timings` getter returns an array of timings for the current frame.\n\nOverall, the `GpuTimer` class provides a way to measure the performance of the GPU and can be used to optimize the performance of the PlayCanvas engine or any other application built on top of it. An example of how it can be used is to measure the time taken to render a particular scene and then optimize the rendering pipeline to reduce the time taken.\n## Questions: \n 1. What is the purpose of this code and how does it relate to the PlayCanvas engine?\n- This code defines a `GpuTimer` class that can be used to measure GPU timings for a PlayCanvas app. It is part of the PlayCanvas engine.\n\n2. What events is the `GpuTimer` class listening for and how are they used?\n- The `GpuTimer` class listens for the `frameupdate`, `framerender`, and `frameend` events of a PlayCanvas app. These events are used to mark the beginning and end of a frame, and to allocate and free GPU queries.\n\n3. What is the significance of the `disjoint` variable in the `_checkDisjoint` method?\n- The `disjoint` variable is used to check if the GPU has been interrupted, which can invalidate all in-flight queries. If `disjoint` is true, all queries are returned to the free list and the current frame and previous frames are discarded.","metadata":{"source":".autodoc/docs/markdown/extras/mini-stats/gpu-timer.md"}}],["161",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/mini-stats/graph.js)\n\nThe `Graph` class is responsible for creating and updating a real-time performance graph visual. It is designed to be used within the PlayCanvas engine project. \n\nThe constructor takes in several parameters including `name`, `app`, `watermark`, `textRefreshRate`, and `timer`. `name` is a string that represents the name of the graph. `app` is an instance of the PlayCanvas application. `watermark` is a number that represents the maximum value of the graph. `textRefreshRate` is the rate at which the text on the graph is refreshed. `timer` is an instance of the `pc.Timer` class that is used to measure the performance of the application.\n\nThe `Graph` class has several properties including `device`, `enabled`, `avgTotal`, `avgTimer`, `avgCount`, `timingText`, `texture`, `yOffset`, `cursor`, and `sample`. `device` is an instance of the PlayCanvas graphics device. `enabled` is a boolean that determines whether the graph is enabled or not. `avgTotal`, `avgTimer`, and `avgCount` are used to calculate the average performance of the application. `timingText` is a string that represents the average performance of the application. `texture` is a WebGL texture that is used to render the graph. `yOffset` is the y-offset of the graph. `cursor` is the current position of the graph. `sample` is a `Uint8ClampedArray` that is used to store the latest sample of the graph.\n\nThe `Graph` class has two methods, `loseContext` and `update`. `loseContext` is called when the context is lost and releases all context-related resources. `update` is called on every frame update and is responsible for updating the graph. It calculates the total performance of the application, updates the average performance, and updates the graph texture.\n\nThe `Graph` class also has a `render` method that is responsible for rendering the graph. It takes in several parameters including `render2d`, `x`, `y`, `w`, and `h`. `render2d` is an instance of the PlayCanvas 2D renderer. `x`, `y`, `w`, and `h` are the x-position, y-position, width, and height of the graph.\n\nOverall, the `Graph` class is an important component of the PlayCanvas engine project that provides real-time performance graph visualizations. It is used to monitor the performance of the application and can be customized to fit the needs of the developer.\n## Questions: \n 1. What is the purpose of the `Graph` class?\n- The `Graph` class is a real-time performance graph visual.\n\n2. What parameters does the `Graph` constructor take?\n- The `Graph` constructor takes `name`, `app`, `watermark`, `textRefreshRate`, and `timer` as parameters.\n\n3. What is the purpose of the `loseContext` method?\n- The `loseContext` method releases all context related resources when the context is lost.","metadata":{"source":".autodoc/docs/markdown/extras/mini-stats/graph.md"}}],["162",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/mini-stats/mini-stats.js)\n\nThe code defines a class called `MiniStats` that is used to render CPU and GPU timing information in a PlayCanvas application. The class imports several modules from the PlayCanvas engine, including `FILTER_NEAREST`, `math`, `Color`, and `Texture`. It also imports several other classes from other files in the project, including `CpuTimer`, `GpuTimer`, `StatsTimer`, `Graph`, `WordAtlas`, and `Render2d`.\n\nThe `MiniStats` class constructor takes two arguments: `app` and `options`. `app` is an instance of the PlayCanvas `Application` class, and `options` is an object that contains various options for configuring the MiniStats display. The constructor first sets up an event listener to handle context lost events, which can occur when the WebGL context is lost and needs to be restored. It then initializes the MiniStats display by creating graphs based on the options, extracting the words needed for the display, creating a word atlas, and assigning the texture to the graphs. It also creates a click region so that the display can be resized, and sets up event listeners to handle canvas resize events and post-render events.\n\nThe `MiniStats` class has several methods for setting and getting various properties of the display, including `activeSizeIndex`, `opacity`, `overallHeight`, `enabled`, `initWordAtlas`, `initGraphs`, `render`, `resize`, and `updateDiv`. These methods are used to resize the display, update the opacity, render the graphs and text, and update the position of the display.\n\nOverall, the `MiniStats` class provides a way to display CPU and GPU timing information in a PlayCanvas application, which can be useful for debugging and performance optimization. The class is highly configurable, allowing developers to customize the display to their needs.\n## Questions: \n 1. What is the purpose of the MiniStats class?\n- The MiniStats class is used for rendering CPU and GPU timing information in the form of graphs and text.\n\n2. What are the options that can be passed to the MiniStats constructor?\n- The options that can be passed to the MiniStats constructor include sizes of the area to render individual graphs in and spacing between individual graphs, refresh rate of text stats, colors used to render graphs, and options for rendering additional graphs based on stats collected into Application.stats.\n\n3. What is the purpose of the initWordAtlas method?\n- The initWordAtlas method is used to create a texture for storing word atlas and graph data, and to initialize the word atlas with the words needed for rendering the graphs.","metadata":{"source":".autodoc/docs/markdown/extras/mini-stats/mini-stats.md"}}],["163",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/mini-stats/render2d.js)\n\nThe code defines a class called `Render2d` that is responsible for rendering 2D textured quads. It is designed to be used in the PlayCanvas engine project. The class constructor takes a `device` object, `colors` object, and an optional `maxQuads` parameter. The `device` object is used to create vertex and index buffers, as well as to set various rendering states. The `colors` object contains color values for various elements of the rendered quads, such as the background, watermark, and graph colors. The `maxQuads` parameter specifies the maximum number of quads that can be rendered at once.\n\nThe `Render2d` class has a `quad` method that is used to add quads to the rendering queue. It takes several parameters that define the position, size, texture, and enabled state of the quad. The `quad` method updates the vertex buffer with the quad data and adds the quad to the rendering queue.\n\nThe `Render2d` class has a `render` method that is used to render all the quads in the rendering queue. It sets various shader uniforms, such as the screen and texture size, colors, and watermark size. It then iterates over the rendering queue and draws each quad using the appropriate texture and primitive type.\n\nThe `Render2d` class uses several constants and objects from the PlayCanvas engine, such as `BLENDEQUATION_ADD`, `VertexBuffer`, `IndexBuffer`, `BlendState`, and `DepthState`. It also defines its own vertex and fragment shaders that are used to render the quads.\n\nOverall, the `Render2d` class provides a simple and efficient way to render 2D textured quads in the PlayCanvas engine project. It can be used to render various UI elements, such as buttons, labels, and graphs. Here is an example of how to use the `Render2d` class to render a simple button:\n\n```javascript\nconst render2d = new Render2d(device, colors);\nrender2d.quad(buttonTexture, x, y, w, h, u, v, uw, uh, enabled);\nrender2d.render(clr, height);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `Render2d` which is used to render 2D textured quads.\n\n2. What are the inputs and outputs of the `quad` method?\n- The `quad` method takes in several parameters including a texture, position and size of the quad, texture coordinates, and a boolean flag indicating whether the quad is enabled or not. The method does not have any output.\n\n3. What is the purpose of the `setupColor` function?\n- The `setupColor` function is used to set up color values for various elements used in rendering. It takes in a color name and value, creates a Float32Array for the color, and resolves the color ID using the device scope.","metadata":{"source":".autodoc/docs/markdown/extras/mini-stats/render2d.md"}}],["164",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/mini-stats/stats-timer.js)\n\nThe `StatsTimer` class is a utility class that provides a way to track and display performance statistics for a PlayCanvas application. It takes in several parameters, including the PlayCanvas `app` object, an array of `statNames` to track, the number of `decimalPlaces` to display, a `unitsName` string, and an optional `multiplier` value. \n\nThe `constructor` method initializes the class properties and limits the `statNames` array to a maximum of 3 elements. It also defines a `resolve` function that recursively looks up properties of objects specified in a string. This function is used to read the specified stats from the `app.stats` object.\n\nThe `app.on` method is used to register a callback function that is called on every frame update. Within this callback function, the `values` array is updated with the current values of the specified stats, multiplied by the optional `multiplier` value. \n\nThe `get timings` getter method returns the `values` array, which can be used to display the performance statistics in the application. \n\nOverall, the `StatsTimer` class provides a simple way to track and display performance statistics for a PlayCanvas application. It can be used in conjunction with other tools and techniques to optimize the performance of the application. \n\nExample usage:\n\n```\nconst app = new pc.Application(canvas, options);\n\n// create a StatsTimer object to track and display performance stats\nconst statsTimer = new StatsTimer(app, ['frame.ms', 'render.timings.foward', 'render.timings.deferred'], 2, 'ms', 0.001);\n\n// add the StatsTimer object to the app's stats panel\napp.stats.add(statsTimer);\n\n// start the application\napp.start();\n```\n## Questions: \n 1. What is the purpose of the StatsTimer class?\n- The StatsTimer class is a timer interface for graph that reads specified stats from the app.stats object and stores them in an array.\n\n2. What parameters does the constructor of the StatsTimer class take?\n- The constructor of the StatsTimer class takes in an app object, an array of stat names, the number of decimal places to round to, a string for the units name, and an optional multiplier.\n\n3. What event does the StatsTimer class listen for and what does it do when the event is triggered?\n- The StatsTimer class listens for the 'frameupdate' event and when triggered, it reads the specified stats from the app.stats object and stores them in the values array, which can be accessed through the timings getter.","metadata":{"source":".autodoc/docs/markdown/extras/mini-stats/stats-timer.md"}}],["165",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/extras/mini-stats/word-atlas.js)\n\nThe `WordAtlas` class is responsible for generating a texture atlas of words that can be used for rendering text in a 2D context. The constructor takes in a texture and an array of words, and generates a canvas with the same dimensions as the texture. It then uses the canvas context to render each word onto the canvas, keeping track of the placement of each word in the atlas. The resulting texture is then used to render text in the 2D context.\n\nThe `render` method takes in a `render2d` object, a word to render, and the x and y coordinates of where to render the word. It looks up the placement of the word in the atlas and uses the `render2d` object to render a quad with the appropriate texture coordinates. The method returns the width of the rendered word.\n\nThis class is useful for rendering text in a 2D context efficiently, as it avoids the need to render each character separately. By generating a texture atlas of words, it reduces the number of draw calls needed to render text. This can be especially useful in games or other applications where text is frequently rendered.\n\nExample usage:\n\n```javascript\nconst texture = new Texture(device, {\n    width: 512,\n    height: 512,\n    format: PIXELFORMAT_R8_G8_B8_A8\n});\n\nconst words = ['hello', 'world', 'foo', 'bar'];\nconst wordAtlas = new WordAtlas(texture, words);\n\n// render the word 'hello' at position (10, 10)\nwordAtlas.render(render2d, 'hello', 10, 10);\n```\n## Questions: \n 1. What is the purpose of the WordAtlas class?\n- The WordAtlas class is used to create a texture atlas of words that can be rendered in a 2D context.\n\n2. What font is used to render the words in the texture atlas?\n- The words are rendered using the \"Lucida Console\", Monaco, monospace font with a font size of 10px.\n\n3. How are the words in the texture atlas colored?\n- Single characters and '.' are colored white, while the rest of the characters are colored grey.","metadata":{"source":".autodoc/docs/markdown/extras/mini-stats/word-atlas.md"}}],["166",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/jsconfig.json)\n\nThis code is a configuration file for the TypeScript compiler used in the PlayCanvas engine project. The `compilerOptions` object specifies various settings for the compiler, including the `baseUrl` which is set to the current directory, `checkJs` which enables type checking in JavaScript files, `module` which is set to ES6 module format, `target` which is set to ES6, `moduleResolution` which is set to node, and `typeRoots` which specifies the directories to search for type definitions.\n\nThe `include` array specifies the files and directories to include in the compilation process. The `src/**/*` pattern includes all files and subdirectories in the `src` directory, while `rollup.config.mjs` includes the Rollup configuration file.\n\nThis configuration file is important for ensuring that the TypeScript code in the PlayCanvas engine project is compiled correctly and with the desired settings. It allows developers to write code in TypeScript and have it transpiled to JavaScript that can run in modern browsers.\n\nAn example of how this configuration file is used in the project can be seen in the `package.json` file, where the `tsc` command is used to compile the TypeScript code using this configuration:\n\n```\n\"scripts\": {\n  \"build\": \"tsc\"\n},\n```\n\nOverall, this configuration file is a crucial part of the PlayCanvas engine project, as it ensures that the TypeScript code is compiled correctly and with the desired settings, allowing developers to write code in a modern and efficient way.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code is a configuration file for the TypeScript compiler options used in the PlayCanvas engine project.\n\n2. **What is the significance of the \"checkJs\" option being set to true?**\\\nThe \"checkJs\" option being set to true means that the TypeScript compiler will perform type checking on JavaScript files as well as TypeScript files.\n\n3. **What is the role of the \"typeRoots\" option in this configuration file?**\\\nThe \"typeRoots\" option specifies the directories where the TypeScript compiler should look for type definitions. In this case, it includes directories for WebGPU and general types.","metadata":{"source":".autodoc/docs/markdown/jsconfig.md"}}],["167",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/release.js)\n\nThe code is a Node.js script that automates the process of creating and finalizing releases for the PlayCanvas engine. The script is designed to be run from the command line and takes two optional arguments: `create-release` and `finalize-release`. If no arguments are provided, the script will determine the appropriate operation based on the current Git branch.\n\nThe script uses several Node.js modules, including `child_process`, `readline`, and `fs`. The `child_process` module is used to execute Git commands, `readline` is used to prompt the user for confirmation, and `fs` is used to read and write files.\n\nThe script defines several functions that are used to read and manipulate the version number in the `package.json` file, create and finalize release branches, and prompt the user for confirmation. The `readPackageVersion` function reads the current version number from `package.json` and returns an object with the major, minor, patch, and build numbers. The `bumpPackageVersion` function takes a version object and a release type (major, minor, or patch) and returns a new version object with the appropriate number incremented. The `evolvePackageVersion` function takes a version object and returns a new version object with the build number set to `null` if it exists, or the patch number incremented if it does not.\n\nThe `createRelease` function creates a new release branch based on the current `main` branch. It prompts the user for confirmation, creates the new branch, updates the version number in `package.json`, and commits the changes. The `finalizeRelease` function finalizes the current release branch by updating the version number in `package.json`, committing the changes, and creating a Git tag for the release.\n\nThe `getUserConfirmation` function prompts the user with a question and invokes a callback if the user responds with `y` or `Y`. The `exec` function executes a Git command and returns the output as a string.\n\nThe `run` function determines the appropriate operation based on the command line arguments or the current Git branch. It calls the `createRelease` or `finalizeRelease` function as appropriate, or prints the usage information if the arguments are invalid.\n\nOverall, this script provides a convenient way to automate the release process for the PlayCanvas engine, reducing the risk of human error and ensuring that the version numbers and Git branches are consistent.\n## Questions: \n 1. What does this code do?\n- This code is a script for managing the release process of the PlayCanvas engine. It allows the user to create a new minor release branch, finalize the package version, and tag the release.\n\n2. What dependencies does this code have?\n- This code requires the `child_process`, `readline`, and `fs` modules.\n\n3. What is the purpose of the `evolvePackageVersion` function?\n- The `evolvePackageVersion` function is used to evolve the package version for releases. If the current version has a build number, it will remove the build number. Otherwise, it will bump the patch version number.","metadata":{"source":".autodoc/docs/markdown/release.md"}}],["168",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/animation/tween.js)\n\nThe code defines a script called \"Tween\" that can be attached to entities in the PlayCanvas engine. The purpose of the script is to create and manage tweens, which are animations that interpolate between two values over time. The script allows the user to define multiple tweens with different properties and settings, and to trigger them in various ways.\n\nThe script uses the TWEEN.js library to create and manage the tweens. The script defines a set of attributes that can be set in the editor or via code, including the start and end values of the tween, the duration, easing function, and events to trigger at various points in the tween's lifecycle. The script also allows the user to specify a path to the property being tweened, which can be a nested property of the entity or a property of a component attached to the entity.\n\nWhen the script is initialized, it creates an array of tween instances and an array of callbacks for triggering the tweens. It also sets up event listeners for the various trigger events specified in the attributes. When a trigger event is fired, the corresponding tween is started. The script also handles enabling and disabling of the entity, pausing and resuming any playing tweens as needed.\n\nThe `start` method is called to start a tween. It creates a new TWEEN.Tween instance with the specified settings and starts it. The method also handles updating the property being tweened on each frame of the animation. The `update` event is fired each time the property is updated, and the `complete` event is fired when the tween is finished. The `repeat` event is fired each time the tween repeats, and the `stop` event is fired if the tween is explicitly stopped.\n\nFinally, the script sets up an event listener to update the TWEEN.js engine each frame. This ensures that all active tweens are updated and animated correctly.\n\nExample usage:\n\n```\n// Attach the Tween script to an entity\nvar entity = new pc.Entity();\nentity.addComponent('script');\nentity.script.create('tween', {\n    tweens: [\n        {\n            autoPlay: true,\n            path: 'position.x',\n            start: { x: 0 },\n            end: { x: 10 },\n            duration: 1000\n        },\n        {\n            event: 'myEvent',\n            path: 'rotation.y',\n            start: { x: 0 },\n            end: { x: 360 },\n            duration: 2000,\n            repeat: -1\n        }\n    ]\n});\n\n// Trigger a tween via an event\nentity.app.fire('myEvent');\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a script called \"Tween\" that allows developers to create and manage tweens (animations) for entity properties in a PlayCanvas project.\n\n2. What types of properties can be tweened using this script?\n- This script supports tweening of number, vec2, vec3, vec4, and color properties of entities.\n\n3. How can a tween be triggered to start?\n- A tween can be triggered to start immediately upon initialization of the script, or it can be triggered by a specified event name that is fired on the global application object. The script also supports delay and repeat options for the tween.","metadata":{"source":".autodoc/docs/markdown/scripts/animation/tween.md"}}],["169",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/camera/fly-camera.js)\n\nThe `FlyCamera` script is a component of the PlayCanvas engine that allows the user to control the camera in a 3D scene. The script provides functionality for moving the camera in response to user input from the keyboard and mouse. \n\nThe script has three attributes: `speed`, `fastSpeed`, and `mode`. `speed` and `fastSpeed` are numbers that determine the speed at which the camera moves. `mode` is an enum that determines the behavior of the camera when the user interacts with it. The `Lock` mode locks the camera's orientation and allows the user to move it around the scene. The `Drag` mode allows the user to drag the camera around the scene.\n\nThe `initialize` function sets up the initial state of the camera. It gets the current rotation of the camera and stores it in `ex` and `ey`. It also sets the `moved` and `lmbDown` flags to false. The function then disables the context menu and sets up event listeners for mouse movement, mouse button down, and mouse button up.\n\nThe `update` function is called every frame and updates the camera's position and orientation based on user input. It checks if the shift key is pressed and updates the speed accordingly. It then checks which keys are pressed and moves the camera in the appropriate direction.\n\nThe `onMouseMove` function is called when the user moves the mouse. It updates the camera's orientation based on the movement of the mouse. It checks if the `mode` is set to `Lock` and if the mouse pointer is locked. If it is not, the function returns. If the `mode` is set to `Drag` and the left mouse button is not down, the function returns. The function then updates the camera's orientation based on the movement of the mouse.\n\nThe `onMouseDown` function is called when the user presses a mouse button. If the left mouse button is pressed, it sets the `lmbDown` flag to true. If the `mode` is set to `Lock` and the mouse pointer is not locked, it enables pointer lock.\n\nThe `onMouseUp` function is called when the user releases a mouse button. If the left mouse button is released, it sets the `lmbDown` flag to false.\n\nOverall, the `FlyCamera` script provides a way for the user to control the camera in a 3D scene. It allows the user to move the camera around the scene and change its orientation. The script can be used in any PlayCanvas project that requires camera control. An example of how to use the `FlyCamera` script can be seen below:\n\n```javascript\n// Create a new entity and add the FlyCamera script to it\nvar cameraEntity = new pc.Entity();\ncameraEntity.addComponent('camera');\ncameraEntity.addComponent('script');\ncameraEntity.script.create('flyCamera');\n\n// Add the entity to the scene\napp.root.addChild(cameraEntity);\n```\n## Questions: \n 1. What does this code do?\n- This code defines a script called `FlyCamera` that allows the user to control the position and orientation of a camera in a 3D scene using the keyboard and mouse.\n\n2. What are the default values for the `speed`, `fastSpeed`, and `mode` attributes?\n- The default value for `speed` is 10, the default value for `fastSpeed` is 20, and the default value for `mode` is 0 (which corresponds to the \"Lock\" option in the enum).\n\n3. What events does the script listen for and how does it respond to them?\n- The script listens for mouse move, mouse down, and mouse up events using the `app.mouse.on` method. It responds to these events by updating the camera's orientation and position based on the user's input.","metadata":{"source":".autodoc/docs/markdown/scripts/camera/fly-camera.md"}}],["170",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/camera/follow-camera.js)\n\nThe code defines a script called `FollowCamera` that can be attached to an entity in a PlayCanvas project. The purpose of this script is to make a camera follow a target entity in a smooth and frame rate independent way. \n\nThe script has three attributes that can be set in the editor: `target`, `cameraOffset`, and `lerpAmount`. `target` is the entity that the camera will follow. `cameraOffset` is a vector that specifies the local space offset of the camera with respect to the target entity coordinate system. `lerpAmount` is a number between 0 and 1 that controls how fast the camera moves towards its desired position. A value of 1 means the camera moves instantly, while a value of 0 means the camera never moves.\n\nThe `initialize` function is called once per entity and initializes some variables used by the script. If a `target` entity is specified, the function calls `updateTargetPosition` to calculate the initial position of the camera. If no `target` entity is specified, the camera will stay at the position of the entity that the script is attached to.\n\nThe `updateTargetPosition` function calculates the desired position of the camera based on the position and orientation of the `target` entity and the `cameraOffset`. It first calculates the angle of the `target` entity around the world Y axis, and then constructs a transformation matrix that rotates the `target` entity to face that angle. It then applies the `cameraOffset` to the `target` entity's position in world space to get the desired position of the camera.\n\nThe `postUpdate` function is called every frame and updates the position and orientation of the camera. It first calls `updateTargetPosition` to calculate the desired position of the camera. It then uses a technique called \"lerping\" to smoothly move the camera towards its desired position. The `lerpAmount` attribute controls how fast the camera moves towards its desired position. Finally, it sets the position of the camera to the current position and makes it look at the `target` entity.\n\nOverall, this script provides a simple and flexible way to make a camera follow a target entity in a smooth and frame rate independent way. It can be used in a variety of games and applications where a camera needs to follow a moving object. For example, it could be used in a racing game to follow the player's car, or in a third-person action game to follow the player's character.\n## Questions: \n 1. What is the purpose of this script in the PlayCanvas engine?\n- This script is a camera-following script that allows the camera to follow a target entity.\n\n2. What are the attributes that can be set for this script?\n- The attributes that can be set for this script are the target entity to follow, the camera offset with respect to the target entity coordinate system, and the amount to lerp the camera towards its desired position over time.\n\n3. How does the lerping work in this script?\n- The lerping in this script is framerate independent and will be correct for every frame rate. The closer the lerp amount is to 1, the faster the camera will move. The current camera position is lerped towards the desired position using a formula from https://www.rorydriscoll.com/2016/03/07/frame-rate-independent-damping-using-lerp/.","metadata":{"source":".autodoc/docs/markdown/scripts/camera/follow-camera.md"}}],["171",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/camera/tracking-camera.js)\n\nThe code above defines a script called `trackingCamera` in the PlayCanvas engine project. This script is responsible for tracking the position of a target entity and updating the camera's position to follow it.\n\nThe `TrackingCamera` script has one attribute called `target`, which is of type `entity`. This attribute is used to specify the entity that the camera should track.\n\nThe `postUpdate` function is called every frame and is responsible for updating the camera's position. If the `target` attribute is set, the function retrieves the position of the target entity and updates the camera's position to look at it using the `lookAt` method.\n\nThis script can be used in a variety of scenarios, such as in a game where the camera needs to follow the player character. By attaching the `TrackingCamera` script to the camera entity and setting the `target` attribute to the player entity, the camera will automatically follow the player's movements.\n\nHere is an example of how to use the `TrackingCamera` script in PlayCanvas:\n\n```javascript\n// create a camera entity\nvar cameraEntity = new pc.Entity('camera');\ncameraEntity.addComponent('camera');\n\n// create a player entity\nvar playerEntity = new pc.Entity('player');\n\n// add the TrackingCamera script to the camera entity\ncameraEntity.addComponent('script');\ncameraEntity.script.create('trackingCamera', {\n    target: playerEntity\n});\n\n// add the camera and player entities to the scene\napp.root.addChild(cameraEntity);\napp.root.addChild(playerEntity);\n```\n\nIn this example, the `TrackingCamera` script is attached to the camera entity and the `target` attribute is set to the player entity. As a result, the camera will follow the player's movements in the scene.\n## Questions: \n 1. **What is the purpose of this script?** \n    This script is called \"trackingCamera\" and it appears to be used for updating the position of the camera to follow a target entity.\n\n2. **What type of attribute is the 'target' attribute?** \n    The 'target' attribute is of type 'entity', which suggests that it is used to reference another entity in the scene.\n\n3. **What does the 'postUpdate' function do?** \n    The 'postUpdate' function is called every frame and checks if there is a target entity. If there is, it updates the position of the camera to look at the target entity's position.","metadata":{"source":".autodoc/docs/markdown/scripts/camera/tracking-camera.md"}}],["172",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/parsers/obj-model.js)\n\n# PlayCanvas Engine - Obj Model Parser\n\nThe `ObjModelParser` is a JavaScript class that provides a parser for OBJ files. OBJ files are a common file format used for storing 3D models. This parser is not included in the PlayCanvas engine library by default, but can be added to the engine by following the instructions provided in the code comments.\n\nThe `ObjModelParser` class provides a `parse` method that takes an OBJ file as input and returns a `pc.Model` object. The `pc.Model` object represents a 3D model that can be rendered in a PlayCanvas application. The `parse` method also takes a callback function that is called when the parsing is complete.\n\nThe `ObjModelParser` class also provides a `_parseIndices` method that is used internally by the `parse` method to parse the face indices in the OBJ file.\n\nThe `ObjModelParser` class is used in the PlayCanvas engine to load OBJ files as model assets. To use the `ObjModelParser`, you first need to register it with the model resource handler. This is done using the following code:\n\n```javascript\nvar objParser = new pc.ObjModelParser(this.app.graphicsDevice);\nthis.app.loader.getHandler(\"model\").addParser(objParser, function (url) {\n    return (pc.path.getExtension(url) === '.obj');\n});\n```\n\nThis code creates a new `pc.ObjModelParser` object and registers it with the model resource handler. The `addParser` method takes two arguments: the parser object and a function that returns `true` if the parser should be used to parse the file with the given URL.\n\nOnce the parser is registered, you can load an OBJ file as a model asset using the following code:\n\n```javascript\nvar asset = new pc.Asset(\"MyObj\", \"model\", {\n   url: \"model.obj\"\n});\nthis.app.assets.add(asset);\nthis.app.assets.load(asset);\n```\n\nThis code creates a new `pc.Asset` object with the name \"MyObj\" and the type \"model\". The `url` property specifies the URL of the OBJ file to load. The asset is then added to the PlayCanvas application's asset registry and loaded using the `load` method.\n\nWhen the asset is loaded, the `pc.ObjModelParser` is used to parse the OBJ file and create a `pc.Model` object. The `pc.Model` object can then be used to render the 3D model in the PlayCanvas application.\n\nOverall, the `ObjModelParser` class provides a convenient way to load OBJ files as model assets in a PlayCanvas application.\n## Questions: \n 1. What is the purpose of this code?\n- This code is a sample Obj model parser that is not built into the PlayCanvas engine library by default. It allows developers to register the parser and load obj as a model asset.\n\n2. What are some known issues with this code?\n- The code cannot handle meshes larger than 65535 verts, assigns default material to all meshes, and doesn't create indexed geometry.\n\n3. How does the code parse the input?\n- The code parses the input by splitting it into lines and then processing each line based on its first character. If the first character is 'v', the code adds the vertex, normal, or uv to the corresponding array. If the first character is 'g', 'o', or 'u', the code splits the input into groups. If the first character is 'f', the code parses the face indices and expands the corresponding vertices, uvs, and normals. Finally, the code creates a new mesh instance for each group and adds it to the model.","metadata":{"source":".autodoc/docs/markdown/scripts/parsers/obj-model.md"}}],["173",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/physics/action-physics-reset.js)\n\nThe code defines a script called `ActionPhysicsReset` that can be attached to entities in a PlayCanvas project. The purpose of the script is to reset the position and orientation of an entity's dynamic rigid body when a specified event is fired on the app. \n\nThe script defines an attribute called `event`, which is a string that represents the name of the event that will trigger the reset. When the script is initialized, it retrieves the position and rotation of the entity and stores them in variables. It then defines a function called `reset` that will be called when the specified event is fired. The `reset` function checks if the entity has a dynamic rigid body and, if so, resets it to its initial position and orientation with zero linear and angular velocity. \n\nThe script also sets up event listeners to handle changes to the `event` attribute and to clean up when the script is destroyed. When the `event` attribute is changed, the script removes the old event listener and adds a new one for the updated event. When the script is destroyed, it removes the event listener for the `event` attribute.\n\nThis script can be useful in a variety of scenarios where an entity's position and orientation need to be reset in response to a specific event. For example, it could be used in a game where the player needs to return to a starting position after completing a level or in a physics simulation where objects need to be reset to their initial state after a certain action is taken. \n\nHere is an example of how to use the `ActionPhysicsReset` script in a PlayCanvas project:\n\n1. Attach the script to an entity that has a dynamic rigid body component.\n2. Set the `event` attribute to the name of the event that should trigger the reset.\n3. Fire the specified event on the app to trigger the reset of the entity's rigid body.\n## Questions: \n 1. What is the purpose of this script in the PlayCanvas engine?\n- This script resets the position and orientation of an entity with a dynamic rigid body when a specified event is fired on the app.\n\n2. What attributes can be added to this script?\n- The only attribute that can be added to this script is 'event', which specifies the event that triggers the reset.\n\n3. What happens when the entity being reset does not have a dynamic rigid body?\n- Nothing happens when the entity being reset does not have a dynamic rigid body, as the reset function only executes if the entity has a dynamic rigid body.","metadata":{"source":".autodoc/docs/markdown/scripts/physics/action-physics-reset.md"}}],["174",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/physics/render-physics.js)\n\nThe `RenderPhysics` script is responsible for rendering debug shapes for physics collision components in the PlayCanvas engine. The script is attached to an entity and has three attributes: `drawShapes`, `opacity`, and `castShadows`. \n\nThe `initialize` function is called once per entity and sets up event listeners for attribute changes and script enable/disable events. When the `castShadows` attribute changes, the script updates the `castShadows` property of each child model in the `debugRoot` entity. When the `opacity` attribute changes, the script updates the `opacity` property of each material in each child mesh instance in the `debugRoot` entity. When the script is enabled, a new `debugRoot` entity is created and added to the app root. When the script is disabled, the `debugRoot` entity is destroyed and any `_debugShape` properties on collision components are deleted.\n\nThe `createModel` function creates a new model with a single mesh instance using the provided mesh and material.\n\nThe `postUpdate` function is called once per frame and is responsible for updating the debug shapes. First, it marks all existing debug shapes as not updated. If `drawShapes` is true, it finds all collision components in the app root and updates their debug shapes. If a collision component does not have a `_debugShape` property, a new debug shape is created and added to the `debugRoot` entity. The debug shape is created using a random color material and a mesh created using the collision component's properties. If the collision component has a rigid body, the debug shape is positioned and rotated using the rigid body's position and rotation. Otherwise, the debug shape is positioned and rotated using the collision component's entity's position and rotation. If the collision component is a capsule, cone, or cylinder, the debug shape is rotated to take into account the component's axis. If a debug shape was not updated during this frame, it is assumed that the corresponding collision component no longer exists and the debug shape is deleted.\n\nOverall, the `RenderPhysics` script provides a way to visualize physics collision components in the PlayCanvas engine, which can be useful for debugging and testing physics interactions. Developers can attach this script to entities with collision components and adjust the `drawShapes`, `opacity`, and `castShadows` attributes to customize the debug shapes.\n## Questions: \n 1. What does this code do?\n- This code defines a script called `RenderPhysics` that can be attached to entities in a PlayCanvas project to render physics collision shapes. It has attributes for controlling the appearance of the shapes and handles events for changing those attributes.\n\n2. What are the possible values for the `type` property of the `drawShapes` attribute?\n- The `drawShapes` attribute is a boolean type, so it can only have the values `true` or `false`. \n\n3. What happens when the `disable` event is triggered on an entity with the `RenderPhysics` script?\n- When the `disable` event is triggered, the script removes any debug shapes associated with collision components on the entity and destroys the `debugRoot` entity that contains them.","metadata":{"source":".autodoc/docs/markdown/scripts/physics/render-physics.md"}}],["175",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/physics/vehicle.js)\n\nThe code defines a script called `vehicle` that creates a vehicle with wheels and allows it to be controlled. The script is used to create a vehicle in the PlayCanvas engine project. The script has several attributes that can be set to customize the vehicle, including the number of wheels, the maximum engine force, the maximum braking force, and the maximum steering angle. \n\nThe `initialize` function is called once per entity and creates a vehicle with wheels. It sets up the vehicle's wheels by adding them to the vehicle and setting their properties, such as friction, suspension, and roll influence. The function also sets up event handling for the vehicle, such as enabling and disabling the vehicle and destroying it when it is no longer needed. \n\nThe `update` function is called every frame and updates the vehicle's position and rotation based on its speed and steering angle. It applies engine and braking forces to the back wheels and steering forces to the front wheels. \n\nThe script also defines two other scripts, `vehicleWheel` and `vehicleControls`. The `vehicleWheel` script defines the properties of a wheel, such as its radius, width, and suspension stiffness. The `vehicleControls` script allows the vehicle to be controlled using buttons or keyboard keys. \n\nOverall, the `vehicle` script is a key component of the PlayCanvas engine project, allowing developers to create and control vehicles with wheels. It provides a customizable and flexible way to create vehicles that can be used in a variety of games and simulations. \n\nExample usage:\n\n```\n// Create a new entity and add the vehicle script to it\nvar car = new pc.Entity();\ncar.addComponent('script');\ncar.script.create('vehicle', {\n    wheels: [wheel1, wheel2, wheel3, wheel4],\n    maxEngineForce: 3000,\n    maxBrakingForce: 200,\n    maxSteering: 0.5\n});\n\n// Add the car to the scene\napp.root.addChild(car);\n\n// Control the car using buttons\nvar controls = new pc.Entity();\ncontrols.addComponent('script');\ncontrols.script.create('vehicleControls', {\n    targetVehicle: car,\n    leftButton: leftButton,\n    rightButton: rightButton,\n    forwardButton: forwardButton,\n    reverseButton: reverseButton\n});\n\n// Add the controls to the scene\napp.root.addChild(controls);\n```\n## Questions: \n 1. What are the attributes of the Vehicle script?\n- The Vehicle script has attributes for wheels (an array of entities), max engine force (a number), max braking force (a number), and max steering (a number).\n\n2. What does the VehicleControls script do?\n- The VehicleControls script handles input for controlling a vehicle, including buttons for left, right, forward, and reverse, as well as keyboard input for the same controls.\n\n3. What is the purpose of the VehicleWheel script?\n- The VehicleWheel script defines attributes for a wheel entity, including whether it is a front wheel, its radius, width, suspension stiffness, damping, compression, rest length, roll influence, friction slip, and debug rendering.","metadata":{"source":".autodoc/docs/markdown/scripts/physics/vehicle.md"}}],["176",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-blend.js)\n\n# BlendEffect and Blend Script\n\nThe `BlendEffect` class is a post-processing effect that blends the input render target with another texture. It is a subclass of the `PostEffect` class and is used to create a new instance of the post effect. The `BlendEffect` constructor takes a `GraphicsDevice` object as its argument.\n\nThe `BlendEffect` class has two properties: `blendMap` and `mixRatio`. The `blendMap` property is a texture that is used to blend the input render target with. The `mixRatio` property is a number that specifies the amount of blending between the input and the blendMap. It ranges from 0 to 1.\n\nThe `BlendEffect` class has a `render` method that takes three arguments: `inputTarget`, `outputTarget`, and `rect`. The `inputTarget` argument is the input render target, the `outputTarget` argument is the output render target, and the `rect` argument is the rectangle that defines the area of the screen to render to. The `render` method sets the values of the uniforms used in the shader and then draws a quad using the `drawQuad` method of the `PostEffect` class.\n\nThe `Blend` script is a script that can be attached to an entity in a PlayCanvas scene. It has two attributes: `mixRatio` and `blendMap`. The `mixRatio` attribute is a number that specifies the amount of blending between the input and the blendMap. It ranges from 0 to 1. The `blendMap` attribute is a texture that is used to blend the input render target with.\n\nThe `Blend` script has an `initialize` method that creates a new instance of the `BlendEffect` class and sets its properties. It then adds the effect to the post effects queue of the entity's camera. The `Blend` script also has event handlers for changes to the `mixRatio` and `blendMap` attributes, which update the corresponding properties of the `BlendEffect` instance.\n\nOverall, the `BlendEffect` class and `Blend` script provide a way to blend the input render target with another texture as a post-processing effect in a PlayCanvas scene.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a post effect called BlendEffect that blends an input render target with another texture using a mix ratio. It also defines a script called Blend that initializes and adds the BlendEffect to a camera's post effects queue.\n\n2. What are the inputs and outputs of the BlendEffect's render function?\n- The render function takes in an inputTarget (the input render target), an outputTarget (the output render target), and a rect (the rectangle defining the area to render to). It does not have any output as it directly renders to the outputTarget.\n\n3. What is the purpose of the Blend script's initialize function?\n- The initialize function creates a new instance of the BlendEffect and sets its mixRatio and blendMap properties based on the script's attributes. It then adds the effect to the camera's post effects queue and sets up event listeners to update the effect's properties when the script's attributes change or when the script is destroyed.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-blend.md"}}],["177",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-bloom.js)\n\n## Code Explanation\n\nThe code defines a post-processing effect called `BloomEffect` that applies a bloom effect to the scene. The bloom effect is a visual effect that simulates the way bright objects in the real world can cause a \"glow\" or \"halo\" effect around them. The effect is achieved by extracting the brightest parts of the image, blurring them, and then combining them back with the original image.\n\nThe `BloomEffect` class extends the `PostEffect` class, which is a base class for all post-processing effects in the PlayCanvas engine. The class has three main properties that control the intensity, threshold, and blur amount of the bloom effect. The class also defines three shaders that are used to extract the bright areas of the image, blur them horizontally and vertically, and combine them back with the original image.\n\nThe `render` method of the `BloomEffect` class is responsible for rendering the bloom effect. The method takes an input target, an output target, and a rectangle that defines the area of the screen to render. The method first resizes the render targets used for the effect, then it renders the scene into the first render target using the `extractShader` shader. The method then applies a horizontal blur to the first render target and stores the result in the second render target using the `blurShader` shader. The method applies a vertical blur to the second render target and stores the result back in the first render target using the same `blurShader` shader. Finally, the method combines the first render target with the original image and stores the result in the output target using the `combineShader` shader.\n\nThe `Bloom` script is a component that can be attached to a camera entity to enable the bloom effect. The script creates an instance of the `BloomEffect` class and adds it to the camera's post-effects queue. The script also listens for changes to the `bloomIntensity`, `bloomThreshold`, and `blurAmount` properties and updates the effect accordingly.\n\n## Example Usage\n\nTo use the `Bloom` script, attach it to a camera entity in your scene. You can then adjust the `bloomIntensity`, `bloomThreshold`, and `blurAmount` properties in the script's attributes panel to control the strength and appearance of the bloom effect. For example, you can increase the `bloomIntensity` property to make the bloom effect more pronounced, or decrease the `bloomThreshold` property to include more pixels in the effect. You can also adjust the `blurAmount` property to control the amount of blurring applied to the bright areas of the image.\n## Questions: \n 1. What is the purpose of the `calculateBlurValues` function?\n- The `calculateBlurValues` function is used to compute the weights and offsets for the gaussian blur filter.\n\n2. What are the default values for `bloomThreshold`, `blurAmount`, and `bloomIntensity`?\n- The default value for `bloomThreshold` is 0.25, the default value for `blurAmount` is 4, and the default value for `bloomIntensity` is 1.25.\n\n3. What is the purpose of the `Bloom` script?\n- The `Bloom` script is used to add a bloom post-processing effect to a camera in a PlayCanvas scene. It allows the developer to adjust the intensity, threshold, and blur amount of the effect.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-bloom.md"}}],["178",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-bokeh.js)\n\n# BokehEffect\n\nThe `BokehEffect` class is a post-processing effect that simulates the depth of field effect of a camera lens. It is a subclass of the `PostEffect` class and is used to create a new instance of the post effect. The `BokehEffect` class takes in a `GraphicsDevice` object as a parameter and sets the `needsDepthBuffer` property to `true`.\n\nThe `BokehEffect` class uses a depth-of-field shader with bokeh, which is a blur filter that simulates the out-of-focus areas of a photograph. The shader is ported from a GLSL shader by Martins Upitis. The shader takes in a color buffer, a maximum blur amount, an aperture value, a focus value, and an aspect ratio. The shader calculates the factor by which to blur the image based on the depth of the pixel and the focus value. The shader then applies the blur to the image using a series of texture samples.\n\nThe `BokehEffect` class has three properties: `maxBlur`, `aperture`, and `focus`. The `maxBlur` property sets the maximum amount of blurring, which ranges from 0 to 1. The `aperture` property sets the size of the aperture, which affects the depth of field. Bigger values create a shallower depth of field. The `focus` property controls the focus of the effect.\n\nThe `BokehEffect` class has a `render` method that takes in an input target, an output target, and a rectangle. The method sets the values of the uniforms in the shader and then draws a quad using the shader and the output target.\n\n# Bokeh Script\n\nThe `Bokeh` script is a script that adds the `BokehEffect` post-processing effect to a camera. The script has three attributes: `maxBlur`, `aperture`, and `focus`. The attributes are used to set the properties of the `BokehEffect` object.\n\nThe `Bokeh` script initializes a new instance of the `BokehEffect` class and sets its properties to the values of the attributes. The script then adds the effect to the camera's post-effects queue. The script also listens for changes to the attributes and updates the effect accordingly. Finally, the script removes the effect from the queue when the script is destroyed.\n## Questions: \n 1. What is the purpose of this code?\n- This code implements the BokehEffect post-processing effect in the PlayCanvas engine.\n\n2. What are the properties of the BokehEffect object?\n- The BokehEffect object has three properties: maxBlur, aperture, and focus.\n\n3. How is the BokehEffect object used in the PlayCanvas engine?\n- The BokehEffect object is used as a post-processing effect in the PlayCanvas engine. It is added to the camera's postEffects queue and applied to the scene during rendering.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-bokeh.md"}}],["179",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-brightnesscontrast.js)\n\nThe code defines a post-processing effect called BrightnessContrastEffect that can be applied to a render target. The effect changes the brightness and contrast of the input render target. The code creates a new instance of the post effect by extending the PostEffect class. The constructor takes a graphics device as a parameter and initializes the brightness and contrast properties to 0.\n\nThe BrightnessContrastEffect class has a render method that takes an inputTarget, outputTarget, and rect as parameters. The method sets the brightness and contrast values as uniforms on the shader, sets the input color buffer as a uniform, and draws a quad to the output target using the shader.\n\nThe code also defines a script called BrightnessContrast that creates an instance of the BrightnessContrastEffect and adds it to the camera's postEffects queue. The script has two attributes, brightness and contrast, that can be set in the editor. When the script is initialized, it creates a new instance of the BrightnessContrastEffect and sets its brightness and contrast properties to the values of the script's attributes. It then adds the effect to the camera's postEffects queue. When the script's attributes are changed, it updates the effect's properties accordingly. When the script is enabled or disabled, it adds or removes the effect from the postEffects queue.\n\nThis code can be used in the larger PlayCanvas engine project to create custom post-processing effects for games and applications. The BrightnessContrastEffect can be used to adjust the brightness and contrast of a scene, which can be useful for creating different moods or emphasizing certain elements. The script can be attached to a camera entity to apply the effect to the scene. The attributes can be adjusted in the editor to fine-tune the effect. Overall, this code provides a flexible and customizable way to add post-processing effects to PlayCanvas projects.\n## Questions: \n 1. What does this code do?\n- This code defines a post effect called BrightnessContrastEffect that changes the brightness and contrast of the input render target. It also creates a script called BrightnessContrast that initializes the effect and adds it to the camera's post effects queue.\n\n2. What are the valid ranges for the brightness and contrast properties?\n- The brightness and contrast properties both have valid ranges from -1 to 1. A value of -1 represents solid black or gray, 0 represents no change, and 1 represents solid white or maximum contrast.\n\n3. Who is the author of the shader used in this code?\n- The shader used in this code was authored by tapio and can be found at http://tapio.github.com/.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-brightnesscontrast.md"}}],["180",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-edgedetect.js)\n\n# PlayCanvas Engine: EdgeDetectEffect\n\nThe `EdgeDetectEffect` class is a post-processing effect that applies an edge detection filter to the rendered scene. It uses the Sobel filter to calculate the intensity of the edges in the scene and applies a color and intensity to the detected edges.\n\n## Usage\n\nTo use the `EdgeDetectEffect` in a PlayCanvas project, you can create an instance of the `EdgeDetect` script and attach it to a camera entity. The script will automatically add the effect to the camera's post-processing queue.\n\n```javascript\nvar cameraEntity = app.root.findByName('Camera');\nvar edgeDetect = cameraEntity.addComponent('script');\nedgeDetect.script.create('edgeDetect', {\n    intensity: 1.5,\n    color: [1, 0, 0, 1]\n});\n```\n\n## Constructor\n\n### EdgeDetectEffect(graphicsDevice)\n\nCreates a new instance of the `EdgeDetectEffect` post effect.\n\n- `graphicsDevice` - The graphics device of the application.\n\n## Properties\n\n### resolution\n\nA `Float32Array` that represents the resolution of the input target.\n\n### intensity\n\nA `float` that represents the intensity of the edge detection effect.\n\n### color\n\nA `pc.Color` that represents the color of the detected edges.\n\n## Methods\n\n### render(inputTarget, outputTarget, rect)\n\nRenders the effect to the output target.\n\n- `inputTarget` - The input target to render.\n- `outputTarget` - The output target to render to.\n- `rect` - The rectangle to render to.\n\n## Script Attributes\n\n### intensity\n\nA `number` that represents the intensity of the edge detection effect. The default value is `1` and the valid range is `0` to `2`.\n\n### color\n\nAn `rgba` color that represents the color of the detected edges. The default value is `[0.5, 0.5, 0.5, 1]`.\n\n## Example\n\n```javascript\nvar cameraEntity = app.root.findByName('Camera');\nvar edgeDetect = cameraEntity.addComponent('script');\nedgeDetect.script.create('edgeDetect', {\n    intensity: 1.5,\n    color: [1, 0, 0, 1]\n});\n```\n\nThis code creates a new instance of the `EdgeDetect` script and attaches it to a camera entity. It sets the intensity of the effect to `1.5` and the color of the detected edges to red. The effect will be automatically added to the camera's post-processing queue.\n## Questions: \n 1. What does this code do?\n- This code defines a post effect called EdgeDetectEffect that uses a Sobel filter to detect edges in an image. It creates a new instance of the effect and sets its intensity and color attributes. It also defines a script called EdgeDetect that initializes the effect and adds it to the camera's post effects queue.\n\n2. What is the purpose of the \"cnv\" array in the fragment shader?\n- The \"cnv\" array is used to store the convolution values for the two masks used in the Sobel filter. These values are squared and then added together to get the final edge detection intensity value.\n\n3. What is the significance of the \"attr\" event listener in the EdgeDetect script?\n- The \"attr\" event listener is used to update the effect's attributes (intensity and color) whenever they are changed in the editor. This allows the effect to be customized without having to modify the script code directly.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-edgedetect.md"}}],["181",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-fxaa.js)\n\n# PlayCanvas Engine: FxaaEffect\n\nThe `FxaaEffect` class is a post-processing effect that implements the Fast Approximate Anti-Aliasing (FXAA) algorithm by NVIDIA. This effect is used to smooth out jagged edges and improve the overall visual quality of the rendered scene. \n\nThe `FxaaEffect` class extends the `PostEffect` class, which is a base class for all post-processing effects in the PlayCanvas engine. It takes a `GraphicsDevice` object as a parameter and creates a new instance of the post effect. \n\nThe `FxaaEffect` class contains a shader that performs the FXAA algorithm. The shader takes a color buffer texture and the resolution of the screen as input, and outputs the final color of each pixel after applying the FXAA algorithm. The shader uses several techniques to smooth out jagged edges, such as edge detection, direction calculation, and color blending. \n\nThe `FxaaEffect` class also contains a `render` method that takes an input target, an output target, and a rectangle as parameters. The method sets the resolution of the screen and the color buffer texture as uniforms in the shader, and then draws a quad using the shader to the output target. \n\nThe `Fxaa` script is a wrapper around the `FxaaEffect` class that can be attached to a camera entity in the scene. The script initializes the `FxaaEffect` object and adds it to the post-effects queue of the camera. It also listens for changes in the state of the script and adds or removes the effect from the queue accordingly. \n\nOverall, the `FxaaEffect` class and the `Fxaa` script are essential components of the PlayCanvas engine's post-processing pipeline. They provide a simple and efficient way to apply the FXAA algorithm to the rendered scene and improve its visual quality. \n\nExample usage:\n\n```javascript\nvar fxaaEffect = new FxaaEffect(graphicsDevice);\nvar fxaaScript = new Fxaa();\nfxaaScript.effect = fxaaEffect;\ncameraEntity.addComponent('script');\ncameraEntity.script.create('fxaa');\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code implements the FXAA post effect by NVIDIA for the PlayCanvas engine.\n\n2. What are the inputs and outputs of the `render` function?\n- The `render` function takes in an `inputTarget` (the target to render the effect onto), an `outputTarget` (the target to output the result to), and a `rect` (the rectangle to render the effect within).\n- The `render` function outputs the result of the effect to the `outputTarget`.\n\n3. What is the `Fxaa` script used for?\n- The `Fxaa` script is used to initialize the `FxaaEffect` and add it to the post effects queue of the entity's camera. It also handles enabling and disabling the effect based on the state of the entity.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-fxaa.md"}}],["182",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-horizontaltiltshift.js)\n\n# PlayCanvas Engine: Horizontal Tilt-Shift Effect\n\nThe `HorizontalTiltShiftEffect` class is a post-processing effect that creates a fake tilt-shift effect by modulating a two-pass Gaussian blur by horizontal position. This effect is used to simulate a miniature scene or to create a shallow depth-of-field effect.\n\n## How it Works\n\nThe `HorizontalTiltShiftEffect` class extends the `PostEffect` class and overrides the `render` method. The `render` method takes an input target, an output target, and a rectangle and applies the tilt-shift effect to the input target, rendering the result to the output target.\n\nThe `HorizontalTiltShiftEffect` class uses a custom fragment shader to apply the tilt-shift effect. The shader takes a color buffer texture, a horizontal offset (`uH`), and a focus value (`uR`) as input. The shader calculates a weighted sum of nine texture samples, each offset horizontally by a multiple of the focus value. The weights of the samples are determined by a Gaussian distribution.\n\nThe `HorizontalTiltShiftEffect` class also defines a `focus` property that controls the position of the \"focused\" vertical line. The `focus` property is used to set the `uR` uniform in the shader.\n\nThe `HorizontalTiltShift` script is a wrapper around the `HorizontalTiltShiftEffect` class that adds the effect to a camera's post-effects queue. The script defines a `focus` attribute that can be set in the editor or via script. The script creates an instance of the `HorizontalTiltShiftEffect` class and adds it to the camera's post-effects queue. The script also listens for changes to the `focus` attribute and updates the effect accordingly.\n\n## Example\n\nTo use the `HorizontalTiltShift` script, attach it to a camera entity in your scene. You can adjust the `focus` attribute to control the position of the \"focused\" vertical line.\n\n```javascript\nvar camera = app.root.findByName('Camera');\ncamera.addComponent('script');\ncamera.script.create('horizontalTiltShift', {\n    focus: 0.5\n});\n```\n\n## Limitations\n\nThe `HorizontalTiltShiftEffect` class only applies the tilt-shift effect horizontally. To apply the effect vertically, you would need to create a separate effect or modify the existing shader. Additionally, the effect is computationally expensive and may impact performance on low-end devices.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a post effect called HorizontalTiltShiftEffect that creates a fake tilt-shift effect by modulating a two-pass Gaussian blur by horizontal position.\n\n2. What graphics device does this code require?\n- This code requires a graphics device of the application as a parameter for creating a new instance of the post effect.\n\n3. What is the range of values for the \"focus\" property?\n- The \"focus\" property has a default value of 0.35 and a range of values from 0 to 1.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-horizontaltiltshift.md"}}],["183",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-huesaturation.js)\n\nThe code defines a post-processing effect called `HueSaturationEffect` that can be applied to a render target. The effect allows for adjusting the hue and saturation of the input render target. The `HueSaturationEffect` class is a subclass of `PostEffect`, which is a base class for all post-processing effects in the PlayCanvas engine.\n\nThe `HueSaturationEffect` constructor takes a `GraphicsDevice` object as an argument and creates a new instance of the post effect. The constructor also defines two properties: `hue` and `saturation`. The `hue` property controls the hue of the input render target and ranges from -1 to 1. A value of -1 represents 180 degrees in the negative direction, 0 represents no change, and 1 represents 180 degrees in the positive direction. The `saturation` property controls the saturation of the input render target and ranges from -1 to 1. A value of -1 represents solid gray, 0 represents no change, and 1 represents maximum saturation.\n\nThe `HueSaturationEffect` class also defines a `render` method that takes an input render target, an output render target, and a rectangle as arguments. The method sets the values of the `uHue`, `uSaturation`, and `uColorBuffer` uniforms in the shader and draws a quad to the output render target using the `drawQuad` method.\n\nThe code also defines a script called `HueSaturation` that can be attached to a camera entity to apply the `HueSaturationEffect` post-processing effect to the camera's output. The script defines two attributes: `hue` and `saturation`, which control the hue and saturation of the effect, respectively. The `initialize` method of the script creates a new instance of the `HueSaturationEffect` class and sets its `hue` and `saturation` properties. The method also adds the effect to the camera's post-effects queue and sets up event listeners to update the effect when the attribute values change or the script is destroyed.\n\nOverall, this code provides a way to adjust the hue and saturation of a rendered scene using a post-processing effect. It can be used in the larger PlayCanvas engine project to create a variety of visual effects, such as color grading or stylized rendering.\n## Questions: \n 1. What does this code do?\n- This code defines a post effect called HueSaturationEffect that allows hue and saturation adjustment of the input render target. It also defines a script called HueSaturation that initializes the effect and adds it to the camera's post effects queue.\n\n2. What are the valid ranges for the hue and saturation properties?\n- The hue and saturation properties range from -1 to 1, where -1 represents the minimum value and 1 represents the maximum value.\n\n3. Who is the author of the shader used in this code?\n- The shader author is tapio, and their website is http://tapio.github.com/.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-huesaturation.md"}}],["184",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-luminosity.js)\n\n## PlayCanvas Engine: Luminosity Effect\n\nThe `LuminosityEffect` class is a post-processing effect that outputs the luminosity of the input render target. This effect is used to create a grayscale version of the scene by converting the RGB color values to a single luminance value.\n\nThe `LuminosityEffect` class is defined as a subclass of the `PostEffect` class, which is a base class for all post-processing effects in the PlayCanvas engine. The constructor of the `LuminosityEffect` class takes a `GraphicsDevice` object as a parameter, which is used to create a shader program for the effect.\n\nThe shader program for the `LuminosityEffect` class is defined in the constructor using GLSL code. The shader program takes a texture sampler as input and outputs a grayscale version of the texture. The shader program calculates the luminance value of each texel in the texture by multiplying the RGB color values with a vector of weights and summing the results. The resulting luminance value is then used to set the RGB color values of the output texel, while preserving the alpha value.\n\nThe `LuminosityEffect` class overrides the `render` method of the `PostEffect` class to implement the effect. The `render` method takes an input render target, an output render target, and a rectangle as parameters. The method sets the input texture sampler of the shader program to the color buffer of the input render target and draws a full-screen quad to the output render target using the shader program.\n\nThe `Luminosity` script is a component that adds the `LuminosityEffect` to a camera's post-effects queue. The `initialize` method of the script creates a new instance of the `LuminosityEffect` class and adds it to the camera's post-effects queue. The script also listens to the `state` event of the entity to enable or disable the effect when the entity is enabled or disabled. Finally, the script removes the effect from the queue when the entity is destroyed.\n\nExample usage:\n\n```javascript\nvar entity = new pc.Entity();\nvar camera = new pc.Camera();\nentity.addComponent('camera', {\n    camera: camera,\n    postEffects: new pc.PostEffectQueue()\n});\nentity.addComponent('script', {\n    scripts: {\n        enabled: true,\n        luminosity: {\n            enabled: true\n        }\n    }\n});\n```\n\nIn this example, a new entity is created with a camera component and a script component. The camera component is configured with a new post-effects queue. The script component is configured with the `Luminosity` script and enabled by default. The `Luminosity` script adds the `LuminosityEffect` to the camera's post-effects queue, which applies the effect to the camera's output render target.\n## Questions: \n 1. What does this code do?\n- This code defines a post effect called LuminosityEffect that outputs the luminosity of the input render target. It also creates a script called Luminosity that initializes the effect and adds it to the camera's post effects queue.\n\n2. What are the inputs and outputs of the LuminosityEffect?\n- The LuminosityEffect takes an input render target and an output render target as parameters in its render function. The input target is used to sample the color buffer, and the output target is used to render the effect.\n\n3. What is the purpose of the dot product calculation in the LuminosityEffect shader?\n- The dot product calculation is used to calculate the luminosity value of the texel by multiplying the RGB values with a vector of weights (0.299, 0.587, 0.114) and summing the results. This is a common method for calculating luminosity that takes into account the sensitivity of the human eye to different colors.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-luminosity.md"}}],["185",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-outline.js)\n\nThe code defines a post-processing effect called `OutlineEffect` that applies an outline effect on an input render target. The effect is created by passing in the graphics device of the application and a thickness value to be used as a constant in the shader. The effect is implemented as a subclass of `PostEffect`, which is a base class for all post-processing effects in the PlayCanvas engine.\n\nThe `OutlineEffect` class defines a fragment shader that samples the input color buffer and an outline texture to create an outline effect. The thickness of the outline is controlled by the `THICKNESS` constant passed in during initialization. The shader loops over a range of pixels around the current pixel and checks if any of them have a non-zero alpha value in the outline texture. If so, the current pixel is considered to be on the outline and is colored with the outline color. Otherwise, the pixel is colored with the input color.\n\nThe `OutlineEffect` class also defines a `render` method that takes an input render target, an output render target, and a rectangle to render. The method sets up the shader uniforms and draws a full-screen quad with the shader to the output render target.\n\nThe code also defines a script called `Outline` that uses the `OutlineEffect` to apply an outline effect to a camera. The script defines attributes for the outline color, thickness, and texture, and creates an instance of `OutlineEffect` in the `initialize` method. The effect is added to the camera's post-effects queue, and the script listens for changes to the color and texture attributes to update the effect accordingly.\n\nOverall, this code provides a simple way to add an outline effect to a camera in a PlayCanvas project. The `Outline` script can be attached to a camera entity, and the effect can be customized by adjusting the script attributes. The `OutlineEffect` class can also be used as a base for more complex outline effects that require custom shaders or additional features.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a post effect called OutlineEffect that applies an outline effect on an input render target. It also defines a script called Outline that initializes and adds the OutlineEffect to a camera's post effects queue.\n\n2. What are the inputs and outputs of the OutlineEffect?\n- The inputs of the OutlineEffect are an input render target and a texture for the outline effect. The outputs of the OutlineEffect are an output render target with the outline effect applied.\n\n3. What is the purpose of the \"thickness\" attribute in the Outline script?\n- The \"thickness\" attribute specifies the thickness of the outline effect and is used as a constant in the shader. Changing the thickness requires reloading the effect.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-outline.md"}}],["186",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-sepia.js)\n\n## PlayCanvas Engine: SepiaEffect\n\nThe `SepiaEffect` class is a post-processing effect that applies a sepia tone filter to the rendered scene. It is a subclass of the `PostEffect` class, which is a base class for all post-processing effects in the PlayCanvas engine.\n\n### Usage\n\nTo use the `SepiaEffect` in a PlayCanvas project, you can create an instance of the `Sepia` script and attach it to a camera entity. This will add the effect to the camera's post-processing queue, which will apply the effect to the rendered scene.\n\n```javascript\nvar cameraEntity = app.root.findByName('Camera');\ncameraEntity.addComponent('script');\ncameraEntity.script.create('sepia', {\n    amount: 0.5\n});\n```\n\n### Properties\n\nThe `SepiaEffect` class has a single property:\n\n- `amount`: Controls the intensity of the effect. Ranges from 0 to 1.\n\n### Implementation\n\nThe `SepiaEffect` class overrides the `render` method of the `PostEffect` class. This method is called by the engine during the post-processing stage of rendering. It takes an input target, an output target, and a rectangle that defines the area of the screen to render to.\n\nThe method sets the value of the `uAmount` uniform in the shader to the value of the `amount` property. It then sets the value of the `uColorBuffer` uniform to the color buffer of the input target. Finally, it calls the `drawQuad` method of the `PostEffect` class to render the effect to the output target using the `SepiaShader`.\n\nThe `SepiaShader` is a fragment shader that applies the sepia tone filter to the rendered scene. It takes the color of each pixel in the scene and applies a matrix transformation to it to produce the sepia tone effect.\n\n### Script Definition\n\nThe `Sepia` script is a PlayCanvas script that provides a user-friendly interface for the `SepiaEffect` class. It allows the user to set the `amount` property of the effect using the PlayCanvas editor.\n\nThe `initialize` method of the script creates an instance of the `SepiaEffect` class and adds it to the camera's post-processing queue. It also sets up event listeners to update the effect's `amount` property when the user changes the value in the editor, and to remove the effect from the queue when the script is destroyed.\n## Questions: \n 1. What does this code do?\n- This code defines a post effect called SepiaEffect that applies a sepia color filter to an input texture.\n\n2. What is the purpose of the `amount` property?\n- The `amount` property controls the intensity of the sepia effect, ranging from 0 to 1.\n\n3. How is the SepiaEffect applied to a camera in the scene?\n- The SepiaEffect is added to the postEffects queue of a camera entity in the scene, and its `amount` property can be adjusted through the `attr:amount` event.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-sepia.md"}}],["187",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-verticaltiltshift.js)\n\n# PlayCanvas Engine: VerticalTiltShiftEffect\n\nThe `VerticalTiltShiftEffect` is a post-processing effect that creates a fake tilt-shift effect by modulating a two-pass Gaussian blur by vertical position. This effect is used to simulate a miniature scene by blurring the top and bottom of the image, creating a shallow depth of field effect. \n\nThe `VerticalTiltShiftEffect` is a class that extends the `PostEffect` class, which is a base class for all post-processing effects in the PlayCanvas engine. The `VerticalTiltShiftEffect` constructor takes a `GraphicsDevice` object as a parameter and creates a new instance of the post effect. The `focus` property controls where the \"focused\" horizontal line lies.\n\nThe `VerticalTiltShiftEffect` uses a fragment shader to apply the effect. The shader takes a color buffer texture as input and applies a vertical blur to it. The amount of blur is controlled by the `focus` property. The shader code is written in GLSL and is compiled at runtime using the `createShaderFromCode` method of the `pc` object.\n\nThe `render` method of the `VerticalTiltShiftEffect` class is responsible for rendering the effect. It takes an input target, an output target, and a rectangle as parameters. The method sets the values of the shader uniforms and draws a quad using the shader.\n\nThe `VerticalTiltShift` script is a wrapper around the `VerticalTiltShiftEffect` class that can be attached to a camera entity in the scene. The script adds the effect to the camera's post-effects queue and provides a user interface for controlling the `focus` property. The `initialize` method of the script creates a new instance of the `VerticalTiltShiftEffect` class and adds it to the camera's post-effects queue. The `on` method is used to listen for changes to the `focus` property and update the effect accordingly. The `on` method is also used to listen for changes to the script's state and enable or disable the effect accordingly. Finally, the `on` method is used to remove the effect from the post-effects queue when the script is destroyed.\n\nExample usage:\n\n```javascript\nvar cameraEntity = new pc.Entity();\ncameraEntity.addComponent('camera', {\n    postEffects: new pc.PostEffectQueue()\n});\ncameraEntity.addComponent('script');\ncameraEntity.script.create('verticalTiltShift', {\n    focus: 0.5\n});\n```\n## Questions: \n 1. What does this code do?\n- This code defines a post effect called VerticalTiltShiftEffect that creates a fake tilt-shift effect by modulating two pass Gaussian blur by vertical position. It also defines a script called VerticalTiltShift that initializes the effect and adds it to the camera's post effects queue.\n\n2. What is the purpose of the `focus` property?\n- The `focus` property controls where the \"focused\" horizontal line lies in the tilt-shift effect.\n\n3. Who is the author of the shader used in this code?\n- The shader author is alteredq from http://alteredqualia.com/.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-verticaltiltshift.md"}}],["188",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/posteffects/posteffect-vignette.js)\n\n# PlayCanvas Engine - Vignette Effect\n\nThe `VignetteEffect` class is a post-processing effect that adds a vignette to the screen. It is a part of the PlayCanvas engine project and is used to enhance the visual quality of the game. \n\n## How it Works\n\nThe `VignetteEffect` class extends the `PostEffect` class and overrides its `render` method. The `render` method takes an input target, an output target, and a rectangle as parameters. It sets the values of the `uColorBuffer`, `uOffset`, and `uDarkness` uniforms in the shader and then draws a quad using the `vignetteShader`. The `vignetteShader` is a fragment shader that applies the vignette effect to the screen.\n\nThe `Vignette` script is a wrapper around the `VignetteEffect` class that makes it easier to use in a PlayCanvas project. It adds two attributes to the script: `offset` and `darkness`. These attributes control the offset and darkness of the vignette effect. The `initialize` method of the script creates a new instance of the `VignetteEffect` class and sets its `offset` and `darkness` properties to the values of the `offset` and `darkness` attributes. It then adds the effect to the camera's post-effects queue. \n\nThe `Vignette` script also listens for changes to the `offset` and `darkness` attributes and updates the `effect` object accordingly. It also listens for changes to the `state` of the script and adds or removes the effect from the camera's post-effects queue accordingly. Finally, it listens for the `destroy` event and removes the effect from the camera's post-effects queue.\n\n## Example Usage\n\nTo use the `Vignette` script in a PlayCanvas project, you can attach it to a camera entity. You can then adjust the `offset` and `darkness` attributes to achieve the desired effect. For example:\n\n```javascript\nvar cameraEntity = app.root.findByName('Camera');\ncameraEntity.addComponent('script');\ncameraEntity.script.create('vignette', {\n    offset: 0.5,\n    darkness: 0.5\n});\n```\n\nThis will add a vignette effect to the camera with an offset of 0.5 and a darkness of 0.5.\n## Questions: \n 1. What is the purpose of this code?\n- This code implements the VignetteEffect post processing effect in the PlayCanvas engine.\n\n2. What are the properties of the VignetteEffect?\n- The VignetteEffect has two properties: offset, which controls the offset of the effect, and darkness, which controls the darkness of the effect.\n\n3. How is the VignetteEffect applied to an entity's camera?\n- The VignetteEffect is added to an entity's camera postEffects queue in the initialize function of the Vignette script, and can be enabled or disabled using the state attribute.","metadata":{"source":".autodoc/docs/markdown/scripts/posteffects/posteffect-vignette.md"}}],["189",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/textmesh/earcut-license.txt)\n\nThis code is a license file for the Mapbox software, which is being used in the PlayCanvas engine project. The license grants permission to use, copy, modify, and distribute the software for any purpose, with or without fee, as long as the copyright notice and permission notice are included in all copies.\n\nThe license also includes a disclaimer that the software is provided \"as is\" and the author (Mapbox) does not provide any warranties with regard to the software, including any implied warranties of merchantability and fitness. The author is not liable for any special, direct, indirect, or consequential damages or any damages whatsoever resulting from loss of use, data, or profits, whether in an action of contract, negligence, or other tortious action, arising out of or in connection with the use or performance of the software.\n\nThis license file is important for the PlayCanvas engine project as it ensures that the project is legally allowed to use and distribute the Mapbox software. It also provides a clear understanding of the terms and conditions under which the software can be used, which is important for maintaining the integrity of the project and avoiding any legal issues.\n\nAn example of how this license file may be used in the PlayCanvas engine project is by including it in the project's documentation or in the source code repository. This ensures that anyone who uses or contributes to the project is aware of the license terms and conditions and can comply with them.\n## Questions: \n 1. Who is the author of this code and what is their affiliation with the PlayCanvas engine?\n- The author of this code is Mapbox, but there is no indication of their affiliation with the PlayCanvas engine.\n2. What is the purpose of this code and how does it relate to the PlayCanvas engine?\n- The code is a license for the software, indicating the terms under which it can be used, copied, and distributed. It does not directly relate to the functionality of the PlayCanvas engine.\n3. What are the limitations of liability for the author of this code?\n- The author disclaims all warranties and is not liable for any damages resulting from the use or performance of the software, including loss of use, data, or profits.","metadata":{"source":".autodoc/docs/markdown/scripts/textmesh/earcut-license.md"}}],["190",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/textmesh/opentype-license.txt)\n\nThis code is a license file for the PlayCanvas engine project, which is a 3D game engine that allows developers to create games and interactive experiences for the web. The license file specifies the terms and conditions under which the software can be used, copied, modified, merged, published, distributed, sublicensed, and sold.\n\nThe license is the MIT License, which is a permissive open-source license that allows users to freely use, modify, and distribute the software as long as they include the copyright notice and permission notice in all copies or substantial portions of the software. The license also disclaims any warranties and limits the liability of the authors and copyright holders.\n\nThis license file is important for the PlayCanvas engine project because it ensures that the software can be used and distributed legally and ethically. It also promotes collaboration and innovation by allowing developers to freely modify and build upon the software.\n\nHere is an example of how this license file can be used in the PlayCanvas engine project:\n\n```javascript\n// This code uses the PlayCanvas engine under the terms of the MIT License.\n\n// Import the PlayCanvas engine module\nimport * as pc from 'playcanvas';\n\n// Create a new PlayCanvas application\nconst app = new pc.Application();\n\n// Add a new scene to the application\nconst scene = new pc.Scene();\napp.scenes.add(scene);\n\n// Create a new entity in the scene\nconst entity = new pc.Entity();\nscene.addEntity(entity);\n\n// Set the position of the entity\nentity.setPosition(0, 0, 0);\n\n// Run the application\napp.start();\n```\n\nIn this example, the developer is using the PlayCanvas engine module under the terms of the MIT License. They are creating a new PlayCanvas application, adding a new scene to the application, creating a new entity in the scene, setting the position of the entity, and running the application. The license file ensures that the developer can legally use and distribute the PlayCanvas engine module in their project.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains the MIT License for the PlayCanvas engine project, granting permission for anyone to use, modify, and distribute the software.\n\n2. Who is the author of this code file?\n- The author of this code file is Frederik De Bleser.\n\n3. What are the conditions for using the software?\n- The software can be used without restriction, but the copyright notice and permission notice must be included in all copies or substantial portions of the software. The software is provided \"as is\" without warranty, and the authors or copyright holders are not liable for any claims, damages, or other liability.","metadata":{"source":".autodoc/docs/markdown/scripts/textmesh/opentype-license.md"}}],["191",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/utils/cubemap-renderer.js)\n\nThe code defines a script called `CubemapRenderer` that generates a cubemap texture from a camera's perspective. The script can be used in the PlayCanvas Editor to create an entity with a camera component, which defines where the cubemap is rendered from and its properties. The cubemap can then be accessed using the `material.cubeMap` property in a script.\n\nThe `CubemapRenderer` script has three attributes: `resolution`, `mipmaps`, and `depth`. `resolution` sets the resolution of one side of the cubemap, `mipmaps` determines whether mipmaps will be allocated and autogenerated, and `depth` specifies whether a depth buffer will be created.\n\nThe `initialize` function is called once per entity and initializes the cubemap texture. It first checks if the entity has a camera component and logs an error if it does not. It then disables the camera component since it is only used as a source of properties. The maximum texture size is limited to the minimum of the `resolution` attribute and the maximum cubemap size of the graphics device. A cubemap render target is created with the specified resolution and mipmap generation. The angles to render the camera for all six faces of the cubemap are defined using quaternions. The rendering is set up for all six faces by creating a child entity with a camera for each face. The camera properties are copied from the original camera component, and the camera renders into the texture target. Before the first camera renders, the `onCubemapPreRender` event is triggered on the entity. When the last camera is finished rendering, the `onCubemapPostRender` event is triggered on the entity. These events can be listened to by the user, and the resulting cubemap can be further processed (e.g., prefiltered).\n\nAn example of how to use the `CubemapRenderer` script in a script is as follows:\n\n```\nvar entity = app.root.findByName('MyEntity');\nvar material = entity.model.meshInstances[0].material;\nmaterial.cubeMap = entity.script.cubemapRenderer.cubeMap;\n```\n\nThis code finds an entity called `MyEntity`, gets the material of its first mesh instance, and sets its `cubeMap` property to the cubemap generated by the `CubemapRenderer` script attached to the entity.\n## Questions: \n 1. What is the purpose of this code?\n- This code is for generating a cubemap texture from a camera in the PlayCanvas engine.\n\n2. What are the attributes that can be set for the CubemapRenderer script?\n- The attributes that can be set for the CubemapRenderer script are resolution, mipmaps, and depth.\n\n3. How is the cubemap texture generated?\n- The cubemap texture is generated by rendering the scene from six different angles using a child camera entity for each angle, and then combining the resulting textures into a cubemap texture.","metadata":{"source":".autodoc/docs/markdown/scripts/utils/cubemap-renderer.md"}}],["192",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/utils/download-texture.js)\n\nThe code in this file provides functionality for downloading a texture as a PNG file. It includes two functions for constructing a PNG file: `constructPngUrl` and `constructPngUrlOld`. The former function constructs an uncompressed PNG file manually, while the latter uses the canvas API to construct the PNG file. The `constructPngUrl` function is preferred over `constructPngUrlOld` because the canvas API suffers from bit loss due to its handling of premultiplied alpha. \n\nThe `download` function downloads the data URI of the PNG file as a file with a specified filename. The `readPixels` function reads the pixel data of a given texture face and returns it as a `Uint8ClampedArray`. The `flipY` function flips the image data in the Y direction. Finally, the `downloadTexture` function downloads the texture as a PNG file. It takes in the texture, filename, face, and a boolean `flipY_` parameter. If the texture is a cubemap, it concatenates the six faces of the cubemap into a single image. \n\nThis code is useful for exporting textures from the PlayCanvas engine. It allows users to download textures as PNG files, which can be used in other applications or for offline use. The `downloadTexture` function can be called from other parts of the PlayCanvas engine to provide a download button for textures. \n\nExample usage of the `downloadTexture` function:\n```\nvar texture = app.assets.find(\"texture_asset\").resource;\ndownloadTexture(texture, \"texture.png\", 0, true);\n```\nThis code downloads the first face of the texture as a PNG file with the filename \"texture.png\". The `true` parameter flips the image data in the Y direction.\n## Questions: \n 1. What is the purpose of the `constructPngUrl` function?\n- The `constructPngUrl` function constructs an uncompressed PNG file manually because the canvas API suffers from bit loss due to its handling of premultiplied alpha.\n\n2. What is the difference between `constructPngUrl` and `constructPngUrlOld` functions?\n- `constructPngUrl` constructs an uncompressed PNG file manually, while `constructPngUrlOld` constructs a PNG using canvas API. The latter is faster but suffers from canvas premultiplied alpha bit loss.\n\n3. What is the purpose of the `downloadTexture` function?\n- The `downloadTexture` function downloads the image as png by calling the `download` function and passing the constructed PNG URL from the `constructPngUrl` function. It also reads the pixel data of the given texture face and flips the image data in Y if specified.","metadata":{"source":".autodoc/docs/markdown/scripts/utils/download-texture.md"}}],["193",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/scripts/utils/planar-renderer.js)\n\nThe `PlanarRenderer` script is used to create a reflection texture of a scene that can be used to create reflections on objects in the scene. The script requires a reflection entity with a camera component and its layers set up to what needs to be reflected. The script is then added to the entity, and the `sceneCameraEntity` attribute is set to the main camera of the scene. The `frameUpdate` function is then called to update the reflection texture. This function needs to be called after the main camera properties, including the transform, have been set already.\n\nThe script has several attributes that can be set to customize the reflection texture. The `scale` attribute sets the scale of the texture compared to the render buffer of the main camera. The `mipmaps` attribute, if set to true, generates mipmaps. The `depth` attribute, if set to true, creates a depth buffer. The `planePoint` attribute sets the point on a reflection plane, and the `planeNormal` attribute sets the normal of a reflection plane.\n\nThe `initialize` function is called once per entity and initializes the `plane` and `reflectionMatrix` variables. It also checks if the `sceneCameraEntity` and `entity` have camera components. When the camera is finished rendering, the `onPlanarPostRender` event is triggered on the entity. This event can be listened to by the user, and the resulting texture can be further processed (e.g., prefiltered).\n\nThe `updateRenderTarget` function updates the reflection texture's resolution based on the main camera's resolution and the `scale` attribute. It also limits the maximum texture size and creates a new texture render target with the specified resolution and mipmap generation. If the render target already exists, it is destroyed, and a new one is created.\n\nThe `frameUpdate` function updates the reflection camera orientation by mirroring the scene camera by the plane. It then copies other properties from the scene camera, such as the field of view, orthographic height, near and far clip planes, aperture, sensitivity, and shutter. Finally, it returns the texture.\n\nOverall, the `PlanarRenderer` script is a useful tool for creating reflection textures in a scene. It is easy to use and customizable, allowing for a wide range of reflection effects.\n## Questions: \n 1. What is the purpose of this code?\n- This code is for creating a reflection entity with a camera component and updating the reflection texture.\n\n2. What attributes can be added to the PlanarRenderer script?\n- The attributes that can be added to the PlanarRenderer script are sceneCameraEntity, scale, mipmaps, depth, planePoint, and planeNormal.\n\n3. What happens in the initialize function of the PlanarRenderer script?\n- In the initialize function of the PlanarRenderer script, the sceneCameraEntity and planarCamera are checked if they have camera components. If they don't, an error message is logged. Then, the onPostRender event is triggered when the camera is finished rendering.","metadata":{"source":".autodoc/docs/markdown/scripts/utils/planar-renderer.md"}}],["194",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/constants.js)\n\nThis code defines a set of constants that are used for logging various events and actions related to rendering in the PlayCanvas engine. Each constant is a string that represents a specific type of event or action that can be logged. \n\nFor example, `TRACEID_RENDER_FRAME` is used to log the frame number, while `TRACEID_RENDER_FRAME_TIME` is used to log the time it takes to render a frame. Other constants are used to log information about render passes, render actions, render targets, textures, shaders, and VRAM usage. \n\nThese constants are likely used throughout the PlayCanvas engine to provide developers with detailed information about the rendering process and to help with debugging and optimization. For example, a developer might use `TRACEID_SHADER_COMPILE` to determine if there are any performance issues related to shader compilation. \n\nOverall, this code is an important part of the PlayCanvas engine's logging and debugging infrastructure, and it provides developers with a powerful tool for understanding and optimizing the rendering process. \n\nExample usage:\n\n```javascript\n// Log the frame number\nconsole.log(TRACEID_RENDER_FRAME + ': ' + frameNumber);\n\n// Log the time it takes to render a frame\nconsole.log(TRACEID_RENDER_FRAME_TIME + ': ' + frameTime + 'ms');\n\n// Log information about a render pass\nconsole.log(TRACEID_RENDER_PASS + ': ' + passInfo);\n\n// Log additional detail for a render pass\nconsole.log(TRACEID_RENDER_PASS_DETAIL + ': ' + passDetail);\n\n// Log the allocation of a render target\nconsole.log(TRACEID_RENDER_TARGET_ALLOC + ': ' + targetInfo);\n\n// Log the allocation of a texture\nconsole.log(TRACEID_TEXTURE_ALLOC + ': ' + textureInfo);\n\n// Log the creation of a shader\nconsole.log(TRACEID_SHADER_ALLOC + ': ' + shaderInfo);\n\n// Log the compilation time of a shader\nconsole.log(TRACEID_SHADER_COMPILE + ': ' + compileTime + 'ms');\n\n// Log VRAM usage by textures\nconsole.log(TRACEID_VRAM_TEXTURE + ': ' + vramUsage);\n\n// Log VRAM usage by vertex buffers\nconsole.log(TRACEID_VRAM_VB + ': ' + vramUsage);\n\n// Log VRAM usage by index buffers\nconsole.log(TRACEID_VRAM_IB + ': ' + vramUsage);\n\n// Log the creation of a bind group\nconsole.log(TRACEID_BINDGROUP_ALLOC + ': ' + bindGroupInfo);\n\n// Log the creation of a bind group format\nconsole.log(TRACEID_BINDGROUPFORMAT_ALLOC + ': ' + bindGroupFormatInfo);\n\n// Log the creation of a render pipeline\nconsole.log(TRACEID_RENDERPIPELINE_ALLOC + ': ' + pipelineInfo);\n\n// Log the creation of a pipeline layout\nconsole.log(TRACEID_PIPELINELAYOUT_ALLOC + ': ' + layoutInfo);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines constants for logging different types of events related to rendering, such as frame numbers, render passes, render actions, and resource allocations.\n\n2. How are these constants used in the PlayCanvas engine?\n- These constants are likely used as keys or identifiers for logging events and tracking performance metrics related to rendering in the PlayCanvas engine.\n\n3. Are there any other types of events or metrics that could be logged using similar constants?\n- Yes, depending on the needs of the PlayCanvas engine or specific applications built with it, additional constants could be defined for logging other types of events or performance metrics related to rendering or other aspects of the engine.","metadata":{"source":".autodoc/docs/markdown/src/core/constants.md"}}],["195",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/core.js)\n\nThe code defines a root namespace for the PlayCanvas Engine and exports several variables and functions. \n\nThe `version` and `revision` variables store the current version and revision of the PlayCanvas SDK. The `config` and `common` variables are empty objects that can be used to store configuration and common data respectively. The `apps` object is used to store information about applications that use the PlayCanvas Engine, while the `data` object is used to store exported entity data.\n\nThe `_typeLookup` function creates a lookup table for JavaScript types, which is used by the `type` function. The `type` function takes an object as an argument and returns a string representing its type. The function first checks if the object is `null` and returns `'null'` if it is. If the object is not `null`, the function uses the `typeof` operator to get the type of the object. If the type is `'undefined'`, `'number'`, `'string'`, or `'boolean'`, the function returns the type string. Otherwise, the function looks up the type string in the `_typeLookup` table based on the object's prototype.\n\nThe `extend` function takes two objects as arguments and merges their contents into a single object. The function iterates over the properties of the second object (`ex`) and copies them to the first object (`target`). If a property is an object or an array, the function recursively calls itself to copy the nested properties. The function returns the modified `target` object.\n\nThese exported variables and functions can be used by other modules in the PlayCanvas Engine to store and manipulate data, check the type of objects, and merge objects.\n## Questions: \n 1. What is the purpose of the `_typeLookup` function?\n    \n    The `_typeLookup` function creates a lookup table for JavaScript types and their corresponding string representations.\n\n2. What is the purpose of the `extend` function?\n    \n    The `extend` function merges the contents of two objects into a single object.\n\n3. What is the purpose of the `version`, `revision`, `config`, `common`, `apps`, and `data` variables?\n    \n    These variables are used to store information about the PlayCanvas Engine, such as the current version and revision, configuration settings, and storage for applications and exported entity data.","metadata":{"source":".autodoc/docs/markdown/src/core/core.md"}}],["196",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/debug.js)\n\nThe code defines two classes, `Debug` and `DebugHelper`, which provide logging and debugging functionality for the PlayCanvas engine. The `Debug` class provides methods for logging messages at different levels of severity, including `deprecated`, `assert`, `log`, `warn`, and `error`. These methods take one or more arguments, which are written to the console using the corresponding console method (`console.warn`, `console.error`, etc.). The `Debug` class also provides methods for logging messages only once (`logOnce`, `warnOnce`, `errorOnce`) and for tracing messages (`trace`). The `DebugHelper` class provides helper methods for setting the `name`, `label`, and `destroyed` properties of objects.\n\nThe purpose of this code is to provide a simple and consistent way to log messages and debug the PlayCanvas engine. The `Debug` class provides a way to log messages at different levels of severity, which can be useful for debugging and troubleshooting. The `DebugHelper` class provides helper methods for setting object properties, which can be useful for tracking objects and debugging memory leaks.\n\nHere is an example of how the `Debug` class might be used in the PlayCanvas engine:\n\n```\nimport { Debug } from \"playcanvas-engine\";\n\n// Log a message\nDebug.log(\"Hello, world!\");\n\n// Log a warning\nDebug.warn(\"Something went wrong!\");\n\n// Log an error\nDebug.error(\"Something went really wrong!\");\n\n// Assert that a variable is true\nDebug.assert(x > 0, \"x must be greater than 0\");\n\n// Trace a message\nDebug.trace(\"physics\", \"Collision detected:\", entity1, entity2);\n```\n\nOverall, this code provides a useful set of debugging tools for the PlayCanvas engine, which can help developers to identify and fix issues more quickly and easily.\n## Questions: \n 1. What is the purpose of the `Tracing` import and how is it used in this code?\n- The `Tracing` import is used to enable tracing for a specific channel, and is used in the `trace` method of the `Debug` class to log trace messages to the console if tracing is enabled for the channel.\n\n2. What is the difference between the `log` and `logOnce` methods in the `Debug` class?\n- The `log` method logs a message to the console every time it is called, while the `logOnce` method logs a message to the console only the first time it is called, and subsequent calls with the same message will be ignored.\n\n3. What is the purpose of the `DebugHelper` class and its methods?\n- The `DebugHelper` class provides helper methods that are used only in the debug build of the engine, such as setting a name or label for an object, or marking an object as destroyed. These methods are used to aid in debugging and are not necessary for the functionality of the engine itself.","metadata":{"source":".autodoc/docs/markdown/src/core/debug.md"}}],["197",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/event-handler.js)\n\nThe code defines a class called `EventHandler` that provides functionality for event handling. The class has several methods that allow subscribing to and firing events, as well as detaching event handlers. The class also defines a callback function type called `HandleEventCallback` that takes up to eight arguments.\n\nThe `EventHandler` class can be used as a base class for other classes that need to implement event handling functionality. The class can be instantiated and used directly, or it can be subclassed to add additional functionality.\n\nThe `EventHandler` class has several methods that allow subscribing to and detaching event handlers. The `on` method is used to subscribe to an event. It takes a name of the event to bind the callback to, a callback function that is called when the event is fired, and an optional scope object to use as `this` when the event is fired. The `off` method is used to detach an event handler from an event. If the `name` parameter is not provided, all events are unbound. If the `callback` parameter is not provided, all callbacks for the specified event are unbound. If the `scope` parameter is not provided, all events with the callback will be unbound.\n\nThe `fire` method is used to fire an event. It takes the name of the event to fire and any additional arguments to pass to the event listener. The `once` method is used to attach an event handler to an event that will be removed after being fired once.\n\nThe `hasEvent` method is used to test if there are any handlers bound to an event name. It takes the name of the event to test and returns `true` if the object has handlers bound to the specified event name.\n\nOverall, the `EventHandler` class provides a simple and flexible way to implement event handling functionality in JavaScript applications. It can be used in a variety of contexts, such as game engines, user interfaces, and web applications.\n## Questions: \n 1. What is the purpose of the `HandleEventCallback` function?\n- The `HandleEventCallback` function is a callback used by `EventHandler` functions and is limited to 8 arguments.\n\n2. What is the purpose of the `EventHandler` class?\n- The `EventHandler` class is an abstract base class that implements functionality for event handling. It allows developers to register, detach, and fire event handlers.\n\n3. What does the `fire` method do?\n- The `fire` method fires an event and calls all the registered event handlers for that event. It takes in the name of the event and up to 8 arguments to pass to the event handlers. If an event handler was registered with the `once` parameter set to `true`, it will be removed after being fired once.","metadata":{"source":".autodoc/docs/markdown/src/core/event-handler.md"}}],["198",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/events.js)\n\nThe code above is a module that exports an object called `events`. This object contains a method called `attach` that attaches event methods to an object passed as an argument. The event methods that are attached to the object are `on`, `off`, `fire`, `once`, and `hasEvent`. These methods are used to add, remove, trigger, and check for events respectively.\n\nThe `attach` method takes an object as an argument and returns the same object with the event methods attached to it. The `on` method is used to add a callback function to an event. The `off` method is used to remove a callback function from an event. The `fire` method is used to trigger an event and call all the callback functions attached to it. The `once` method is used to add a callback function that will only be called once when the event is triggered. The `hasEvent` method is used to check if an event has any callback functions attached to it.\n\nThe `events` object also contains properties that reference the corresponding methods of the `EventHandler` class. These properties are `_addCallback`, `on`, `off`, `fire`, `once`, and `hasEvent`. These properties are used internally by the `attach` method to attach the event methods to the object passed as an argument.\n\nThis code is part of the PlayCanvas engine project and is used to add event handling functionality to objects in the engine. This allows developers to add custom behavior to objects based on events triggered in the engine. For example, a developer could attach a callback function to the `update` event of a script component attached to an entity in the engine. This callback function would be called every frame when the `update` event is triggered, allowing the developer to update the behavior of the entity based on the current state of the engine.\n## Questions: \n 1. What is the purpose of the `EventHandler` import?\n- The `EventHandler` import is used to define the prototype methods that will be attached to the target object.\n\n2. What methods are being attached to the target object?\n- The methods being attached to the target object are `on`, `off`, `fire`, `once`, and `hasEvent`.\n\n3. What is the purpose of the `_callbacks` and `_callbackActive` properties?\n- The `_callbacks` property is used to store the callback functions for each event, while the `_callbackActive` property is used to keep track of which callbacks are currently active.","metadata":{"source":".autodoc/docs/markdown/src/core/events.md"}}],["199",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/guid.js)\n\nThe code defines a namespace called `guid` which contains a single method called `create`. The purpose of this code is to generate a unique identifier for entities in the PlayCanvas engine. The unique identifier is a GUID (Globally Unique Identifier) which is a 128-bit random number. The probability of generating two GUIDs that clash is vanishingly small, making it a reliable way to identify entities.\n\nThe `create` method generates a new GUID that is compliant with the RFC4122 version 4 standard. It does this by replacing the `x` and `y` characters in the string `'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'` with random hexadecimal digits. The `x` characters are replaced with random digits between 0 and 15, while the `y` character is replaced with a random digit between 8 and 11. This ensures that the GUID is compliant with the RFC4122 version 4 standard.\n\nHere is an example of how the `create` method can be used:\n\n```\nimport { guid } from 'playcanvas-engine';\n\nconst entityId = guid.create();\nconsole.log(entityId); // e.g. '6f5c7d8a-9b0c-4d3e-a1b2-8c9d0e1f2g3h'\n```\n\nThis code imports the `guid` namespace from the PlayCanvas engine and uses the `create` method to generate a new GUID. The GUID is then assigned to the `entityId` variable and logged to the console.\n\nOverall, this code provides a reliable way to generate unique identifiers for entities in the PlayCanvas engine. This is important for managing and manipulating entities within the engine, as it ensures that each entity has a unique identifier that can be used to reference it.\n## Questions: \n 1. What is the purpose of this code?\n- This code generates unique identifiers for entities using GUIDs.\n\n2. What is the format of the generated GUID?\n- The generated GUID follows the RFC4122 version 4 format, which consists of 128-bit random numbers separated by hyphens.\n\n3. How does the code ensure uniqueness of the generated GUIDs?\n- The code uses a very large random number to create the GUID, which makes the probability of creating two that clash vanishingly small.","metadata":{"source":".autodoc/docs/markdown/src/core/guid.md"}}],["200",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/hash.js)\n\nThe code above defines a function called `hashCode` that calculates a simple hash value of a given string. The purpose of this function is to generate a unique identifier for a string that can be used for various purposes, such as caching or indexing. \n\nThe function takes a single parameter, `str`, which is the string to be hashed. It then initializes a variable called `hash` to 0 and loops through each character in the string. For each character, it performs a series of bitwise operations to update the `hash` value. Finally, it returns the resulting `hash` value.\n\nThe algorithm used to calculate the hash value is designed for performance rather than perfect accuracy. As such, it may not produce completely unique hash values for all strings. However, it should be sufficient for most use cases where a simple hash value is needed.\n\nThis function may be used in various parts of the PlayCanvas engine where a unique identifier is needed for a string. For example, it could be used to generate a unique ID for a resource that is being loaded, or to generate a key for a cache that stores previously loaded resources. \n\nHere is an example of how this function could be used:\n\n```javascript\nimport { hashCode } from 'playcanvas-engine';\n\nconst myString = 'Hello, world!';\nconst hashValue = hashCode(myString);\n\nconsole.log(hashValue); // Output: -1399148729\n```\n\nIn this example, we import the `hashCode` function from the PlayCanvas engine and use it to generate a hash value for the string \"Hello, world!\". The resulting hash value is then logged to the console.\n## Questions: \n 1. What is the purpose of this function and how is it used within the PlayCanvas engine?\n- This function calculates a simple hash value of a string and is likely used for some internal functionality within the PlayCanvas engine.\n\n2. How does the hash value calculation work and what is the significance of converting it to a 32bit integer?\n- The hash value is calculated using a loop that shifts the hash value left by 5 bits, subtracts the previous hash value, and adds the character code of the current character in the string. Converting it to a 32bit integer ensures that the hash value is within a certain range and can be used more efficiently in certain contexts.\n\n3. Why is the @ignore tag used in the JSDoc comment for this function?\n- The @ignore tag is used to indicate that this function should not be included in the generated documentation for the PlayCanvas engine, likely because it is an internal utility function that is not intended for external use or documentation.","metadata":{"source":".autodoc/docs/markdown/src/core/hash.md"}}],["201",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/indexed-list.js)\n\nThe `IndexedList` class is a data structure that allows for the storage of items in an ordered list, with the ability to look up items by a unique key. The class contains a private `_list` property, which is an array that stores the items in the order they were added, and a private `_index` property, which is an object that maps keys to the index of the corresponding item in the `_list` array.\n\nThe `push` method is used to add a new item to the list with a unique key. If the key already exists in the index, an error is thrown. Otherwise, the item is added to the end of the `_list` array, and its index is stored in the `_index` object using the provided key.\n\nThe `has` method is used to test whether a key has been added to the index. It returns `true` if the key is in the index, and `false` otherwise.\n\nThe `get` method is used to retrieve an item from the list using its key. If the key is in the index, the method returns the corresponding item from the `_list` array. If the key is not in the index, the method returns `null`.\n\nThe `remove` method is used to remove an item from the list using its key. If the key is in the index, the corresponding item is removed from the `_list` array, and its index is removed from the `_index` object. The method returns `true` if an item was removed, and `false` otherwise.\n\nThe `list` method is used to return the entire list of items in the order they were added.\n\nThe `clear` method is used to remove all items from the list and reset the index.\n\nThis data structure can be useful in a variety of scenarios where items need to be stored in an ordered list and accessed by a unique key. For example, it could be used to store entities in a game engine, where each entity has a unique ID that can be used to look it up quickly. Here is an example of how the `IndexedList` class could be used:\n\n```\nconst entities = new IndexedList();\n\n// Add some entities\nentities.push('player', { name: 'Player', health: 100 });\nentities.push('enemy1', { name: 'Enemy 1', health: 50 });\nentities.push('enemy2', { name: 'Enemy 2', health: 75 });\n\n// Get an entity by key\nconst player = entities.get('player');\nconsole.log(player); // { name: 'Player', health: 100 }\n\n// Remove an entity by key\nentities.remove('enemy1');\n\n// Get the entire list of entities\nconst allEntities = entities.list();\nconsole.log(allEntities); // [{ name: 'Player', health: 100 }, { name: 'Enemy 2', health: 75 }]\n```\n## Questions: \n 1. What is the purpose of this `IndexedList` class?\n- The `IndexedList` class is a data structure that allows for item lookup by key and can also return a list.\n\n2. What are the private properties of the `IndexedList` class?\n- The private properties of the `IndexedList` class are `_list`, which is an array of objects, and `_index`, which is an object with string keys and number values.\n\n3. What happens when a key already exists in the index during a `push` operation?\n- If a key already exists in the index during a `push` operation, an error will be thrown with the message \"Key already in index\" followed by the key that already exists.","metadata":{"source":".autodoc/docs/markdown/src/core/indexed-list.md"}}],["202",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/bit-packing.js)\n\nThe `BitPacking` object provides functionality for operating on values stored as bits in a number. It contains four methods: `set`, `get`, `all`, and `any`.\n\nThe `set` method sets a value to specified bits of a number. It takes four parameters: `storage`, which is the number to store the bits into; `value`, which is the value to store; `shift`, which is the number of bits to shift the value; and `mask`, which is an optional parameter that limits the number of storage bits and defaults to 1. The method first clears the space by performing a bitwise AND operation on `storage` and the complement of the mask shifted by the shift amount. It then sets the bits by performing a bitwise OR operation on the result and the value shifted by the shift amount. The method returns the updated storage value.\n\nThe `get` method gets the value of specified bits from a number. It takes three parameters: `storage`, which is the number to extract the bits from; `shift`, which is the number of bits to shift the mask; and `mask`, which is an optional parameter that limits the number of storage bits and defaults to 1. The method extracts the bits by performing a bitwise right shift on `storage` by the shift amount and a bitwise AND operation on the result and the mask. The method returns the extracted value.\n\nThe `all` method tests if all specified bits are set. It takes three parameters: `storage`, which is the number to test; `shift`, which is the number of bits to shift the mask; and `mask`, which is an optional parameter that limits the number of storage bits and defaults to 1. The method first shifts the mask by the shift amount and assigns the result to a variable. It then tests if the bitwise AND operation between `storage` and the shifted mask is equal to the shifted mask. The method returns true if all bits in the mask are set in the storage.\n\nThe `any` method tests if any specified bits are set. It takes three parameters: `storage`, which is the number to test; `shift`, which is the number of bits to shift the mask; and `mask`, which is an optional parameter that limits the number of storage bits and defaults to 1. The method tests if the bitwise AND operation between `storage` and the mask shifted by the shift amount is not equal to 0. The method returns true if any bits in the mask are set in the storage.\n\nThese methods can be used in the larger project to manipulate and extract values stored as bits in a number. For example, they could be used in a game engine to store and retrieve information about game objects, such as their position, rotation, and scale. The `set` method could be used to set the position of an object by storing its x, y, and z coordinates as bits in a single number. The `get` method could be used to extract the x, y, or z coordinate from the number. The `all` and `any` methods could be used to test if certain flags or properties are set for an object.\n## Questions: \n 1. What is the purpose of the BitPacking API?\n    \n    The BitPacking API provides functionality for operating on values stored as bits in a number.\n\n2. How does the `set` method work?\n    \n    The `set` method sets a value to specified bits of a number by clearing the space, then setting the bits, and finally returning the updated storage.\n\n3. What is the difference between the `all` and `any` methods?\n    \n    The `all` method tests if all specified bits are set in the storage, while the `any` method tests if any specified bits are set in the storage.","metadata":{"source":".autodoc/docs/markdown/src/core/math/bit-packing.md"}}],["203",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/color.js)\n\nThe code defines a class called `Color` that represents an RGBA color. The class provides methods for creating, manipulating, and comparing colors. The `Color` class can be used in a larger project that requires color manipulation, such as a game engine or a graphics application.\n\nThe `Color` class has four properties: `r`, `g`, `b`, and `a`, which represent the red, green, blue, and alpha components of the color, respectively. The constructor of the `Color` class takes four optional parameters that can be used to set the values of these properties. If the first parameter is an array of length 3 or 4, the array will be used to populate all components.\n\nThe `Color` class provides several methods for manipulating colors. The `clone()` method returns a duplicate color object. The `copy(rhs)` method copies the contents of a source color to a destination color. The `equals(rhs)` method reports whether two colors are equal. The `set(r, g, b, a)` method assigns values to the color components, including alpha. The `lerp(lhs, rhs, alpha)` method returns the result of a linear interpolation between two specified colors. The `fromString(hex)` method sets the values of the color from a string representation '#RRGGBBAA' or '#RRGGBB'. The `toString(alpha)` method converts the color to string form.\n\nThe `Color` class also provides several static properties that represent common colors, such as `BLACK`, `BLUE`, `CYAN`, `GRAY`, `GREEN`, `MAGENTA`, `RED`, `WHITE`, and `YELLOW`. These properties are read-only and are instances of the `Color` class.\n\nHere is an example of how the `Color` class can be used:\n\n```\nimport { Color } from 'playcanvas-engine';\n\nconst red = new Color(1, 0, 0, 1);\nconst green = new Color(0, 1, 0, 1);\n\nconsole.log(red.equals(green)); // false\n\nconst yellow = new Color();\nyellow.lerp(red, green, 0.5);\n\nconsole.log(yellow.toString()); // #808000ff\n```\n\nIn this example, two colors are created using the `Color` constructor. The `equals(rhs)` method is used to check if the two colors are equal. The `lerp(lhs, rhs, alpha)` method is used to interpolate between the two colors and create a new color. The `toString(alpha)` method is used to convert the new color to a string representation.\n## Questions: \n 1. What is the purpose of the `math` import at the beginning of the file?\n- The `math` import is used to call the `intToBytes24` and `intToBytes32` functions in the `fromString` method.\n\n2. What is the difference between the `set` and `copy` methods?\n- The `set` method assigns new values to the color components, including alpha, while the `copy` method copies the contents of a source color to a destination color.\n\n3. What is the format of the string returned by the `toString` method?\n- The format of the string returned by the `toString` method is `#RRGGBBAA`, where RR, GG, BB, AA are the red, green, blue, and alpha values. If the alpha value is not included, the format is the same as used in HTML/CSS.","metadata":{"source":".autodoc/docs/markdown/src/core/math/color.md"}}],["204",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/constants.js)\n\nThis code defines several constants that represent different interpolation schemes. Interpolation is the process of estimating values between two known values. In the context of computer graphics, interpolation is often used to create smooth animations between two keyframes. \n\nThe first two constants, `CURVE_LINEAR` and `CURVE_SMOOTHSTEP`, represent simple linear and smooth step interpolation schemes, respectively. Linear interpolation simply draws a straight line between two points, while smooth step interpolation uses a smooth curve to transition between two values. \n\nThe next two constants, `CURVE_CATMULL` and `CURVE_CARDINAL`, represent deprecated interpolation schemes that were once used in the PlayCanvas engine. These have been replaced by the `CURVE_SPLINE` constant, which represents a more general spline interpolation scheme. Spline interpolation uses a series of curves to create a smooth transition between two values. \n\nFinally, the `CURVE_STEP` constant represents a stepped interpolator, which does not blend between values but instead jumps directly from one value to the next. This can be useful for creating discrete animations or for representing digital signals that can only take on certain values. \n\nThese constants can be used throughout the PlayCanvas engine to specify different interpolation schemes for animations and other visual effects. For example, a developer might use the `CURVE_SPLINE` constant to create a smooth camera movement between two points in a 3D scene. Alternatively, they might use the `CURVE_STEP` constant to create a flashing effect for a warning sign. \n\nOverall, this code provides a useful set of tools for creating smooth and realistic animations in the PlayCanvas engine. By defining these constants, the engine makes it easy for developers to experiment with different interpolation schemes and find the one that works best for their particular use case.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines different interpolation schemes for use in animations.\n\n2. What is the difference between the deprecated interpolation schemes and the CURVE_SPLINE scheme?\n- The deprecated schemes are no longer recommended for use and should be replaced with CURVE_SPLINE. \n\n3. How can the CURVE_SPLINE scheme be customized?\n- For Catmull-Rom interpolation, a curve tension of 0.5 can be specified.","metadata":{"source":".autodoc/docs/markdown/src/core/math/constants.md"}}],["205",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/curve-evaluator.js)\n\nThe code defines a class called `CurveEvaluator` that is used to evaluate a curve at a specific time. The class takes a `Curve` object and an initial time as input and provides a method called `evaluate` that returns the value of the curve at the given time. The class also has a private method called `_reset` that is used to calculate the weights for the curve interval at the given time.\n\nThe `CurveEvaluator` class supports different types of curves, including linear, step, smoothstep, and hermite curves. The type of curve is determined by the `type` property of the `Curve` object. If the curve is a step curve, the `evaluate` method returns the value of the first key of the curve. If the curve is a linear or smoothstep curve, the `evaluate` method calculates the normalized time `t` and uses it to interpolate between the two keys of the curve. If the curve is a hermite curve, the `evaluate` method calculates the tangents for the curve and uses them to evaluate the curve at the given time.\n\nThe `CurveEvaluator` class is used in the larger PlayCanvas engine project to provide a way to evaluate curves used in animations and other parts of the engine. For example, the `Curve` object is used in the `Animation` class to define the animation curves for each property of an animated entity. The `CurveEvaluator` class is then used to evaluate these curves at each frame of the animation to determine the value of the property at that time.\n\nExample usage:\n\n```javascript\nimport { CurveEvaluator } from 'playcanvas';\n\n// create a curve object\nconst curve = {\n    type: 'linear',\n    keys: [\n        [0, 0],\n        [1, 1]\n    ]\n};\n\n// create a curve evaluator object\nconst evaluator = new CurveEvaluator(curve);\n\n// evaluate the curve at time 0.5\nconst value = evaluator.evaluate(0.5);\n\nconsole.log(value); // 0.5\n```\n## Questions: \n 1. What is the purpose of the `CurveEvaluator` class?\n- The `CurveEvaluator` class is used to evaluate a curve at a specific time.\n\n2. What types of curves are supported by this code?\n- The code supports several types of curves, including linear, smoothstep, and hermite curves (specifically, Catmull-Rom, Cardinal, and cubic splines).\n\n3. How are tangents calculated for hermite curves?\n- Tangents for hermite curves are calculated using a formula that takes into account the tension of the curve and the spacing of the knots. The specific formula used depends on the type of hermite curve being evaluated.","metadata":{"source":".autodoc/docs/markdown/src/core/math/curve-evaluator.md"}}],["206",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/curve-set.js)\n\nThe code defines a class called `CurveSet` which is a collection of curves. It imports two other classes, `Curve` and `CurveEvaluator`, and a constant `CURVE_SMOOTHSTEP` from other files. \n\nThe `CurveSet` class has a constructor that takes an array of arrays of keys (pairs of numbers with the time first and value second) as an argument. It creates a new `Curve` instance for each array of keys and adds it to the `curves` array. If no argument is provided, it creates a new `Curve` instance and adds it to the `curves` array. If a single number is provided as an argument, it creates that many `Curve` instances and adds them to the `curves` array. If an array of arrays of keys is provided as an argument, it creates a new `Curve` instance for each array of keys and adds it to the `curves` array.\n\nThe `CurveSet` class has a `length` property that returns the number of curves in the curve set. It also has a `type` property that sets and gets the interpolation scheme applied to all curves in the curve set. The interpolation scheme can be `CURVE_LINEAR`, `CURVE_SMOOTHSTEP`, `CURVE_SPLINE`, or `CURVE_STEP`. It defaults to `CURVE_SMOOTHSTEP`. \n\nThe `CurveSet` class has a `get` method that returns a specific curve in the curve set. It takes an index as an argument and returns the curve at the specified index.\n\nThe `CurveSet` class has a `value` method that returns the interpolated value of all curves in the curve set at the specified time. It takes a time and an optional result array as arguments. If the result array is not supplied, the function allocates a new array internally to return the result.\n\nThe `CurveSet` class has a `clone` method that returns a clone of the specified curve set object.\n\nThe `CurveSet` class has two private methods, `quantize` and `quantizeClamped`, that are used internally to sample the curveset at regular intervals over the range [0..1] and return the set of quantized values. The `quantizeClamped` method also clamps the result to a specified minimum and maximum value.\n\nOverall, the `CurveSet` class provides a way to create and manipulate a collection of curves with different interpolation schemes. It can be used in the larger project to define animations, particle effects, and other dynamic behaviors.\n## Questions: \n 1. What is the purpose of the `CurveSet` class and how is it used?\n- The `CurveSet` class is a collection of curves that can be used to interpolate values over time. It can be instantiated with an array of arrays of keys, and its `value` method can be used to get the interpolated curve values at a specified time.\n\n2. What interpolation schemes are available for the curves in a `CurveSet` and how can they be set?\n- The interpolation schemes available for the curves in a `CurveSet` are `CURVE_LINEAR`, `CURVE_SMOOTHSTEP`, `CURVE_SPLINE`, and `CURVE_STEP`. They can be set using the `type` property of the `CurveSet` instance.\n\n3. What is the purpose of the `quantize` and `quantizeClamped` methods of the `CurveSet` class?\n- The `quantize` method samples the curveset at regular intervals over the range [0..1] and returns a set of quantized values. The `quantizeClamped` method does the same, but clamps the result to a specified minimum and maximum value. These methods are used internally and are not intended to be called directly by developers.","metadata":{"source":".autodoc/docs/markdown/src/core/math/curve-set.md"}}],["207",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/curve.js)\n\nThe code defines a class called `Curve` which represents a collection of keys (time/value pairs) that define a curve's shape. The curve's shape is defined by its type, which specifies an interpolation scheme for the keys. The class has several methods for manipulating and evaluating the curve.\n\nThe `Curve` class has a constructor that takes an optional array of keys and initializes the `keys` property with them. The `add` method adds a new key to the curve at the specified time and value. The `get` method returns the key at the specified index. The `sort` method sorts the keys by time. The `value` method returns the interpolated value of the curve at the specified time. The `closest` method returns the key closest to the specified time. The `clone` method returns a clone of the curve object.\n\nThe `Curve` class also has two methods for quantizing the curve at regular intervals over the range [0..1]. The `quantize` method samples the curve and returns a set of quantized values. The `quantizeClamped` method samples the curve and clamps the resulting samples to a specified range.\n\nThe `Curve` class has several properties, including `type`, which specifies the interpolation scheme for the curve, and `tension`, which controls how tangents are calculated for spline curves. The class also has a private property `_eval` which is an instance of the `CurveEvaluator` class. The `CurveEvaluator` class is not defined in this file, but it is likely used to evaluate the curve at a given time using the specified interpolation scheme.\n\nOverall, the `Curve` class provides a way to define and manipulate curves with different interpolation schemes. It can be used in the larger project to create animations, control movement, or define other types of curves.\n## Questions: \n 1. What is the purpose of the `Curve` class?\n- The `Curve` class represents a collection of keys (time/value pairs) that define the shape of a curve using a specified interpolation scheme.\n\n2. What are the different interpolation schemes that can be used with a `Curve`?\n- The interpolation scheme can be one of the following: `CURVE_LINEAR`, `CURVE_SMOOTHSTEP`, `CURVE_SPLINE`, or `CURVE_STEP`. The default is `CURVE_SMOOTHSTEP`.\n\n3. What is the purpose of the `quantize` and `quantizeClamped` methods?\n- These methods sample the curve at regular intervals over the range [0..1] and return a set of quantized values. The `quantizeClamped` method additionally clamps the resulting samples to a specified range. These methods are used internally and are not intended for external use.","metadata":{"source":".autodoc/docs/markdown/src/core/math/curve.md"}}],["208",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/float-packing.js)\n\nThe code defines a static class called `FloatPacking` that provides utility functions for packing float values to various storage representations. The class contains three static methods: `float2Half`, `float2Bytes`, and `float2BytesRange`. \n\nThe `float2Half` method packs a float value to a 16-bit half-float representation used by the GPU. The method takes a float value as input and returns the packed value. The implementation is based on a method described in a post on esdiscuss.org. The method first stores the float value in a Float32Array and then extracts the sign, mantissa, and exponent bits from the corresponding Int32Array. The method then performs various checks and calculations to pack the bits into a 16-bit value that represents the half-float value.\n\nThe `float2Bytes` method packs a float value in the [0..1) range to a specified number of bytes and stores them in an array with a start offset. The method takes a float value, a Uint8ClampedArray, an offset, and a number of bytes as input. The method calculates the byte values by multiplying the float value by 255 and taking the integer part of the result. The method then subtracts the integer part from the float value and multiplies the result by 255 to get the next byte value. This process is repeated for each byte until the specified number of bytes is reached.\n\nThe `float2BytesRange` method packs a float value into a specified number of bytes, using a minimum and maximum range for the float to normalize it to the [0..1) range. The method takes a float value, a Uint8ClampedArray, an offset, a minimum range value, a maximum range value, and a number of bytes as input. The method first checks if the input value is within the specified range and issues a warning if not. The method then normalizes the input value to the [0..1) range using the minimum and maximum range values and calls the `float2Bytes` method to pack the value into the specified number of bytes.\n\nThe `FloatPacking` class is used in the larger PlayCanvas engine project to provide functionality for packing float values to various storage representations. The `float2Half` method is used to pack float values to a 16-bit half-float representation used by the GPU. The `float2Bytes` method is used to pack float values to an array of bytes for storage or transmission. The `float2BytesRange` method is used to pack float values to an array of bytes with a specified range for normalization. \n\nExample usage of the `FloatPacking` class:\n\n```javascript\nimport { FloatPacking } from 'playcanvas-engine';\n\nconst value = 0.5;\nconst halfFloat = FloatPacking.float2Half(value);\nconsole.log(halfFloat); // 15360\n\nconst bytes = new Uint8ClampedArray(4);\nFloatPacking.float2BytesRange(value, bytes, 0, 0, 1, 4);\nconsole.log(bytes); // Uint8ClampedArray [127, 0, 0, 0]\n\nFloatPacking.float2MantissaExponent(value, bytes, 0, 4);\nconsole.log(bytes); // Uint8ClampedArray [127, 0, 0, 0]\n```\n## Questions: \n 1. What is the purpose of the `math` import from `math.js`?\n- It is unclear from this code snippet what the `math` module contains or how it is used.\n\n2. What is the purpose of the `FloatPacking` class?\n- The `FloatPacking` class provides utility functions for packing float values into various storage representations, such as 16-bit half-float or a specified number of bytes.\n\n3. Why is there a conditional check for `value` being out of range in the `float2BytesRange` method?\n- The check is only included if `_DEBUG` is defined, and it is used to warn the developer if a value being packed is outside of the specified range. It is unclear why this check is only included in debug mode.","metadata":{"source":".autodoc/docs/markdown/src/core/math/float-packing.md"}}],["209",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/mat3.js)\n\n# Mat3.js\n\nThe `Mat3.js` file contains the implementation of a 3x3 matrix class called `Mat3`. This class is used to represent 3x3 matrices and provides various methods to manipulate and transform them. \n\n## Class Definition\n\nThe `Mat3` class is defined using the `class` keyword in JavaScript. It has the following properties and methods:\n\n### Properties\n\n- `data`: A `Float32Array` that stores the matrix elements in the form of a flat array.\n\n### Methods\n\n- `constructor()`: A constructor method that creates a new `Mat3` instance and initializes it to the identity matrix.\n- `clone()`: A method that creates a duplicate of the specified matrix.\n- `copy(rhs)`: A method that copies the contents of a source 3x3 matrix to a destination 3x3 matrix.\n- `set(src)`: A method that copies the contents of a source array[9] to a destination 3x3 matrix.\n- `equals(rhs)`: A method that reports whether two matrices are equal.\n- `isIdentity()`: A method that reports whether the specified matrix is the identity matrix.\n- `setIdentity()`: A method that sets the matrix to the identity matrix.\n- `toString()`: A method that converts the matrix to string form.\n- `transpose()`: A method that generates the transpose of the specified 3x3 matrix.\n- `setFromMat4(m)`: A method that converts the specified 4x4 matrix to a Mat3.\n- `transformVector(vec, res)`: A method that transforms a 3-dimensional vector by a 3x3 matrix.\n\n## Usage\n\nThe `Mat3` class can be used to represent 3x3 matrices and perform various operations on them. Here are some examples of how to use this class:\n\n### Creating a new Mat3 instance\n\n```javascript\nconst mat = new Mat3();\n```\n\nThis creates a new `Mat3` instance and initializes it to the identity matrix.\n\n### Cloning a matrix\n\n```javascript\nconst mat1 = new Mat3();\nconst mat2 = mat1.clone();\n```\n\nThis creates a new `Mat3` instance `mat1` and clones it to create another instance `mat2`.\n\n### Copying a matrix\n\n```javascript\nconst mat1 = new Mat3();\nconst mat2 = new Mat3();\nmat2.copy(mat1);\n```\n\nThis creates two new `Mat3` instances `mat1` and `mat2`, and copies the contents of `mat1` to `mat2`.\n\n### Setting a matrix from an array\n\n```javascript\nconst mat = new Mat3();\nmat.set([1, 2, 3, 4, 5, 6, 7, 8, 9]);\n```\n\nThis creates a new `Mat3` instance `mat` and sets its elements to the values in the array `[1, 2, 3, 4, 5, 6, 7, 8, 9]`.\n\n### Checking if two matrices are equal\n\n```javascript\nconst mat1 = new Mat3();\nconst mat2 = new Mat3();\nconst isEqual = mat1.equals(mat2);\n```\n\nThis creates two new `Mat3` instances `mat1` and `mat2`, and checks if they are equal.\n\n### Checking if a matrix is the identity matrix\n\n```javascript\nconst mat = new Mat3();\nconst isIdentity = mat.isIdentity();\n```\n\nThis creates a new `Mat3` instance `mat` and checks if it is the identity matrix.\n\n### Setting a matrix to the identity matrix\n\n```javascript\nconst mat = new Mat3();\nmat.setIdentity();\n```\n\nThis creates a new `Mat3` instance `mat` and sets it to the identity matrix.\n\n### Converting a matrix to string form\n\n```javascript\nconst mat = new Mat3();\nconst str = mat.toString();\n```\n\nThis creates a new `Mat3` instance `mat` and converts it to string form.\n\n### Generating the transpose of a matrix\n\n```javascript\nconst mat = new Mat3();\nmat.transpose();\n```\n\nThis creates a new `Mat3` instance `mat` and generates its transpose.\n\n### Converting a 4x4 matrix to a Mat3\n\n```javascript\nconst mat4 = new Mat4();\nconst mat3 = new Mat3();\nmat3.setFromMat4(mat4);\n```\n\nThis creates a new `Mat4` instance `mat4` and a new `Mat3` instance `mat3`, and converts `mat4` to `mat3`.\n\n### Transforming a vector by a matrix\n\n```javascript\nconst mat = new Mat3();\nconst vec = new Vec3(1, 2, 3);\nconst res = new Vec3();\nmat.transformVector(vec, res);\n```\n\nThis creates a new `Mat3` instance `mat`, a new `Vec3` instance `vec`, and a new `Vec3` instance `res`, and transforms `vec` by `mat`. The result is stored in `res`.\n## Questions: \n 1. What is the purpose of the `Vec3` import statement at the beginning of the code?\n- The `Vec3` import statement is used to import the `Vec3` class from the `vec3.js` file.\n\n2. What is the purpose of the `Mat3` class?\n- The `Mat3` class represents a 3x3 matrix and provides methods for matrix operations such as cloning, copying, setting, transforming vectors, and more.\n\n3. What are the `IDENTITY` and `ZERO` properties of the `Mat3` class?\n- The `IDENTITY` property is a constant matrix set to the identity, while the `ZERO` property is a constant matrix with all elements set to 0. Both properties are instances of the `Mat3` class and are frozen to prevent modification.","metadata":{"source":".autodoc/docs/markdown/src/core/math/mat3.md"}}],["210",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/math.js)\n\nThe code above is a collection of utility functions for mathematical operations. The `math` object contains a set of methods that can be used to perform various mathematical operations such as conversion, interpolation, rounding, and checking whether a number is between two other numbers. \n\nThe `math` object contains the following methods:\n\n- `DEG_TO_RAD` and `RAD_TO_DEG`: These are conversion factors between degrees and radians. They are used to convert angles from degrees to radians and vice versa.\n\n- `clamp`: This method takes a number and two other numbers representing the minimum and maximum values. It returns the original number if it is within the range of the minimum and maximum values. If the number is less than the minimum value, the minimum value is returned. If the number is greater than the maximum value, the maximum value is returned.\n\n- `intToBytes24` and `intToBytes32`: These methods take an integer and convert it into an array of bytes. `intToBytes24` returns an array of 3 bytes, while `intToBytes32` returns an array of 4 bytes.\n\n- `bytesToInt24` and `bytesToInt32`: These methods take an array of bytes and convert it into an integer. `bytesToInt24` takes an array of 3 bytes and returns a 24-bit integer, while `bytesToInt32` takes an array of 4 bytes and returns a 32-bit integer.\n\n- `lerp` and `lerpAngle`: These methods perform linear interpolation between two numbers or two angles. `lerp` takes two numbers and a value between 0 and 1, and returns a value that is linearly interpolated between the two numbers. `lerpAngle` takes two angles in degrees and a value between 0 and 1, and returns an angle that is linearly interpolated between the two angles.\n\n- `powerOfTwo`, `nextPowerOfTwo`, and `nearestPowerOfTwo`: These methods perform operations related to powers of two. `powerOfTwo` takes a number and returns true if it is a power of two, and false otherwise. `nextPowerOfTwo` takes a number and returns the next power of two that is greater than or equal to the number. `nearestPowerOfTwo` takes a number and returns the nearest power of two.\n\n- `random`: This method takes two numbers representing a range and returns a random number between those two numbers.\n\n- `smoothstep` and `smootherstep`: These methods perform smooth interpolation between two numbers. `smoothstep` takes two numbers and a value between 0 and 1, and returns a value that is smoothly interpolated between the two numbers. `smootherstep` is an improved version of `smoothstep` that has zero 1st and 2nd order derivatives at t=0 and t=1.\n\n- `roundUp`: This method takes a number and a multiple, and returns the number rounded up to the nearest multiple.\n\n- `between`: This method takes a number and two other numbers representing upper or lower thresholds, and returns true if the number is between the two thresholds. If the `inclusive` parameter is true, the method also returns true if the number is equal to either of the thresholds.\n\nThese methods can be used in various parts of the PlayCanvas engine project that require mathematical operations. For example, `lerp` and `lerpAngle` can be used to interpolate between two values over time, while `powerOfTwo`, `nextPowerOfTwo`, and `nearestPowerOfTwo` can be used to perform operations related to textures and rendering.\n## Questions: \n 1. What is the purpose of the `math` namespace?\n- The `math` namespace contains various math-related functions and constants.\n\n2. What is the difference between `bytesToInt24` and `bytesToInt32` functions?\n- `bytesToInt24` converts 3 8-bit numbers into a single unsigned 24-bit number, while `bytesToInt32` converts 4 8-bit numbers into a single unsigned 32-bit number.\n\n3. What is the purpose of the `smootherstep` function?\n- The `smootherstep` function is an improved version of the `smoothstep` function that has zero 1st and 2nd order derivatives at t=0 and t=1. It is used for smoothly interpolating values between a range.","metadata":{"source":".autodoc/docs/markdown/src/core/math/math.md"}}],["211",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/math/random.js)\n\nThe code defines a module called `random` that provides several functions for generating random points on a 2D circle, a 3D sphere, and a pseudo-random sequence using radical inverse. \n\nThe `circlePoint` function generates a random 2D point inside a unit circle with uniform distribution. It takes a `Vec2` object as an argument and sets its `x` and `y` properties to the generated values. \n\nThe `circlePointDeterministic` function generates evenly distributed deterministic points inside a unit circle using Fermat's spiral and Vogel's method. It takes a `Vec2` object, an index, and the total number of points as arguments, and sets the `x` and `y` properties of the `Vec2` object to the generated values. \n\nThe `spherePointDeterministic` function generates evenly distributed deterministic points on a unit sphere using Fibonacci sphere algorithm. It takes a `Vec3` object, an index, the total number of points, and optional `start` and `end` parameters as arguments, and sets the `x`, `y`, and `z` properties of the `Vec3` object to the generated values. The `start` and `end` parameters specify the part of the sphere along the y-axis to generate points for, with 0 being the top of the sphere and 1 being the bottom. If not specified, the function generates points for the entire sphere. \n\nThe `radicalInverse` function generates a repeatable pseudo-random sequence using radical inverse. It takes an index as an argument and returns a pseudo-random value. \n\nThese functions can be used in various parts of the PlayCanvas engine that require random point generation, such as particle systems, procedural generation, and physics simulations. For example, the `spherePointDeterministic` function can be used to generate points on a sphere for placing objects in a game world, while the `radicalInverse` function can be used for generating random numbers for physics simulations.\n## Questions: \n 1. What is the purpose of the `math` import?\n- The `math` import is used in the `spherePointDeterministic` function to call the `lerp` method.\n\n2. What is the significance of the `_goldenAngle` constant?\n- The `_goldenAngle` constant represents the golden angle in radians, which is used in the `circlePointDeterministic` and `spherePointDeterministic` functions to generate evenly distributed points.\n\n3. What is the purpose of the `radicalInverse` function?\n- The `radicalInverse` function generates a repeatable pseudo-random sequence using radical inverse, which can be used for various purposes such as generating random numbers or sampling points.","metadata":{"source":".autodoc/docs/markdown/src/core/math/random.md"}}],["212",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/object-pool.js)\n\n# ObjectPool Class\n\nThe `ObjectPool` class is a utility class that provides a pool of reusable objects of the same type. The purpose of this class is to promote the reuse of objects to reduce garbage collection. \n\n## Properties\n\n### _pool\n\nAn array of object instances.\n\n### _count\n\nThe number of object instances that are currently allocated.\n\n## Constructor\n\n### constructor(constructorFunc, size)\n\nThe constructor function takes two parameters:\n\n- `constructorFunc`: The constructor function for the objects in the pool.\n- `size`: The initial number of object instances to allocate.\n\n## Methods\n\n### _resize(size)\n\nResizes the pool to the specified size. If the new size is greater than the current size, new object instances will be created using the constructor function specified in the constructor.\n\n### allocate()\n\nReturns an object instance from the pool. If no instances are available, the pool will be doubled in size and a new instance will be returned.\n\n### freeAll()\n\nAll object instances in the pool will be available again. The pool itself will not be resized.\n\n## Usage\n\nThe `ObjectPool` class can be used in any project that requires the reuse of objects to reduce garbage collection. For example, in a game engine, the `ObjectPool` class can be used to manage the creation and reuse of game objects such as bullets, enemies, or power-ups. \n\nHere is an example of how to use the `ObjectPool` class:\n\n```javascript\nimport { ObjectPool } from 'playcanvas-engine';\n\nclass Bullet {\n    constructor() {\n        // initialize bullet properties\n    }\n}\n\nconst bulletPool = new ObjectPool(Bullet, 10);\n\n// allocate a bullet from the pool\nconst bullet = bulletPool.allocate();\n\n// use the bullet\n\n// free the bullet\nbulletPool.freeAll();\n```\n\nIn this example, a pool of 10 `Bullet` objects is created using the `ObjectPool` class. When a bullet is needed, it is allocated from the pool using the `allocate()` method. When the bullet is no longer needed, it is freed using the `freeAll()` method. This way, the `Bullet` objects are reused instead of being created and destroyed every time they are needed, which reduces garbage collection and improves performance.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `ObjectPool` which is a pool of reusable objects of the same type, designed to reduce garbage collection.\n\n2. What parameters does the constructor take?\n- The constructor takes two parameters: `constructorFunc`, which is the constructor function for the objects in the pool, and `size`, which is the initial number of object instances to allocate.\n\n3. What does the `allocate` method do?\n- The `allocate` method returns an object instance from the pool. If no instances are available, the pool will be doubled in size and a new instance will be returned.","metadata":{"source":".autodoc/docs/markdown/src/core/object-pool.md"}}],["213",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/path.js)\n\nThe code defines a namespace called `path` that provides a set of functions for working with file paths. The `path` namespace has several functions that can be used to manipulate file paths, including `join`, `normalize`, `split`, `getBasename`, `getDirectory`, `getExtension`, `isRelativePath`, and `extractPath`.\n\nThe `join` function takes two or more path segments and joins them together using the `delimiter` property of the `path` namespace. If the second segment starts with the delimiter, it replaces the first segment. If the first segment does not end with the delimiter and the second segment does not start with the delimiter, the delimiter is inserted between them. The `normalize` function removes any `.` or `..` segments from the path and returns the normalized path. The `split` function splits the path into two parts: the directory and the filename. The `getBasename` function returns the filename from the path, and the `getDirectory` function returns the directory from the path. The `getExtension` function returns the extension of the file from the path. The `isRelativePath` function checks if the path is relative or absolute. The `extractPath` function returns the path without the filename.\n\nThese functions can be used to manipulate file paths in a cross-platform way, regardless of the operating system. The `path` namespace is useful for working with file paths in web applications, game engines, and other software projects that need to manipulate file paths. For example, the `join` function can be used to join a base path with a filename to create a full path to a file. The `getExtension` function can be used to determine the file type of a file based on its extension. The `normalize` function can be used to ensure that a path is in a standard format before using it in other functions.\n## Questions: \n 1. What is the purpose of the `Debug` import and how is it used in the `join` function?\n- The `Debug` import is used for debugging purposes and is used to assert that the `one` and `two` arguments in the `join` function are not undefined.\n2. What is the difference between `getBasename` and `getDirectory` functions?\n- `getBasename` returns the filename of the path while `getDirectory` returns the directory part of the path.\n3. What is the purpose of the `isRelativePath` function and how does it determine if a path is relative?\n- The `isRelativePath` function determines if a path is relative by checking if it does not start with a slash and does not include a colon and double slash. Its purpose is to check if a string is a relative path.","metadata":{"source":".autodoc/docs/markdown/src/core/path.md"}}],["214",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/platform.js)\n\nThe code is responsible for detecting the platform environment and features support. It sets a number of boolean flags based on the user agent string and other environment-specific properties. The `platform` object is then created, which stores these flags and provides a convenient way to check for platform-specific features.\n\nThe code first initializes a number of boolean flags to `false`. It then checks if the `navigator` object is defined, which is only true in a browser environment. If it is defined, it extracts the user agent string and uses regular expressions to determine the platform type. It sets the `desktop`, `mobile`, `windows`, `xbox`, `android`, and `ios` flags accordingly. It also checks if the `window` object is defined, which is also only true in a browser environment. If it is defined, it checks if the browser supports touch input and sets the `touch` flag accordingly. It also checks if the browser supports gamepads and sets the `gamepads` flag accordingly. Finally, it checks if the browser supports Web Workers and sets the `workers` flag accordingly.\n\nThe `environment` variable is then set to either `'browser'` or `'node'` depending on whether the `window` object is defined. The `platform` object is then created, which contains the following properties:\n\n- `environment`: A string identifying the current runtime environment. Either `'browser'` or `'node'`.\n- `global`: The global object. This will be the `window` object when running in a browser and the `global` object when running in Node.js.\n- `browser`: A convenience boolean indicating whether we're running in the browser.\n- `desktop`: A boolean indicating whether it is a desktop or laptop device.\n- `mobile`: A boolean indicating whether it is a mobile or tablet device.\n- `ios`: A boolean indicating whether it is iOS.\n- `android`: A boolean indicating whether it is Android.\n- `windows`: A boolean indicating whether it is Windows.\n- `xbox`: A boolean indicating whether it is Xbox.\n- `gamepads`: A boolean indicating whether the platform supports gamepads.\n- `touch`: A boolean indicating whether the platform supports touch input.\n- `workers`: A boolean indicating whether the platform supports Web Workers.\n- `passiveEvents`: A boolean indicating whether the platform supports an options object as the third parameter to `EventTarget.addEventListener()` and the passive property is supported.\n\nDevelopers can use the `platform` object to check for platform-specific features. For example, if they want to check if touch input is supported, they can use `if (pc.platform.touch) { ... }`. If they want to check if gamepads are supported, they can use `if (pc.platform.gamepads) { ... }`.\n## Questions: \n 1. What does this code do?\n- This code detects the platform environment and features support of the device that is running the code. It sets boolean values for various properties such as desktop, mobile, touch, gamepads, etc. based on the user agent string of the device.\n\n2. What platforms and devices are supported by this code?\n- This code supports Windows, Mac OS, Linux, Chrome OS, Xbox, Windows Phone, Android, and iOS platforms. It also detects whether the device is a desktop or laptop, or a mobile or tablet device.\n\n3. What is the purpose of the `passiveEvents` property?\n- The `passiveEvents` property is used to check whether the platform supports an options object as the third parameter to `EventTarget.addEventListener()` and the passive property is supported. This is used to improve scrolling performance on touch devices.","metadata":{"source":".autodoc/docs/markdown/src/core/platform.md"}}],["215",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/preprocessor.js)\n\n# Preprocessor.js\n\nThe `Preprocessor.js` file is a static class that implements a subset of C-style preprocessor. It is used to preprocess the source code and resolve the code based on the defines and ifdefs. The class is inspired by the `Preprocessor.js` library.\n\nThe class contains several regular expressions that are used to match different keywords and expressions. The accepted keywords are `#ifdef`, `#ifndef`, `#if`, `#endif`, `#else`, `#elif`, `#define`, `#undef`, and `#extension`. The class also contains a list of unsupported characters in the expression, which includes `|`, `&`, `<`, `>`, `=`, `+`, and `-`.\n\nThe `run` method is the main method of the class. It takes a string of source code as input and returns the preprocessed source code. The method first strips comments from the source code and then preprocesses the defines and ifdefs. If the preprocessing is successful, the method removes empty lines and consecutive empty lines from the source code.\n\nThe `_preprocess` method is a private method that preprocesses the defines and ifdefs. The method uses a stack to store information about ifdef blocks. The method also uses a map to store active defines, which maps define name to its value. The method then matches the keywords and expressions using regular expressions and evaluates the expressions using the `evaluate` method. The method then adds the information to the stack and handles the ifdef blocks.\n\nThe `_keep` method is a private method that returns true if the evaluation is inside keep branches. The `evaluate` method is a simple expression evaluation method that handles cases like `expression`, `defined(expression)`, and `!defined(expression)`. The method does not handle more complex cases that would require a more complex system.\n\nThe `Preprocessor` class is a useful tool for preprocessing source code and resolving the code based on the defines and ifdefs. It is a powerful tool that can be used in a variety of projects.\n## Questions: \n 1. What is the purpose of this code?\n- This code is a pure static class that implements a subset of C-style preprocessor. It preprocesses the source code and resolves the code based on the defines and ifdefs.\n\n2. What are the accepted keywords in this code?\n- The accepted keywords in this code are #ifdef, #ifndef, #if, #endif, #else, #elif, #define, #undef, and #extension.\n\n3. What does the evaluate() function do?\n- The evaluate() function performs a very simple expression evaluation. It handles cases like expression, defined(expression), and !defined(expression). It tests if the expression define exists and handles inversion.","metadata":{"source":".autodoc/docs/markdown/src/core/preprocessor.md"}}],["216",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/read-stream.js)\n\nThe `ReadStream` class is a helper class that provides methods for reading data from an ArrayBuffer in an organized way. It is designed to be used internally within the PlayCanvas engine and is not intended for external use.\n\nThe `ReadStream` constructor takes an ArrayBuffer as its only argument and initializes the class with a DataView object that is used to read data from the ArrayBuffer. The `offset` property is set to 0, which represents the current position in the buffer that is being read. The `stack` property is an array that is used to keep track of the current state of the read stream.\n\nThe `remainingBytes` method returns the number of bytes that are left to be read in the buffer.\n\nThe `reset` method can be used to reset the current position in the buffer to a specified offset.\n\nThe `skip` method can be used to skip a specified number of bytes in the buffer.\n\nThe `align` method can be used to align the current position in the buffer to a specified number of bytes. This is useful when reading data that is aligned to a specific byte boundary.\n\nThe `readChar` method reads a single character from the buffer and returns it as a string.\n\nThe `readChars` method reads a specified number of characters from the buffer and returns them as a string.\n\nThe `readU8`, `readU16`, `readU32`, and `readU64` methods read unsigned integers of the specified size from the buffer and return them as numbers. The `readU64` method reads two 32-bit unsigned integers and combines them into a single 64-bit unsigned integer.\n\nThe `readU32be` method reads a 32-bit unsigned integer from the buffer in big-endian byte order.\n\nThe `readArray` method reads a specified number of bytes from the buffer into an existing array.\n\nThe `readLine` method reads a line of text from the buffer and returns it as a string. A line is terminated by a newline character (`\\n`).\n\nOverall, the `ReadStream` class provides a convenient and organized way to read data from an ArrayBuffer in the PlayCanvas engine. It is used extensively throughout the engine to read data from various file formats and network protocols.\n## Questions: \n 1. What is the purpose of the `ReadStream` class?\n    \n    The `ReadStream` class is a helper class for organized reading of memory.\n\n2. What methods are available in the `ReadStream` class for reading data?\n    \n    The `ReadStream` class has methods for reading different types of data, including characters, unsigned 8-bit integers, unsigned 16-bit integers, unsigned 32-bit integers, unsigned 64-bit integers, arrays, and lines.\n\n3. What is the purpose of the `align` method in the `ReadStream` class?\n    \n    The `align` method is used to align the offset of the `ReadStream` instance to a multiple of a specified number of bytes.","metadata":{"source":".autodoc/docs/markdown/src/core/read-stream.md"}}],["217",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/ref-counted-cache.js)\n\nThe `RefCountedCache` class is a utility class that implements a reference counting cache for objects. The purpose of this class is to keep track of the number of references to an object and to destroy the object when the reference count reaches zero. This is useful in situations where objects are shared between different parts of the code and need to be cleaned up when they are no longer needed.\n\nThe `cache` property is a `Map` that stores the objects and their reference counts. When an object is added to the cache using the `incRef` method, its reference count is incremented. When an object is removed from the cache using the `decRef` method, its reference count is decremented. If the reference count reaches zero, the object is destroyed and removed from the cache.\n\nThe `destroy` method is used to destroy all the objects in the cache. It iterates over the `cache` map and calls the `destroy` method on each object. After all the objects have been destroyed, the `cache` map is cleared.\n\nThis class is used internally by the PlayCanvas engine to manage the lifecycle of objects. For example, when a scene is loaded, the engine creates a number of objects such as entities, lights, and cameras. These objects are added to the `RefCountedCache` so that they can be cleaned up when the scene is unloaded. Here is an example of how this class might be used:\n\n```javascript\nimport { RefCountedCache } from 'playcanvas-engine';\n\n// create a new cache\nconst cache = new RefCountedCache();\n\n// create a new object and add it to the cache\nconst obj = { name: 'my object' };\ncache.incRef(obj);\n\n// remove the object from the cache\ncache.decRef(obj);\n\n// destroy all the objects in the cache\ncache.destroy();\n```\n\nIn this example, a new `RefCountedCache` is created and an object is added to the cache using the `incRef` method. The object is then removed from the cache using the `decRef` method. Finally, all the objects in the cache are destroyed using the `destroy` method.\n## Questions: \n 1. What is the purpose of this class and how is it used in the PlayCanvas engine?\n- This class is a reference counting cache for objects, used to keep track of the number of references to an object and destroy it when the reference count reaches zero. It is likely used in the PlayCanvas engine to manage memory and prevent memory leaks.\n\n2. What is the data structure used to implement the cache and why was it chosen?\n- The cache is implemented using a Map object, where the key is the object being stored and the value is the reference count. This was likely chosen because Maps provide efficient lookup and insertion of key-value pairs.\n\n3. What happens when an object is removed from the cache?\n- When an object is removed from the cache, its destroy function is called and it is deleted from the cache. This ensures that the object is properly cleaned up and any resources it was using are released.","metadata":{"source":".autodoc/docs/markdown/src/core/ref-counted-cache.md"}}],["218",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/ref-counted-object.js)\n\nThe code defines a base class called `RefCountedObject` that implements reference counting for objects. Reference counting is a technique used to manage memory in programming languages where objects are automatically deleted when they are no longer needed. The purpose of this class is to keep track of the number of references to an object and delete it when the reference count reaches zero.\n\nThe class has a private property `_refCount` that is initialized to zero. It also has two methods `incRefCount()` and `decRefCount()` that increment and decrement the reference count respectively. The `get` accessor method `refCount` returns the current reference count.\n\nThis class can be used as a base class for other classes that need reference counting. For example, in the PlayCanvas engine, this class may be used in the implementation of game objects, resources, and other objects that need to be managed by the engine. When an object is created, its reference count is set to one. When another object references it, the reference count is incremented. When an object is no longer needed, the reference count is decremented. If the reference count reaches zero, the object is deleted.\n\nHere is an example of how this class can be used:\n\n```\nimport { RefCountedObject } from 'playcanvas-engine';\n\nclass MyObject extends RefCountedObject {\n    constructor() {\n        super();\n        // other initialization code\n    }\n}\n\nlet obj1 = new MyObject();\nlet obj2 = new MyObject();\n\nobj1.incRefCount();\nobj2.incRefCount();\n\nobj1.decRefCount();\nobj2.decRefCount();\n```\n\nIn this example, two instances of `MyObject` are created and their reference count is incremented. When the reference count is decremented, the objects are not deleted because their reference count is still greater than zero. When the `decRefCount()` method is called again, the reference count reaches zero and the objects are deleted.\n## Questions: \n 1. What is the purpose of the `RefCountedObject` class?\n- The `RefCountedObject` class implements reference counting for objects.\n\n2. What is the significance of the `_refCount` property being marked as `@private`?\n- The `@private` tag indicates that `_refCount` is intended to be used only within the class and not accessed from outside.\n\n3. How can the reference count be incremented or decremented?\n- The reference count can be incremented using the `incRefCount()` method and decremented using the `decRefCount()` method.","metadata":{"source":".autodoc/docs/markdown/src/core/ref-counted-object.md"}}],["219",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/set-utils.js)\n\nThe code defines a module that exports a set object with a single method called equals. The purpose of the equals method is to compare two sets for equality. \n\nThe equals method takes two arguments, set1 and set2, which are both sets. It first checks if the size of set1 is equal to the size of set2. If they are not equal, it returns false, indicating that the sets are not equal. If they are equal, it iterates over the items in set1 using a for...of loop. For each item, it checks if set2 has the same item using the has method. If set2 does not have the item, it returns false, indicating that the sets are not equal. If all items in set1 are found in set2, it returns true, indicating that the sets are equal.\n\nThis code can be used in the larger PlayCanvas engine project to compare sets of objects or values. For example, it could be used to check if two sets of entities in a game are the same, or if two sets of properties on an object are equal. \n\nHere is an example usage of the equals method:\n\n```\nconst set1 = new Set([1, 2, 3]);\nconst set2 = new Set([3, 2, 1]);\nconst set3 = new Set([1, 2, 4]);\n\nconsole.log(set.equals(set1, set2)); // true\nconsole.log(set.equals(set1, set3)); // false\n```\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code defines a set object with an equals method that compares two sets for equality.\n\n2. **What parameters does the equals method take?**\\\nThe equals method takes two parameters, set1 and set2, which are the sets being compared for equality.\n\n3. **What does the equals method return?**\\\nThe equals method returns a boolean value indicating whether the two sets are equal or not.","metadata":{"source":".autodoc/docs/markdown/src/core/set-utils.md"}}],["220",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/shape/bounding-sphere.js)\n\nThe code defines a class called `BoundingSphere` which represents a sphere-shaped volume used for fast intersection testing. The class has two properties: `center`, which is a `Vec3` object representing the center of the sphere, and `radius`, which is a number representing the radius of the sphere. \n\nThe constructor of the `BoundingSphere` class takes two optional parameters: `center` and `radius`. If `center` is not provided, a new `Vec3` object is created with default values. If `radius` is not provided, it defaults to 0.5. \n\nThe `BoundingSphere` class has three methods: \n\n1. `containsPoint(point)`: This method takes a `Vec3` object representing a point in 3D space and returns a boolean indicating whether the point is inside the sphere or not. It does this by calculating the distance between the point and the center of the sphere, and checking if it is less than the radius of the sphere. \n\n2. `intersectsRay(ray, point)`: This method takes a `Ray` object and an optional `Vec3` object. It returns a boolean indicating whether the ray intersects with the sphere or not. If there is an intersection and `point` is provided, the intersection point is copied into the `point` object. The method first calculates the distance between the origin of the ray and the center of the sphere, and then checks if the ray is pointing towards or away from the sphere. If the ray is pointing away from the sphere, there is no intersection. If the ray is pointing towards the sphere, the method calculates the smallest `t` value of intersection and uses it to compute the intersection point. \n\n3. `intersectsBoundingSphere(sphere)`: This method takes another `BoundingSphere` object and returns a boolean indicating whether the two spheres are overlapping, enveloping, or one is inside the other. It does this by calculating the distance between the centers of the two spheres and checking if it is less than the sum of their radii. \n\nOverall, the `BoundingSphere` class is used to represent a sphere-shaped volume for fast intersection testing. It can be used in various parts of the PlayCanvas engine where such volumes are needed, such as collision detection, physics simulation, and raycasting. For example, it can be used to check if a ray fired by the player intersects with an enemy's hitbox, or if a player's character is inside a trigger volume.\n## Questions: \n 1. What is the purpose of the `BoundingSphere` class?\n- The `BoundingSphere` class is used to create a volume for fast intersection testing.\n\n2. What parameters does the constructor of the `BoundingSphere` class accept?\n- The constructor of the `BoundingSphere` class accepts two optional parameters: `center` (a `Vec3` object representing the center of the sphere) and `radius` (a number representing the radius of the sphere).\n\n3. What is the purpose of the `intersectsRay` method?\n- The `intersectsRay` method is used to test if a ray intersects with the sphere and returns `true` if there is an intersection. It also has an optional `point` parameter that, if provided, will contain the intersection point.","metadata":{"source":".autodoc/docs/markdown/src/core/shape/bounding-sphere.md"}}],["221",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/shape/frustum.js)\n\n# PlayCanvas Engine - Frustum Class\n\nThe `Frustum` class is a shape that defines the viewing space of a camera. It can be used to determine visibility of points and bounding spheres. Typically, you would not create a Frustum shape directly, but instead query `CameraComponent#frustum`.\n\nThe `Frustum` class has the following methods:\n\n## constructor()\n\nThe constructor creates a new `Frustum` instance. It initializes the `planes` array with six empty arrays.\n\n```javascript\nvar frustum = new pc.Frustum();\n```\n\n## setFromMat4(matrix)\n\nThe `setFromMat4` method updates the frustum shape based on the supplied 4x4 matrix. It extracts the numbers for the six planes of the frustum and normalizes the result. The `matrix` parameter is an instance of `Mat4` class.\n\n```javascript\n// Create a perspective projection matrix\nvar projMat = pc.Mat4();\nprojMat.setPerspective(45, 16 / 9, 1, 1000);\n\n// Create a frustum shape that is represented by the matrix\nvar frustum = new pc.Frustum();\nfrustum.setFromMat4(projMat);\n```\n\n## containsPoint(point)\n\nThe `containsPoint` method tests whether a point is inside the frustum. Note that points lying in a frustum plane are considered to be outside the frustum. The `point` parameter is an instance of `Vec3` class.\n\n```javascript\nvar point = new pc.Vec3(1, 2, 3);\nvar isInside = frustum.containsPoint(point);\n```\n\n## containsSphere(sphere)\n\nThe `containsSphere` method tests whether a bounding sphere intersects the frustum. If the sphere is outside the frustum, zero is returned. If the sphere intersects the frustum, 1 is returned. If the sphere is completely inside the frustum, 2 is returned. Note that a sphere touching a frustum plane from the outside is considered to be outside the frustum. The `sphere` parameter is an instance of `BoundingSphere` class.\n\n```javascript\nvar sphere = new pc.BoundingSphere(new pc.Vec3(1, 2, 3), 4);\nvar result = frustum.containsSphere(sphere);\n```\n## Questions: \n 1. What is a Frustum and how is it used in PlayCanvas engine?\n- A Frustum is a shape that defines the viewing space of a camera and is used to determine visibility of points and bounding spheres. It is typically queried through the CameraComponent#frustum method.\n2. What does the setFromMat4 method do?\n- The setFromMat4 method updates the frustum shape based on the supplied 4x4 matrix. It extracts the numbers for each plane of the frustum and normalizes the result.\n3. How does the containsSphere method determine if a sphere intersects the frustum?\n- The containsSphere method checks if the sphere is outside the frustum by calculating the distance between the sphere center and each plane of the frustum. If the distance is less than or equal to the negative radius of the sphere, it is outside the frustum. If the distance is greater than the radius, it intersects the frustum. If the sphere is completely inside the frustum, it is contained by the frustum.","metadata":{"source":".autodoc/docs/markdown/src/core/shape/frustum.md"}}],["222",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/shape/oriented-box.js)\n\nThe code defines a class called `OrientedBox` which represents an oriented bounding box in 3D space. An oriented bounding box is a box that is not necessarily aligned with the world axes, but rather with an arbitrary set of axes defined by a transformation matrix. The purpose of this class is to provide methods for testing intersections between the bounding box and other geometric primitives.\n\nThe class imports several other classes from the PlayCanvas engine, including `Debug`, `Mat4`, `Vec3`, `BoundingBox`, `BoundingSphere`, and `Ray`. These classes are used to perform various calculations and transformations on the bounding box.\n\nThe `OrientedBox` class has several properties and methods. The `halfExtents` property is a `Vec3` object that represents the half-width, half-height, and half-depth of the bounding box. The `_modelTransform` and `_worldTransform` properties are `Mat4` objects that represent the local and world transformations of the bounding box, respectively. The `_aabb` property is a `BoundingBox` object that represents the axis-aligned bounding box of the oriented bounding box.\n\nThe constructor of the `OrientedBox` class takes two optional parameters: a `Mat4` object representing the world transformation of the bounding box, and a `Vec3` object representing the half-extents of the bounding box. If no parameters are provided, the world transformation is set to the identity matrix and the half-extents are set to (0.5, 0.5, 0.5).\n\nThe `intersectsRay` method takes a `Ray` object and an optional `Vec3` object as parameters, and returns a boolean indicating whether the ray intersects with the bounding box. If an intersection is found and a `Vec3` object is provided, the intersection point is copied into the `Vec3` object. The method first transforms the ray into the local space of the bounding box using the `_modelTransform` property, then tests for intersection with the axis-aligned bounding box using the `_aabb` property. If an intersection is found, the intersection point is transformed back into world space using the inverse of the `_modelTransform` property.\n\nThe `containsPoint` method takes a `Vec3` object as a parameter and returns a boolean indicating whether the point is inside the bounding box. The method transforms the point into local space using the `_modelTransform` property, then tests for containment using the `_aabb` property.\n\nThe `intersectsBoundingSphere` method takes a `BoundingSphere` object as a parameter and returns a boolean indicating whether the bounding sphere intersects with the bounding box. The method transforms the center of the sphere into local space using the `_modelTransform` property, then creates a temporary `BoundingSphere` object with the same radius and transformed center. The method then tests for intersection between the temporary sphere and the axis-aligned bounding box using the `_aabb` property.\n\nOverall, the `OrientedBox` class provides a way to represent and test intersections with an oriented bounding box in 3D space. It can be used in the larger PlayCanvas engine project for collision detection, physics simulation, and other applications that require spatial queries.\n## Questions: \n 1. What is the purpose of the `tmpRay`, `tmpVec3`, `tmpSphere`, and `tmpMat4` variables?\n- These variables are used as temporary storage for calculations within the `OrientedBox` class methods.\n\n2. What is the `_aabb` property and how is it used?\n- The `_aabb` property is an instance of the `BoundingBox` class and represents the axis-aligned bounding box of the oriented box. It is used in the `intersectsRay` and `intersectsBoundingSphere` methods to check for intersections.\n\n3. What is the purpose of the `worldTransform` property and how is it used?\n- The `worldTransform` property represents the transformation matrix of the oriented box in world space. It is used to transform points and rays from world space to model space for intersection tests. It can be set by the user to update the position and orientation of the box.","metadata":{"source":".autodoc/docs/markdown/src/core/shape/oriented-box.md"}}],["223",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/shape/plane.js)\n\nThe code defines a class called `Plane` that represents an infinite plane in 3D space. The plane is internally represented in a parametric equation form: `ax + by + cz + distance = 0`, where `a`, `b`, and `c` are the components of the plane's normal vector, and `distance` is the distance from the plane to the origin, along its normal. \n\nThe `Plane` class has two properties: `normal` and `distance`. `normal` is a `Vec3` object that represents the normal vector of the plane, and `distance` is a number that represents the distance from the plane to the origin, along its normal. Both properties are read-only.\n\nThe `Plane` class has a constructor that takes two optional parameters: `normal` and `distance`. If `normal` is not provided, it defaults to `Vec3.UP`, which is a constant vector pointing in the positive y-direction. If `distance` is not provided, it defaults to 0.\n\nThe `Plane` class has several methods:\n\n- `setFromPointNormal(point, normal)`: sets the plane based on a specified normal and a point on the plane. `point` is a `Vec3` object that represents a point on the plane, and `normal` is a `Vec3` object that represents the normal vector of the plane. The method returns the `Plane` object for chaining.\n\n- `intersectsLine(start, end, point)`: tests if the plane intersects between two points. `start` and `end` are `Vec3` objects that represent the start and end positions of a line. If there is an intersection, the intersection point will be copied into `point` (if provided). The method returns `true` if there is an intersection, `false` otherwise.\n\n- `intersectsRay(ray, point)`: tests if a ray intersects with the infinite plane. `ray` is a `Ray` object that represents the ray to test against (the direction of the ray must be normalized). If there is an intersection, the intersection point will be copied into `point` (if provided). The method returns `true` if there is an intersection, `false` otherwise.\n\n- `copy(src)`: copies the contents of a source `Plane` object. `src` is the `Plane` object to copy from. The method returns the `Plane` object for chaining.\n\n- `clone()`: returns a clone of the `Plane` object.\n\nThe `Plane` class is used in the larger PlayCanvas engine project to represent infinite planes in 3D space. It can be used for various purposes, such as collision detection, raycasting, and physics simulations. For example, the `intersectsLine` method can be used to check if a line intersects with a plane, which can be useful for implementing collision detection between objects. The `intersectsRay` method can be used to check if a ray intersects with a plane, which can be useful for implementing raycasting in a 3D scene. Overall, the `Plane` class provides a convenient and efficient way to work with infinite planes in 3D space.\n## Questions: \n 1. What is the purpose of the `Plane` class?\n    \n    The `Plane` class represents an infinite plane in 3D space, and is internally represented in a parametric equation form.\n\n2. What parameters does the `setFromPointNormal` method take, and what does it do?\n    \n    The `setFromPointNormal` method takes a `point` and a `normal` vector as parameters, and sets the plane based on these values. The `distance` property of the plane is calculated based on the dot product of the `normal` vector and the `point` vector.\n\n3. What is the purpose of the `intersectsRay` method, and what does it return?\n    \n    The `intersectsRay` method tests if a ray intersects with the infinite plane, and returns a boolean value indicating whether or not there is an intersection. If there is an intersection, the intersection point will be copied into the `point` parameter (if provided).","metadata":{"source":".autodoc/docs/markdown/src/core/shape/plane.md"}}],["224",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/shape/ray.js)\n\n## PlayCanvas Engine - Ray Class\n\nThe `Ray` class is a part of the PlayCanvas engine project and is used to represent an infinite ray in 3D space. It is defined in a file located in the `math` directory of the project and is imported using ES6 module syntax.\n\nThe `Ray` class has two properties - `origin` and `direction`. `origin` is a `Vec3` object that represents the starting point of the ray, while `direction` is a `Vec3` object that represents the direction of the ray. Both properties are read-only and can be accessed using dot notation.\n\nThe `Ray` class has three methods - `constructor()`, `set()`, `copy()`, and `clone()`. \n\nThe `constructor()` method is used to create a new instance of the `Ray` class. It takes two optional parameters - `origin` and `direction`. If `origin` is provided, it is copied to the `origin` property of the new instance. If `direction` is provided, it is copied to the `direction` property of the new instance. If neither parameter is provided, the `origin` property is set to the origin (0, 0, 0) and the `direction` property is set to a direction down the world negative Z axis (0, 0, -1).\n\n```javascript\n// Create a new ray starting at the position of this entity and pointing down\n// the entity's negative Z axis\nvar ray = new pc.Ray(this.entity.getPosition(), this.entity.forward);\n```\n\nThe `set()` method is used to set the `origin` and `direction` properties of the `Ray` instance to the supplied vector values. It takes two parameters - `origin` and `direction`, both of which are `Vec3` objects. The method copies the values of these vectors to the `origin` and `direction` properties of the `Ray` instance and returns the instance for chaining.\n\n```javascript\nvar ray = new pc.Ray();\nray.set(new pc.Vec3(1, 2, 3), new pc.Vec3(0, 1, 0));\n```\n\nThe `copy()` method is used to copy the contents of a source `Ray` instance to the current instance. It takes one parameter - `src`, which is the source `Ray` instance. The method copies the `origin` and `direction` properties of the `src` instance to the `origin` and `direction` properties of the current instance and returns the instance for chaining.\n\n```javascript\nvar ray1 = new pc.Ray(new pc.Vec3(1, 2, 3), new pc.Vec3(0, 1, 0));\nvar ray2 = new pc.Ray();\nray2.copy(ray1);\n```\n\nThe `clone()` method is used to create a duplicate of the current `Ray` instance. It returns a new `Ray` instance with the same `origin` and `direction` properties as the current instance.\n\n```javascript\nvar ray1 = new pc.Ray(new pc.Vec3(1, 2, 3), new pc.Vec3(0, 1, 0));\nvar ray2 = ray1.clone();\n```\n## Questions: \n 1. What is the purpose of the `Vec3` import and how is it used in this code?\n   - The `Vec3` import is used to define the starting point and direction of the ray as instances of `Vec3`. It is also used in the `set`, `copy`, and `clone` methods to manipulate the origin and direction of the ray.\n2. What is the default direction of the ray and how can it be changed?\n   - The default direction of the ray is down the world negative Z axis (0, 0, -1). It can be changed by passing a different `Vec3` instance as the `direction` parameter when creating a new `Ray` instance or by calling the `set` method with a different `Vec3` instance for the `direction` parameter.\n3. What is the purpose of the `clone` method and how is it used?\n   - The `clone` method returns a duplicate `Ray` instance with the same origin and direction as the original `Ray`. It is used to create a copy of the `Ray` that can be modified without affecting the original `Ray`.","metadata":{"source":".autodoc/docs/markdown/src/core/shape/ray.md"}}],["225",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/sort.js)\n\nThe code above defines two functions, `cmpPriority` and `sortPriority`, which are used to sort an array of objects based on their `priority` property. \n\nThe `cmpPriority` function takes two objects as parameters, each with a `priority` property, and returns a number indicating their relative position. It does this by subtracting the `priority` property of `b` from the `priority` property of `a`. If `a` has a higher priority than `b`, the result will be a positive number, indicating that `a` should come before `b` in the sorted array. If `b` has a higher priority, the result will be negative, indicating that `b` should come before `a`. If the priorities are equal, the result will be zero, indicating that their relative position doesn't matter.\n\nThe `sortPriority` function takes an array of objects as a parameter, where each object has at least a `priority` property. It then sorts the array in place based on the `priority` property of each object, using the `cmpPriority` function. The sorted array is then returned.\n\nThis code is likely used in the larger PlayCanvas engine project to sort arrays of objects based on their priority. This could be useful in a variety of contexts, such as rendering objects in a scene based on their distance from the camera, or updating game objects based on their importance to the gameplay. \n\nHere is an example of how this code could be used:\n\n```\nconst objects = [\n  { name: 'object1', priority: 2 },\n  { name: 'object2', priority: 1 },\n  { name: 'object3', priority: 3 }\n];\n\nsortPriority(objects);\n\nconsole.log(objects);\n// Output: [{ name: 'object2', priority: 1 }, { name: 'object1', priority: 2 }, { name: 'object3', priority: 3 }]\n```\n\nIn this example, an array of objects is defined, each with a `name` and `priority` property. The `sortPriority` function is then called on the array, which sorts the objects based on their `priority` property. The sorted array is then logged to the console.\n## Questions: \n 1. What is the purpose of the `cmpPriority` function?\n   - The `cmpPriority` function is used to compare two objects based on their `priority` property and return a number indicating their relative position.\n\n2. What is the input and output of the `sortPriority` function?\n   - The `sortPriority` function takes an array of objects, each containing at least a `priority` property, and sorts the array in place based on the `priority` property. The function returns the sorted array.\n\n3. Why is the `@ignore` tag used in the JSDoc comments?\n   - The `@ignore` tag is used to indicate that the function should be ignored by the documentation generator. This is likely because the functions are internal and not intended to be used or documented externally.","metadata":{"source":".autodoc/docs/markdown/src/core/sort.md"}}],["226",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/sorted-loop-array.js)\n\nThe `SortedLoopArray` class is a helper class used to hold an array of items in a specific order. The class is designed to hold objects that need to be sorted based on one of their fields. The class is safe to modify while looping through it. \n\nThe class has several properties and methods. The `items` property is an internal array that holds the actual array elements. The `length` property is the number of elements in the array. The `loopIndex` property is the current index used to loop through the array. This gets modified if we add or remove elements from the array while looping. \n\nThe class has a constructor that takes an object with a `sortBy` property. The `sortBy` property is the name of the field that each element in the array is going to be sorted by. The class has an `_binarySearch` method that searches for the right spot to insert the specified item. The method uses a binary search algorithm to find the right index. \n\nThe class has an `insert` method that inserts the specified item into the array at the right index based on the `sortBy` field passed into the constructor. This also adjusts the `loopIndex` accordingly. The class has an `append` method that appends the specified item to the end of the array. This method is faster than `insert()` as it does not binary search for the right index. This also adjusts the `loopIndex` accordingly. The class has a `remove` method that removes the specified item from the array.\n\nThe class has a `sort` method that sorts elements in the array based on the `sortBy` field passed into the constructor. This also updates the `loopIndex` if we are currently looping. The method uses the `_doSort` method to sort the array. The `_doSort` method is a private method that takes two items and compares them based on the `sortBy` field. \n\nThe `SortedLoopArray` class can be used in the larger project to hold an array of objects that need to be sorted based on one of their fields. The class can be used to insert, append, and remove items from the array while looping through it. The class can also be used to sort the array based on the `sortBy` field. \n\nExample usage:\n\n```\nvar array = new SortedLoopArray({ sortBy: 'priority' });\narray.insert(item); // adds item to the right slot based on item.priority\narray.append(item); // adds item to the end of the array\narray.remove(item); // removes item from array\nfor (array.loopIndex = 0; array.loopIndex < array.length; array.loopIndex++) {\n  // do things with array elements\n  // safe to remove and add elements into the array while looping\n}\n```\n## Questions: \n 1. What is the purpose of this class and how is it used?\n- This class is a helper class used to hold an array of items in a specific order that is safe to modify while looping through it. It assumes that it holds objects that need to be sorted based on one of their fields. It can be used to insert, append, and remove items from the array, as well as sort the elements based on the specified field.\n2. What is the significance of the `_binarySearch` method?\n- The `_binarySearch` method is used to search for the right spot to insert the specified item into the array based on the `sortBy` field passed into the constructor. It performs a binary search on the array to find the correct index to insert the item, which makes it more efficient than a linear search.\n3. What is the potential issue with sorting the array while iterating through it?\n- If the array is sorted while iterating through it, the element that is currently being processed might be moved behind other elements, which could cause it to be iterated over more than once. This can lead to unexpected behavior and should be avoided.","metadata":{"source":".autodoc/docs/markdown/src/core/sorted-loop-array.md"}}],["227",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/string.js)\n\nThe code defines a set of constants and functions that extend the functionality of the built-in JavaScript String object. The constants define ranges of Unicode code points for various types of characters, such as ASCII letters, high and low surrogate pairs, and emoji modifiers. The functions provide methods for working with strings that contain these types of characters.\n\nThe `getCodePointData` function takes a string and an optional index and returns an object with two properties: `code` and `long`. The `code` property is the Unicode code point for the character at the specified index, or the first character if no index is specified. If the character is part of a high-low surrogate pair, the function calculates the corresponding code point using a formula. The `long` property is a boolean that indicates whether the character is part of a surrogate pair.\n\nThe `isCodeBetween` function takes a string and two code point values and returns a boolean indicating whether the code point for the first character in the string is between the two values. This function is used to check whether a character is part of a particular range of code points.\n\nThe `numCharsToTakeForNextSymbol` function takes a string and an index and returns the number of characters to take from the string to form the next symbol. A symbol is a visible character, which may be composed of multiple code points. The function checks whether the character at the specified index is part of a high-low surrogate pair, and if so, whether the next character is a modifier or another surrogate pair. It also checks for zero-width joiners and accent characters. The function returns the number of characters to take based on these checks.\n\nThe `string` object is an extension of the built-in JavaScript String object. It defines several constants for working with ASCII letters and various types of Unicode characters. It also provides several methods for working with strings that contain these types of characters. The `format` method takes a string and replaces placeholders with values from additional arguments. The `toBool` method converts a string to a boolean value, with an optional strict mode that throws an error for invalid values. The `getCodePoint` method returns the Unicode code point for a character at a specified index. The `getCodePoints` method returns an array of code points for all characters in a string. The `getSymbols` method returns an array of symbols for all visible characters in a string. The `fromCodePoint` method converts one or more code points to a string.\n\nOverall, this code provides a set of tools for working with strings that contain various types of Unicode characters, including emoji and accent characters. These tools can be used to manipulate and format strings in a way that takes into account the complexity of these characters.\n## Questions: \n 1. What is the purpose of the `getCodePointData` function?\n- The `getCodePointData` function is used to get the code point data for a character in a string, including whether it is a surrogate pair or not.\n\n2. What is the difference between `isCodeBetween` and `numCharsToTakeForNextSymbol` functions?\n- The `isCodeBetween` function checks if a given code point is between two other code points, while the `numCharsToTakeForNextSymbol` function determines how many characters to take for the next symbol in a string, taking into account special cases like emoji modifiers and surrogate pairs.\n\n3. What is the purpose of the `string` object and its methods?\n- The `string` object provides an extended API for working with strings, including methods for formatting, converting to boolean, getting code points and symbols, and converting from code points to strings.","metadata":{"source":".autodoc/docs/markdown/src/core/string.md"}}],["228",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/tags-cache.js)\n\nThe `TagsCache` class is used to cache and index items based on their tags. It provides methods to add and remove items from the cache, as well as a `find` method to retrieve items based on their tags.\n\nThe `_index` property is an object that stores the cached items indexed by their tags. The `_key` property is an optional parameter that specifies a key to use for indexing the items. If `_key` is specified, the items are indexed by the value of the specified key.\n\nThe `addItem` method adds an item to the cache by iterating over its tags and calling the `add` method for each tag. The `removeItem` method removes an item from the cache by iterating over its tags and calling the `remove` method for each tag.\n\nThe `add` method adds an item to the cache for a given tag. If the tag is already in the cache and the item is already associated with the tag, the method returns without doing anything. If the tag is not in the cache, a new index is created for the tag. If `_key` is specified, the item is also indexed by the value of the specified key.\n\nThe `remove` method removes an item from the cache for a given tag. If the tag is not in the cache, the method returns without doing anything. If `_key` is specified, the item is removed from the index by the value of the specified key.\n\nThe `find` method retrieves items from the cache based on their tags. It takes an array of tags as an argument and returns an array of items that match the tags. If a tag is an array, it is treated as a set of tags that must all be present in the items. The items are sorted by the number of matching tags, with the items that match the most tags first.\n\nOverall, the `TagsCache` class provides a way to efficiently cache and retrieve items based on their tags. It can be used in the larger project to implement features such as searching, filtering, and grouping of items based on their tags. For example, it could be used to implement a search bar that allows users to search for items based on their tags.\n## Questions: \n 1. What is the purpose of the TagsCache class?\n- The TagsCache class is used to cache items based on their tags.\n\n2. What is the significance of the _key property?\n- The _key property is used to index items by a specific key, if available.\n\n3. How does the find() method work?\n- The find() method takes an array of tags and returns an array of items that match those tags. It can handle single tags, multiple tags, and nested arrays of tags.","metadata":{"source":".autodoc/docs/markdown/src/core/tags-cache.md"}}],["229",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/tags.js)\n\nThe code defines a class called `Tags` that extends the `EventHandler` class. The `Tags` class is used to manage a set of tags that can be associated with an object. The tags are stored as an array of strings in the `_list` property and as a dictionary in the `_index` property. The `_index` property is used to quickly check if a tag is already in the set.\n\nThe `Tags` class provides methods to add, remove, and clear tags. The `add` method takes one or more tag names as arguments and adds them to the set. If a tag is already in the set, it is ignored. The `remove` method takes one or more tag names as arguments and removes them from the set. If a tag is not in the set, it is ignored. The `clear` method removes all tags from the set.\n\nThe `Tags` class also provides a `has` method that takes one or more tag names or arrays of tag names as arguments and returns `true` if the set contains any of the specified tags. The `has` method can be used to filter objects based on their tags.\n\nThe `Tags` class fires events when tags are added, removed, or changed. The `add` and `remove` events are fired for each tag operation, while the `change` event is fired once on bulk changes.\n\nThe `Tags` class can be used to add tags to entities and assets in the PlayCanvas engine. For example, an entity representing a player character could have tags like \"player\", \"human\", \"male\", \"warrior\", etc. These tags could be used to filter entities based on their properties, e.g. to find all entities that represent male characters or all entities that represent warriors.\n\nHere is an example of how to use the `Tags` class:\n\n```javascript\nimport { Tags } from 'playcanvas-engine';\n\nconst tags = new Tags();\n\ntags.add('player', 'human', 'male', 'warrior');\ntags.add(['level-1', 'mob']);\n\nconsole.log(tags.has('player')); // true\nconsole.log(tags.has('female')); // false\nconsole.log(tags.has('human', 'female')); // false\nconsole.log(tags.has('human', 'male')); // true\nconsole.log(tags.has(['level-1', 'mob'])); // true\n\ntags.remove('male', 'warrior');\n\nconsole.log(tags.has('male')); // false\nconsole.log(tags.has('warrior')); // false\n\ntags.clear();\n\nconsole.log(tags.has('player')); // false\nconsole.log(tags.has('level-1')); // false\n```\n## Questions: \n 1. What is the purpose of the `Tags` class?\n    \n    The `Tags` class is a subclass of `EventHandler` and provides a set of tag names that are automatically available on `Entity` and `Asset` as `tags` field.\n\n2. What events can be fired by the `Tags` class?\n    \n    The `Tags` class can fire `add`, `remove`, and `change` events. The `add` and `remove` events are fired on each tag operation, while the `change` event is fired once on bulk changes.\n\n3. How can a developer check if tags satisfy filters?\n    \n    A developer can use the `has` method of the `Tags` class to check if tags satisfy filters. Filters can be provided by simple name of tag, as well as by array of tags. If any of comma separated argument is satisfied, then it will return true.","metadata":{"source":".autodoc/docs/markdown/src/core/tags.md"}}],["230",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/time.js)\n\nThe code above defines a function called `now` that returns the current time in milliseconds. This function is used to measure time differences in the PlayCanvas engine project. \n\nThe function first checks if the `window` object is defined and if the `performance` object exists within it. If both conditions are true, it checks if the `now` method exists within the `performance` object and if the `timing` property exists within the `performance` object. If all of these conditions are true, the `now` method is bound to the `performance` object and returned. \n\nIf any of these conditions are false, the `Date.now` method is returned instead. \n\nThis function is useful for measuring time differences in the PlayCanvas engine project, such as for calculating frame rates or animation timings. \n\nHere is an example of how this function could be used in the larger project:\n\n```\nconst startTime = now();\n\n// Perform some task\n\nconst endTime = now();\nconst elapsedTime = endTime - startTime;\nconsole.log(`Task took ${elapsedTime} milliseconds to complete.`);\n```\n## Questions: \n 1. What is the purpose of this code?\n   This code defines a function called `now` that returns the current time in milliseconds, which can be used to measure time differences.\n\n2. How does the `now` function work?\n   The `now` function checks if the `window` object and the `performance` object are defined, and if so, it uses the `performance.now` method to get the current time in milliseconds. Otherwise, it falls back to using the `Date.now` method.\n\n3. Why is the `@ignore` tag used in the JSDoc comment?\n   The `@ignore` tag is used to indicate that this function should not be included in the generated documentation, likely because it is an internal implementation detail that is not intended to be used directly by external code.","metadata":{"source":".autodoc/docs/markdown/src/core/time.md"}}],["231",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/tracing.js)\n\n# PlayCanvas Engine - Tracing\n\nThe `Tracing` class is responsible for providing log tracing functionality to the PlayCanvas engine. It allows for tracing of the internal functionality of the engine. Note that the trace logging only takes place in the debug build of the engine and is stripped out in other builds.\n\nThe class has a static `_traceChannels` set that stores the names of enabled trace channels. It also has a static `stack` property that enables call stack logging for trace calls. By default, it is set to `false`.\n\nThe `set` method is used to enable or disable a trace channel. It takes two parameters - `channel` and `enabled`. `channel` is the name of the trace channel and can be one of the following:\n\n- `TRACEID_RENDER_FRAME`\n- `TRACEID_RENDER_FRAME_TIME`\n- `TRACEID_RENDER_PASS`\n- `TRACEID_RENDER_PASS_DETAIL`\n- `TRACEID_RENDER_ACTION`\n- `TRACEID_RENDER_TARGET_ALLOC`\n- `TRACEID_TEXTURE_ALLOC`\n- `TRACEID_SHADER_ALLOC`\n- `TRACEID_SHADER_COMPILE`\n- `TRACEID_VRAM_TEXTURE`\n- `TRACEID_VRAM_VB`\n- `TRACEID_VRAM_IB`\n- `TRACEID_RENDERPIPELINE_ALLOC`\n- `TRACEID_PIPELINELAYOUT_ALLOC`\n\n`enabled` is the new enabled state for the channel. If `enabled` is `true`, the channel is added to the `_traceChannels` set. If `enabled` is `false`, the channel is removed from the set.\n\nThe `get` method is used to test if the trace channel is enabled. It takes one parameter - `channel`, which is the name of the trace channel. It returns `true` if the trace channel is enabled, otherwise `false`.\n\nThis class can be used to enable or disable trace channels for debugging purposes. For example, if a developer wants to trace the rendering pipeline, they can enable the `TRACEID_RENDER_PASS` channel using the `set` method. They can then use the `get` method to check if the channel is enabled before logging any trace messages.\n\nExample usage:\n\n```javascript\nimport { Tracing } from 'playcanvas';\n\n// enable the TRACEID_RENDER_PASS channel\nTracing.set(Tracing.TRACEID_RENDER_PASS, true);\n\n// check if the TRACEID_RENDER_PASS channel is enabled\nif (Tracing.get(Tracing.TRACEID_RENDER_PASS)) {\n    console.log('TRACEID_RENDER_PASS is enabled');\n}\n```\n## Questions: \n 1. What is the purpose of the Tracing class?\n    \n    The Tracing class provides log tracing functionality to trace the internal functionality of the engine, and it only takes place in the debug build of the engine.\n\n2. What is the significance of the `_DEBUG` flag in the `set` method?\n    \n    The `_DEBUG` flag is used to ensure that trace logging only takes place in the debug build of the engine and is stripped out in other builds.\n\n3. What are some examples of trace channels that can be enabled or disabled using the `set` method?\n    \n    Some examples of trace channels that can be enabled or disabled using the `set` method include `TRACEID_RENDER_FRAME`, `TRACEID_RENDER_PASS`, `TRACEID_SHADER_COMPILE`, etc.","metadata":{"source":".autodoc/docs/markdown/src/core/tracing.md"}}],["232",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/uri.js)\n\n## Code Explanation: PlayCanvas Engine - URI\n\nThe `createURI` function and `URI` class are part of the PlayCanvas engine project. The purpose of this code is to provide a way to create and manipulate URI objects. \n\nThe `createURI` function takes an object as an argument, which contains the different parts of a URI. It then constructs a URI string from these parts and returns it. The different parts of the URI that can be passed as options include the scheme, authority, host, path, hostpath, query, and fragment. \n\nThe `URI` class is used to represent a URI object. It takes a URI string as an argument and parses it into its constituent parts. The different parts of the URI are then stored as properties of the `URI` object. The `toString` method is used to convert the `URI` object back into a URI string. \n\nThe `getQuery` method is used to extract the query parameters from the URI and return them as an object. The `setQuery` method is used to set the query parameters of the URI from an object. \n\nOverall, this code provides a way to create, parse, and manipulate URI objects in the PlayCanvas engine project. \n\nExample usage:\n\n```javascript\nconst uriString = 'https://www.example.com/path/to/resource?key=value#fragment';\nconst uri = new URI(uriString);\n\nconsole.log(uri.scheme); // logs \"https\"\nconsole.log(uri.authority); // logs \"www.example.com\"\nconsole.log(uri.path); // logs \"/path/to/resource\"\nconsole.log(uri.query); // logs \"key=value\"\nconsole.log(uri.fragment); // logs \"fragment\"\n\nuri.setQuery({ key: 'newvalue' });\nconsole.log(uri.toString()); // logs \"https://www.example.com/path/to/resource?key=newvalue#fragment\"\n```\n## Questions: \n 1. What is the purpose of the `createURI` function?\n- The `createURI` function is used to create a URI string from constituent parts such as scheme, authority, host, path, query, and fragment.\n\n2. What is the purpose of the `URI` class?\n- The `URI` class is used to represent a URI object and provides methods to manipulate and retrieve information from the URI.\n\n3. What is the purpose of the `getQuery` method in the `URI` class?\n- The `getQuery` method is used to extract the query parameters from the URI and return them as an object.","metadata":{"source":".autodoc/docs/markdown/src/core/uri.md"}}],["233",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/core/wasm-module.js)\n\nThe code defines a utility class called `WasmModule` that supports immediate and lazy loading of WebAssembly (wasm) modules. The class provides three static methods: `setConfig`, `getConfig`, and `getInstance`. \n\nThe `setConfig` method sets the configuration for a wasm module. It takes two parameters: `moduleName` and `config`. The `moduleName` parameter is a string that specifies the name of the module. The `config` parameter is an object that contains the following properties: `glueUrl`, `wasmUrl`, `fallbackUrl`, `numWorkers`, and `errorHandler`. The `glueUrl` property is the URL of the glue script, the `wasmUrl` property is the URL of the wasm script, and the `fallbackUrl` property is the URL of the fallback script to use when wasm modules aren't supported. The `numWorkers` property is the number of threads to use for modules running on worker threads, and the `errorHandler` property is a function to be called if the module fails to download.\n\nThe `getConfig` method gets the configuration for a wasm module. It takes one parameter: `moduleName`, which is a string that specifies the name of the module. The method returns the previously set configuration object or `undefined` if the module has not been configured.\n\nThe `getInstance` method gets a wasm module instance. It takes two parameters: `moduleName` and `callback`. The `moduleName` parameter is a string that specifies the name of the module. The `callback` parameter is a function that is called when the instance is available. If the instance has already been created, the callback is called immediately with the instance as the parameter. If the instance has not been created, the callback is added to the list of callbacks for the module. If the module has been configured, the `initialize` method is called to create the instance.\n\nThe `Impl` class is used internally by the `WasmModule` class. It defines several static methods and properties that are used to load and initialize wasm modules. The `modules` property is an object that stores the state of each module. The `wasmSupported` method returns `true` if the running host supports wasm modules. The `loadScript` method loads a script and calls a callback when the script has loaded or failed to load. The `loadWasm` method loads a wasm module and calls a callback when the module has been instantiated or failed to load. The `getModule` method gets the state object for a module. The `initialize` method initializes a module if it has not already been initialized.\n\nThe `cachedResult` function is a wrapper function that caches the result of a function on the first invocation and subsequently returns the cached value. It takes one parameter: `func`, which is the function to be cached. The function returns a new function that checks if the result has been cached. If the result has not been cached, the function calls the original function and caches the result. If the result has been cached, the function returns the cached result.\n\nOverall, the `WasmModule` class provides a simple interface for loading and initializing wasm modules. It allows modules to be configured with fallback scripts and error handlers, and provides a way to lazily load modules only when they are needed.\n## Questions: \n 1. What is the purpose of the `cachedResult` function?\n- The `cachedResult` function is a wrapper function that caches the result of a function on the first invocation and returns the cached value on subsequent invocations.\n\n2. What is the purpose of the `loadWasm` function?\n- The `loadWasm` function is used to load a wasm module. It checks if the running host supports wasm modules and loads the module from the appropriate URL based on the configuration provided.\n\n3. What is the purpose of the `WasmModule` class?\n- The `WasmModule` class is a static utility class that supports immediate and lazy loading of wasm modules. It provides methods to set and get a module's configuration, and to get a module instance.","metadata":{"source":".autodoc/docs/markdown/src/core/wasm-module.md"}}],["234",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/binder/anim-binder.js)\n\nThe code defines a class called `AnimBinder` which is used by the `AnimEvaluator` class in the PlayCanvas engine to resolve animation target path strings into instances of `AnimTarget`. The purpose of this class is to provide utility functions for working with animation target paths, as well as to define methods that are called by the `AnimEvaluator` during the animation process.\n\nThe `AnimBinder` class provides two static methods for working with animation target paths. The `joinPath` method takes a list of path segments and joins them into a path string using the full stop character (or another character if one is supplied). The `splitPath` method takes a path string and splits it into its segments, resolving any character escaping that may be present.\n\nThe class also provides three instance methods that are called by the `AnimEvaluator`. The `resolve` method takes an animation curve path and returns an instance of `AnimTarget` that will handle setting the value of the target, or null if no such target exists. The `unresolve` method is called when the `AnimEvaluator` no longer has a curve driving a given key. The `update` method is called by the `AnimEvaluator` once per frame after animation updates are done.\n\nFinally, the class provides a static `encode` method that takes an entity path, component, and property path and returns a string that encodes the locator. This method is used to convert a locator array into its string version.\n\nOverall, the `AnimBinder` class provides utility functions for working with animation target paths and defines methods that are called by the `AnimEvaluator` during the animation process. It is an important part of the PlayCanvas engine and is used to ensure that animations are handled correctly.\n## Questions: \n 1. What is the purpose of the `AnimBinder` class?\n    \n    The `AnimBinder` class is used by the `AnimEvaluator` to resolve animation target path strings into instances of `AnimTarget`.\n\n2. What do the `joinPath` and `splitPath` methods do?\n    \n    The `joinPath` method joins a list of path segments into a path string using the full stop character or another character if supplied. The `splitPath` method splits a path string into its segments and resolves character escaping.\n\n3. What is the purpose of the `encode` method?\n    \n    The `encode` method converts a locator array into its string version, which is used to encode the entity location, component, and property location as a string.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/binder/anim-binder.md"}}],["235",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/binder/default-anim-binder.js)\n\nThe `DefaultAnimBinder` class is an implementation of the `AnimBinder` interface for animating a skeleton in the graph-node hierarchy. It is used to bind animation data to the properties of a skeleton, such as its position, rotation, and scale. The class is part of the PlayCanvas engine project and is located in a file within the project.\n\nThe `DefaultAnimBinder` class has a constructor that takes a `graph` parameter, which is a reference to the root node of the graph-node hierarchy. The constructor initializes several properties of the class, including a cache of node names, a cache of animation targets, and a list of active nodes. It also defines several handler functions that are used to create animation targets for different types of properties.\n\nThe `findNode` method is used to find a node in the graph-node hierarchy based on a given path. If the path is not found, the method returns null. The `resolve` method is used to create an animation target for a given path. If the path is not found or the property is not supported, the method returns null. The `unresolve` method is used to remove an animation target for a given path. The `update` method is used to flag animating nodes as dirty, which causes them to be updated during the next animation frame.\n\nThe `DefaultAnimBinder` class is used in the larger PlayCanvas engine project to animate skeletons in 3D models. It is used by other classes in the project that need to animate the position, rotation, or scale of a skeleton. For example, the `AnimationComponent` class uses the `DefaultAnimBinder` class to bind animation data to the properties of a skeleton. The `DefaultAnimBinder` class is also used by the `AnimationSystem` class to update the animation state of entities in the scene.\n## Questions: \n 1. What is the purpose of the `DefaultAnimBinder` class?\n- The `DefaultAnimBinder` class is an implementation of the `AnimBinder` interface used for animating a skeleton in the graph-node hierarchy.\n\n2. What are the properties and methods of the `DefaultAnimBinder` class?\n- The `DefaultAnimBinder` class has properties such as `graph`, `nodes`, `targetCache`, `nodeCounts`, `activeNodes`, and `handlers`. It also has methods such as `_isPathInMask`, `_isPathActive`, `findNode`, `createAnimTarget`, `resolve`, `unresolve`, `update`, and `assignMask`.\n\n3. What is the purpose of the `_mask` property and the `assignMask` method?\n- The `_mask` property is used to filter which nodes are active during animation. The `assignMask` method is used to assign a new mask to the `DefaultAnimBinder` instance and returns `true` if the mask is different from the current one.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/binder/default-anim-binder.md"}}],["236",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/constants.js)\n\nThis code defines three constants that represent different interpolation schemes: stepped, linear, and cubic spline. Interpolation is the process of estimating values between two known values. In the context of game development, interpolation is often used to smooth out movement or animation between frames. \n\nThe `INTERPOLATION_STEP` constant represents a stepped interpolation scheme, where the value jumps directly from one known value to the next without any in-between values. This can create a choppy or jerky effect, but may be useful in certain situations where precise control is needed.\n\nThe `INTERPOLATION_LINEAR` constant represents a linear interpolation scheme, where the value changes at a constant rate between the two known values. This creates a smooth and natural-looking transition between values.\n\nThe `INTERPOLATION_CUBIC` constant represents a cubic spline interpolation scheme, which is a more complex method of interpolation that uses a mathematical formula to calculate the in-between values. This can create even smoother and more natural-looking transitions than linear interpolation, but may be more computationally expensive.\n\nThese constants can be used throughout the PlayCanvas engine to specify which interpolation scheme to use in different contexts. For example, when animating a character's movement, the developer may choose to use linear interpolation to create a smooth and natural-looking motion. Alternatively, when animating a game object's rotation, the developer may choose to use stepped interpolation to create a more precise and controlled effect.\n\nOverall, this code provides a simple and flexible way for developers to specify different interpolation schemes in their projects, allowing for greater control over the look and feel of their games and applications.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code defines three constants that represent different interpolation schemes: stepped, linear, and cubic spline.\n\n2. **How are these constants used in the PlayCanvas engine?**\\\nWithout additional context, it is unclear how these constants are used in the PlayCanvas engine. It is possible that they are used in animation or movement calculations.\n\n3. **What are the possible values for the constants?**\\\nThe possible values for the constants are 0, 1, and 2, which correspond to the stepped, linear, and cubic spline interpolation schemes, respectively.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/constants.md"}}],["237",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/anim-blend-tree-1d.js)\n\nThe code defines a class called `AnimBlendTree1D` which extends another class called `AnimBlendTree`. This class is used to create a blend tree that calculates its weights using a 1D algorithm based on a thesis by Rune Skovbo Johansen. \n\nThe purpose of this class is to create a blend tree that can be used to blend animations together. The `calculateWeights` method of this class is used to calculate the weights of the child nodes of the blend tree based on a given parameter value. The weights are calculated using a 1D algorithm that takes into account the distance between the child nodes and the parameter value. \n\nThe constructor of this class takes in several parameters including the `AnimState` that this `AnimBlendTree` belongs to, the parent of the `AnimBlendTree`, the name of the `BlendTree`, the coordinate/vector that is used to determine the weight of this node when it's part of a `AnimBlendTree`, the anim component parameters which are used to calculate the current weights of the blend trees children, the child nodes that this blend tree should create, a boolean value that determines whether the speed of each blended animation will be synchronized, and two functions that are used to create child blend trees of varying types and to get the current parameter values at runtime.\n\nThe `calculateWeights` method of this class first checks if the parameter values have been updated. If they have not been updated, it calculates the weights of the child nodes of the blend tree based on the 1D algorithm. It then updates the weighted speed of each child node if the `syncAnimations` parameter is set to true.\n\nOverall, this class is an important part of the PlayCanvas engine project as it allows for the creation of blend trees that can be used to blend animations together. The 1D algorithm used to calculate the weights of the child nodes is based on a thesis by Rune Skovbo Johansen and is an efficient way to blend animations together.\n## Questions: \n 1. What is the purpose of the `calculateWeights` method in the `AnimBlendTree1D` class?\n- The `calculateWeights` method is used to calculate the weights of the child nodes in the blend tree based on the current parameter values.\n\n2. What is the significance of the `point` parameter in the constructor of the `AnimBlendTree1D` class?\n- The `point` parameter is used to determine the weight of the node when it's part of a blend tree, based on a 1D algorithm.\n\n3. What is the purpose of the `syncAnimations` parameter in the constructor of the `AnimBlendTree1D` class?\n- The `syncAnimations` parameter is used to determine whether the speed of each blended animation should be synchronized or not.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/anim-blend-tree-1d.md"}}],["238",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/anim-blend-tree-2d-cartesian.js)\n\nThe code defines a class called `AnimBlendTreeCartesian2D` that extends `AnimBlendTree`. This class is used to calculate the weights of child nodes in an animation blend tree using a 2D Cartesian algorithm based on a thesis by Rune Skovbo Johansen. \n\nThe `calculateWeights` method of this class is responsible for calculating the weights of the child nodes based on the current parameter values. It first checks if the parameter values have been updated and returns if they have not. It then iterates over each child node and calculates its weight based on its position in the 2D Cartesian space. The weight is calculated by finding the minimum distance between the current parameter value and the child node's position, and then clamping it between 0 and 1. The child node's weight is set to this value, and the sum of all weights is calculated. \n\nIf the `_syncAnimations` flag is set to true, the method also calculates the weighted duration sum of all child nodes. This is done by dividing each child node's animation duration by its absolute speed and multiplying it by its weight. The sum of these values is then calculated. \n\nFinally, the weights of all child nodes are normalized by dividing each child node's weight by the sum of all weights. If `_syncAnimations` is true, the weighted speed of each child node is also calculated by dividing its animation duration by its absolute speed and the weighted duration sum. \n\nThe purpose of this class is to provide a way to blend animations in a 2D Cartesian space, where each child node represents a different animation and its position represents its parameter values. This can be useful in games or other applications where animations need to be blended based on multiple parameters. \n\nExample usage:\n\n```javascript\nimport { AnimBlendTreeCartesian2D } from 'path/to/anim-blend-tree-cartesian-2d.js';\n\n// create a new blend tree\nconst blendTree = new AnimBlendTreeCartesian2D();\n\n// add child nodes to the blend tree\nblendTree.addChild(node1);\nblendTree.addChild(node2);\nblendTree.addChild(node3);\n\n// set the parameter values\nblendTree.setParameterValues(0.5, 0.8);\n\n// calculate the weights of the child nodes\nblendTree.calculateWeights();\n\n// get the weights of the child nodes\nconst weights = blendTree.getChildren().map(child => child.weight);\n```\n## Questions: \n 1. What is the purpose of the `AnimBlendTreeCartesian2D` class and how does it differ from the `AnimBlendTree` class it extends?\n- The `AnimBlendTreeCartesian2D` class is an implementation of an `AnimBlendTree` that calculates weights using a 2D Cartesian algorithm based on a thesis. It differs from the `AnimBlendTree` class in how it calculates the weights for its children.\n\n2. What is the significance of the `pointDistanceCache` method and how is it used in the `calculateWeights` method?\n- The `pointDistanceCache` method calculates the distance between two points and caches the result to avoid redundant calculations. It is used in the `calculateWeights` method to calculate the weight of each child based on its distance from the current point.\n\n3. What is the purpose of the `weightSum` and `weightedDurationSum` variables in the `calculateWeights` method?\n- The `weightSum` variable is used to keep track of the total weight of all children, while the `weightedDurationSum` variable is used to keep track of the total duration of all animations weighted by their respective weights. These variables are used to normalize the weights and calculate the weighted speed of each child if animation synchronization is enabled.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/anim-blend-tree-2d-cartesian.md"}}],["239",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/anim-blend-tree-2d-directional.js)\n\nThe code defines a class called `AnimBlendTreeDirectional2D` which extends another class called `AnimBlendTree`. This class calculates the weights of its children nodes using a 2D directional algorithm based on a thesis by Rune Skovbo Johansen. \n\nThe `AnimBlendTreeDirectional2D` class has two static properties `_p` and `_pip` which are instances of the `Vec2` class from the `../../../core/math/vec2.js` module. These properties are used to store 2D vectors that are used in the calculation of the weights of the child nodes.\n\nThe `pointCache` method is used to cache the 2D vector values for each pair of child nodes. It takes two arguments `i` and `j` which are indices of the child nodes. It calculates the 2D vector value for the pair of child nodes using the formulae from the thesis and returns the value. If the value has already been calculated, it returns the cached value.\n\nThe `calculateWeights` method is used to calculate the weights of the child nodes. It first checks if the parameter values have been updated and returns if they have. It then calculates the length of the parameter vector and initializes two variables `weightSum` and `weightedDurationSum` to 0.0. It then iterates over each child node and calculates the weight of the node using the formulae from the thesis. It also calculates the weighted duration sum if `_syncAnimations` is true. Finally, it iterates over each child node again and sets the weight and weighted speed of the node.\n\nThis class is used in the larger project to blend animations based on the direction of the parameter vector. It is a specific implementation of the `AnimBlendTree` class that uses a directional algorithm to calculate the weights of the child nodes. It can be used to blend animations in a more natural way based on the direction of the parameter vector. \n\nExample usage:\n\n```javascript\nimport { AnimBlendTreeDirectional2D } from './anim-blend-tree-directional-2d.js';\n\nconst blendTree = new AnimBlendTreeDirectional2D();\n// add child nodes to the blend tree\nblendTree.addChild(childNode1);\nblendTree.addChild(childNode2);\n// set parameter values\nblendTree.setParameterValues(0.5, 0.5);\n// calculate weights of child nodes\nblendTree.calculateWeights();\n// get the blended animation\nconst blendedAnimation = blendTree.getBlendedAnimation();\n```\n## Questions: \n 1. What is the purpose of the `AnimBlendTreeDirectional2D` class and how does it differ from the `AnimBlendTree` class it extends?\n    \n    Answer: The `AnimBlendTreeDirectional2D` class is a subclass of `AnimBlendTree` that calculates its weights using a 2D directional algorithm based on a thesis. It differs from `AnimBlendTree` in that it uses a different algorithm to calculate weights.\n\n2. What is the significance of the `pointCache` method and how is it used in the `calculateWeights` method?\n    \n    Answer: The `pointCache` method is used to cache the results of a calculation involving two child nodes of the blend tree. It is used in the `calculateWeights` method to avoid redundant calculations and improve performance.\n\n3. What is the purpose of the `weightSum` and `weightedDurationSum` variables in the `calculateWeights` method?\n    \n    Answer: The `weightSum` variable is used to calculate the sum of the weights of all child nodes in the blend tree, while the `weightedDurationSum` variable is used to calculate the sum of the durations of all child animations weighted by their respective weights. These variables are used to normalize the weights and durations of child nodes in the blend tree.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/anim-blend-tree-2d-directional.md"}}],["240",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/anim-blend-tree-direct.js)\n\nThe code defines a class called `AnimBlendTreeDirect` which extends another class called `AnimBlendTree`. The purpose of this class is to calculate normalized weight values for a blend tree. A blend tree is a data structure used in animation systems to blend multiple animations together based on certain parameters. \n\nThe `calculateWeights()` method is the main function of this class. It first checks if the parameter values have been updated and returns if they have. It then calculates the sum of the weights of all the children nodes in the blend tree. If the `_syncAnimations` flag is set to true, it also calculates the weighted duration sum of all the child animations. \n\nNext, it iterates through all the children nodes and calculates the weight of each child based on its parameter value and the total weight sum. If `_syncAnimations` is true, it also calculates the weighted speed of each child based on its duration, absolute speed, and the weighted duration sum. If the total weight sum is zero, it sets the weight and weighted speed of the child to zero. \n\nThis class is used in the larger PlayCanvas engine project to provide animation blending functionality. Developers can create instances of this class and add child nodes to create blend trees for their animations. They can then call the `calculateWeights()` method to calculate the normalized weight values for each child node based on their parameter values and the total weight sum. This allows for smooth blending between multiple animations based on certain parameters. \n\nExample usage:\n\n```\nimport { AnimBlendTreeDirect } from 'playcanvas-engine';\n\n// create a new blend tree\nconst blendTree = new AnimBlendTreeDirect();\n\n// add child nodes to the blend tree\nblendTree.addChild(node1);\nblendTree.addChild(node2);\nblendTree.addChild(node3);\n\n// set parameter values for each child node\nblendTree.setParameterValue(node1, 0.5);\nblendTree.setParameterValue(node2, 0.2);\nblendTree.setParameterValue(node3, 0.3);\n\n// calculate the normalized weight values for each child node\nblendTree.calculateWeights();\n\n// use the normalized weight values to blend animations\n```\n## Questions: \n 1. What is the purpose of this code and how does it fit into the PlayCanvas engine?\n- This code defines a class called `AnimBlendTreeDirect` which extends `AnimBlendTree` and calculates normalized weight values for animations. It is part of the PlayCanvas engine's animation system.\n\n2. What is the significance of the `_syncAnimations` property and how does it affect the calculations?\n- The `_syncAnimations` property determines whether the animation durations and speeds of child nodes should be taken into account when calculating weights. If it is `true`, then `weightedDurationSum` and `child.weightedSpeed` will be calculated and used in the weight calculation.\n\n3. What is the purpose of the `updateParameterValues()` method and when would it return `true`?\n- The `updateParameterValues()` method updates the parameter values of the blend tree. It returns `true` if any of the parameter values have changed since the last update, indicating that the weights need to be recalculated.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/anim-blend-tree-direct.md"}}],["241",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/anim-blend-tree.js)\n\nThe `AnimBlendTree` class is a part of the PlayCanvas engine project and is used to blend multiple `AnimNodes` together. It is used to create a hierarchy of `AnimNodes` and can be the child of other `AnimBlendTrees`. The class takes a blend type as an argument which defines which function should be used to determine the weights of each of its children, based on the current parameter value.\n\nThe constructor of the `AnimBlendTree` class takes several arguments including the `AnimState` that this `AnimBlendTree` belongs to, the parent of the `AnimBlendTree`, the name of the `BlendTree`, the coordinate/vector that is used to determine the weight of this node when it's part of a `AnimBlendTree`, the anim component parameters which are used to calculate the current weights of the blend trees children, the child nodes that this blend tree should create, and functions used to create child blend trees of varying types and to get the current parameter values at runtime.\n\nThe `AnimBlendTree` class has several methods including `get weight()` which calculates the weight of the `AnimBlendTree` based on the weights of its children and its parent, `get syncAnimations()` which returns whether the speed of each blended animation will be synchronized, `getChild(name)` which returns the child node with the given name, `updateParameterValues()` which updates the parameter values of the `AnimBlendTree`, `getNodeWeightedDuration(i)` which returns the weighted duration of the child node at index `i`, and `getNodeCount()` which returns the number of child nodes in the `AnimBlendTree`.\n\nOverall, the `AnimBlendTree` class is an important part of the PlayCanvas engine project as it allows for the creation of complex animations by blending multiple `AnimNodes` together and creating a hierarchy of `AnimNodes`.\n## Questions: \n 1. What is the purpose of the `AnimBlendTree` class and how is it used in the PlayCanvas engine?\n   \n   The `AnimBlendTree` class is used to store and blend multiple `AnimNodes` together, and can be the child of other `AnimBlendTrees` to create a hierarchy of `AnimNodes`. It takes a blend type as an argument which defines which function should be used to determine the weights of each of its children, based on the current parameter value.\n\n2. What parameters are required to create a new instance of the `AnimBlendTree` class?\n   \n   To create a new instance of the `AnimBlendTree` class, the following parameters are required: `state` (the `AnimState` that this `AnimBlendTree` belongs to), `parent` (the parent of the `AnimBlendTree`, if not null), `name` (the name of the `BlendTree`), `point` (the coordinate/vector used to determine the weight of this node when it's part of a `BlendTree`), `parameters` (the anim component parameters used to calculate the current weights of the blend trees children), `children` (the child nodes that this blend tree should create), `syncAnimations` (a boolean indicating whether the speed of each blended animation should be synchronized), `createTree` (a function used to create child blend trees of varying types), and `findParameter` (a function used at runtime to get the current parameter values).\n\n3. What is the purpose of the `getChild` method in the `AnimBlendTree` class?\n   \n   The `getChild` method is used to retrieve a child node of the `AnimBlendTree` by name. It iterates through the `_children` array and returns the child node with a matching `name` property, or `null` if no matching child node is found.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/anim-blend-tree.md"}}],["242",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/anim-node.js)\n\nThe code defines a class called `AnimNode` which is used to represent a single animation track in the current state. The purpose of this class is to store information about an animation track, such as its name, speed, weight, and coordinate/vector. \n\nAn `AnimNode` can be part of a `BlendTree` hierarchy, which controls the weight (contribution to the state's final animation) of its child `AnimNodes`. The `AnimNode` class has a constructor that takes in several parameters, including the `AnimState` that it belongs to, its parent `BlendTree`, its name, its coordinate/vector, and its speed. \n\nThe `AnimNode` class has several getter and setter methods that allow access to its properties, such as its parent, name, path, point, weight, speed, and animTrack. The `weight` property is calculated based on the weight of its parent and its own weight. The `normalizedWeight` property is calculated based on the total weight of the state that it belongs to. The `speed` property is calculated based on its weighted speed and its speed. \n\nThis class is used in the larger PlayCanvas engine project to store information about animation tracks and their weights in a state. It is also used to calculate the final animation based on the weights of the `AnimNodes` in the `BlendTree` hierarchy. \n\nExample usage:\n\n```javascript\nimport { AnimNode } from 'playcanvas';\n\nconst state = new AnimState();\nconst parent = new BlendTree(state);\nconst name = 'animNode';\nconst point = [0, 0];\nconst speed = 1;\n\nconst animNode = new AnimNode(state, parent, name, point, speed);\nanimNode.weight = 0.5;\nanimNode.weightedSpeed = 0.8;\nanimNode.animTrack = new AnimTrack();\n```\n## Questions: \n 1. What is the purpose of the `Vec2` import from `../../../core/math/vec2.js`?\n- The `Vec2` import is used to create a coordinate/vector that determines the weight of the `AnimNode` when it's part of a `BlendTree`.\n\n2. What is the significance of the `speed` parameter in the `constructor` method?\n- The `speed` parameter sets the speed that the `AnimTrack` should play at, with a default value of 1.\n\n3. What is the purpose of the `normalizedWeight` getter?\n- The `normalizedWeight` getter returns the weight of the `AnimNode` divided by the total weight of the `AnimState`, ensuring that the weights of all `AnimNodes` in the state add up to 1.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/anim-node.md"}}],["243",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/anim-state.js)\n\nThe code defines a class called `AnimState` which represents a single state that the controller can be in. Each state contains either a single `AnimNode` or a `AnimBlendTree` of multiple `AnimNodes`, which will be used to animate the Entity while the state is active. An `AnimState` will stay active and play as long as there is no `AnimTransition` with its conditions met that has that `AnimState` as its source state.\n\nThe `AnimState` class has several properties and methods that allow for the manipulation of the state's animations. The `_animations` and `_animationList` properties are used to store the animations associated with the state. The `addAnimation` method is used to add an animation to the state. The `animations` property is used to get or set the list of animations associated with the state. The `hasAnimations` property is used to determine if the state has any animations associated with it. The `speed` and `loop` properties are used to set the speed and loop properties of the state's animations.\n\nThe `AnimState` class also has several methods that are used to get information about the state's animations. The `nodeCount` property is used to get the number of nodes in the state's blend tree. The `playable` property is used to determine if the state is playable. The `looping` property is used to determine if the state's animations should loop. The `totalWeight` property is used to get the total weight of the state's animations. The `timelineDuration` property is used to get the duration of the state's animations.\n\nThe `AnimState` class is used in the larger project to define the different states that the controller can be in. Each state has a set of animations associated with it that will be played while the state is active. The `AnimState` class is also used to get information about the state's animations, such as the number of nodes in the blend tree and the duration of the animations. Overall, the `AnimState` class is an important part of the PlayCanvas engine that allows for the creation and manipulation of animations in the engine.\n## Questions: \n 1. What is the purpose of the `AnimState` class?\n- The `AnimState` class defines a single state that the controller can be in, which contains either a single `AnimNode` or a `AnimBlendTree` of multiple `AnimNodes`, used to animate the Entity while the state is active.\n\n2. What is the `_createTree` method used for?\n- The `_createTree` method is used to create a new `AnimBlendTree` hierarchy based on the specified type, parameters, and children.\n\n3. What is the purpose of the `addAnimation` method?\n- The `addAnimation` method is used to add a new animation to the `AnimState` instance, which is associated with a specific `AnimNode` or `AnimBlendTree` based on the specified path.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/anim-state.md"}}],["244",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/anim-transition.js)\n\nThe code defines a class called `AnimTransition` which represents connections between `AnimStates` in a state graph. The purpose of this class is to allow for the creation of transitions between different states in an animation controller. During each frame, the controller checks if any of the `AnimTransitions` have the current `AnimState` as their source state. If so, and the conditions for the transition are met, the controller will transition to the destination state.\n\nThe `AnimTransition` class has several properties that can be set when creating a new instance of the class. These properties include the source state (`from`), the destination state (`to`), the duration of the transition (`time`), the priority of the transition (`priority`), a list of conditions that must be met for the transition to occur (`conditions`), the time at which the transition should be exited (`exitTime`), the time at which the destination state should begin playing its animation (`transitionOffset`), and the type of interruption source (`interruptionSource`).\n\nThe `from`, `to`, `time`, and `priority` properties are straightforward and define the basic parameters of the transition. The `conditions` property is an array of conditions that must be met for the transition to occur. These conditions are defined elsewhere in the code and can be used to check things like the value of a parameter or the state of a trigger.\n\nThe `exitTime` property is used to specify the exact frame during which the source state's progress passes the time specified. This is given as a normalized value of the source state's duration. If the value is less than 1, it will be checked every animation loop. The `transitionOffset` property is used to specify the time at which the destination state should begin playing its animation. This is given in normalized time, based on the state's duration and must be between 0 and 1.\n\nThe `interruptionSource` property is used to define whether another transition can interrupt this one and which of the current or previous states transitions can do so. This is one of `pc.ANIM_INTERRUPTION_*` and defaults to `pc.ANIM_INTERRUPTION_NONE`.\n\nOverall, the `AnimTransition` class is an important part of the animation controller in the PlayCanvas engine. It allows for the creation of transitions between different states in an animation, which is essential for creating complex animations with multiple states. Here is an example of how the `AnimTransition` class might be used:\n\n```javascript\nimport { AnimTransition } from 'playcanvas-engine';\n\nconst transition = new AnimTransition({\n  from: 'idle',\n  to: 'walk',\n  time: 0.5,\n  priority: 1,\n  conditions: [\n    { parameter: 'speed', operator: '>', value: 0 }\n  ],\n  exitTime: 0.9,\n  transitionOffset: 0.2,\n  interruptionSource: pc.ANIM_INTERRUPTION_PREVIOUS\n});\n```\n\nIn this example, a new `AnimTransition` instance is created with the source state set to `'idle'` and the destination state set to `'walk'`. The transition has a duration of `0.5` seconds and a priority of `1`. The transition will only occur if the value of the `'speed'` parameter is greater than `0`. The transition will be exited at `0.9` normalized time and the destination state will begin playing its animation at `0.2` normalized time. Finally, the transition can be interrupted by transitions from the previous state.\n## Questions: \n 1. What is the purpose of the `AnimTransition` class?\n    \n    The `AnimTransition` class represents connections in the controller's state graph between `AnimStates`, and is used to transition between states based on parameter-based conditions.\n\n2. What options can be passed to the `AnimTransition` constructor?\n    \n    The `AnimTransition` constructor can be passed options such as the source and destination states, the duration of the transition, a priority value, a list of conditions that must be met for the transition to be used, and more.\n\n3. What is the purpose of the `interruptionSource` option in the `AnimTransition` constructor?\n    \n    The `interruptionSource` option defines whether another transition can interrupt this one and which of the current or previous states transitions can do so, and defaults to `pc.ANIM_INTERRUPTION_NONE`.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/anim-transition.md"}}],["245",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/controller/constants.js)\n\nThis code defines a set of constants that are used in the PlayCanvas engine for defining and manipulating animations in an anim state graph. \n\nThe constants define various properties of the animation state graph, such as the type of interruption source for a transition between states, the type of condition predicate for a transition, the type of parameter for a state, and the type of blending for a layer. \n\nFor example, the `ANIM_INTERRUPTION_NONE` constant is used to set the anim state graph transition interruption source to no state, while the `ANIM_GREATER_THAN` constant is used to set an anim state graph transition condition predicate as '>'. \n\nThese constants can be used throughout the PlayCanvas engine to define and manipulate animations in a consistent and standardized way. For example, a developer could use the `ANIM_PARAMETER_INTEGER` constant to set a state parameter as an integer value, or use the `ANIM_LAYER_ADDITIVE` constant to indicate that a layer's animations should blend additively with previous layers.\n\nOverall, this code provides a useful set of constants for working with animations in the PlayCanvas engine, making it easier for developers to create and manipulate complex animations in their projects.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines constants used for setting animation state graph transition interruption sources, condition predicates, parameter types, blend types, and layer types in the PlayCanvas engine.\n\n2. What are some examples of how these constants might be used in the PlayCanvas engine?\n- These constants might be used to specify how animations should transition between states, what conditions should trigger those transitions, what types of parameters should be used to control animations, what types of blending should be used to combine animations, and what types of layers should be used to organize animations.\n\n3. Are there any other related constants or functions that are not defined in this file?\n- It is possible that there are other related constants or functions that are not defined in this file, as this file only defines a subset of the constants used in the PlayCanvas engine for animation state graphs.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/controller/constants.md"}}],["246",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-blend.js)\n\nThe code defines a class called `AnimBlend` that contains several static methods for blending and manipulating animations. The purpose of this class is to provide a set of utility functions that can be used to blend different types of animations, such as vector and quaternion animations.\n\nThe `dot` method calculates the dot product of two arrays of numbers. This method is used to calculate the length of a vector, which is used in the `normalize` method to normalize a vector. The `normalize` method takes an array of numbers and normalizes it to a unit vector. This method is used to normalize quaternions, which are used to represent rotations.\n\nThe `set` method sets the values of an array to the values of another array. This method is used to set the values of a quaternion to the values of another quaternion. If the type parameter is set to 'quaternion', the method normalizes the quaternion before setting its values.\n\nThe `blendVec` method blends two arrays of numbers using a linear interpolation. This method is used to blend vector animations.\n\nThe `blendQuat` method blends two quaternions using a spherical linear interpolation. This method is used to blend quaternion animations.\n\nThe `blend` method is a wrapper method that calls either `blendVec` or `blendQuat` depending on the type parameter.\n\nThe `stableSort` method sorts an array using a stable sorting algorithm. This method is used to sort animations in a way that preserves their order.\n\nOverall, the `AnimBlend` class provides a set of utility functions that can be used to blend and manipulate animations in the PlayCanvas engine. These methods are used to create smooth and seamless animations that can be used in games and other interactive applications.\n## Questions: \n 1. What is the purpose of the `AnimBlend` class?\n- The `AnimBlend` class provides static methods for blending and sorting animations.\n\n2. What types of data can be blended using the `AnimBlend` class?\n- The `AnimBlend` class can blend vectors and quaternions.\n\n3. What is the purpose of the `stableSort` method in the `AnimBlend` class?\n- The `stableSort` method performs a stable sort on an array of items using a provided comparison function.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-blend.md"}}],["247",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-cache.js)\n\nThe code defines a class called `AnimCache` that is used to cache data for the evaluation of a single curve timeline. The purpose of this class is to optimize the performance of the animation system by pre-calculating certain values that are used repeatedly during the evaluation of an animation curve.\n\nThe `AnimCache` class has several members that are calculated per-segment, such as the time of the left and right knots, the distance between the current knots, and the index of the left and right knots. These members are recalculated whenever the time value being evaluated falls outside the current segment.\n\nThe class also has members that are calculated per-time evaluation, such as the normalized time and the hermite weights. The normalized time is calculated based on the current time value and the left and right knots of the current segment. The hermite weights are used to calculate the output anim data for cubic interpolation.\n\nThe `AnimCache` class has a method called `eval` that is used to evaluate the output anim data at the current time. The method takes three arguments: `result`, `interpolation`, and `output`. The `result` argument is an array that will contain the output anim data. The `interpolation` argument is an integer that specifies the type of interpolation to use (either cubic, linear, or step). The `output` argument is an object that contains the input and output data for the animation curve.\n\nIf the interpolation type is step, the method simply copies the data from the current segment to the `result` array. If the interpolation type is linear, the method uses the `math.lerp` function to interpolate between the data from the left and right knots of the current segment. If the interpolation type is cubic, the method uses the hermite weights to calculate the output anim data.\n\nOverall, the `AnimCache` class is an important part of the PlayCanvas engine's animation system. It helps to optimize the performance of the system by pre-calculating certain values that are used repeatedly during the evaluation of an animation curve.\n## Questions: \n 1. What is the purpose of the `AnimCache` class?\n- The `AnimCache` class is used for internal cache data for the evaluation of a single curve timeline.\n\n2. What are the different types of interpolation supported by this code?\n- The different types of interpolation supported by this code are `INTERPOLATION_CUBIC`, `INTERPOLATION_LINEAR`, and `INTERPOLATION_STEP`.\n\n3. What is the purpose of the `update` method in the `AnimCache` class?\n- The `update` method in the `AnimCache` class is used to update the internal cache data based on the current time and input data.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-cache.md"}}],["248",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-clip.js)\n\nThe `AnimClip` class in the PlayCanvas engine project is responsible for managing the running state of an animation track. It contains and updates the animation cursor and performs looping logic. The class is not intended to be used directly by developers, but rather as a helper class for other parts of the engine that require animation functionality.\n\nThe constructor of the `AnimClip` class takes in several parameters, including the animation data, the initial time of the clip, the speed of the animation playback, whether the clip is playing or not, whether the clip should loop, and an event handler to call when an event is fired by the clip. The class also has several getter and setter methods for various properties, such as the name of the clip, the animation track, the time, the speed, the loop, the blend weight, and the blend order.\n\nThe `alignCursorToCurrentTime` method is responsible for moving the event cursor to the event that should fire after the current time. The `activeEventsForFrame` method checks whether the next event occurs during the current frame and fires the event if it does. The `progressForTime` method returns the progress of the animation for a given time. The `_update` method updates the time of the clip, performs looping, and updates the snapshot if the time has changed. The `play`, `stop`, `pause`, `resume`, and `reset` methods are used to control the playback of the clip.\n\nOverall, the `AnimClip` class is an important part of the PlayCanvas engine project as it provides a way to manage the running state of an animation track. It is used by other parts of the engine that require animation functionality, such as the animation system and the entity component system. Developers can use the engine's animation system to create and manage animations for their games and applications. For example, they can create an animation clip for a character's walk cycle and use the `AnimClip` class to manage the playback of the animation.\n## Questions: \n 1. What is the purpose of the `AnimClip` class?\n- The `AnimClip` class wraps the running state of an animation track, contains and updates the animation 'cursor', and performs looping logic.\n\n2. What are the properties that can be set and retrieved using the `set` and `get` methods?\n- The properties that can be set and retrieved using the `set` and `get` methods are `name`, `track`, `time`, `speed`, `loop`, `blendWeight`, `blendOrder`, and `eventCursor`.\n\n3. What is the purpose of the `activeEventsForFrame` method?\n- The `activeEventsForFrame` method checks whether the next event occurs during the current frame and fires the event if it does. It also checks for events that occur during the clipped duration of the current frame if the frame overlaps with the end of the track.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-clip.md"}}],["249",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-curve.js)\n\nThe `AnimCurve` class is a part of the PlayCanvas engine project and is used to define an animation curve. An animation curve links an input data set to an output data set and defines the interpolation method to use. The purpose of this class is to provide a way to create and manipulate animation curves in the PlayCanvas engine.\n\nThe `AnimCurve` class has a constructor that takes four parameters: `paths`, `input`, `output`, and `interpolation`. The `paths` parameter is an array of path strings identifying the targets of this curve. The `input` parameter is the index of the curve which specifies the key data. The `output` parameter is the index of the curve which specifies the value data. The `interpolation` parameter is the interpolation method to use, which can be one of the following: `INTERPOLATION_STEP`, `INTERPOLATION_LINEAR`, or `INTERPOLATION_CUBIC`.\n\nThe `AnimCurve` class has four getter methods: `paths`, `input`, `output`, and `interpolation`. The `paths` getter method returns the list of paths which identify targets of this curve. The `input` getter method returns the index of the `AnimTrack` input which contains the key data for this curve. The `output` getter method returns the index of the `AnimTrack` input which contains the value data for this curve. The `interpolation` getter method returns the interpolation method used by this curve.\n\nHere is an example of how to create an `AnimCurve` object:\n\n```\nconst paths = [\"rootNode.translation\"];\nconst input = 0;\nconst output = 1;\nconst interpolation = INTERPOLATION_LINEAR;\n\nconst animCurve = new AnimCurve(paths, input, output, interpolation);\n```\n\nIn this example, we create a new `AnimCurve` object with a single path, an input index of 0, an output index of 1, and a linear interpolation method. This `AnimCurve` object can then be used to define an animation in the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of the `AnimCurve` class?\n    \n    The `AnimCurve` class is used to link an input data set to an output data set and define the interpolation method to use for animation.\n\n2. What parameters are required to create a new instance of the `AnimCurve` class?\n    \n    To create a new instance of the `AnimCurve` class, you need to provide an array of path strings identifying the targets of the curve, the index of the curve which specifies the key data, the index of the curve which specifies the value data, and the interpolation method to use.\n\n3. What are the available interpolation methods that can be used with `AnimCurve`?\n    \n    The available interpolation methods that can be used with `AnimCurve` are `INTERPOLATION_STEP`, `INTERPOLATION_LINEAR`, and `INTERPOLATION_CUBIC`.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-curve.md"}}],["250",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-data.js)\n\nThe code defines a class called `AnimData` that represents a set of data used in animation. The purpose of this class is to provide a convenient way to store and manipulate animation data in the PlayCanvas engine. \n\nThe constructor of the `AnimData` class takes two parameters: `components` and `data`. The `components` parameter specifies how many components make up an element of data. For example, if `components` is set to 3, then the data represents a set of 3-dimensional vectors. The `data` parameter is an array of numbers that represents the actual animation data. The length of the `data` array must be a multiple of `components`.\n\nThe `AnimData` class has two getter methods: `components` and `data`. The `components` getter returns the number of components that make up an element of data. The `data` getter returns the actual animation data as either a `Float32Array` or a regular array of numbers.\n\nThis class can be used in the PlayCanvas engine to store and manipulate animation data. For example, it can be used to store keyframe data for skeletal animations or to store vertex data for morph target animations. Here is an example of how the `AnimData` class can be used to store keyframe data for a skeletal animation:\n\n```\nconst keyframeData = new AnimData(7, [\n    0, 0, 0, 0, 0, 0, 0, // position and rotation of bone 1 at time 0\n    1, 1, 1, 0, 0, 0, 0, // position and rotation of bone 1 at time 1\n    2, 2, 2, 0, 0, 0, 0, // position and rotation of bone 1 at time 2\n    0, 0, 0, 0, 0, 0, 0, // position and rotation of bone 2 at time 0\n    1, 1, 1, 0, 0, 0, 0, // position and rotation of bone 2 at time 1\n    2, 2, 2, 0, 0, 0, 0, // position and rotation of bone 2 at time 2\n]);\n\n// Get the position of bone 1 at time 1\nconst bone1Pos = keyframeData.data.subarray(7, 10);\n``` \n\nIn this example, the `keyframeData` object represents the position and rotation of two bones at three different times. The `components` parameter is set to 7 because each element of data consists of a 3-dimensional position vector and a 4-dimensional rotation quaternion. The `data` parameter is an array of numbers that represents the actual keyframe data. The `subarray` method is used to extract the position of bone 1 at time 1.\n## Questions: \n 1. What is the purpose of the `AnimData` class?\n- The `AnimData` class is used to wrap a set of data used in animation.\n\n2. What parameters are required to create a new instance of `AnimData`?\n- Two parameters are required to create a new instance of `AnimData`: `components`, which specifies how many components make up an element of data, and `data`, which is the set of data.\n\n3. What types of data can be returned by the `data` getter?\n- The `data` getter can return either a `Float32Array` or an array of numbers (`number[]`).","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-data.md"}}],["251",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-evaluator.js)\n\nThe `AnimEvaluator` class is responsible for blending multiple sets of animation clips together. It is a part of the PlayCanvas engine project. The purpose of this class is to evaluate and blend animation clips and set the results on the animation targets. \n\nThe `AnimEvaluator` class has a constructor that takes an instance of `AnimBinder` as a parameter. The `AnimBinder` interface resolves curve paths to instances of `AnimTarget`. The class has a list of animation clips, inputs, outputs, and targets. The `addClip` method adds a clip to the evaluator. It stores a list of input/output arrays, curves, and snapshots. It creates a new target if it doesn't exist yet. It also sets the mask for the animation component. The `removeClip` method removes a clip from the evaluator. The `removeClips` method removes all clips from the evaluator. The `updateClipTrack` method updates the clip track. The `findClip` method finds the first clip that matches the given name. The `rebind` method rebinds the evaluator. The `assignMask` method assigns a mask. The `update` method updates the evaluator. It updates the clip, blends the results, and sets the results on the animation targets. \n\nThe `AnimEvaluator` class is used in the PlayCanvas engine project to blend multiple sets of animation clips together. It is used to evaluate and blend animation clips and set the results on the animation targets. It is a part of the animation system in the PlayCanvas engine project. \n\nExample usage:\n\n```javascript\nconst animEvaluator = new AnimEvaluator(animBinder);\nanimEvaluator.addClip(animClip);\nanimEvaluator.update(deltaTime);\n```\n## Questions: \n 1. What is the purpose of the `AnimEvaluator` class?\n- The `AnimEvaluator` class blends multiple sets of animation clips together.\n\n2. What is the role of the `binder` parameter in the `AnimEvaluator` constructor?\n- The `binder` parameter is an interface that resolves curve paths to instances of `AnimTarget`.\n\n3. What is the purpose of the `update` method in the `AnimEvaluator` class?\n- The `update` method is the evaluator frame update function that evaluates, blends, and sets the results of the update to the `AnimTarget`.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-evaluator.md"}}],["252",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-events.js)\n\nThe code defines a class called `AnimEvents` which is used to store a sorted array of animation events that should be fired sequentially during the playback of an `pc.AnimTrack`. The purpose of this class is to provide a way to define and manage animation events that occur during the playback of an animation track. \n\nThe `AnimEvents` class has a constructor that takes an array of objects representing animation events. Each event object has a `name` property which is a string representing the name of the event, a `time` property which is a number representing the time in seconds at which the event should occur, and any additional properties that are optional and will be available in the `EventHandler` callback's event object. \n\nThe constructor sorts the events array in ascending order based on the `time` property of each event object. This ensures that the events are fired in the correct order during the playback of the animation track. \n\nThe `AnimEvents` class has a getter method called `events` which returns the sorted array of animation events. This allows other parts of the code to access the events array without being able to modify it directly. \n\nThis class can be used in the larger PlayCanvas engine project to define and manage animation events for various types of animations. For example, it could be used to define events that occur during the playback of a character's walk cycle animation, such as footstep sounds or dust particles being kicked up. \n\nHere is an example of how the `AnimEvents` class could be used in the PlayCanvas engine project:\n\n```\nconst events = new pc.AnimEvents([\n    {\n        name: 'footstep',\n        time: 0.5,\n        sound: 'footstep.wav',\n        particles: 'dust.png'\n    },\n    {\n        name: 'footstep',\n        time: 1.5,\n        sound: 'footstep.wav',\n        particles: 'dust.png'\n    }\n]);\n\nconst animTrack = new pc.AnimTrack();\nanimTrack.events = events;\n```\n\nIn this example, an `AnimEvents` instance is created with two events representing footstep sounds and dust particles being kicked up during a character's walk cycle animation. The `AnimEvents` instance is then assigned to an `AnimTrack` instance which is used to play the animation. During the playback of the animation, the events will be fired at the specified times, triggering the footstep sounds and particle effects.\n## Questions: \n 1. What is the purpose of the AnimEvents class?\n- The AnimEvents class stores a sorted array of animation events that should fire sequentially during the playback of an AnimTrack.\n\n2. What parameters does the constructor of the AnimEvents class take?\n- The constructor takes an array of objects representing animation events.\n\n3. What is the format of an animation event object and what properties can it have?\n- An animation event object has a name and time property (given in seconds), and can have additional optional properties that will be available in the EventHandler callback's event object.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-events.md"}}],["253",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-snapshot.js)\n\nThe code defines a class called `AnimSnapshot` which is used to store the state of an animation track at a particular time. This class is not meant to be used directly by developers, hence the `@ignore` tag in the JSDoc comment. \n\nThe constructor of the `AnimSnapshot` class takes an instance of the `AnimTrack` class as an argument. It initializes several properties of the `AnimSnapshot` instance, including the name of the snapshot, the time at which the snapshot was taken, and two arrays for caching input and output values of the animation curves. \n\nThe `AnimSnapshot` class pre-allocates input caches and storage for evaluation results. It loops through the input values of the `AnimTrack` instance and creates a new `AnimCache` instance for each input. It also loops through the curves and outputs of the `AnimTrack` instance and creates a new array to store the evaluation results for each curve. \n\nThe purpose of the `AnimSnapshot` class is to provide a way to efficiently store and retrieve the state of an animation track at a particular time. This is useful for implementing features such as animation blending and state machines, where multiple animations need to be combined or switched based on certain conditions. \n\nHere is an example of how the `AnimSnapshot` class might be used in the larger PlayCanvas engine project:\n\n```javascript\nconst animTrack = new AnimTrack('myAnimation');\nconst animSnapshot = new AnimSnapshot(animTrack);\n\n// update the animation track to a certain time\nanimTrack.update(0.5);\n\n// take a snapshot of the animation track at the current time\nconst snapshotTime = animTrack.getTime();\nconst snapshot = new AnimSnapshot(animTrack);\n\n// blend two snapshots together\nconst blendFactor = 0.5;\nconst blendedSnapshot = new AnimSnapshot(animTrack);\nfor (let i = 0; i < animTrack._curves.length; ++i) {\n    const curve = animTrack._curves[i];\n    const input = animTrack._inputs[curve._input];\n    const output = animTrack._outputs[curve._output];\n    const result = blendedSnapshot._results[i];\n    for (let j = 0; j < output._components; ++j) {\n        const valueA = snapshot._cache[curve._input].get(input[snapshotTime]);\n        const valueB = animSnapshot._cache[curve._input].get(input[animTrack._time]);\n        result[j] = valueA * (1 - blendFactor) + valueB * blendFactor;\n    }\n}\n``` \n\nIn this example, we create an `AnimTrack` instance for an animation called \"myAnimation\". We then create an `AnimSnapshot` instance for the animation track and update it to a certain time. We take a snapshot of the animation track at the current time and blend it with another snapshot using a blend factor. The resulting blended snapshot can then be used to update the animation track and play the blended animation.\n## Questions: \n 1. What is the purpose of the `AnimSnapshot` class?\n    \n    The `AnimSnapshot` class stores the state of an animation track at a particular time.\n\n2. What is the `animTrack` parameter in the constructor used for?\n    \n    The `animTrack` parameter is used as the source track for the animation snapshot.\n\n3. What is the purpose of the `AnimCache` class imported from `anim-cache.js`?\n    \n    The `AnimCache` class is used to store per-curve input cache for the animation snapshot.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-snapshot.md"}}],["254",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-target-value.js)\n\nThe code defines a class called `AnimTargetValue` that is used to store and update the value of an animation target. This class is used in the PlayCanvas engine project to combine the values of multiple layer targets into a single value. \n\nThe `AnimTargetValue` class has two static properties called `TYPE_QUAT` and `TYPE_VEC3` that define the type of value stored, either quaternion or vector3. It also has several static properties that are used to store and manipulate quaternion and vector3 values. \n\nThe constructor of the `AnimTargetValue` class takes two parameters: `component` and `type`. The `component` parameter is an instance of the `AnimComponent` class that this target value is associated with, while the `type` parameter is the type of value stored, either quaternion or vector3. \n\nThe `AnimTargetValue` class has several methods that are used to update the value of the target. The `getWeight` method is used to get the weight of a layer at a given index. The `setMask` method is used to set the mask of a layer at a given index. The `updateWeights` method is used to update the weights of all layers. The `updateValue` method is used to update the value of the target at a given index. The `unbind` method is used to unbind the target value. \n\nThe `updateValue` method is the most complex method in the `AnimTargetValue` class. It takes two parameters: `index` and `value`. The `index` parameter is the index of the layer to update, while the `value` parameter is the value to update the target with. The method first checks if the layer at the given index is masked or if its weight is zero. If either of these conditions is true, the method returns without updating the target value. If the layer blend type is additive and the weights are not normalized, the method calculates the additive value and scales it by its weight. If the layer blend type is not additive or the weights are normalized, the method blends the value with the target value using the `AnimBlend` class. Finally, if a setter function is defined, the method calls it with the updated target value. \n\nOverall, the `AnimTargetValue` class is an important part of the PlayCanvas engine project that is used to store and update the value of an animation target. It is used to combine the values of multiple layer targets into a single value, and it provides methods for updating the target value based on the weights and blend types of the layers.\n## Questions: \n 1. What is the purpose of the `AnimTargetValue` class?\n- The `AnimTargetValue` class is used to store and update the value of an animation target by combining the values of multiple layer targets into a single value.\n\n2. What types of values can be stored in an `AnimTargetValue` instance?\n- An `AnimTargetValue` instance can store values of either quaternion or vector3 type.\n\n3. What is the significance of the `dirty` property in an `AnimTargetValue` instance?\n- The `dirty` property is used to indicate whether the weights of the layer targets have been updated since the last time the value was updated. If `dirty` is `true`, the weights need to be updated before the value can be updated.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-target-value.md"}}],["255",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-target.js)\n\nThe code defines two classes, `AnimTarget` and `AnimSetter`, that are used in the PlayCanvas engine for animation. \n\nThe `AnimSetter` class is a callback function that is used by the `AnimEvaluator` class to set final animation values. The `AnimSetter` function takes an array of numbers as input, which represents the updated animation value. This function is stored in `AnimTarget` instances, which are constructed by an `AnimBinder`. \n\nThe `AnimTarget` class stores the information required by the `AnimEvaluator` for updating a target value. It takes four parameters: `func`, `type`, `components`, and `targetPath`. `func` is the `AnimSetter` function that will be called when a new animation value is output by the `AnimEvaluator`. `type` is the type of animation data that this target expects, which can be either `'vector'` or `'quaternion'`. `components` is the number of components on this target, which should ideally match the number of components found on all attached animation curves. `targetPath` is the path to the target value. \n\nThe `AnimTarget` class has several getter methods that return the values of its properties. The `set` and `get` methods return the `set` and `get` functions of the `AnimSetter` instance, respectively. The `type` method returns the type of animation data that this target expects. The `components` method returns the number of components on this target. The `targetPath` method returns the path to the target value. The `isTransform` method returns a boolean value that indicates whether the target value is a transform. \n\nOverall, the `AnimTarget` and `AnimSetter` classes are used in the PlayCanvas engine for animation. The `AnimSetter` class is a callback function that is used to set final animation values, while the `AnimTarget` class stores the information required by the `AnimEvaluator` for updating a target value. These classes are used in conjunction with other classes in the PlayCanvas engine to create and manipulate animations. \n\nExample usage of the `AnimTarget` class:\n\n```\nconst animTarget = new AnimTarget(\n  (value) => {\n    console.log(value);\n  },\n  'vector',\n  3,\n  'position'\n);\n\nconsole.log(animTarget.type); // 'vector'\nconsole.log(animTarget.components); // 3\nconsole.log(animTarget.targetPath); // 'position'\nconsole.log(animTarget.isTransform); // false\n```\n## Questions: \n 1. What is the purpose of the `AnimSetter` callback function?\n- The `AnimSetter` callback function is used by the `AnimEvaluator` to set final animation values.\n\n2. What is the `AnimTarget` class used for?\n- The `AnimTarget` class stores information required by the `AnimEvaluator` for updating a target value.\n\n3. What is the significance of the `_isTransform` property in the `AnimTarget` constructor?\n- The `_isTransform` property is used to determine if the target value is a transform (localRotation, localPosition, or localScale).","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-target.md"}}],["256",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/evaluator/anim-track.js)\n\nThe `AnimTrack` class is a component of the PlayCanvas engine that stores the curve data required to animate a set of target nodes. It can be linked to the nodes it should animate using the `AnimComponent#assignAnimation` method. \n\nThe class has a constructor that takes in a name, duration, inputs, outputs, curves, and animEvents. The name is a string that represents the name of the AnimTrack. The duration is a number that represents the duration of the track in seconds. The inputs and outputs are lists of curve key data and curve value data, respectively. The curves are a list of curves, and animEvents is a sequence of animation events. \n\nThe class has several getter methods that return the name, duration, inputs, outputs, and curves of the AnimTrack. It also has a setter method that sets the animation events that will fire during the playback of this anim track. \n\nThe `eval` method of the class evaluates all track curves at the specified time and stores the results in the provided snapshot. It takes in a time and a snapshot as parameters. The snapshot is an object that contains a cache and results. The cache is an array that stores the evaluated inputs, and the results are an array that stores the evaluated outputs. \n\nThe `AnimTrack` class can be used to create animations for objects in a 3D scene. For example, if you have a character in a game that needs to walk, you can create an AnimTrack that stores the curve data for the character's walking animation. You can then link the AnimTrack to the character using the `AnimComponent#assignAnimation` method. When the game is played, the AnimTrack will be used to animate the character's walking motion. \n\nOverall, the `AnimTrack` class is an essential component of the PlayCanvas engine that enables developers to create complex animations for objects in a 3D scene.\n## Questions: \n 1. What is the purpose of the `AnimEvents` class imported at the beginning of the file?\n- The `AnimEvents` class is used to store a sequence of animation events that will fire during the playback of an animation track.\n\n2. What is the significance of the `EMPTY` static property of the `AnimTrack` class?\n- The `EMPTY` property is a pre-defined instance of the `AnimTrack` class that can be used as a placeholder track when creating a state graph before having all associated animation data available.\n\n3. What is the purpose of the `eval` method in the `AnimTrack` class?\n- The `eval` method is used to evaluate all track curves at a specified time and store the results in a provided snapshot. It updates the cache and results arrays for the inputs and outputs of the curves, respectively.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/evaluator/anim-track.md"}}],["257",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/anim/state-graph/anim-state-graph.js)\n\nThe `AnimStateGraph` class is a resource asset that represents an animation state graph. It can be loaded into an animation component using the `loadStateGraph` method. \n\nThe purpose of this class is to provide a way to define complex animation states and transitions between them. It is designed to be used by game developers who want to create complex animations for their game characters or objects. \n\nThe `AnimStateGraph` class has two properties: `parameters` and `layers`. The `parameters` property is an object that contains the parameters used by the animation states and transitions. The `layers` property is an array of objects that represent the layers of the animation state graph. Each layer contains an array of states and transitions. \n\nThe `constructor` method of the `AnimStateGraph` class takes a JSON object as its parameter. The JSON object contains the data needed to create the animation state graph. The constructor then parses the JSON object and creates the `parameters` and `layers` properties. \n\nThe `AnimStateGraph` class can be used in the following way:\n\n```javascript\nconst animStateGraph = app.assets.get(ASSET_ID).resource;\nconst entity = new pc.Entity();\nentity.addComponent('anim');\nentity.anim.loadStateGraph(animStateGraph);\n```\n\nIn this example, the `animStateGraph` variable is an instance of the `AnimStateGraph` class. It is loaded from an asset using the `app.assets.get` method. The `entity` variable is a new entity that is created. An animation component is added to the entity using the `addComponent` method. Finally, the `loadStateGraph` method is called on the animation component, passing in the `animStateGraph` variable as its parameter. \n\nOverall, the `AnimStateGraph` class provides a way for game developers to define complex animation states and transitions for their game characters or objects. It is a key component of the PlayCanvas engine and is used extensively throughout the engine to provide advanced animation capabilities.\n## Questions: \n 1. What is the purpose of the `AnimStateGraph` class?\n- The `AnimStateGraph` class is an asset resource that represents an animation state graph and can be loaded into an animation component.\n\n2. How can a script retrieve an instance of `AnimStateGraph`?\n- A script can retrieve an instance of `AnimStateGraph` from assets of type 'animstategraph' using `app.assets.get(ASSET_ID).resource`.\n\n3. What properties can be accessed from an instance of `AnimStateGraph`?\n- An instance of `AnimStateGraph` has two properties that can be accessed: `parameters` and `layers`. `parameters` is an object containing the parameters of the animation state graph, while `layers` is an array of layers in the animation state graph.","metadata":{"source":".autodoc/docs/markdown/src/framework/anim/state-graph/anim-state-graph.md"}}],["258",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/app-options.js)\n\nThe code defines a class called `AppOptions` which is used to store various options and settings for the PlayCanvas engine. The purpose of this class is to provide a centralized location for storing and accessing these options throughout the engine.\n\nThe class contains a number of properties, each of which represents a different option or setting. These include input handlers for keyboard, mouse, touch, and gamepad, as well as prefixes for script and asset URLs, a list of scripts to load in order, a sound manager, a graphics device, a lightmapper, a batch manager, an XR manager, and arrays of component systems and resource handlers.\n\nEach property is annotated with a JSDoc comment that describes its purpose and type. For example, the `elementInput` property is described as an input handler for `ElementComponent`s, and its type is specified as `import('./input/element-input.js').ElementInput`. This provides useful information for developers who are working with the code and need to understand what each property does and how it should be used.\n\nOverall, the `AppOptions` class is an important part of the PlayCanvas engine, as it provides a way to configure and customize the engine's behavior. Developers can create an instance of this class and set its properties to the desired values before initializing the engine, allowing them to tailor the engine to their specific needs. For example, they can set the `scriptPrefix` property to load scripts from a different location, or add custom component systems to the `componentSystems` array to extend the engine's functionality.\n## Questions: \n 1. What is the purpose of the `AppOptions` class?\n    \n    The `AppOptions` class is used to store various options and handlers related to input, loading, sound, graphics, batching, XR, and resource handling for a PlayCanvas app.\n\n2. What is the `componentSystems` property used for?\n    \n    The `componentSystems` property is an array that stores the component systems required by the PlayCanvas app.\n\n3. What is the `lightmapper` property used for?\n    \n    The `lightmapper` property is used to store the lightmapper object that handles lightmapping for the PlayCanvas app.","metadata":{"source":".autodoc/docs/markdown/src/framework/app-options.md"}}],["259",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/application.js)\n\nThe code defines the `Application` class, which represents and manages a PlayCanvas application. It extends the `AppBase` class and provides additional functionality for creating and configuring an application instance. \n\nThe `Application` constructor takes a `canvas` element and an optional `options` object. The `options` object can be used to configure various input handlers, prefixes for script and asset URLs, and options for the graphics device. \n\nThe `createDevice` method creates a new `WebglGraphicsDevice` instance with the specified canvas and graphics device options. If the `xr` property is available on the `navigator` object, the `xrCompatible` option is set to `true`. \n\nThe `addComponentSystems` method adds various component systems to the `appOptions` object, which is used to configure the application instance. These component systems include those for handling rigid bodies, collisions, joints, animations, models, rendering, cameras, lights, scripts, audio sources, audio listeners, particles, screens, elements, buttons, scroll views, scrollbars, sprites, layout groups, layout children, and zones. \n\nThe `addResourceHandles` method adds various resource handlers to the `appOptions` object. These resource handlers include those for handling rendering, animations, animation clips, animation state graphs, models, materials, textures, text, JSON, audio, scripts, scenes, cubemaps, HTML, CSS, shaders, hierarchies, folders, fonts, binary data, texture atlases, sprites, templates, and containers. \n\nFinally, the `init` method is called with the `appOptions` object to initialize the application instance. \n\nOverall, this code provides a high-level interface for creating and configuring a PlayCanvas application instance. It abstracts away many of the low-level details of setting up input handlers, graphics devices, component systems, and resource handlers, allowing developers to focus on building their applications. \n\nExample usage:\n\n```javascript\n// create a new PlayCanvas application instance\nconst canvas = document.getElementById('application-canvas');\nconst app = new pc.Application(canvas, {\n    keyboard: new pc.Keyboard(window),\n    mouse: new pc.Mouse(canvas),\n    touch: new pc.TouchDevice(canvas),\n    scriptPrefix: '/scripts/',\n    assetPrefix: '/assets/',\n    graphicsDeviceOptions: {\n        antialias: true,\n        alpha: false\n    }\n});\n\n// start the application's main loop\napp.start();\n```\n## Questions: \n 1. What is the purpose of the `Application` class?\n- The `Application` class represents and manages a PlayCanvas application, and can be accessed in scripts to perform various tasks such as initializing and updating.\n\n2. What input handlers can be passed as options to the `Application` constructor?\n- The `Application` constructor can be passed options for `ElementInput`, `Keyboard`, `Mouse`, `TouchDevice`, and `GamePads` input handlers.\n\n3. What are some of the component systems and resource handlers that are added to the `Application` instance?\n- Some of the component systems added to the `Application` instance include `RigidBodyComponentSystem`, `CollisionComponentSystem`, `AnimationComponentSystem`, `ModelComponentSystem`, and `ZoneComponentSystem`. Some of the resource handlers added include `RenderHandler`, `AnimationHandler`, `MaterialHandler`, `TextureHandler`, and `ScriptHandler`.","metadata":{"source":".autodoc/docs/markdown/src/framework/application.md"}}],["260",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/asset/asset-file.js)\n\nThe code defines a class called `AssetFile` that is used to wrap a source of asset data. The class has a constructor that takes in six parameters: `url`, `filename`, `hash`, `size`, `opt`, and `contents`. These parameters are used to initialize the properties of the class with the same names. \n\nThe `equals` method is also defined in the class, which takes in another `AssetFile` object as a parameter and compares its properties with the current object. If all the properties are the same, the method returns `true`, otherwise, it returns `false`.\n\nThis class is likely used in the larger PlayCanvas engine project to represent asset files that are loaded from external sources, such as images, audio files, or 3D models. The `AssetFile` class provides a convenient way to store and compare information about these files, which can be useful when managing and loading assets in the engine.\n\nFor example, the `AssetFile` class could be used in the following code snippet to load an image asset:\n\n```\nconst url = 'https://example.com/image.png';\nconst filename = 'image.png';\nconst hash = 'abc123';\nconst size = 1024;\nconst opt = { compression: 'none' };\nconst contents = null;\n\nconst assetFile = new AssetFile(url, filename, hash, size, opt, contents);\n\n// Load the image asset using the asset file\nengine.loadAsset(assetFile);\n```\n\nOverall, the `AssetFile` class provides a simple and flexible way to represent asset files in the PlayCanvas engine, which can be used to manage and load assets in a more efficient and organized way.\n## Questions: \n 1. What is the purpose of the AssetFile class?\n- The AssetFile class is a wrapper for a source of asset data.\n\n2. What parameters does the AssetFile constructor take?\n- The AssetFile constructor takes six parameters: url (string), filename (string), hash (null or string), size (null or number), opt (null or object), and contents (null or string).\n\n3. What does the equals() method do?\n- The equals() method compares the data of two AssetFile instances and returns true if they have the same data, and false otherwise.","metadata":{"source":".autodoc/docs/markdown/src/framework/asset/asset-file.md"}}],["261",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/asset/asset-list-loader.js)\n\nThe `AssetListLoader` class is used to load a group of assets and fire a callback when all assets are loaded. It is a subclass of `EventHandler` and is used to manage a list of assets that need to be loaded. \n\nThe constructor takes two parameters: an array of `Asset` objects or an array of asset IDs to load, and an `AssetRegistry` object that is used to load and manage the assets. The `AssetListLoader` object creates a new `Set` for each of the following: `_assets`, `_loadingAssets`, and `_waitingAssets`. The `_assets` set contains all the assets that need to be loaded, the `_loadingAssets` set contains the assets that are currently being loaded, and the `_waitingAssets` set contains the assets that are waiting to be loaded. \n\nThe `load` method starts loading the asset list and calls the `done` callback when all assets have loaded or failed to load. The `ready` method sets a callback that will be called when all assets in the list have been loaded. \n\nThe `_onLoad` method is called when an asset is loaded. It checks if the asset is one that the `AssetListLoader` object cares about, and if so, it removes the asset from the `_loadingAssets` set and fires a `progress` event. If all assets have been loaded, it calls the `_loadingComplete` method. \n\nThe `_onError` method is called when an asset fails to load. It checks if the asset is one that the `AssetListLoader` object cares about, and if so, it adds the asset to the `_failed` array, removes the asset from the `_loadingAssets` set, and fires an `error` event. If all assets have been loaded, it calls the `_loadingComplete` method. \n\nThe `_loadingComplete` method is called when all assets have been loaded or failed to load. It sets the `_loaded` flag to true, removes the event listeners for `load` and `error`, and fires either a `load` or `error` event depending on whether any assets failed to load. \n\nThe `destroy` method removes all references to the `AssetListLoader` object. It removes any outstanding listeners, clears the `_waitingAssets` set, and removes the `progress` and `load` event listeners. \n\nOverall, the `AssetListLoader` class is an important part of the PlayCanvas engine project as it allows developers to load a group of assets and fire a callback when all assets are loaded. This is useful for loading complex scenes that require multiple assets to be loaded before they can be displayed.\n## Questions: \n 1. What is the purpose of the `AssetListLoader` class?\n- The `AssetListLoader` class is used to load a group of assets and fires a callback when all assets are loaded.\n\n2. What parameters does the `AssetListLoader` constructor take?\n- The `AssetListLoader` constructor takes an array of `Asset` objects or an array of asset IDs to load, and the application's asset registry.\n\n3. What happens if an asset fails to load?\n- If an asset fails to load, it is added to the `_failed` list and the `load` function's callback is called with an error message and the list of failed assets.","metadata":{"source":".autodoc/docs/markdown/src/framework/asset/asset-list-loader.md"}}],["262",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/asset/asset-localized.js)\n\nThe `LocalizedAsset` class is a subclass of the `EventHandler` class and is used to manage localized assets in the PlayCanvas engine. It provides functionality to load and manage localized assets based on the current locale set in the `app.i18n` object. \n\nWhen an instance of the `LocalizedAsset` class is created, it takes an instance of the `app` object as a parameter. The `app` object is an instance of the `Application` class and is used to manage the PlayCanvas application. The `LocalizedAsset` class listens for the `set:locale` event on the `app.i18n` object and calls the `_onSetLocale` method when the event is fired.\n\nThe `LocalizedAsset` class has several properties that can be set or retrieved using getter and setter methods. These properties include `defaultAsset`, `localizedAsset`, `autoLoad`, and `disableLocalization`. \n\nThe `defaultAsset` property is used to set the default asset that will be used if a localized asset is not available for the current locale. The `localizedAsset` property is used to set the localized asset that will be used if available for the current locale. The `autoLoad` property is used to automatically load the localized asset when it is set. The `disableLocalization` property is used to disable localization and always use the default asset.\n\nThe `LocalizedAsset` class also has several private methods that are used to bind and unbind events to assets and handle events related to assets. These methods include `_bindDefaultAsset`, `_unbindDefaultAsset`, `_onDefaultAssetAdd`, `_onDefaultAssetRemove`, `_bindLocalizedAsset`, `_unbindLocalizedAsset`, `_onLocalizedAssetAdd`, `_onLocalizedAssetLoad`, `_onLocalizedAssetChange`, `_onLocalizedAssetRemove`, `_onLocaleAdd`, `_onLocaleRemove`, and `_onSetLocale`.\n\nThe `LocalizedAsset` class can be used in the larger PlayCanvas project to manage localized assets and provide localized content to users based on their locale. For example, a game developer could use the `LocalizedAsset` class to provide different audio files for different languages or different textures for different regions. The `LocalizedAsset` class provides a simple and efficient way to manage localized assets in the PlayCanvas engine. \n\nExample usage:\n\n```javascript\nconst app = new pc.Application();\nconst localizedAsset = new pc.LocalizedAsset(app);\n\n// set the default asset\nlocalizedAsset.defaultAsset = 'myAudioFile.mp3';\n\n// set the localized asset for French\nlocalizedAsset.localizedAsset = 'myAudioFile_fr.mp3';\n\n// automatically load the localized asset\nlocalizedAsset.autoLoad = true;\n\n// disable localization and always use the default asset\nlocalizedAsset.disableLocalization = true;\n```\n## Questions: \n 1. What is the purpose of this code and how does it fit into the PlayCanvas engine?\n- This code defines a class called `LocalizedAsset` that extends `EventHandler` and is used to manage localized assets in the PlayCanvas engine.\n2. What properties and methods are available on the `LocalizedAsset` class?\n- The `LocalizedAsset` class has properties for `defaultAsset`, `localizedAsset`, `autoLoad`, and `disableLocalization`, as well as methods for binding and unbinding assets and handling events related to asset loading and localization.\n3. How does the `LocalizedAsset` class determine which asset to use for a given locale?\n- The `LocalizedAsset` class checks the `defaultAsset` for a localized version of the asset corresponding to the current locale, and if one exists, it uses that asset. If no localized version exists, it falls back to the `defaultAsset`.","metadata":{"source":".autodoc/docs/markdown/src/framework/asset/asset-localized.md"}}],["263",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/asset/asset-reference.js)\n\nThe `AssetReference` class is responsible for managing the case where an object holds a reference to an asset and needs to be notified when changes occur in the asset. This class is part of the PlayCanvas engine project and is used to enable updating of assets in the engine.\n\nThe constructor of the `AssetReference` class takes in several parameters, including the name of the property that the asset is stored under, the parent object that contains the asset reference, the asset registry that stores all assets, and a set of functions called when the asset state changes. These functions include load, add, remove, and unload. The scope to call the callbacks in can also be passed in as a parameter.\n\nThe `AssetReference` class has two methods, `set id` and `set url`, which are used to initialize an asset reference. One of either id or url must be set to initialize an asset reference. The `set id` method sets the asset id which this references, while the `set url` method sets the asset url which this references.\n\nThe `AssetReference` class also has several private methods, including `_bind`, `_unbind`, `_onLoad`, `_onAdd`, `_onRemove`, and `_onUnload`. These methods are used to bind and unbind events to the asset registry, and to handle the load, add, remove, and unload events.\n\nOverall, the `AssetReference` class is an important part of the PlayCanvas engine project, as it enables updating of assets in the engine. Developers can use this class to manage asset references and to be notified when changes occur in the asset. Below is an example of how to use the `AssetReference` class:\n\n```\nvar reference = new pc.AssetReference('textureAsset', this, this.app.assets, {\n    load: this.onTextureAssetLoad,\n    add: this.onTextureAssetAdd,\n    remove: this.onTextureAssetRemove\n}, this);\nreference.id = this.textureAsset.id;\n```\n## Questions: \n 1. What is the purpose of the `AssetReference` class?\n- The `AssetReference` class manages the case where an object holds a reference to an asset and needs to be notified when changes occur in the asset, such as load, add, and remove events.\n\n2. What parameters are required to create a new instance of the `AssetReference` class?\n- To create a new instance of the `AssetReference` class, the following parameters are required: `propertyName` (string), `parent` (an `Asset` or an object), `registry` (an `AssetRegistry`), and `callbacks` (an object containing functions called when the asset state changes).\n\n3. What are the differences between the `id` and `url` properties of an `AssetReference` instance?\n- The `id` and `url` properties of an `AssetReference` instance are used to initialize an asset reference. One of either `id` or `url` must be set. `id` is a number that represents the asset's unique identifier, while `url` is a string that represents the asset's URL.","metadata":{"source":".autodoc/docs/markdown/src/framework/asset/asset-reference.md"}}],["264",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/asset/constants.js)\n\nThis file contains a set of constants and a regular expression that are used throughout the PlayCanvas engine project to define and validate various types of assets and URLs.\n\nThe `ABSOLUTE_URL` constant is a regular expression that matches absolute URLs, including protocol-relative URLs, data URLs, and blob URLs. This regular expression is used to validate URLs throughout the PlayCanvas engine, ensuring that only valid URLs are used to load assets and resources.\n\nThe remaining constants define the various types of assets that can be loaded and used in the PlayCanvas engine. These include animations, audio files, images, JSON data, models, materials, text files, textures, texture atlases, cubemaps, shaders, CSS files, HTML files, scripts, and containers. These constants are used throughout the PlayCanvas engine to identify and load the appropriate assets for a given scene or project.\n\nFor example, to load an image asset in PlayCanvas, you would use the `ASSET_IMAGE` constant to identify the asset type, like this:\n\n```\nconst asset = app.assets.find('my-image.png', ASSET_IMAGE);\n```\n\nOverall, this file provides a set of useful constants and regular expressions that are used throughout the PlayCanvas engine to ensure that assets and resources are loaded correctly and efficiently.\n## Questions: \n 1. What is the purpose of the `ABSOLUTE_URL` regular expression?\n- The `ABSOLUTE_URL` regular expression is used to match absolute URLs, including data URLs and blob URLs.\n\n2. What are the different asset types defined in this code?\n- The code defines asset types for animation, audio, image, JSON, model, material, text, texture, texture atlas, cubemap, shader, CSS, HTML, script, and container.\n\n3. What is the format of the documentation comments for the asset type constants?\n- The documentation comments for the asset type constants use JSDoc format, with a `@type` tag followed by a string indicating the name of the asset type.","metadata":{"source":".autodoc/docs/markdown/src/framework/asset/constants.md"}}],["265",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/bundle/bundle-registry.js)\n\n# BundleRegistry Class\n\nThe `BundleRegistry` class is responsible for keeping track of which assets are in bundles and loading files from bundles. This class is used internally by the PlayCanvas engine and is not intended to be used directly by developers.\n\n## Constructor\n\nThe constructor takes an instance of the `AssetRegistry` class as its only argument. It initializes several internal indexes and registers event listeners for when assets are added or removed from the registry.\n\n## Methods\n\n### _onAssetAdded(asset)\n\nThis method is called when an asset is added to the registry. If the asset is a bundle, it is added to the `_bundleAssets` index and its referenced assets are indexed in the `_assetsInBundles` index. If the asset is not a bundle, its URLs are indexed in the `_urlsInBundles` index.\n\n### _registerBundleEventListeners(bundleAssetId)\n\nThis method registers event listeners for when a bundle asset is loaded or fails to load.\n\n### _unregisterBundleEventListeners(bundleAssetId)\n\nThis method unregisters event listeners for a bundle asset.\n\n### _indexAssetInBundle(assetId, bundleAsset)\n\nThis method indexes an asset and its file URLs in the `_assetsInBundles` and `_urlsInBundles` indexes, respectively.\n\n### _indexAssetFileUrls(asset)\n\nThis method indexes the file URLs of an asset in the `_urlsInBundles` index.\n\n### _getAssetFileUrls(asset)\n\nThis method returns an array of all possible URLs for an asset.\n\n### _normalizeUrl(url)\n\nThis method removes query parameters from a URL.\n\n### _onAssetRemoved(asset)\n\nThis method is called when an asset is removed from the registry. If the asset is a bundle, it is removed from the `_bundleAssets` index and its referenced assets are removed from the `_assetsInBundles` and `_urlsInBundles` indexes. If the asset is not a bundle, its URLs are removed from the `_urlsInBundles` index.\n\n### _onBundleLoaded(bundleAsset)\n\nThis method is called when a bundle asset is loaded. It resolves any pending file requests that can be satisfied by the bundle.\n\n### _onBundleError(err, bundleAsset)\n\nThis method is called when a bundle asset fails to load. It searches for other bundles that can satisfy any pending file requests for the URLs in the failed bundle. If no other bundles are found, the file requests are failed with the specified error.\n\n### _findLoadedOrLoadingBundleForUrl(url)\n\nThis method finds a bundle that contains the specified URL and is either loaded or currently being loaded.\n\n### listBundlesForAsset(asset)\n\nThis method lists all available bundles that reference the specified asset.\n\n### list()\n\nThis method lists all available bundles, including those that are not loaded.\n\n### hasUrl(url)\n\nThis method returns true if there is a bundle that contains the specified URL.\n\n### canLoadUrl(url)\n\nThis method returns true if there is a bundle that contains the specified URL and that bundle is either loaded or currently being loaded.\n\n### loadUrl(url, callback)\n\nThis method loads the specified file URL from a bundle that is either loaded or currently being loaded. The callback is called when the file has been loaded or if an error occurs.\n\n### destroy()\n\nThis method destroys the registry and releases its resources.\n## Questions: \n 1. What is the purpose of the `BundleRegistry` class?\n    \n    The `BundleRegistry` class keeps track of which assets are in bundles and loads files from bundles.\n\n2. What methods are available in the `BundleRegistry` class?\n    \n    The `BundleRegistry` class has methods such as `listBundlesForAsset`, `list`, `hasUrl`, `canLoadUrl`, `loadUrl`, and `destroy`.\n\n3. What is the role of the `_fileRequests` property in the `BundleRegistry` class?\n    \n    The `_fileRequests` property contains requests to load file URLs indexed by URL. If there are any pending file requests that can be satisfied by a specified bundle, then they are resolved.","metadata":{"source":".autodoc/docs/markdown/src/framework/bundle/bundle-registry.md"}}],["266",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/bundle/bundle.js)\n\nThe `Bundle` class represents the resource of a Bundle Asset in the PlayCanvas engine. A Bundle Asset is a collection of files that are loaded together as a single unit. The purpose of this class is to map the URLs of the files in the bundle to their corresponding blob URLs. Blob URLs are used to load binary data as a URL that can be used as the source of an image, audio, or video element.\n\nThe `constructor` method of the `Bundle` class takes an array of objects as its argument. Each object in the array represents a file in the bundle and has a `name` field and a `getBlobUrl()` function. The `name` field is used as the key in the `_blobUrls` object to map the file URL to its blob URL. The `getBlobUrl()` function is called to get the blob URL for the file.\n\nThe `hasBlobUrl()` method takes a file URL as its argument and returns `true` if the blob URL for the file exists in the `_blobUrls` object, otherwise it returns `false`.\n\nThe `getBlobUrl()` method takes a file URL as its argument and returns the blob URL for the file if it exists in the `_blobUrls` object.\n\nThe `destroy()` method is used to free up the blob URLs when the bundle is no longer needed. It iterates over the `_blobUrls` object and calls `URL.revokeObjectURL()` for each blob URL to free up memory.\n\nThis class is used internally by the PlayCanvas engine to manage the loading and unloading of Bundle Assets. Developers using the engine can use this class to get the blob URL for a file in a Bundle Asset. For example:\n\n```javascript\nconst bundle = new Bundle(files);\nconst blobUrl = bundle.getBlobUrl('file.png');\nconst image = new Image();\nimage.src = blobUrl;\n```\n## Questions: \n 1. What is the purpose of the `Bundle` class and how is it used in the PlayCanvas engine?\n- The `Bundle` class represents the resource of a Bundle Asset, which contains an index that maps URLs to blob URLs. It is used to manage blob URLs for files in a loaded bundle.\n\n2. What is the format of the `files` parameter in the `Bundle` constructor?\n- The `files` parameter is an array of objects that have a `name` field and contain a `getBlobUrl()` function.\n\n3. What happens when the `destroy()` method is called on a `Bundle` instance?\n- The `destroy()` method frees up blob URLs by revoking object URLs for each key in the `_blobUrls` object. After calling `destroy()`, the `_blobUrls` property is set to `null`.","metadata":{"source":".autodoc/docs/markdown/src/framework/bundle/bundle.md"}}],["267",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/anim/component-binder.js)\n\nThe code defines a class called `AnimComponentBinder` that extends the `DefaultAnimBinder` class. This class is used to bind animation data to properties of an entity or a graph node. The `AnimComponentBinder` constructor takes in an `animComponent`, `graph`, `layerName`, `mask`, and `layerIndex`. The `animComponent` is the entity that the animation is bound to, the `graph` is the graph node that the animation is bound to, the `layerName` is the name of the animation layer, the `mask` is the mask of the animation layer, and the `layerIndex` is the index of the animation layer.\n\nThe class has several static methods that are used to pack animation data into the correct format for the target property. These methods include `_packFloat`, `_packBoolean`, `_packVec2`, `_packVec3`, `_packVec4`, `_packColor`, and `_packQuat`. These methods take in an array of animation values and return the packed animation data in the correct format.\n\nThe `AnimComponentBinder` class has a `resolve` method that takes in a `path` object and returns an `AnimTarget` object. The `path` object contains information about the entity, component, and property that the animation is bound to. The `resolve` method first encodes the `path` object into a string and checks if the target is already in the cache. If the target is in the cache, it returns the cached target. If the target is not in the cache, it resolves the `path` object to get the target entity, property component, and target path. It then creates an `AnimTarget` object for the target property and caches the target.\n\nThe `AnimComponentBinder` class has an `update` method that takes in a `deltaTime` value. This method flags active nodes as dirty.\n\nThe `AnimComponentBinder` class has a `_getEntityFromHierarchy` method that takes in an `entityHierarchy` array and returns the entity that matches the hierarchy. This method checks if the `animComponent` entity name matches the first element of the `entityHierarchy` array. If it does not match, it returns null. If it matches, it gets the current entity and checks if the `entityHierarchy` array has only one element. If it has only one element, it returns the current entity. If it has more than one element, it recursively searches for the entity in the hierarchy.\n\nThe `AnimComponentBinder` class has a `_resolvePath` method that takes in an `object`, `path`, and `resolveLeaf` value. This method resolves an object path by iterating over the path and returning the object at the end of the path.\n\nThe `AnimComponentBinder` class has a `_setter` method that takes in an `object`, `path`, and `packFunc`. This method constructs a setter function for the property located at `path` from the base object. The `packFunc` is a function that takes the animation values array and packages them for the target property in the correct format. The `_setter` method first resolves the path to get the object and key. If the object has a setter function, it uses it. If the target property has a copy function, it uses it. If the object is a vector, color, or quaternion, it invokes the object's setter. Otherwise, it sets the property directly.\n\nThe `AnimComponentBinder` class has a `_createAnimTargetForProperty` method that takes in a `propertyComponent`, `propertyHierarchy`, and `targetPath`. This method creates an `AnimTarget` object for the target property. If the property is a weight, it calls the `handlers.weight` method. If the property is a material texture, it calls the `handlers.materialTexture` method. Otherwise, it resolves the property and creates a setter function for the property. It then creates an `AnimTarget` object with the setter function and returns it.\n\nThe `AnimComponentBinder` class has a `rebind` method that resets the target cache and sets the graph to the root bone or entity. It also caches the node names for quick resolution of animation paths.\n\nOverall, the `AnimComponentBinder` class is used to bind animation data to properties of an entity or a graph node. It provides methods for resolving animation paths, creating setter functions for properties, and creating `AnimTarget` objects for the target properties.\n## Questions: \n 1. What is the purpose of the `AnimComponentBinder` class?\n- The `AnimComponentBinder` class is a subclass of `DefaultAnimBinder` and is responsible for binding animation data to properties of an entity or graph.\n\n2. What is the significance of the `packFunc` parameter in the `_setter` method?\n- The `packFunc` parameter is a function that takes an array of animation values and packages them in the correct format for the target property, such as `Vec2`, `Color`, or `Quat`.\n\n3. What is the purpose of the `rebind` method in the `AnimComponentBinder` class?\n- The `rebind` method resets the target cache and updates the graph and node cache based on the current state of the `AnimComponentBinder` instance.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/anim/component-binder.md"}}],["268",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/anim/component-layer.js)\n\nThe `AnimComponentLayer` class is a component of the PlayCanvas engine that allows managers to create a single layer of the animation state graph. It is responsible for managing the animations of a single layer and provides methods to control the playback of animations, assign animation tracks to states or blend tree nodes, and transition between states.\n\nThe class constructor takes in several parameters, including the name of the layer, the controller to manage the layer's animations, the component that the layer is a member of, the weight of the layer, the blend type of the layer, and whether the weight of the layer should be normalized using the total weight of all layers.\n\nThe class provides several getter and setter methods to access and modify the properties of the layer, including the name of the layer, whether the layer is currently playing, whether the layer is playable, the currently active state name, the previously active state name, the currently active state's progress, the currently active state's duration, and whether the layer is currently transitioning between states.\n\nThe class also provides methods to control the playback of animations, including `play()`, `pause()`, and `reset()`. Additionally, it provides methods to assign animation tracks to states or blend tree nodes, remove animations from a node in the loaded state graph, and transition between states.\n\nThe `update()` method is called every frame and updates the weight of the layer if a blend is in progress and updates the controller.\n\nOverall, the `AnimComponentLayer` class provides a simple and efficient way to manage animations in a single layer of the animation state graph. It is a crucial component of the PlayCanvas engine and is used extensively throughout the project.\n## Questions: \n 1. What is the purpose of the `AnimComponentLayer` class?\n- The `AnimComponentLayer` class allows managers to have a single layer of the animation state graph.\n\n2. What are the parameters of the `constructor` method?\n- The `constructor` method takes in the name of the layer, the controller to manage this layer's animations, the component that this layer is a member of, an optional weight value (defaults to 1), an optional blend type (defaults to ANIM_LAYER_OVERWRITE), and an optional boolean value to determine whether the weight of this layer should be normalized using the total weight of all layers.\n\n3. What is the purpose of the `assignAnimation` method?\n- The `assignAnimation` method assigns an animation track to a state or blend tree node in the current graph. If a state for the given nodePath doesn't exist, it will be created. If all states nodes are linked and the `AnimComponent`'s `activate` value was set to true then the component will begin playing.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/anim/component-layer.md"}}],["269",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/anim/data.js)\n\nThe code above defines a class called `AnimComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to animation components that can be attached to entities in the game world. \n\nThe `constructor` method initializes a property called `enabled` to `true`. This property is used to determine whether or not the animation component is currently active. If `enabled` is set to `false`, the animation will not play. \n\nThis class can be used in conjunction with other classes and methods in the PlayCanvas engine to create complex animations for game entities. For example, a developer could create an instance of `AnimComponentData` and attach it to an entity using the `addComponent` method. They could then use the `setAnimation` method to specify which animation should be played, and the `play` method to start the animation. \n\nHere is an example of how this class might be used in a larger project:\n\n```\nimport { AnimComponentData } from 'playcanvas-engine';\n\nconst myEntity = new pc.Entity();\nconst animData = new AnimComponentData();\n\nmyEntity.addComponent('animation', {\n    animComponentData: animData\n});\n\nanimData.enabled = false;\n\n// Later on, when we want to play the animation:\nanimData.enabled = true;\nmyEntity.animation.play('walk');\n```\n\nIn this example, we create a new entity and a new instance of `AnimComponentData`. We then attach the animation component to the entity and pass in the `animData` object we just created. We set `enabled` to `false` initially, so the animation won't play yet. Later on, when we want to play the animation, we set `enabled` to `true` and call the `play` method on the animation component to start the animation.\n## Questions: \n 1. What is the purpose of the `AnimComponentData` class?\n   - The `AnimComponentData` class is likely used to store data related to animation components in the PlayCanvas engine.\n\n2. What does the `enabled` property do?\n   - The `enabled` property is a boolean value that determines whether the animation component is enabled or disabled.\n\n3. Why is the `AnimComponentData` class being exported?\n   - The `AnimComponentData` class is being exported so that it can be used in other parts of the PlayCanvas engine or in external code that utilizes the engine.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/anim/data.md"}}],["270",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/anim/system.js)\n\nThe `AnimComponentSystem` is a class that manages the creation and deletion of `AnimComponents`. It extends the `ComponentSystem` class and is part of the PlayCanvas engine project. \n\nThe `AnimComponentSystem` class has several methods that are used to initialize, update, and destroy `AnimComponents`. The `initializeComponentData` method initializes the data for a new `AnimComponent`. It takes in the `component`, `data`, and `properties` as parameters. The `data` parameter is an object that contains the data for the `AnimComponent`. The `properties` parameter is an array of strings that specifies the properties that should be initialized. The `initializeComponentData` method initializes the `component` with the data from the `data` parameter. It also initializes the `component` with the properties specified in the `properties` parameter.\n\nThe `onAnimationUpdate` method is called every frame and updates all `AnimComponents` that are currently playing. It takes in the `dt` parameter, which is the time since the last frame. The `cloneComponent` method is used to clone an `AnimComponent`. It takes in the `entity` and `clone` parameters. The `entity` parameter is the entity that the `AnimComponent` is attached to. The `clone` parameter is the entity that the cloned `AnimComponent` will be attached to. The `cloneComponent` method returns the cloned `AnimComponent`.\n\nThe `onBeforeRemove` method is called before an `AnimComponent` is removed. It takes in the `entity` and `component` parameters. The `entity` parameter is the entity that the `AnimComponent` is attached to. The `component` parameter is the `AnimComponent` that is being removed. The `onBeforeRemove` method calls the `onBeforeRemove` method of the `component`.\n\nThe `AnimComponentSystem` class is used to manage the creation and deletion of `AnimComponents`. It is used in the larger PlayCanvas engine project to provide animation functionality to entities. The `AnimComponentSystem` class is used to initialize, update, and destroy `AnimComponents`. It is also used to clone `AnimComponents`.\n## Questions: \n 1. What is the purpose of the `AnimComponentSystem` class?\n- The `AnimComponentSystem` class manages creating and deleting `AnimComponents`.\n\n2. What are the properties that can be initialized manually in the `initializeComponentData` method?\n- The properties that can be initialized manually are `animationAssets`, `stateGraph`, `layers`, and `masks`.\n\n3. What is the purpose of the `onAnimationUpdate` method?\n- The `onAnimationUpdate` method updates the animation components that are currently playing and enabled.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/anim/system.md"}}],["271",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/animation/data.js)\n\nThe code above defines a class called `AnimationComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to animation components in the engine. \n\nThe `constructor` method initializes a property called `enabled` to `true`. This property is used to determine whether or not an animation component is enabled and should be played. \n\nThis class can be used in conjunction with other classes and methods in the PlayCanvas engine to create and manipulate animations. For example, a developer could create an instance of `AnimationComponentData` and pass it as an argument to a method that creates an animation component. \n\n```\nconst animationData = new AnimationComponentData();\ncreateAnimationComponent(entity, animationData);\n```\n\nIn this example, `createAnimationComponent` is a method that takes an entity and an instance of `AnimationComponentData` as arguments and creates an animation component for that entity using the provided data. \n\nOverall, the `AnimationComponentData` class serves as a simple data container for animation component properties and can be used to create and manipulate animations in the PlayCanvas engine.\n## Questions: \n 1. **What is the purpose of the AnimationComponentData class?** \nThe AnimationComponentData class is likely used to store data related to animation components within the PlayCanvas engine.\n\n2. **What does the `enabled` property do?** \nThe `enabled` property is a boolean value that determines whether the animation component is enabled or disabled.\n\n3. **How is this code used within the PlayCanvas engine?** \nWithout additional context, it is unclear how this code is used within the PlayCanvas engine. It may be necessary to review other files or documentation to understand its role within the larger project.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/animation/data.md"}}],["272",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/animation/system.js)\n\nThe code defines the AnimationComponentSystem class, which is responsible for managing the creation and deletion of AnimationComponents. This class extends the ComponentSystem class, which is a base class for all component systems in the PlayCanvas engine. \n\nThe AnimationComponentSystem class has a constructor that takes an instance of the AppBase class as a parameter. It sets the id property to 'animation', which is used to identify the system. It also sets the ComponentType and DataType properties to AnimationComponent and AnimationComponentData, respectively. These properties are used to create new instances of components and their associated data. The schema property is set to an array of strings that define the properties of the component.\n\nThe class has an initializeComponentData method that is called during the initialization of a component. This method sets the properties of the component in a specific order due to some setters in the component having extra logic. The method then calls the initializeComponentData method of the base class.\n\nThe class has a cloneComponent method that creates a clone of a component. This method creates a copy of all component data variables and returns the newly cloned component.\n\nThe class has an onBeforeRemove method that is called when a component is about to be removed from an entity. This method calls the onBeforeRemove method of the component being removed.\n\nThe class has an onUpdate method that is called every frame to update the animation components. This method updates the animation components if they are enabled and their associated entities are enabled.\n\nThe class exports the AnimationComponentSystem class. \n\nThis code is used in the PlayCanvas engine to manage the creation and deletion of AnimationComponents. It provides methods for initializing, cloning, and updating animation components. Developers can use this class to create and manage animation components in their PlayCanvas projects. For example, they can create an instance of the AnimationComponentSystem class and use it to create and manage animation components for their entities. \n\nExample usage:\n\n```javascript\nimport { AnimationComponentSystem } from 'playcanvas';\n\nconst app = new pc.Application();\nconst animationSystem = new AnimationComponentSystem(app);\n\n// create an entity with an animation component\nconst entity = new pc.Entity();\nentity.addComponent('animation', {\n    assets: [1, 2, 3],\n    speed: 1,\n    loop: true,\n    activate: true,\n    enabled: true\n});\n\n// add the entity to the scene\napp.root.addChild(entity);\n\n// update the animation components every frame\napp.on('update', (dt) => {\n    animationSystem.onUpdate(dt);\n});\n```\n## Questions: \n 1. What is the purpose of the `AnimationComponentSystem` class?\n- The `AnimationComponentSystem` class manages creating and deleting `AnimationComponents`.\n\n2. What properties can be set in the `initializeComponentData` method?\n- The `activate`, `enabled`, `loop`, `speed`, and `assets` properties can be set in the `initializeComponentData` method.\n\n3. What does the `onUpdate` method do?\n- The `onUpdate` method updates the animation of all enabled `AnimationComponents` belonging to enabled entities.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/animation/system.md"}}],["273",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/audio-listener/component.js)\n\nThe code defines a class called `AudioListenerComponent` which extends the `Component` class. This class represents an audio listener in a 3D world, which is responsible for ensuring that audio sources positioned in 3D space are heard correctly. \n\nThe `AudioListenerComponent` class has a constructor that takes two parameters: a `system` object of type `AudioListenerComponentSystem` and an `entity` object of type `Entity`. The `system` parameter is the component system that created this component, while the `entity` parameter is the entity that this component is attached to. \n\nThe `AudioListenerComponent` class has two methods: `setCurrentListener()` and `onDisable()`. The `setCurrentListener()` method sets the current audio listener to the entity that this component is attached to, if the component is enabled and the entity has an `audiolistener` component that is also enabled. It then sets the position of the audio listener to the position of the current entity. The `onDisable()` method is called when the component is disabled, and it sets the current audio listener to `null` if the current audio listener is the entity that this component is attached to. \n\nThis code is part of the PlayCanvas engine project and is used to enable 3D audio in the engine. It allows developers to attach an `AudioListenerComponent` to an entity in their scene, which will then act as the audio listener for that scene. This is useful for creating immersive audio experiences in games and other interactive applications. \n\nFor example, a developer could create an entity called `player` and attach an `AudioListenerComponent` to it. They could then create other entities in the scene that have `AudioSourceComponent` components attached to them, which represent audio sources. The `AudioListenerComponent` would ensure that the audio from these sources is heard correctly based on the position of the `player` entity in the scene. \n\nOverall, the `AudioListenerComponent` class is an important part of the PlayCanvas engine's audio system, and it allows developers to create rich, immersive audio experiences in their applications.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines an AudioListenerComponent class that represents an audio listener in a 3D world for correct audio positioning.\n\n2. What other classes does this code depend on?\n- This code depends on the Component class from '../component.js', the AudioListenerComponentSystem class from './system.js', and the Entity class from '../../entity.js'.\n\n3. What is the significance of the setCurrentListener() method?\n- The setCurrentListener() method sets the current audio listener to this entity if it is enabled and has an audio listener component, and updates the listener's position to match the entity's position.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/audio-listener/component.md"}}],["274",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/audio-listener/data.js)\n\nThe code above defines a class called `AudioListenerComponentData` which is used to store data related to an audio listener component in the PlayCanvas engine. The `constructor` method initializes the `enabled` property to `true`. \n\nIn the PlayCanvas engine, an audio listener component is used to define the position and orientation of the listener in the 3D world. This component is attached to an entity in the scene and is responsible for receiving audio from other entities that have audio sources attached to them. \n\nThe `AudioListenerComponentData` class is used to store the data related to the audio listener component. This data is serialized, which means it can be saved and loaded from disk or transmitted over a network. The `enabled` property is a boolean value that determines whether the audio listener component is enabled or not. \n\nHere is an example of how this class might be used in the PlayCanvas engine:\n\n```javascript\n// Create a new entity and add an audio listener component to it\nconst entity = new pc.Entity();\nentity.addComponent('audiolistener', {\n    enabled: true\n});\n\n// Get the data for the audio listener component\nconst data = entity.audiolistener.data;\n\n// Disable the audio listener component\ndata.enabled = false;\n```\n\nIn this example, a new entity is created and an audio listener component is added to it with the `enabled` property set to `true`. The `data` property is then used to get the `AudioListenerComponentData` object for the component. Finally, the `enabled` property is set to `false` to disable the audio listener component. \n\nOverall, the `AudioListenerComponentData` class is an important part of the PlayCanvas engine as it allows developers to store and manipulate data related to audio listener components in a standardized way.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code defines a class called `AudioListenerComponentData` which has a constructor that initializes a boolean property called `enabled` to `true`. It is also being exported for use in other parts of the PlayCanvas engine.\n\n2. **What is the significance of the `enabled` property?**\\\nThe `enabled` property is likely used to determine whether or not an audio listener component is active or inactive. If it is set to `true`, the audio listener component is enabled and can receive audio input.\n\n3. **Where else in the PlayCanvas engine is this class being used?**\\\nWithout further context, it is difficult to determine where else this class is being used in the PlayCanvas engine. However, it is being exported, so it is likely being used in other parts of the engine where audio listener components are needed.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/audio-listener/data.md"}}],["275",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/audio-listener/system.js)\n\nThe code defines a Component System for adding and removing AudioListenerComponent objects to entities in the PlayCanvas engine. An AudioListenerComponent is a component that can be added to an entity to enable it to listen to audio. The AudioListenerComponentSystem extends the ComponentSystem class and is responsible for managing the creation and destruction of AudioListenerComponents.\n\nThe AudioListenerComponentSystem class has a constructor that takes an instance of the AppBase class as a parameter. It sets the id of the system to 'audiolistener', sets the ComponentType and DataType to AudioListenerComponent and AudioListenerComponentData respectively, and initializes the schema with the _schema constant. The _schema constant is an array that contains the names of the properties that can be set on an AudioListenerComponent.\n\nThe AudioListenerComponentSystem class also has an onUpdate method that is called every frame. This method updates the position and orientation of the audio listener based on the position and world transform of the current entity that has an AudioListenerComponent attached to it. The current entity is set by calling the setCurrentEntity method on the AudioListenerComponentManager class.\n\nThe AudioListenerComponentSystem class also has a destroy method that removes the onUpdate method from the update event of the app systems.\n\nThe code exports the AudioListenerComponentSystem class and imports the Debug, Component, ComponentSystem, AudioListenerComponent, and AudioListenerComponentData classes from other files in the PlayCanvas engine.\n\nThis code can be used to add audio listener functionality to entities in the PlayCanvas engine. For example, if a game has a player character that needs to listen to audio, an AudioListenerComponent can be added to the player entity. The AudioListenerComponentSystem will manage the creation and destruction of the AudioListenerComponent and update the position and orientation of the audio listener every frame.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a component system for adding and removing audio listener components to entities in the PlayCanvas engine.\n\n2. What other components does this code interact with?\n- This code interacts with the AudioListenerComponent and AudioListenerComponentData components.\n\n3. What is the significance of the Debug.assert statement?\n- The Debug.assert statement ensures that the sound manager is defined before creating an instance of the AudioListenerComponentSystem, and will throw an error if it is not defined.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/audio-listener/system.md"}}],["276",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/audio-source/component.js)\n\nThe `AudioSourceComponent` class is a component that controls the playback of an audio sample. It is a part of the PlayCanvas engine project. This class will be deprecated in favor of the `SoundComponent`. \n\nThe `AudioSourceComponent` class has several properties that can be set to control the audio playback. These properties include:\n\n- `assets`: The list of audio assets - can also be an array of asset ids.\n- `activate`: If true the audio will begin playing as soon as the scene is loaded.\n- `volume`: The volume modifier to play the audio with. In range 0-1.\n- `pitch`: The pitch modifier to play the audio with. Must be larger than 0.01.\n- `loop`: If true the audio will restart when it finishes playing.\n- `3d`: If true the audio will play back at the location of the entity in space, so the audio will be affected by the position of the `AudioListenerComponent`.\n- `distanceModel`: Determines which algorithm to use to reduce the volume of the audio as it moves away from the listener. Can be:\n  - \"linear\"\n  - \"inverse\"\n  - \"exponential\"\n  Default is \"inverse\".\n- `minDistance`: The minimum distance from the listener at which audio falloff begins.\n- `maxDistance`: The maximum distance from the listener at which audio falloff stops. Note the volume of the audio is not 0 after this distance, but just doesn't fall off anymore.\n- `rollOffFactor`: The factor used in the falloff equation.\n\nThe `AudioSourceComponent` class has several methods that can be used to control the audio playback. These methods include:\n\n- `play(name)`: Begin playback of an audio asset in the component attached to an entity.\n- `pause()`: Pause playback of the audio that is playing on the Entity. Playback can be resumed by calling `unpause()`.\n- `unpause()`: Resume playback of the audio if paused. Playback is resumed at the time it was paused.\n- `stop()`: Stop playback on an Entity. Playback can not be resumed after being stopped.\n\nThe `AudioSourceComponent` class also has several event handlers that are called when the properties of the component are changed. These event handlers include:\n\n- `onSetAssets(name, oldValue, newValue)`: Called when the `assets` property is changed.\n- `onSetLoop(name, oldValue, newValue)`: Called when the `loop` property is changed.\n- `onSetVolume(name, oldValue, newValue)`: Called when the `volume` property is changed.\n- `onSetPitch(name, oldValue, newValue)`: Called when the `pitch` property is changed.\n- `onSetMinDistance(name, oldValue, newValue)`: Called when the `minDistance` property is changed.\n- `onSetMaxDistance(name, oldValue, newValue)`: Called when the `maxDistance` property is changed.\n- `onSetRollOffFactor(name, oldValue, newValue)`: Called when the `rollOffFactor` property is changed.\n- `onSetDistanceModel(name, oldValue, newValue)`: Called when the `distanceModel` property is changed.\n- `onSet3d(name, oldValue, newValue)`: Called when the `3d` property is changed.\n\nThe `AudioSourceComponent` class also has several methods that are called when the component is enabled or disabled. These methods include:\n\n- `onEnable()`: Called when the component is enabled.\n- `onDisable()`: Called when the component is disabled.\n\nOverall, the `AudioSourceComponent` class is a useful component for controlling the playback of audio samples in a PlayCanvas project. It provides a variety of properties and methods that can be used to customize the audio playback experience.\n## Questions: \n 1. What is the purpose of the `AudioSourceComponent` class?\n- The `AudioSourceComponent` class controls the playback of an audio sample and is attached to an entity in the PlayCanvas engine.\n2. What is the difference between the `play()` and `pause()` methods?\n- The `play()` method begins playback of an audio asset, while the `pause()` method pauses playback of the audio that is currently playing on the entity.\n3. What is the significance of the `3d` property?\n- If the `3d` property is set to true, the audio will play back at the location of the entity in space and will be affected by the position of the `AudioListenerComponent`.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/audio-source/component.md"}}],["277",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/audio-source/data.js)\n\nThe code defines a class called `AudioSourceComponentData` which is used to store data related to an audio source component in the PlayCanvas engine. The class has several properties that are used to configure the behavior of the audio source component.\n\nThe `enabled` property is a boolean that determines whether the audio source component is enabled or not. If it is disabled, the audio source will not play.\n\nThe `assets` property is an array of audio assets that can be played by the audio source component. The audio assets are specified as URLs to audio files.\n\nThe `activate` property is a boolean that determines whether the audio source component should start playing automatically when it is added to an entity.\n\nThe `volume` property is a number between 0 and 1 that determines the volume of the audio source component.\n\nThe `pitch` property is a number that determines the pitch of the audio source component.\n\nThe `loop` property is a boolean that determines whether the audio source component should loop or not.\n\nThe `3d` property is a boolean that determines whether the audio source component should be treated as a 3D sound or not.\n\nThe `minDistance` property is a number that determines the minimum distance at which the audio source component can be heard.\n\nThe `maxDistance` property is a number that determines the maximum distance at which the audio source component can be heard.\n\nThe `rollOffFactor` property is a number that determines how quickly the volume of the audio source component decreases as the listener moves away from it.\n\nThe `distanceModel` property is a constant that determines the algorithm used to calculate the volume of the audio source component based on its distance from the listener.\n\nThe `paused` property is a boolean that determines whether the audio source component is currently paused or not.\n\nThe `sources` property is an object that contains references to the audio sources that are used to play the audio assets.\n\nThe `currentSource` property is a reference to the currently playing audio source.\n\nThe `channel` property is a reference to the audio channel that the audio source component is playing on.\n\nThis class is used to store the configuration of an audio source component and is used by other parts of the PlayCanvas engine to play audio. For example, when an entity with an audio source component is added to the scene, the engine will use the data stored in the `AudioSourceComponentData` instance to create an audio source and start playing the audio. \n\nHere is an example of how this class might be used:\n\n```\nimport { AudioSourceComponentData } from 'playcanvas';\n\nconst audioSourceData = new AudioSourceComponentData();\naudioSourceData.assets = ['https://example.com/audio.mp3'];\naudioSourceData.volume = 0.5;\naudioSourceData.loop = true;\n\n// Add the audio source component to an entity\nconst entity = new pc.Entity();\nentity.addComponent('audiosource', audioSourceData);\n```\n## Questions: \n 1. What is the purpose of the `AudioSourceComponentData` class?\n- The `AudioSourceComponentData` class is a data structure that holds information about an audio source component in the PlayCanvas engine.\n\n2. What is the significance of the `DISTANCE_INVERSE` constant imported from `constants.js`?\n- The `DISTANCE_INVERSE` constant is likely used as a value for the `distanceModel` property of the `AudioSourceComponentData` class, which determines how the volume of the audio source decreases with distance.\n\n3. What is the difference between the `minDistance` and `maxDistance` properties of the `AudioSourceComponentData` class?\n- The `minDistance` property represents the distance at which the audio source will be at its maximum volume, while the `maxDistance` property represents the distance at which the audio source will be at its minimum volume.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/audio-source/data.md"}}],["278",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/audio-source/system.js)\n\nThe code defines a class called `AudioSourceComponentSystem` that controls the playback of an audio sample. This class is an extension of `ComponentSystem` and is used to manage `AudioSourceComponent` instances. The `AudioSourceComponent` class is used to attach audio sources to entities in the PlayCanvas engine. \n\nThe `AudioSourceComponentSystem` class has a constructor that takes an instance of the PlayCanvas application as an argument. It sets the `id` property to `'audiosource'` and initializes the `ComponentType` and `DataType` properties to `AudioSourceComponent` and `AudioSourceComponentData`, respectively. It also sets the `schema` property to an array of strings that define the properties of the `AudioSourceComponent`. \n\nThe `AudioSourceComponentSystem` class has a method called `initializeComponentData` that initializes the properties of an `AudioSourceComponent` instance. It sets the `paused` property to `true` if the `enabled` and `activate` properties of the component are `false`. \n\nThe `AudioSourceComponentSystem` class has an `onInitialize` method that is called when the system is initialized. It plays the current audio source if the `audiosource` property of the root entity is enabled and activated. It also recursively initializes all child entities. \n\nThe `AudioSourceComponentSystem` class has an `onUpdate` method that is called every frame. It updates the position of the audio source if it is a 3D sound. \n\nThe `AudioSourceComponentSystem` class has an `onRemove` method that is called when an entity is removed. It stops the audio source and sets the `channel` property to `null`. \n\nThe `AudioSourceComponentSystem` class has a `setVolume` method that sets the volume for the entire `AudioSourceComponentSystem`. \n\nThe `AudioSourceComponentSystem` class is used to manage `AudioSourceComponent` instances in the PlayCanvas engine. It provides methods for initializing, updating, and removing audio sources. It also provides a method for setting the volume for all audio sources.\n## Questions: \n 1. What is the purpose of this code?\n- This code is a module for the PlayCanvas engine that controls the playback of an audio sample through an AudioSourceComponent.\n\n2. What is the difference between AudioSourceComponent and SoundComponentSystem?\n- AudioSourceComponent is a class that controls the playback of an audio sample, while SoundComponentSystem is a newer class that is intended to replace AudioSourceComponent and provides more features.\n\n3. What is the purpose of the `onUpdate` method?\n- The `onUpdate` method updates the position of the channel for 3D sounds based on the position of the entity that the AudioSourceComponent is attached to.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/audio-source/system.md"}}],["279",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/button/constants.js)\n\nThis code defines two constants that are used to specify different transition modes for buttons in the PlayCanvas engine. The first constant, `BUTTON_TRANSITION_MODE_TINT`, is used to specify different color tints for the hover, pressed, and inactive states of a button. The second constant, `BUTTON_TRANSITION_MODE_SPRITE_CHANGE`, is used to specify different sprites for the hover, pressed, and inactive states of a button.\n\nThese constants are likely used in conjunction with a button component in the PlayCanvas engine. The button component allows developers to create interactive buttons that respond to user input, such as clicks or touches. The transition mode of the button determines how the button changes appearance when it is interacted with.\n\nFor example, if a developer wants a button to change color when it is hovered over, they would set the transition mode to `BUTTON_TRANSITION_MODE_TINT` and specify the different tints for each state. They could do this using the `hoverTint`, `pressedTint`, and `inactiveTint` properties of the button component.\n\nSimilarly, if a developer wants a button to change its sprite when it is pressed, they would set the transition mode to `BUTTON_TRANSITION_MODE_SPRITE_CHANGE` and specify the different sprites for each state. They could do this using the `hoverSprite`, `pressedSprite`, and `inactiveSprite` properties of the button component.\n\nOverall, these constants provide a way for developers to customize the appearance of buttons in their PlayCanvas projects, making them more engaging and interactive for users.\n## Questions: \n 1. **What is the purpose of this code?**\\\nA smart developer might ask what this code is used for and how it fits into the PlayCanvas engine. This code defines constants for different transition modes for buttons in the engine.\n\n2. **What are the different transition modes available?**\\\nA smart developer might ask what the different transition modes are and how they differ. This code defines two transition modes: BUTTON_TRANSITION_MODE_TINT for color tints and BUTTON_TRANSITION_MODE_SPRITE_CHANGE for sprite changes.\n\n3. **How are these constants used in the PlayCanvas engine?**\\\nA smart developer might ask how these constants are used in the PlayCanvas engine and where they are referenced. Without further context, it is unclear how these constants are used in the engine.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/button/constants.md"}}],["280",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/button/data.js)\n\nThe code defines a class called `ButtonComponentData` that represents the data associated with a button component in the PlayCanvas engine. The purpose of this class is to store the various properties and settings that define the behavior and appearance of a button in a PlayCanvas application.\n\nThe class imports two other classes from the PlayCanvas engine: `Color` and `Vec4`. `Color` is used to define the various colors associated with the button, such as the hover tint, pressed tint, and inactive tint. `Vec4` is used to define the hit padding, which is the amount of padding around the button that should be considered part of the button's clickable area.\n\nThe `ButtonComponentData` class has a number of properties that can be set to customize the behavior and appearance of the button. These include:\n\n- `enabled`: A boolean value that determines whether the button is currently enabled or disabled.\n- `active`: A boolean value that determines whether the button is currently active or inactive.\n- `imageEntity`: A reference to the entity that contains the button's image.\n- `hitPadding`: A `Vec4` object that defines the hit padding around the button.\n- `transitionMode`: An enum value that determines the transition mode for the button. The available options are defined in the `constants.js` file.\n- `hoverTint`: A `Color` object that defines the tint to apply to the button when it is hovered over.\n- `pressedTint`: A `Color` object that defines the tint to apply to the button when it is pressed.\n- `inactiveTint`: A `Color` object that defines the tint to apply to the button when it is inactive.\n- `fadeDuration`: The duration of the fade animation when the button is transitioned between states.\n- `hoverSpriteAsset`: A reference to the asset that contains the sprite to use for the button when it is hovered over.\n- `hoverSpriteFrame`: The frame of the sprite to use for the button when it is hovered over.\n- `pressedSpriteAsset`: A reference to the asset that contains the sprite to use for the button when it is pressed.\n- `pressedSpriteFrame`: The frame of the sprite to use for the button when it is pressed.\n- `inactiveSpriteAsset`: A reference to the asset that contains the sprite to use for the button when it is inactive.\n- `inactiveSpriteFrame`: The frame of the sprite to use for the button when it is inactive.\n\nThis class can be used in the larger PlayCanvas project to define the behavior and appearance of buttons in a PlayCanvas application. For example, a developer could create a new instance of the `ButtonComponentData` class and set its properties to customize the behavior and appearance of a specific button in their application. They could then attach this instance to the entity that represents the button in the PlayCanvas scene.\n## Questions: \n 1. What is the purpose of the `ButtonComponentData` class?\n- The `ButtonComponentData` class is used to store data related to a button component, such as its enabled state, active state, image entity, and various tints and sprites.\n\n2. What is the `BUTTON_TRANSITION_MODE_TINT` constant and where is it defined?\n- The `BUTTON_TRANSITION_MODE_TINT` constant is used to specify the transition mode for the button component, and it is imported from a `constants.js` file located in the same directory as this code file.\n\n3. What are the `hoverSpriteAsset`, `hoverSpriteFrame`, `pressedSpriteAsset`, `pressedSpriteFrame`, `inactiveSpriteAsset`, and `inactiveSpriteFrame` properties used for?\n- These properties are used to specify the sprite assets and frames to use for the button component in different states, such as when the button is hovered over, pressed, or inactive.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/button/data.md"}}],["281",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/button/system.js)\n\nThe code defines a `ButtonComponentSystem` class that manages the creation of `ButtonComponent`s. The `ButtonComponent` is a component that can be attached to an entity in the PlayCanvas engine to create a button that can be clicked or tapped. \n\nThe `ButtonComponentSystem` extends the `ComponentSystem` class, which is a base class for all component systems in the PlayCanvas engine. It defines the `ButtonComponent` and `ButtonComponentData` classes, which are used to create and store instances of the `ButtonComponent`. \n\nThe `_schema` constant is an array that defines the properties of the `ButtonComponent`. These properties include whether the button is enabled or active, the image entity that represents the button, the padding around the button, the transition mode, and the sprite assets and frames for the button in different states (hover, pressed, and inactive). \n\nThe `ButtonComponentSystem` constructor sets the `id`, `ComponentType`, `DataType`, and `schema` properties of the class. It also registers an event listener for the `beforeremove` event, which is triggered when a component is about to be removed from an entity. Additionally, it registers an event listener for the `update` event, which is triggered every frame to update the state of the engine. \n\nThe `initializeComponentData` method initializes the data for a `ButtonComponent` instance. It calls the `super` method to initialize the base component data, and passes in the `_schema` constant to define the properties of the component. \n\nThe `onUpdate` method is called every frame to update the state of all `ButtonComponent` instances. It loops through all the `ButtonComponent` instances and calls the `onUpdate` method of each enabled and active component. \n\nThe `_onRemoveComponent` method is called when a `ButtonComponent` is about to be removed from an entity. It calls the `onRemove` method of the component to perform any necessary cleanup. \n\nThe `destroy` method is called when the `ButtonComponentSystem` is destroyed. It unregisters the `update` event listener. \n\nFinally, the `Component._buildAccessors` method is called to build getter and setter methods for the properties defined in the `_schema` constant. \n\nOverall, the `ButtonComponentSystem` class provides a way to create and manage `ButtonComponent` instances in the PlayCanvas engine. It defines the properties of the component, initializes the component data, updates the state of the components every frame, and performs cleanup when a component is removed. Developers can use this class to create buttons in their PlayCanvas projects.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file manages the creation of `ButtonComponent`s in the PlayCanvas engine.\n\n2. What is the structure of the `_schema` array?\n- The `_schema` array contains a list of properties for the `ButtonComponent`, including their data types and whether they are required or optional.\n\n3. What does the `onUpdate` method do?\n- The `onUpdate` method loops through all `ButtonComponent`s in the system and calls their `onUpdate` method if they are both enabled and their entity is enabled.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/button/system.md"}}],["282",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/camera/data.js)\n\nThe code above defines a class called `CameraComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to a camera component in a PlayCanvas entity. \n\nThe `constructor` method initializes a property called `enabled` to `true`. This property determines whether the camera component is enabled or disabled. \n\nThis class can be used in conjunction with other classes and methods in the PlayCanvas engine to create and manipulate camera components in a 3D scene. For example, a developer could create a new entity in the scene and add a camera component to it using the `Entity.addComponent` method:\n\n```\nconst entity = new pc.Entity();\nentity.addComponent('camera', new pc.CameraComponentData());\n```\n\nThis would create a new entity with a camera component that is enabled by default. The developer could then modify the `enabled` property of the camera component to disable it if necessary:\n\n```\nentity.camera.enabled = false;\n```\n\nOverall, the `CameraComponentData` class provides a simple and standardized way to store and manipulate camera component data in the PlayCanvas engine.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code defines a class called `CameraComponentData` which has a constructor that sets the `enabled` property to `true`. It is exported for use in other parts of the PlayCanvas engine.\n\n2. **What other properties or methods does the `CameraComponentData` class have?**\\\nWithout further information, it is unclear what other properties or methods the `CameraComponentData` class has. A smart developer may want to investigate the class definition or documentation to learn more.\n\n3. **How is the `CameraComponentData` class used in the PlayCanvas engine?**\\\nIt is unclear from this code snippet how the `CameraComponentData` class is used in the PlayCanvas engine. A smart developer may want to search for other parts of the codebase that import and use this class.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/camera/data.md"}}],["283",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/camera/post-effect-queue.js)\n\nThe code defines two classes, `PostEffect` and `PostEffectQueue`, which are used to manage post-processing effects for a camera in the PlayCanvas engine. \n\nThe `PostEffect` class represents a single post-processing effect and has two properties: `effect` and `inputTarget`. `effect` is an instance of a post-processing effect, and `inputTarget` is a `RenderTarget` object that represents the input to the effect. `outputTarget` is another `RenderTarget` object that represents the output of the effect, and `name` is a string that represents the name of the effect.\n\nThe `PostEffectQueue` class is used to manage multiple post-processing effects for a camera. It has several properties, including `app`, which is an instance of the PlayCanvas application, `camera`, which is the camera component that the post-processing effects are applied to, `destinationRenderTarget`, which is a `RenderTarget` object that represents the final output of the post-processing effects, `effects`, which is an array of `PostEffect` objects that represent the post-processing effects to apply, and `enabled`, which is a boolean that indicates whether the post-processing effects are currently enabled.\n\nThe `PostEffectQueue` class has several methods, including `addEffect`, which adds a post-processing effect to the queue, `removeEffect`, which removes a post-processing effect from the queue, `destroy`, which removes all post-processing effects from the queue and disables it, `enable`, which enables the post-processing effects, and `disable`, which disables the post-processing effects.\n\nThe `PostEffectQueue` class also has several private methods, including `_allocateColorBuffer`, which allocates a color buffer texture for a post-processing effect, `_createOffscreenTarget`, which creates a render target with the dimensions of the canvas, `_resizeOffscreenTarget`, which resizes a render target, `_destroyOffscreenTarget`, which destroys a render target, `_requestDepthMaps`, which requests depth maps for all post-processing effects, `_releaseDepthMaps`, which releases depth maps for all post-processing effects, `_requestDepthMap`, which requests a depth map for a post-processing effect, `_releaseDepthMap`, which releases a depth map for a post-processing effect, `_onCanvasResized`, which is a handler called when the application's canvas element is resized, and `resizeRenderTargets`, which resizes all render targets.\n\nOverall, the `PostEffectQueue` class provides a way to manage post-processing effects for a camera in the PlayCanvas engine, allowing developers to easily add and remove effects and control when they are applied.\n## Questions: \n 1. What is the purpose of the `PostEffectQueue` class?\n- The `PostEffectQueue` class is used to manage multiple post effects for a camera in a game engine.\n2. What is the significance of the `needsDepthBuffer` property of a post effect?\n- The `needsDepthBuffer` property of a post effect indicates whether the effect requires a depth buffer to be rendered properly.\n3. What happens when the `enable` method of the `PostEffectQueue` class is called?\n- When the `enable` method of the `PostEffectQueue` class is called, the queue and all of its effects are enabled, and the camera renders to the first effect's render target. The `onPostprocessing` callback is also set to handle postprocessing.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/camera/post-effect-queue.md"}}],["284",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/camera/system.js)\n\nThe code defines a class called `CameraComponentSystem` that is responsible for managing `CameraComponent`s in the PlayCanvas engine. The `CameraComponent` is a component that can be added to an entity in the engine to make it a camera. The `CameraComponentSystem` class extends the `ComponentSystem` class, which is a base class for all component systems in the engine.\n\nThe `CameraComponentSystem` class has an array called `cameras` that holds all the active camera components. The class has methods to add and remove cameras from this array. The `addCamera` method adds a camera to the array and sorts the array based on the priority of the cameras. The `removeCamera` method removes a camera from the array and sorts the array again.\n\nThe `CameraComponentSystem` class has a constructor that sets up the class. It sets the `id` of the class to `'camera'`, sets the `ComponentType` to `CameraComponent`, and sets the `DataType` to `CameraComponentData`. It also sets up an event listener for the `beforeremove` event and the `prerender` event. The `beforeremove` event is triggered when a component is about to be removed from an entity, and the `prerender` event is triggered before the scene is rendered.\n\nThe `CameraComponentSystem` class has a method called `initializeComponentData` that initializes the data for a camera component. It takes a `component`, a `data`, and a `properties` parameter. The `properties` parameter is an array of strings that represent the properties of the camera component. The method loops through the `properties` array and sets the properties of the `component` based on the values in the `data` object. If the value is an array, it creates a new `Vec4` or `Color` object and sets the property to that object. Otherwise, it sets the property to the value in the `data` object.\n\nThe `CameraComponentSystem` class has a method called `cloneComponent` that clones a camera component. It takes an `entity` and a `clone` parameter. The method creates a new camera component and sets its properties to the properties of the camera component in the `entity`. It then adds the new camera component to the `clone` entity.\n\nThe `CameraComponentSystem` class has an `onBeforeRemove` method that is called when a camera component is about to be removed from an entity. The method removes the camera component from the `cameras` array.\n\nThe `CameraComponentSystem` class has an `onUpdate` method that is called every frame. The method does not do anything.\n\nThe `CameraComponentSystem` class has an `onAppPrerender` method that is called before the scene is rendered. The method calls the `onAppPrerender` method of each camera component in the `cameras` array.\n\nThe `CameraComponentSystem` class exports the `CameraComponentSystem` class.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file is a CameraComponentSystem class used to add and remove CameraComponents from entities and hold an array of all active cameras.\n\n2. What properties can be initialized for a CameraComponent?\n- Properties that can be initialized for a CameraComponent include aspectRatio, aspectRatioMode, calculateProjection, calculateTransform, clearColor, clearColorBuffer, clearDepthBuffer, clearStencilBuffer, renderSceneColorMap, renderSceneDepthMap, cullFaces, farClip, flipFaces, fov, frustumCulling, horizontalFov, layers, renderTarget, nearClip, orthoHeight, projection, priority, rect, scissorRect, aperture, shutter, and sensitivity.\n\n3. What events does the CameraComponentSystem listen for?\n- The CameraComponentSystem listens for the 'beforeremove' event to remove a camera component before an entity is removed, and the 'prerender' event to call the onAppPrerender method for each active camera. It also listens for the 'update' event from the app system.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/camera/system.md"}}],["285",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/collision/data.js)\n\nThe code defines a class called `CollisionComponentData` which is used to store data related to collision detection in the PlayCanvas engine. The class has several properties that can be used to configure the collision component, such as `enabled`, `type`, `halfExtents`, `linearOffset`, `angularOffset`, `radius`, `axis`, `height`, `asset`, and `renderAsset`. \n\nThe `enabled` property is a boolean that determines whether the collision component is active or not. The `type` property specifies the type of collision shape that should be used, such as a box or a sphere. The `halfExtents` property is a `Vec3` object that defines the size of the collision shape. The `linearOffset` and `angularOffset` properties are used to specify the position and orientation of the collision shape relative to the entity that it is attached to. The `radius`, `axis`, and `height` properties are used to configure the collision shape for a cylinder. The `asset` and `renderAsset` properties are used to specify the collision and rendering assets for the component.\n\nThe class also has several non-serialized properties, such as `shape`, `model`, `render`, and `initialized`. These properties are used internally by the engine and are not meant to be modified directly by the user.\n\nOverall, the `CollisionComponentData` class provides a way to configure and store data related to collision detection in the PlayCanvas engine. It can be used in conjunction with other components and systems to create complex game objects that can interact with the environment and other objects in the scene. \n\nExample usage:\n\n```javascript\nimport { CollisionComponentData } from 'playcanvas-engine';\n\n// create a new collision component data object\nconst collisionData = new CollisionComponentData();\n\n// configure the collision component\ncollisionData.type = 'box';\ncollisionData.halfExtents.set(1, 1, 1);\ncollisionData.linearOffset.set(0, 1, 0);\n\n// attach the collision component to an entity\nentity.addComponent('collision', collisionData);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `CollisionComponentData` that contains properties related to collision detection in the PlayCanvas engine.\n\n2. What are the default values for the properties of `CollisionComponentData`?\n- The default values for the properties of `CollisionComponentData` are: `enabled` is `true`, `type` is `'box'`, `halfExtents` is a `Vec3` with values of `(0.5, 0.5, 0.5)`, `linearOffset` is a `Vec3` with default values, `angularOffset` is a `Quat` with default values, `radius` is `0.5`, `axis` is `1`, `height` is `2`, `asset` and `renderAsset` are both `null`, and the non-serialized properties are all `null` and `initialized` is `false`.\n\n3. What other classes or modules are imported in this code?\n- This code imports the `Quat` and `Vec3` classes from the `math` module located in the `core` directory of the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/collision/data.md"}}],["286",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/collision/trigger.js)\n\nThe code defines a class called `Trigger` that creates an object used to create internal physics objects that interact with rigid bodies and trigger collision events with no collision response. The purpose of this class is to create a trigger object that can be used to detect collisions between two objects without actually causing a physical collision response. This is useful for creating events that should be triggered when two objects come into contact, but where the objects should not actually collide with each other.\n\nThe `Trigger` class has several methods that are used to initialize, update, enable, and disable the trigger object. When the `Trigger` object is initialized, it creates a new physics body using the `app.systems.rigidbody.createBody()` method. This method takes in a mass, shape, and transform and creates a new physics body with the specified properties. The `Trigger` object also sets various properties of the physics body, such as its restitution, friction, and damping.\n\nThe `Trigger` object has an `enable()` method that adds the physics body to the physics world and sets its activation state to active so that it is properly simulated. The `disable()` method removes the physics body from the physics world and sets its activation state to disable simulation so that it properly deactivates after it is removed from the physics world.\n\nOverall, the `Trigger` class is an important part of the PlayCanvas engine because it allows developers to create trigger objects that can detect collisions between two objects without actually causing a physical collision response. This is useful for creating events that should be triggered when two objects come into contact, but where the objects should not actually collide with each other.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `Trigger` that creates a trigger object used to create internal physics objects that interact with rigid bodies and trigger collision events with no collision response.\n\n2. What external dependencies does this code have?\n- This code imports constants from a file located at `'../rigid-body/constants.js'` and checks if `Ammo` is defined before creating new instances of `btVector3`, `btQuaternion`, and `btTransform` from the `Ammo` library.\n\n3. What methods does the `Trigger` class have and what do they do?\n- The `Trigger` class has methods called `initialize`, `destroy`, `_getEntityTransform`, `updateTransform`, `enable`, and `disable`. These methods initialize the trigger object, destroy the trigger object, get the transform of the entity associated with the trigger, update the transform of the trigger object, enable the trigger object, and disable the trigger object, respectively.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/collision/trigger.md"}}],["287",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/component.js)\n\nThe code defines a class called `Component` that is used to attach functionality to an `Entity` in the PlayCanvas engine. The `Component` class extends the `EventHandler` class, which allows it to receive update events each frame and expose properties to the PlayCanvas Editor. \n\nThe `Component` class has two properties: `system` and `entity`. The `system` property is of type `ComponentSystem` and is used to create the `Component`. The `entity` property is of type `Entity` and is the `Entity` that the `Component` is attached to. \n\nThe `Component` class has a constructor that takes two parameters: `system` and `entity`. The constructor sets the `system` and `entity` properties of the `Component`. If the `system` has a `schema` property and the `_accessorsBuilt` property is not set, the `buildAccessors` method is called to create getter/setter pairs for each property defined in the `schema`. \n\nThe `Component` class has several methods that are used internally and are not meant to be called directly. The `onSetEnabled` method is called when the `enabled` property of the `Component` is set. If the `enabled` property changes from `false` to `true`, the `onEnable` method is called. If the `enabled` property changes from `true` to `false`, the `onDisable` method is called. The `onPostStateChange` method is called after the state of the `Component` has changed. \n\nThe `Component` class has a `data` property that allows direct access to the `Component` data. However, it is recommended to access the data properties via the individual properties as modifying the data directly will not fire 'set' events. \n\nOverall, the `Component` class is an important part of the PlayCanvas engine as it allows developers to attach functionality to `Entity` objects. The `Component` class is used extensively throughout the engine and is a crucial part of the engine's architecture. \n\nExample usage:\n\n```javascript\nimport { Component } from 'playcanvas';\n\nclass MyComponent extends Component {\n    constructor(system, entity) {\n        super(system, entity);\n\n        // set up component properties\n        this.enabled = true;\n        this.speed = 10;\n    }\n\n    onEnable() {\n        console.log('MyComponent enabled');\n    }\n\n    onDisable() {\n        console.log('MyComponent disabled');\n    }\n}\n\nexport { MyComponent };\n```\n## Questions: \n 1. What is the purpose of the `Component` class?\n    \n    The `Component` class is used to attach functionality to an `Entity`, and can receive update events each frame and expose properties to the PlayCanvas Editor.\n\n2. What is the `system` property used for?\n    \n    The `system` property is used to store the `ComponentSystem` that was used to create the `Component`.\n\n3. Why does the `buildAccessors` method check if `this._accessorsBuilt` is false before building accessors?\n    \n    The `buildAccessors` method checks if `this._accessorsBuilt` is false before building accessors to ensure that accessors are only built once for each `Component` instance.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/component.md"}}],["288",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/element/constants.js)\n\nThis code defines several constants that are used in the PlayCanvas engine project to specify different types of {@link ElementComponent}s and fit modes for content within those elements. \n\nThe first three constants, `ELEMENTTYPE_GROUP`, `ELEMENTTYPE_IMAGE`, and `ELEMENTTYPE_TEXT`, define the different types of elements that can be used in the project. These types are used to specify the type of content that will be displayed within an element. For example, an `ELEMENTTYPE_IMAGE` element would be used to display an image, while an `ELEMENTTYPE_TEXT` element would be used to display text.\n\nThe next three constants, `FITMODE_STRETCH`, `FITMODE_CONTAIN`, and `FITMODE_COVER`, define different fit modes for content within an element. These fit modes are used to specify how the content should be scaled to fit within the element's bounding box. For example, `FITMODE_STRETCH` would stretch the content to fill the entire bounding box, while `FITMODE_CONTAIN` would scale the content to fit within the bounding box while preserving its aspect ratio.\n\nOverall, these constants are used throughout the PlayCanvas engine project to specify the type of content and how it should be displayed within an element. For example, when creating a new element, the developer would specify the type of element using one of the `ELEMENTTYPE` constants, and the fit mode for the content using one of the `FITMODE` constants. \n\nExample usage:\n\n```javascript\nimport { ELEMENTTYPE_IMAGE, FITMODE_COVER } from 'playcanvas-engine';\n\n// create a new image element with content that covers the entire bounding box\nconst imageElement = new ElementComponent(app, {\n  type: ELEMENTTYPE_IMAGE,\n  fitWidth: FITMODE_COVER,\n  fitHeight: FITMODE_COVER\n});\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines constants for different types of ElementComponents and fit modes used in the PlayCanvas engine.\n\n2. How are these constants used in the PlayCanvas engine?\n- These constants are likely used as parameters or options for various functions and methods within the PlayCanvas engine that deal with ElementComponents and their display properties.\n\n3. Are there any other types of ElementComponents or fit modes available in the PlayCanvas engine?\n- It's possible that there are other types of ElementComponents or fit modes available, but they are not defined in this particular code file. A smart developer might want to check other files in the PlayCanvas engine to see if there are additional constants or options available.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/element/constants.md"}}],["289",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/element/data.js)\n\nThe code above defines a class called `ElementComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to an element component, which is a component that can be attached to an entity in the PlayCanvas engine to give it a visual representation in the scene.\n\nThe `ElementComponentData` class has a single property called `enabled`, which is set to `true` by default in the constructor. This property determines whether or not the element component is enabled and visible in the scene. If `enabled` is set to `false`, the element component will not be rendered.\n\nThis class can be used in conjunction with other classes and components in the PlayCanvas engine to create complex scenes with multiple entities and visual elements. For example, a developer could create an entity with an element component attached to it, and then use other components like the transform component to position and scale the entity in the scene.\n\nHere is an example of how the `ElementComponentData` class might be used in a PlayCanvas project:\n\n```\nimport { ElementComponentData } from 'playcanvas-engine';\n\nconst myElementComponent = new ElementComponentData();\nmyElementComponent.enabled = false;\n\n// Attach the element component to an entity\nconst myEntity = new pc.Entity();\nmyEntity.addComponent('element', {\n    type: 'image',\n    spriteAsset: mySpriteAsset,\n    data: myElementComponent\n});\n\n// Add a transform component to position and scale the entity\nmyEntity.addComponent('transform', {\n    position: new pc.Vec3(0, 0, 0),\n    scale: new pc.Vec3(1, 1, 1)\n});\n\n// Add the entity to the scene\nthis.app.root.addChild(myEntity);\n```\n\nIn this example, we create a new `ElementComponentData` object and set its `enabled` property to `false`. We then attach the element component to a new entity and specify that it should render an image using a sprite asset. Finally, we add a transform component to position and scale the entity, and add it to the scene. Because the `enabled` property of the element component is set to `false`, the image will not be visible in the scene.\n## Questions: \n 1. What is the purpose of the ElementComponentData class?\n   - The ElementComponentData class is likely used to store data related to an element component in the PlayCanvas engine.\n2. What does the `enabled` property do?\n   - The `enabled` property is a boolean value that determines whether the element component is enabled or disabled.\n3. How is this code used in the PlayCanvas engine?\n   - This code is likely used as a data structure for element components in the PlayCanvas engine, but further investigation would be needed to determine its exact usage.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/element/data.md"}}],["290",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/element/element-drag-helper.js)\n\nThe code defines a class called `ElementDragHelper` that provides functionality to make it easy to create elements that can be dragged by the mouse or touch. The class extends the `EventHandler` class and has several methods and properties that enable it to perform its functionality.\n\nThe class constructor takes two parameters: `element` and `axis`. `element` is an instance of the `ElementComponent` class, which is the element that should become draggable. `axis` is an optional parameter that constrains the drag movement to either the x or y axis. If `axis` is not provided, the drag movement is not constrained.\n\nThe class has several events that are fired during the drag operation. The `drag:start` event is fired when a new drag operation starts. The `drag:end` event is fired when the current drag operation ends. The `drag:move` event is fired whenever the position of the dragged element changes.\n\nThe class has several private properties that are used during the drag operation. `_dragScale` is a `Vec3` that stores the scale of the dragged element. `_dragStartMousePosition` is a `Vec3` that stores the starting position of the mouse or touch input. `_dragStartHandlePosition` is a `Vec3` that stores the starting position of the dragged element. `_deltaMousePosition` is a `Vec3` that stores the change in position of the mouse or touch input. `_deltaHandlePosition` is a `Vec3` that stores the change in position of the dragged element. `_isDragging` is a boolean that indicates whether a drag operation is currently in progress.\n\nThe class has several private methods that are used during the drag operation. `_toggleLifecycleListeners` is a method that adds or removes event listeners for the `mousedown`, `touchstart`, and `selectstart` events on the dragged element. `_toggleDragListeners` is a method that adds or removes event listeners for the `mousemove`, `mouseup`, `touchmove`, `touchend`, `touchcancel`, `selectmove`, and `selectend` events on the dragged element. `_onMouseDownOrTouchStart` is a method that is called when the `mousedown`, `touchstart`, or `selectstart` event is fired on the dragged element. It initializes the drag operation by setting the starting positions of the mouse or touch input and the dragged element. `_onMouseUpOrTouchEnd` is a method that is called when the `mouseup`, `touchend`, or `touchcancel` event is fired on the dragged element. It ends the drag operation by resetting the `_isDragging` property and removing the event listeners. `_screenToLocal` is a method that calculates the intersection point of the plane and ray based on the mouse or touch input. `_determineInputPosition` is a method that determines the position of the mouse or touch input. `_chooseRayOriginAndDirection` is a method that chooses the origin and direction of the ray based on the screen space of the dragged element. `_calculateDragScale` is a method that calculates the scale of the dragged element based on its parent elements.\n\nThe class has several public properties that can be accessed and modified. `enabled` is a boolean that indicates whether the drag operation is enabled or not. `isDragging` is a boolean that indicates whether a drag operation is currently in progress.\n\nOverall, the `ElementDragHelper` class provides a simple way to create draggable elements in the PlayCanvas engine. It handles the mouse and touch input, calculates the intersection point of the plane and ray, and updates the position of the dragged element. It also fires events during the drag operation that can be used to perform additional actions.\n## Questions: \n 1. What is the purpose of the `ElementDragHelper` class?\n- The `ElementDragHelper` class is a helper class that makes it easy to create Elements that can be dragged by the mouse or touch.\n\n2. What are the events that can be fired by the `ElementDragHelper` class?\n- The `ElementDragHelper` class can fire three events: `drag:start`, `drag:end`, and `drag:move`.\n\n3. What is the purpose of the `_calculateDragScale` method?\n- The `_calculateDragScale` method calculates the drag scale of the element being dragged, taking into account the scale of its parent elements.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/element/element-drag-helper.md"}}],["291",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/joint/constants.js)\n\nThis code defines three constants that are used to specify the degree of freedom for movement in a 3D space. The constants are `MOTION_FREE`, `MOTION_LIMITED`, and `MOTION_LOCKED`. \n\n`MOTION_FREE` indicates that the degree of freedom has free movement, meaning that it can move in any direction without any restrictions. This is useful for objects that need to move freely in a 3D space, such as a camera or a player character.\n\n`MOTION_LIMITED` indicates that the degree of freedom has limited movement, meaning that it can only move within a certain range or along a certain axis. This is useful for objects that need to move within a specific area or along a specific path, such as a door or a moving platform.\n\n`MOTION_LOCKED` indicates that the degree of freedom is locked and cannot move at all. This is useful for objects that need to remain stationary, such as a wall or a floor.\n\nThese constants can be used throughout the PlayCanvas engine to specify the degree of freedom for movement in various components and systems. For example, in the physics system, these constants can be used to specify the type of joint between two rigid bodies. \n\n```javascript\n// Example usage in the physics system\nconst joint = new pc.PhysicalJoint();\njoint.setMotion(0, pc.MOTION_FREE); // Set the first degree of freedom to have free movement\njoint.setMotion(1, pc.MOTION_LIMITED); // Set the second degree of freedom to have limited movement\njoint.setMotion(2, pc.MOTION_LOCKED); // Set the third degree of freedom to be locked\n```\n\nOverall, this code provides a simple and standardized way to specify the degree of freedom for movement in a 3D space, making it easier to develop and maintain the PlayCanvas engine.\n## Questions: \n 1. **What is the purpose of this code?**\\\nA smart developer might ask what this code is used for and how it fits into the larger PlayCanvas engine project. This code defines three constants that represent different degrees of freedom for movement.\n    \n2. **What do the different constants represent?**\\\nA smart developer might ask for more information about what each of the three constants (`MOTION_FREE`, `MOTION_LIMITED`, and `MOTION_LOCKED`) actually mean and how they are used within the PlayCanvas engine. The code comments provide some information, but more context may be needed.\n    \n3. **Why are the constants marked with `@ignore`?**\\\nA smart developer might ask why the constants are marked with the `@ignore` JSDoc tag. This tag is typically used to indicate that a particular piece of code should be excluded from documentation generated by tools like JSDoc. The developer may want to know why these constants are excluded from documentation and whether there are any implications for using them in code.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/joint/constants.md"}}],["292",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/joint/data.js)\n\nThe code above defines a class called `JointComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to provide a data structure for storing information about joints in a 3D model.\n\nThe `constructor` method initializes a property called `enabled` to `true`. This property is used to determine whether the joint is active or not. If `enabled` is set to `false`, the joint will not be included in any calculations or animations.\n\nThis class can be used in conjunction with other classes in the PlayCanvas engine to create complex 3D models with joints that can be animated and manipulated. For example, a `ModelComponent` class might use `JointComponentData` objects to store information about the joints in a model. The `AnimationComponent` class might then use this information to animate the model by manipulating the joints.\n\nHere is an example of how `JointComponentData` might be used in a `ModelComponent` class:\n\n```\nimport { JointComponentData } from 'playcanvas-engine';\n\nclass ModelComponent {\n    constructor() {\n        this.joints = [\n            new JointComponentData(),\n            new JointComponentData(),\n            new JointComponentData()\n        ];\n    }\n}\n```\n\nIn this example, the `ModelComponent` class has an array of `JointComponentData` objects called `joints`. These objects are used to store information about the joints in the model. The `enabled` property of each `JointComponentData` object can be set to `false` to disable a joint and prevent it from being animated.\n## Questions: \n 1. **What is the purpose of the `JointComponentData` class?** \n    The `JointComponentData` class is likely used to store data related to joints in the PlayCanvas engine.\n    \n2. **What does the `enabled` property do?** \n    The `enabled` property is a boolean value that determines whether the joint is enabled or disabled.\n    \n3. **How is this code used within the PlayCanvas engine?** \n    It is unclear how this code is used within the PlayCanvas engine without additional context. It is possible that it is used to create and manage joints between entities in a 3D scene.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/joint/data.md"}}],["293",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/joint/system.js)\n\nThe code above is a module that creates and manages physics joint components in the PlayCanvas engine project. The module imports the `Component` and `ComponentSystem` classes from other modules in the project. It also imports the `JointComponent` and `JointComponentData` classes from another module in the same directory.\n\nThe `JointComponentSystem` class extends the `ComponentSystem` class and is responsible for creating and managing instances of `JointComponent`. It defines a constructor that takes an instance of the `AppBase` class as a parameter. The constructor sets the `id` property of the `JointComponentSystem` instance to `'joint'` and sets the `ComponentType` and `DataType` properties to `JointComponent` and `JointComponentData`, respectively. It also sets the `schema` property to `['enabled']`.\n\nThe `initializeComponentData` method is defined to initialize the `JointComponent` instance with the provided `data` object. The `initFromData` method of the `JointComponent` class is called to perform the initialization.\n\nThe `Component._buildAccessors` method is called to build accessor methods for the `JointComponent` class based on the `_schema` array.\n\nThis module can be used to create and manage physics joint components in the PlayCanvas engine project. Developers can use this module to create instances of `JointComponent` and add them to entities in their game or application. The `JointComponent` instances can be used to define joints between rigid bodies in the physics simulation. The `JointComponentSystem` class provides a convenient way to manage these components and their data.\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code creates and manages physics joint components in the PlayCanvas engine.\n\n2. What is the inheritance hierarchy of `JointComponentSystem`?\n    \n    `JointComponentSystem` inherits from `ComponentSystem`, which in turn inherits from `Object`.\n\n3. What is the significance of the `initializeComponentData` method?\n    \n    The `initializeComponentData` method initializes a `JointComponent` instance with data and properties.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/joint/system.md"}}],["294",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/layout-child/component.js)\n\nThe `LayoutChildComponent` class is a component of the PlayCanvas engine that enables an entity to control the sizing applied to it by its parent `LayoutGroupComponent`. This component is used to define the minimum and maximum width and height of an entity, as well as the amount of additional horizontal and vertical space that the entity should take up, if necessary to satisfy a Stretch/Shrink fitting calculation. \n\nThis component is created by passing a `LayoutChildComponentSystem` and an `Entity` to its constructor. The `LayoutChildComponent` class extends the `Component` class, which is a base class for all components in the PlayCanvas engine. \n\nThe `LayoutChildComponent` class has several properties that can be set and retrieved using getter and setter methods. These properties include:\n\n- `minWidth`: The minimum width the element should be rendered at.\n- `minHeight`: The minimum height the element should be rendered at.\n- `maxWidth`: The maximum width the element should be rendered at.\n- `maxHeight`: The maximum height the element should be rendered at.\n- `fitWidthProportion`: The amount of additional horizontal space that the element should take up, if necessary to satisfy a Stretch/Shrink fitting calculation. This is specified as a proportion, taking into account the proportion values of other siblings.\n- `fitHeightProportion`: The amount of additional vertical space that the element should take up, if necessary to satisfy a Stretch/Shrink fitting calculation. This is specified as a proportion, taking into account the proportion values of other siblings.\n- `excludeFromLayout`: If set to true, the child will be excluded from all layout calculations.\n\nWhen any of these properties are set, the `resize` event is fired. This event can be used to trigger a resize of the entity in response to changes in the layout.\n\nHere is an example of how to use the `LayoutChildComponent` class to set the minimum and maximum width and height of an entity:\n\n```javascript\nconst entity = new pc.Entity();\nconst layoutChildComponent = new pc.LayoutChildComponent(system, entity);\n\nlayoutChildComponent.minWidth = 100;\nlayoutChildComponent.minHeight = 50;\nlayoutChildComponent.maxWidth = 500;\nlayoutChildComponent.maxHeight = 300;\n``` \n\nIn summary, the `LayoutChildComponent` class is a component of the PlayCanvas engine that enables an entity to control the sizing applied to it by its parent `LayoutGroupComponent`. It provides properties for setting the minimum and maximum width and height of an entity, as well as the amount of additional horizontal and vertical space that the entity should take up.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a `LayoutChildComponent` class that enables an entity to control its sizing when it is a child of a `LayoutGroupComponent`.\n\n2. What methods and properties are available in the `LayoutChildComponent` class?\n- The `LayoutChildComponent` class has methods and properties for setting and getting the minimum and maximum width and height of the entity, as well as the amount of additional horizontal and vertical space that the entity should take up, and whether the entity should be excluded from layout calculations.\n\n3. What other classes does this code interact with?\n- This code imports the `Component` class from a `component.js` file, and references a `LayoutChildComponentSystem` class and an `Entity` class from other files. It also mentions a `LayoutGroupComponent` class, which is likely defined elsewhere in the project.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/layout-child/component.md"}}],["295",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/layout-child/data.js)\n\nThe code above defines a class called `LayoutChildComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to the layout of child entities within a parent entity. \n\nThe class has a single property called `enabled`, which is set to `true` by default in the constructor. This property determines whether the child entity should be included in the layout calculations or not. \n\nThis class can be used in conjunction with other components in the PlayCanvas engine to create complex layouts of entities within a scene. For example, a `LayoutGroupComponent` could be used to define a parent entity that contains multiple child entities with `LayoutChildComponentData` attached. The `LayoutGroupComponent` would then use the data from the `LayoutChildComponentData` instances to determine the position and size of each child entity within the parent entity. \n\nHere is an example of how this class could be used in code:\n\n```\nimport { LayoutChildComponentData } from 'playcanvas';\n\n// create a new instance of the LayoutChildComponentData class\nconst childLayoutData = new LayoutChildComponentData();\n\n// disable the layout calculations for this child entity\nchildLayoutData.enabled = false;\n```\n\nOverall, the `LayoutChildComponentData` class is a small but important piece of the PlayCanvas engine that enables developers to create complex layouts of entities within a scene.\n## Questions: \n 1. **What is the purpose of the `LayoutChildComponentData` class?** \n   The `LayoutChildComponentData` class is likely a data structure used to store information about a child component in a layout system.\n\n2. **What does the `enabled` property do?** \n   The `enabled` property is a boolean value that likely determines whether or not the child component is currently enabled or disabled within the layout system.\n\n3. **How is this code used within the PlayCanvas engine?** \n   Without additional context, it is unclear how this code is used within the PlayCanvas engine. It may be necessary to examine other files or documentation to understand its role within the larger system.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/layout-child/data.md"}}],["296",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/layout-child/system.js)\n\nThe code defines a class called `LayoutChildComponentSystem` that manages the creation of `LayoutChildComponent`s. This class extends the `ComponentSystem` class, which is a base class for all component systems in the PlayCanvas engine. \n\nThe `LayoutChildComponent` is a component that can be added to an entity to specify how it should be laid out within a parent entity. The `LayoutChildComponentSystem` is responsible for creating and managing instances of this component. \n\nThe constructor of the `LayoutChildComponentSystem` class takes an instance of the `AppBase` class as a parameter. This is the main application object that is used throughout the PlayCanvas engine. The constructor sets the `id` property of the component system to `'layoutchild'` and sets the `ComponentType` and `DataType` properties to `LayoutChildComponent` and `LayoutChildComponentData`, respectively. It also sets the `schema` property to `['enabled']`, which is an array of strings that defines the properties that can be set on instances of the `LayoutChildComponent`.\n\nThe `initializeComponentData` method is called when a new instance of the `LayoutChildComponent` is created. It takes three parameters: the component instance, the data object that contains the component's properties, and an array of property names. This method sets the properties of the component instance based on the values in the data object. If a property is not defined in the data object, it is not set on the component instance. \n\nThe `cloneComponent` method is called when a new entity is created as a clone of an existing entity. It takes two parameters: the original entity and the new clone entity. This method creates a new instance of the `LayoutChildComponent` for the clone entity and sets its properties to the same values as the original entity's `LayoutChildComponent`.\n\nFinally, the code exports the `LayoutChildComponentSystem` class. This allows other parts of the PlayCanvas engine to import and use this class to manage `LayoutChildComponent`s.\n\nOverall, the `LayoutChildComponentSystem` class is an important part of the PlayCanvas engine's layout system. It provides a way to create and manage instances of the `LayoutChildComponent`, which is used to specify how entities should be laid out within a parent entity. Developers can use this class to create custom layout systems that suit their specific needs. For example, a developer could create a custom `LayoutChildComponentSystem` that lays out entities in a grid pattern or in a circular pattern.\n## Questions: \n 1. What is the purpose of this code?\n   \n   This code defines a `LayoutChildComponentSystem` class that manages the creation of `LayoutChildComponent`s in the PlayCanvas engine.\n\n2. What other files does this code depend on?\n   \n   This code depends on `../component.js`, `../system.js`, `./component.js`, and `./data.js`.\n\n3. What is the significance of the `initializeComponentData` and `cloneComponent` methods?\n   \n   The `initializeComponentData` method initializes the properties of a `LayoutChildComponent` based on the provided data, while the `cloneComponent` method creates a clone of a `LayoutChildComponent` with the same properties.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/layout-child/system.md"}}],["297",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/layout-group/component.js)\n\nThe `LayoutGroupComponent` is a component that enables an entity to position and scale child `ElementComponent`s according to configurable layout rules. This component is part of the PlayCanvas engine project and is used to create UI layouts in 2D and 3D applications.\n\nThe `LayoutGroupComponent` class extends the `Component` class and has several properties that can be set to configure the layout of child elements. These properties include `orientation`, `reverseX`, `reverseY`, `alignment`, `padding`, `spacing`, `widthFitting`, `heightFitting`, and `wrap`. The `reflow()` method is used to calculate the layout of child elements based on the current configuration of the component.\n\nThe `LayoutGroupComponent` class listens for events related to child elements being added, removed, or resized, and schedules a reflow of the layout when necessary. The `reflow()` method calculates the layout of child elements based on the current configuration of the component and the size of the container element. The layout is calculated using the `LayoutCalculator` class, which takes an array of child elements and layout options as input and returns an object containing the position and size of each child element.\n\nThe `LayoutGroupComponent` class can be used to create a variety of UI layouts, including horizontal and vertical lists, grids, and more. It is designed to be flexible and customizable, allowing developers to create complex layouts with ease. The component is part of the PlayCanvas engine project and is used in many of the engine's built-in UI components, such as buttons, panels, and scroll views.\n## Questions: \n 1. What is the purpose of the LayoutGroupComponent class?\n- The LayoutGroupComponent class enables an entity to position and scale child ElementComponents according to configurable layout rules.\n\n2. What are the different fitting modes that can be applied when positioning and scaling child elements?\n- The different fitting modes are FITTING_NONE, FITTING_STRETCH, FITTING_SHRINK, and FITTING_BOTH.\n\n3. How does the LayoutGroupComponent prevent recursive reflow?\n- The LayoutGroupComponent prevents recursive reflow by flagging that a reflow is currently in progress, and only scheduling a new reflow if one is not already in progress.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/layout-group/component.md"}}],["298",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/layout-group/constants.js)\n\nThis code defines four constants that are used to control the fitting logic of child elements within a parent container. The purpose of this code is to provide a simple way to specify how child elements should be sized and positioned within a container.\n\nThe first constant, FITTING_NONE, disables all fitting logic. This means that child elements will not be resized or repositioned in any way.\n\nThe second constant, FITTING_STRETCH, stretches child elements to fit the parent container. This means that child elements will be resized to fill the available space within the container.\n\nThe third constant, FITTING_SHRINK, shrinks child elements to fit the parent container. This means that child elements will be resized to fit within the available space, even if this means making them smaller than their natural size.\n\nThe fourth constant, FITTING_BOTH, applies both STRETCH and SHRINK fitting logic where applicable. This means that child elements will be resized to fill the available space if they are too small, and shrunk to fit within the available space if they are too large.\n\nThese constants can be used in conjunction with other code to create flexible and responsive user interfaces. For example, a layout engine might use these constants to determine how to position and size child elements within a container based on the available space and the desired layout.\n\nHere is an example of how these constants might be used in a layout engine:\n\n```\nimport { FITTING_STRETCH, FITTING_SHRINK } from 'playcanvas-engine';\n\nfunction layout(container, children) {\n  // Determine the available space within the container\n  const availableWidth = container.clientWidth;\n  const availableHeight = container.clientHeight;\n\n  // Determine the desired size and position of each child element\n  children.forEach(child => {\n    const { fitting } = child;\n\n    if (fitting === FITTING_STRETCH) {\n      // Stretch the child element to fill the available space\n      child.width = availableWidth;\n      child.height = availableHeight;\n    } else if (fitting === FITTING_SHRINK) {\n      // Shrink the child element to fit within the available space\n      child.width = Math.min(child.width, availableWidth);\n      child.height = Math.min(child.height, availableHeight);\n    } else {\n      // Apply both STRETCH and SHRINK fitting logic\n      child.width = Math.min(child.width, availableWidth);\n      child.height = Math.min(child.height, availableHeight);\n      if (child.width < availableWidth || child.height < availableHeight) {\n        child.width = availableWidth;\n        child.height = availableHeight;\n      }\n    }\n\n    // Position the child element within the container\n    child.x = (availableWidth - child.width) / 2;\n    child.y = (availableHeight - child.height) / 2;\n  });\n}\n```\n\nIn this example, the `layout` function takes a container element and an array of child elements, and positions and sizes the child elements within the container based on their `fitting` property. The `FITTING_STRETCH`, `FITTING_SHRINK`, and `FITTING_BOTH` constants are used to determine how each child element should be sized and positioned.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines constants for fitting logic options in a PlayCanvas engine.\n\n2. What are the possible values for the FITTING_* constants?\n- The possible values are FITTING_NONE (0), FITTING_STRETCH (1), FITTING_SHRINK (2), and FITTING_BOTH (3).\n\n3. How can these constants be used in PlayCanvas engine?\n- These constants can be used to specify fitting logic for child elements in a parent container. For example, FITTING_STRETCH can be used to stretch child elements to fit the parent container.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/layout-group/constants.md"}}],["299",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/layout-group/data.js)\n\nThe code above defines a class called `LayoutGroupComponentData` and exports it for use in other parts of the PlayCanvas engine project. \n\nThe purpose of this class is to store data related to layout groups, which are used to organize and position entities within a scene. The `enabled` property is a boolean value that determines whether the layout group is currently active or not. \n\nThis class can be used in conjunction with other classes and components within the PlayCanvas engine to create complex layouts for game objects. For example, a developer could create a layout group component that positions a group of buttons in a specific formation on the screen. They could then use the `LayoutGroupComponentData` class to store information about the layout group, such as whether it is currently enabled or not. \n\nHere is an example of how this class might be used in a larger project:\n\n```\nimport { LayoutGroupComponentData } from 'playcanvas-engine';\n\nclass MyLayoutGroupComponent {\n  constructor() {\n    this.data = new LayoutGroupComponentData();\n  }\n\n  enable() {\n    this.data.enabled = true;\n    // code to enable layout group goes here\n  }\n\n  disable() {\n    this.data.enabled = false;\n    // code to disable layout group goes here\n  }\n}\n```\n\nIn this example, we define a custom layout group component that uses the `LayoutGroupComponentData` class to store information about the component's state. The `enable` and `disable` methods update the `enabled` property of the `LayoutGroupComponentData` instance, which can then be used to control the behavior of the layout group. \n\nOverall, the `LayoutGroupComponentData` class is a small but important piece of the PlayCanvas engine project that helps developers create dynamic and flexible layouts for their games and applications.\n## Questions: \n 1. **What is the purpose of the `LayoutGroupComponentData` class?** \n    The `LayoutGroupComponentData` class is likely a data structure used to store information related to layout groups within the PlayCanvas engine.\n\n2. **What does the `enabled` property do?** \n    The `enabled` property is a boolean value that determines whether the layout group is enabled or disabled.\n\n3. **How is this code used within the PlayCanvas engine?** \n    It is unclear how this code is used within the PlayCanvas engine without additional context or documentation.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/layout-group/data.md"}}],["300",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/layout-group/system.js)\n\nThe code defines a class called `LayoutGroupComponentSystem` that manages the creation of `LayoutGroupComponent`s. It extends the `ComponentSystem` class and is used to create instances of `LayoutGroupComponent` which are used to layout entities in a scene. \n\nThe `LayoutGroupComponentSystem` class has a constructor that takes an instance of the `AppBase` class as a parameter. It sets the `id` property to `'layoutgroup'`, the `ComponentType` property to `LayoutGroupComponent`, and the `DataType` property to `LayoutGroupComponentData`. It also sets the `schema` property to `['enabled']`. The `_reflowQueue` property is initialized to an empty array. \n\nThe `LayoutGroupComponentSystem` class has a method called `initializeComponentData` that initializes the properties of a `LayoutGroupComponent` instance. It takes a `component`, `data`, and `properties` as parameters. It sets the properties of the `component` instance based on the values in the `data` object. \n\nThe `LayoutGroupComponentSystem` class has a method called `cloneComponent` that clones a `LayoutGroupComponent` instance. It takes an `entity` and a `clone` as parameters. It returns a new instance of `LayoutGroupComponent` with the same properties as the `entity` instance. \n\nThe `LayoutGroupComponentSystem` class has a method called `scheduleReflow` that adds a `component` to the `_reflowQueue` array. \n\nThe `LayoutGroupComponentSystem` class has a method called `_onPostUpdate` that processes the `_reflowQueue` array. It calls the `_processReflowQueue` method to process the queue. \n\nThe `LayoutGroupComponentSystem` class has a method called `_processReflowQueue` that processes the `_reflowQueue` array. It sorts the array in ascending order of depth within the graph, so that any layout groups which are children of other layout groups will always have their new size set before their own reflow is calculated. It then iterates over the sorted array and calls the `reflow` method on each component. If the number of iterations exceeds `MAX_ITERATIONS`, it logs a warning message to the console. \n\nThe `LayoutGroupComponentSystem` class has a method called `_onRemoveComponent` that calls the `onRemove` method on a `component` instance when it is removed from an `entity`. \n\nThe `LayoutGroupComponentSystem` class has a `destroy` method that removes the `_onPostUpdate` event listener. \n\nOverall, the `LayoutGroupComponentSystem` class is used to manage the creation and layout of `LayoutGroupComponent`s in a scene. It provides methods for initializing, cloning, and scheduling the reflow of `LayoutGroupComponent`s. The `_processReflowQueue` method is responsible for processing the `_reflowQueue` array and calling the `reflow` method on each component.\n## Questions: \n 1. What is the purpose of the `LayoutGroupComponentSystem` class?\n    \n    The `LayoutGroupComponentSystem` class manages the creation of `LayoutGroupComponent`s and performs reflow when running in the engine.\n\n2. What is the significance of the `scheduleReflow` method?\n    \n    The `scheduleReflow` method adds a `LayoutGroupComponent` to the `_reflowQueue` array, which is processed during the next post-update cycle.\n\n3. What is the purpose of the `cloneComponent` method?\n    \n    The `cloneComponent` method clones a `LayoutGroupComponent` from one entity to another by adding a new `LayoutGroupComponent` to the target entity with the same properties as the source component.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/layout-group/system.md"}}],["301",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/light/data.js)\n\nThe code above defines a class called `LightComponentData` that is used to store data related to a light component in the PlayCanvas engine. The purpose of this class is to provide a default set of properties for a light component, which can be customized by the user as needed.\n\nThe `LightComponentData` class has a constructor that initializes two private variables `_props` and `_propsDefault` with values from the `component.js` file. These variables are arrays that contain the names of properties and their default values for a light component.\n\nThe constructor then loops through the `_props` array and sets the corresponding property of the `LightComponentData` instance to its default value. If the default value is an object that has a `clone` method, the method is called to create a copy of the object. This is done to ensure that each instance of `LightComponentData` has its own copy of the default values, rather than sharing a reference to the same object.\n\nThe `LightComponentData` class is exported so that it can be used by other parts of the PlayCanvas engine. For example, when a new light component is created, an instance of `LightComponentData` is created and used to initialize the component's properties.\n\nHere is an example of how the `LightComponentData` class might be used:\n\n```javascript\nimport { LightComponentData } from 'playcanvas-engine';\n\nclass LightComponent {\n    constructor() {\n        this.data = new LightComponentData();\n    }\n}\n\nconst light = new LightComponent();\nconsole.log(light.data.color); // outputs the default color value for a light component\n```\n\nIn this example, a new `LightComponent` instance is created, which initializes its `data` property with a new instance of `LightComponentData`. The default color value for a light component can then be accessed through the `data` property.\n## Questions: \n 1. What is the purpose of the `component.js` file that is being imported?\n   - The `component.js` file is being imported to access the `_lightProps` and `_lightPropsDefault` variables.\n\n2. What is the significance of the `clone()` method being used in the `for` loop?\n   - The `clone()` method is used to create a copy of the value being assigned to `this[_props[i]]` if the value has a `clone` property.\n\n3. What is the expected output of this file?\n   - This file exports a class called `LightComponentData` that initializes its properties based on the values in `_lightPropsDefault`.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/light/data.md"}}],["302",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/light/system.js)\n\nThe code defines a `LightComponentSystem` class that extends the `ComponentSystem` class. This class is responsible for managing `LightComponent` instances, which are used to dynamically light the scene in the PlayCanvas engine project. \n\nThe `LightComponentSystem` class has a constructor that sets the `id` property to `'light'` and the `ComponentType` and `DataType` properties to `LightComponent` and `LightComponentData`, respectively. It also registers an event listener for the `'beforeremove'` event that calls the `_onRemoveComponent` method. \n\nThe `initializeComponentData` method initializes the data for a `LightComponent` instance. It first duplicates the `_lightProps` array and then loops through it to create a new `data` object with the same properties as the `_data` object. If the `type` property is not defined in the `_data` object, it is set to the `type` property of the `component.data` object. The `data` object is then modified to ensure that the `layers`, `color`, `cookieOffset`, `cookieScale`, and `shape` properties are of the correct type. Finally, a new `Light` instance is created and assigned to the `component.data.light` property. \n\nThe `_onRemoveComponent` method is called when a `LightComponent` instance is removed from an entity. It calls the `onRemove` method of the `component` instance. \n\nThe `cloneComponent` method is used to clone a `LightComponent` instance. It creates a new `data` object that contains the same properties as the original `LightComponent` instance, except for the `light` property. It then adds a new `LightComponent` instance to the `clone` entity with the `data` object. \n\nThe `changeType` method is called when the `type` property of a `LightComponent` instance changes. It updates the `type` property of the `component.light` instance to the new value. \n\nOverall, the `LightComponentSystem` class is an important part of the PlayCanvas engine project as it manages the lighting of the scene. It ensures that `LightComponent` instances are properly initialized and updated, and provides methods for cloning and changing the type of these instances.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a `LightComponentSystem` class that is used to dynamically light the scene in the PlayCanvas engine.\n\n2. What other modules or files does this code depend on?\n- This code depends on several other modules and files located in the `core/math`, `scene`, and `system` directories.\n\n3. What is the significance of the `beforeremove` event listener in the constructor?\n- The `beforeremove` event listener is used to call the `_onRemoveComponent` method when a component is about to be removed, which allows for any necessary cleanup to be performed.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/light/system.md"}}],["303",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/model/data.js)\n\nThe code above defines a class called `ModelComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to a model component, which is a component that can be attached to an entity in the PlayCanvas engine to render a 3D model.\n\nThe `constructor` method initializes a property called `enabled` to `true`. This property is used to determine whether the model component is enabled or disabled. When the `enabled` property is set to `false`, the model component will not be rendered.\n\nThis class can be used in conjunction with other classes and methods in the PlayCanvas engine to create and manipulate 3D models. For example, a developer could create an instance of the `ModelComponentData` class and attach it to an entity in the engine using the `addComponent` method:\n\n```\nconst modelComponentData = new ModelComponentData();\nentity.addComponent('model', {\n    type: 'asset',\n    asset: modelAsset,\n    castShadows: true,\n    receiveShadows: true,\n    enabled: modelComponentData.enabled\n});\n```\n\nIn this example, `entity` is an instance of the `Entity` class in the PlayCanvas engine, and `modelAsset` is an instance of the `Asset` class that represents the 3D model to be rendered. The `addComponent` method is used to attach a model component to the entity, and the `enabled` property of the `ModelComponentData` instance is used to determine whether the component is initially enabled or disabled.\n\nOverall, the `ModelComponentData` class is a small but important part of the PlayCanvas engine project that helps developers create and manipulate 3D models in their applications.\n## Questions: \n 1. **What is the purpose of the ModelComponentData class?** \nThe ModelComponentData class is likely used to store data related to a model component in the PlayCanvas engine.\n\n2. **What does the `enabled` property do?** \nThe `enabled` property is a boolean value that determines whether the model component is enabled or disabled.\n\n3. **How is this code used within the PlayCanvas engine?** \nThis code is likely used as part of the PlayCanvas engine's internal implementation of model components, but further context would be needed to determine exactly how it is used.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/model/data.md"}}],["304",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/model/system.js)\n\nThe `ModelComponentSystem` class is a component system that allows an entity to render a 3D model or a primitive shape like a box, capsule, sphere, cylinder, cone, etc. This class extends the `ComponentSystem` class and is used to manage `ModelComponent` instances. \n\nThe `ModelComponentSystem` class has a constructor that initializes the `id`, `ComponentType`, `DataType`, `schema`, and `defaultMaterial` properties. The `id` property is a string that identifies the component system. The `ComponentType` property is a reference to the `ModelComponent` class. The `DataType` property is a reference to the `ModelComponentData` class. The `schema` property is an array of strings that define the properties of the component. The `defaultMaterial` property is the default material used for rendering.\n\nThe `ModelComponentSystem` class has an `initializeComponentData` method that initializes the data of a `ModelComponent` instance. This method takes a `component`, `_data`, and `properties` parameter. The `component` parameter is the `ModelComponent` instance to initialize. The `_data` parameter is the data to initialize the component with. The `properties` parameter is an array of strings that define the properties to initialize. \n\nThe `ModelComponentSystem` class has a `cloneComponent` method that clones a `ModelComponent` instance. This method takes an `entity` and `clone` parameter. The `entity` parameter is the entity to clone the component from. The `clone` parameter is the entity to clone the component to.\n\nThe `ModelComponentSystem` class has an `onRemove` method that is called when a component is removed from an entity. This method takes an `entity` and `component` parameter. The `entity` parameter is the entity the component was removed from. The `component` parameter is the component that was removed.\n\nOverall, the `ModelComponentSystem` class is an important part of the PlayCanvas engine project as it allows entities to render 3D models and primitive shapes. It provides methods for initializing, cloning, and removing `ModelComponent` instances.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file defines the ModelComponentSystem class, which allows an entity to render a model or a primitive shape.\n\n2. What properties can be initialized for a ModelComponent?\n- Properties that can be initialized for a ModelComponent include material, materialAsset, asset, castShadows, receiveShadows, castShadowsLightmap, lightmapped, lightmapSizeMultiplier, type, mapping, layers, isStatic, and batchGroupId.\n\n3. What does the cloneComponent method do?\n- The cloneComponent method creates a new ModelComponent instance with the same properties as the original entity's ModelComponent, and clones the original model if it is of type asset but has no specified asset. It also copies relevant mesh instance properties and custom AABB if they exist.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/model/system.md"}}],["305",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/particle-system/data.js)\n\nThe code defines a class called `ParticleSystemComponentData` which represents the data for a particle system component in the PlayCanvas engine. The purpose of this class is to store all the necessary parameters and settings for a particle system, which can then be used to create and render particle effects in a game or application.\n\nThe class contains a number of properties that define the behavior of the particle system, such as the number of particles to be emitted, the emission rate, the lifetime of each particle, the shape and size of the emitter, the texture maps to be used for coloring and normal mapping, and various other settings related to animation, lighting, and rendering.\n\nOne important property is `mode`, which determines whether the particle system is rendered using the GPU or CPU. If set to `PARTICLEMODE_GPU`, the particle system will be rendered using WebGL and the GPU, which is faster and more efficient for large numbers of particles. If set to `PARTICLEMODE_CPU`, the particle system will be rendered using the CPU, which is slower but allows for more complex particle behavior and interactions.\n\nAnother important property is `mesh`, which specifies the mesh to be used for each particle. If left undefined, the particle system will use simple quads for each particle. However, if a mesh is specified, the vertex buffer of the mesh is expected to hold the vertex position in the first 3 floats of each vertex.\n\nThe class also contains a number of time-dependent parameters, such as `scaleGraph`, `colorGraph`, and `velocityGraph`, which define how the particle properties change over time. These graphs can be used to create complex and dynamic particle effects.\n\nOverall, the `ParticleSystemComponentData` class provides a flexible and customizable way to create and render particle effects in the PlayCanvas engine. By adjusting the various properties and settings, developers can create a wide range of particle effects, from simple explosions and smoke to complex fluid simulations and dynamic environments. \n\nExample usage:\n\n```javascript\nimport { ParticleSystemComponentData } from 'playcanvas-engine';\n\n// create a new particle system component data object\nconst particleSystemData = new ParticleSystemComponentData();\n\n// set the number of particles to be emitted\nparticleSystemData.numParticles = 100;\n\n// set the emission rate\nparticleSystemData.rate = 10;\n\n// set the lifetime of each particle\nparticleSystemData.lifetime = 5;\n\n// set the shape and size of the emitter\nparticleSystemData.emitterShape = EMITTERSHAPE_BOX;\nparticleSystemData.emitterExtents.set(1, 1, 1);\n\n// set the texture maps to be used for coloring and normal mapping\nparticleSystemData.colorMapAsset = 'textures/particle-color.png';\nparticleSystemData.normalMapAsset = 'textures/particle-normal.png';\n\n// set the blend type\nparticleSystemData.blendType = BLEND_NORMAL;\n\n// set the particle orientation\nparticleSystemData.orientation = PARTICLEORIENTATION_SCREEN;\n\n// set the layers to be assigned to the particle system\nparticleSystemData.layers = [LAYERID_WORLD];\n```\n## Questions: \n 1. What is the purpose of the ParticleSystemComponentData class?\n- The ParticleSystemComponentData class is used to store data related to a particle system component, such as emission rate, particle lifetime, and sorting mode.\n\n2. What is the significance of the Vec3 class and how is it used in this code?\n- The Vec3 class is used to represent a 3D vector and is used in this code to define the emitter extents and wrap bounds of the particle system.\n\n3. What is the difference between PARTICLEMODE_GPU and PARTICLEORIENTATION_SCREEN constants?\n- PARTICLEMODE_GPU specifies that particles should be rendered using the GPU, while PARTICLEORIENTATION_SCREEN specifies that particles should be oriented towards the screen.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/particle-system/data.md"}}],["306",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/particle-system/system.js)\n\nThe code defines a ParticleSystemComponentSystem class that extends the ComponentSystem class. This class allows an entity to render a particle system. It imports several classes from the PlayCanvas engine, including Curve, CurveSet, Vec3, and Asset. \n\nThe ParticleSystemComponentSystem class has a constructor that initializes the schema and property types of the particle system component. It also sets up event listeners for when a component is removed and when the app is updated. \n\nThe class has an initializeComponentData method that initializes the data for a particle system component. It maps the mesh asset id to the meshAsset property and converts vec3, curve, and curveset data to their respective classes. \n\nThe class has a cloneComponent method that clones a particle system component. It clones the data for the component and returns a new component. \n\nThe class has an onUpdate method that updates the particle system components. It iterates through all the particle system components and updates them if they are enabled and their entity is enabled. It bakes ambient and directional lighting into one ambient cube and updates the emitter if it is not paused. \n\nOverall, the ParticleSystemComponentSystem class is an important part of the PlayCanvas engine that allows entities to render particle systems. It provides methods for initializing, cloning, and updating particle system components.\n## Questions: \n 1. What is the purpose of the ParticleSystemComponentSystem class?\n- The ParticleSystemComponentSystem class allows an entity to render a particle system and handles the initialization, cloning, and updating of particle system components.\n\n2. What is the significance of the _schema variable?\n- The _schema variable is an array that lists the properties of a particle system component. It is used to define the schema of the component and to build accessors for the component properties.\n\n3. What does the onUpdate method do?\n- The onUpdate method updates the particle system components by iterating through the components and updating their emitter's simulation time, adding time steps, and finishing the frame. It also bakes ambient and directional lighting into one ambient cube.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/particle-system/system.md"}}],["307",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/registry.js)\n\nThe code defines a class called `ComponentSystemRegistry` that is responsible for storing, accessing, and deleting instances of various `ComponentSystems`. These `ComponentSystems` are used in the PlayCanvas engine to define the behavior of entities in a scene. \n\nThe class extends the `EventHandler` class, which provides functionality for registering and unregistering event listeners. It also defines a number of properties, each of which corresponds to a specific `ComponentSystem` used in the engine. These properties are defined using the `@type` and `@readonly` JSDoc tags, which provide information about the type of the property and indicate that it cannot be modified after it is set. \n\nFor example, the `anim` property is defined as follows:\n\n```\n/**\n * Gets the {@link AnimComponentSystem} from the registry.\n *\n * @type {import('./anim/system.js').AnimComponentSystem|undefined}\n * @readonly\n */\nanim;\n```\n\nThis indicates that the `anim` property is of type `AnimComponentSystem`, which is defined in the `./anim/system.js` file. The `@readonly` tag indicates that the property cannot be modified after it is set. \n\nThe class also defines a constructor that initializes an empty array called `list`. This array is used to keep track of all the `ComponentSystems` that have been added to the registry. \n\nThe class provides two methods for adding and removing `ComponentSystems` from the registry: `add` and `remove`. These methods take a `ComponentSystem` instance as an argument and add or remove it from the registry. If a `ComponentSystem` with the same name already exists in the registry, an error is thrown. \n\nFinally, the class provides a `destroy` method that removes all registered `ComponentSystems` from the registry and calls their `destroy` methods. \n\nOverall, the `ComponentSystemRegistry` class provides a centralized way to manage `ComponentSystems` in the PlayCanvas engine. It allows other parts of the engine to easily access and use `ComponentSystems` without having to worry about their implementation details. For example, if a developer wants to add a new `ComponentSystem` to the engine, they can simply create a new instance of the `ComponentSystem` class and add it to the registry using the `add` method. Other parts of the engine can then access this `ComponentSystem` using the appropriate property on the `ComponentSystemRegistry` instance.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `ComponentSystemRegistry` that stores, accesses, and deletes instances of various component systems used in the PlayCanvas engine.\n\n2. What are some examples of component systems that can be accessed through this registry?\n- Examples of component systems that can be accessed through this registry include `AnimComponentSystem`, `AudioListenerComponentSystem`, `ModelComponentSystem`, and `SpriteComponentSystem`.\n\n3. What methods are available for adding and removing component systems from the registry?\n- The `add` method adds a component system to the registry, while the `remove` method removes a component system from the registry. Both methods take a `system` parameter that represents the `ComponentSystem` instance to be added or removed.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/registry.md"}}],["308",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/render/data.js)\n\nThe code above defines a class called `RenderComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to rendering components in the engine. \n\nThe `constructor` method initializes two properties of the class: `enabled` and `rootBone`. The `enabled` property is a boolean value that determines whether the rendering component is enabled or disabled. The `rootBone` property is a reference to the root bone of the rendering component, which is used for skeletal animation.\n\nThis class can be used in other parts of the PlayCanvas engine project to store and manipulate data related to rendering components. For example, if a developer wants to create a new rendering component, they can create an instance of the `RenderComponentData` class and set its properties accordingly. \n\nHere is an example of how this class might be used in the PlayCanvas engine project:\n\n```\nimport { RenderComponentData } from 'playcanvas-engine';\n\nconst myRenderComponent = new RenderComponentData();\nmyRenderComponent.enabled = true;\nmyRenderComponent.rootBone = 'myRootBone';\n```\n\nIn this example, a new instance of the `RenderComponentData` class is created and its `enabled` and `rootBone` properties are set. This instance can then be used to create a new rendering component in the engine. \n\nOverall, the `RenderComponentData` class is an important part of the PlayCanvas engine project as it allows developers to store and manipulate data related to rendering components.\n## Questions: \n 1. **What is the purpose of the `RenderComponentData` class?** \nThe `RenderComponentData` class is likely used to store data related to rendering components in the PlayCanvas engine.\n\n2. **What does the `enabled` property do?** \nThe `enabled` property is a boolean value that determines whether the component is enabled or disabled.\n\n3. **What is the `rootBone` property used for?** \nThe `rootBone` property is likely used to store a reference to the root bone of a skeletal animation, if applicable.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/render/data.md"}}],["309",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/render/system.js)\n\nThe code defines a `RenderComponentSystem` class that extends the `ComponentSystem` class. This class allows an entity to render a mesh or a primitive shape like a box, capsule, sphere, cylinder, cone, etc. It initializes the component data and clones the component. \n\nThe `RenderComponentSystem` class has a constructor that takes an `app` parameter and sets the `id`, `ComponentType`, `DataType`, `schema`, and `defaultMaterial` properties. It also registers an event listener for the `beforeremove` event. \n\nThe `initializeComponentData` method initializes the component data by setting the `batchGroupId` property to `-1` if it is not defined. It duplicates the layer list if it exists. It then iterates over the `_properties` array and sets the component properties if they exist in the data. If the `aabbCenter` and `aabbHalfExtents` properties exist, it creates a custom AABB (Axis-Aligned Bounding Box) for the component. Finally, it calls the `initializeComponentData` method of the `ComponentSystem` class. \n\nThe `cloneComponent` method clones the component by copying its properties and creating a new component with the copied data. It then clones the mesh instances and assigns materials to them. If the original component has a custom AABB, it clones it and assigns it to the new component. \n\nThe code also imports several modules, including `Vec3`, `BoundingBox`, `getDefaultMaterial`, `Component`, `ComponentSystem`, `RenderComponent`, and `RenderComponentData`. It defines a `_schema` array that contains the component schema and an `_properties` array that contains the component properties. \n\nOverall, this code provides a system for rendering meshes and primitive shapes in the PlayCanvas engine. It allows developers to create and manipulate render components for entities in their projects.\n## Questions: \n 1. What is the purpose of this code file?\n    \n    This code file is a module for the PlayCanvas engine that allows an entity to render a mesh or a primitive shape like a box, capsule, sphere, cylinder, cone, etc.\n\n2. What properties can be set for a RenderComponent?\n    \n    The properties that can be set for a RenderComponent include material, meshInstances, asset, materialAssets, castShadows, receiveShadows, castShadowsLightmap, lightmapped, lightmapSizeMultiplier, renderStyle, type, layers, isStatic, and batchGroupId.\n\n3. How does the cloneComponent method work?\n    \n    The cloneComponent method copies the properties of an entity's RenderComponent to a new entity's RenderComponent, removes the mesh instances, manually clones them later, and assigns materials to the new mesh instances. If the original entity has a customAabb, it is also cloned and assigned to the new entity's RenderComponent.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/render/system.md"}}],["310",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/rigid-body/constants.js)\n\nThis code defines various constants related to rigid body physics simulation in the PlayCanvas engine. Rigid bodies are objects in the simulation that have mass and can interact with other objects through forces and collisions. The constants defined in this file are used to specify the type of rigid body, collision flags, activation states, groups, and masks.\n\nThe `BODYTYPE_STATIC` constant is used to specify a rigid body that has infinite mass and cannot move. This type of body is typically used for static objects in the scene, such as walls or floors.\n\nThe `BODYTYPE_DYNAMIC` constant is used to specify a rigid body that is simulated according to applied forces. This type of body can move and interact with other objects in the scene.\n\nThe `BODYTYPE_KINEMATIC` constant is used to specify a rigid body that has infinite mass and does not respond to forces but can still be moved by setting its velocity or position. This type of body is typically used for objects that are controlled by the player or the game logic.\n\nThe `BODYFLAG_STATIC_OBJECT`, `BODYFLAG_KINEMATIC_OBJECT`, and `BODYFLAG_NORESPONSE_OBJECT` constants are used to specify collision flags for rigid bodies. These flags determine how the body should respond to collisions with other objects in the scene.\n\nThe `BODYSTATE_ACTIVE_TAG`, `BODYSTATE_ISLAND_SLEEPING`, `BODYSTATE_WANTS_DEACTIVATION`, `BODYSTATE_DISABLE_DEACTIVATION`, and `BODYSTATE_DISABLE_SIMULATION` constants are used to specify activation states for rigid bodies. These states determine whether the body is active and being simulated by the physics engine.\n\nThe `BODYGROUP_*` constants are used to specify groups that rigid bodies can belong to. These groups are used to filter which objects can interact with each other in the simulation. For example, objects in the `BODYGROUP_STATIC` group may only interact with objects in the `BODYGROUP_DYNAMIC` group.\n\nThe `BODYMASK_*` constants are used to specify masks that define which groups of objects a rigid body can collide with. For example, a body with a mask of `BODYMASK_STATIC` will only collide with objects in the `BODYGROUP_STATIC` group.\n\nOverall, this code provides a set of constants that can be used throughout the PlayCanvas engine to define and control the behavior of rigid bodies in the physics simulation. For example, when creating a new rigid body, the developer can specify its type using one of the `BODYTYPE_*` constants, and its collision flags using the `BODYFLAG_*` constants. Similarly, when defining collision detection rules, the developer can use the `BODYGROUP_*` and `BODYMASK_*` constants to filter which objects should interact with each other.\n## Questions: \n 1. What are the different types of rigid body available in this engine?\n- There are three types of rigid body available: `BODYTYPE_STATIC`, `BODYTYPE_DYNAMIC`, and `BODYTYPE_KINEMATIC`.\n\n2. What are the different activation states of a rigid body?\n- There are five different activation states of a rigid body: `BODYSTATE_ACTIVE_TAG`, `BODYSTATE_ISLAND_SLEEPING`, `BODYSTATE_WANTS_DEACTIVATION`, `BODYSTATE_DISABLE_DEACTIVATION`, and `BODYSTATE_DISABLE_SIMULATION`.\n\n3. What are the different body groups and masks available in this engine?\n- There are multiple body groups and masks available in this engine, including `BODYGROUP_DEFAULT`, `BODYGROUP_DYNAMIC`, `BODYGROUP_STATIC`, `BODYGROUP_KINEMATIC`, and many more. These can be used to filter which objects interact with each other.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/rigid-body/constants.md"}}],["311",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/rigid-body/data.js)\n\nThe code above defines a class called `RigidBodyComponentData` and exports it for use in other parts of the PlayCanvas engine project. This class is used to store data related to rigid body components, which are used to simulate physics in 3D environments.\n\nThe `constructor` method initializes a property called `enabled` to `true`. This property is used to determine whether or not the rigid body component is active and should be included in physics calculations. By default, all rigid body components are enabled.\n\nThis class is likely used in conjunction with other classes and methods in the PlayCanvas engine to create and manipulate rigid body components. For example, a developer might use this class to create a new rigid body component and set its properties:\n\n```\nimport { RigidBodyComponentData } from 'playcanvas';\n\nconst rigidBodyData = new RigidBodyComponentData();\nrigidBodyData.enabled = false;\n\n// create a new rigid body component using the data\nconst rigidBodyComponent = new pc.RigidBodyComponent({\n    data: rigidBodyData\n});\n```\n\nIn this example, a new `RigidBodyComponentData` object is created and its `enabled` property is set to `false`. This data is then used to create a new `pc.RigidBodyComponent` object, which is added to a 3D entity in the scene. By setting the `enabled` property to `false`, the rigid body component will not be included in physics calculations until it is explicitly enabled.\n\nOverall, the `RigidBodyComponentData` class is an important part of the PlayCanvas engine's physics system, allowing developers to create and manipulate rigid body components with ease.\n## Questions: \n 1. **What is the purpose of this class?** \nThis class is a data component for a rigid body in the PlayCanvas engine. It contains information about whether the rigid body is enabled or not.\n\n2. **What other properties or methods does this class have?** \nBased on this code snippet, it appears that this class only has one property called `enabled` and a constructor method.\n\n3. **How is this class used in the PlayCanvas engine?** \nWithout additional context, it is unclear how this class is used in the PlayCanvas engine. It is possible that it is used as a component for a game object that has a rigid body, but more information is needed to confirm this.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/rigid-body/data.md"}}],["312",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/screen/component.js)\n\nThe code defines a class called `ScreenComponent` that extends the `Component` class. The purpose of this class is to enable an entity to render child `ElementComponent`s using anchors and positions in the `ScreenComponent`'s space. This is useful for creating 2D user interfaces. \n\nThe `ScreenComponent` class has several properties and methods that allow for customization of the screen space. The `resolution` property sets the width and height of the `ScreenComponent`. When `screenSpace` is set to true, the resolution will always be equal to the width and height of the graphics device. The `referenceResolution` property sets the resolution that the `ScreenComponent` is designed for. This is only taken into account when `screenSpace` is true and `scaleMode` is set to `SCALEMODE_BLEND`. If the actual resolution is different, then the `ScreenComponent` will be scaled according to the `scaleBlend` value. \n\nThe `scaleMode` property can either be `SCALEMODE_NONE` or `SCALEMODE_BLEND`. When set to `SCALEMODE_BLEND`, the `scaleBlend` property is used to scale the `ScreenComponent` with width as a reference (when `scaleBlend` is 0), the height as a reference (when `scaleBlend` is 1), or anything in between. \n\nThe `priority` property determines the order in which `ScreenComponent`s in the same layer are rendered. The number must be an integer between 0 and 255. Priority is set into the top 8 bits of the `drawOrder` property in an element. \n\nThe `ScreenComponent` class also has several methods. The `syncDrawOrder()` method sets the `drawOrder` of each child `ElementComponent` so that `ElementComponents` which are last in the hierarchy are rendered on top. The `onRemove()` method removes all events and fires an internal event after all screen hierarchy is synced. \n\nOverall, the `ScreenComponent` class is an important part of the PlayCanvas engine project as it enables the creation of 2D user interfaces. It provides a way to customize the screen space and set the order in which `ScreenComponent`s are rendered.\n## Questions: \n 1. What is the purpose of the `ScreenComponent` class?\n- The `ScreenComponent` class enables an entity to render child `ElementComponent`s using anchors and positions in the `ScreenComponent`'s space, and is used to create 2D user interfaces.\n\n2. What is the significance of the `scaleMode` property?\n- The `scaleMode` property can either be `SCALEMODE_NONE` or `SCALEMODE_BLEND`, and determines how the `ScreenComponent` is scaled when the actual resolution is different from the reference resolution. If `scaleMode` is `SCALEMODE_BLEND`, the `scaleBlend` property is used to determine how the `ScreenComponent` is scaled.\n\n3. What is the purpose of the `syncDrawOrder` method?\n- The `syncDrawOrder` method sets the draw order of each child `ElementComponent` so that the ones that are last in the hierarchy are rendered on top. The draw order sync is queued and will be updated by the next update loop.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/screen/component.md"}}],["313",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/screen/constants.js)\n\nThis code defines two constants, `SCALEMODE_NONE` and `SCALEMODE_BLEND`, which are used to determine how the `ScreenComponent` should be scaled in relation to the application's resolution. \n\nThe `ScreenComponent` is a component in the PlayCanvas engine that represents a 2D screen or UI element. It contains properties such as width, height, and resolution, which determine how it is displayed on the screen. \n\nThe `SCALEMODE_NONE` constant indicates that the `ScreenComponent` should always use the application's resolution as its own resolution. This means that the `ScreenComponent` will not be scaled up or down to fit the screen, but will instead be displayed at its original size. \n\nThe `SCALEMODE_BLEND` constant indicates that the `ScreenComponent` should be scaled when the application's resolution is different than the `ScreenComponent`'s reference resolution. The reference resolution is the resolution at which the `ScreenComponent` was designed to be displayed. If the application's resolution is higher or lower than the reference resolution, the `ScreenComponent` will be scaled up or down to fit the screen. \n\nThese constants can be used when creating or modifying a `ScreenComponent` to determine how it should be displayed on the screen. For example, if a developer wants a UI element to always be displayed at its original size, they would set the `scaleMode` property of the `ScreenComponent` to `SCALEMODE_NONE`. If they want the UI element to be scaled to fit the screen, they would set the `scaleMode` property to `SCALEMODE_BLEND`. \n\nOverall, this code provides a simple way for developers to control how their UI elements are displayed on the screen in relation to the application's resolution.\n## Questions: \n 1. What is the purpose of the `ScreenComponent` in the PlayCanvas engine?\n   - The `ScreenComponent` is used to manage the resolution of the application's screen.\n\n2. What is the difference between `SCALEMODE_NONE` and `SCALEMODE_BLEND`?\n   - `SCALEMODE_NONE` means that the resolution of the `ScreenComponent` will always match the application's resolution, while `SCALEMODE_BLEND` means that the `ScreenComponent` will be scaled if the application's resolution is different than the `ScreenComponent`'s reference resolution.\n\n3. How can a developer implement these scale modes in their code?\n   - The developer can use the constants `SCALEMODE_NONE` and `SCALEMODE_BLEND` to set the scale mode for the `ScreenComponent` in their code.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/screen/constants.md"}}],["314",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/screen/data.js)\n\nThe code above defines a class called `ScreenComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to a screen component, which is a visual element that can be added to a game or application. \n\nThe `constructor` method initializes a property called `enabled` to `true`. This property determines whether the screen component is currently enabled or disabled. By default, all screen components are enabled when they are created. \n\nDevelopers can use this class to create and manage screen components in their projects. For example, they might create a new instance of `ScreenComponentData` for each screen component they add to their game or application. They can then set the `enabled` property to `false` to disable a screen component when it is not needed. \n\nHere is an example of how a developer might use this class in their code:\n\n```\nimport { ScreenComponentData } from 'playcanvas-engine';\n\n// Create a new screen component\nconst myScreenComponent = new ScreenComponentData();\n\n// Disable the screen component\nmyScreenComponent.enabled = false;\n```\n\nIn summary, the `ScreenComponentData` class provides a simple way for developers to store and manage data related to screen components in their projects. By using this class, they can easily enable or disable screen components as needed, which can help improve performance and reduce visual clutter in their games or applications.\n## Questions: \n 1. What is the purpose of the ScreenComponentData class?\n   - The ScreenComponentData class is likely used to store data related to a screen component in the PlayCanvas engine.\n\n2. What does the `enabled` property do?\n   - The `enabled` property is a boolean value that determines whether the screen component is enabled or disabled.\n\n3. How is this code used within the PlayCanvas engine?\n   - It is unclear how this code is used within the PlayCanvas engine without further context or documentation.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/screen/data.md"}}],["315",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/screen/system.js)\n\nThe `ScreenComponentSystem` class is responsible for managing the creation of `ScreenComponent`s. It extends the `ComponentSystem` class and is used to create instances of `ScreenComponent` and `ScreenComponentData`. \n\nWhen a new instance of `ScreenComponentSystem` is created, it sets the `id` property to `'screen'`, which is used to identify the system. It also sets the `ComponentType` and `DataType` properties to `ScreenComponent` and `ScreenComponentData`, respectively. The `schema` property is set to `['enabled']`, which defines the properties that can be set on a `ScreenComponent`.\n\nThe `ScreenComponentSystem` class also creates a new `Vec2` object called `windowResolution`, which is used to store the current window resolution. It also creates a new `IndexedList` object called `_drawOrderSyncQueue`, which is used to store a queue of callbacks.\n\nThe `initializeComponentData` method is used to initialize the properties of a `ScreenComponent`. It sets the `priority`, `screenSpace`, `scaleMode`, `scaleBlend`, `resolution`, and `referenceResolution` properties of the component. It also calls the `syncDrawOrder` method of the component to queue up a draw order sync.\n\nThe `destroy` method is used to destroy the `ScreenComponentSystem` instance. It removes event listeners that were added in the constructor.\n\nThe `_onUpdate` method is called on every update of the system. It loops through all the `ScreenComponent`s and calls the `update` method of the `screen` property of the entity if it exists.\n\nThe `_onResize` method is called when the window is resized. It updates the `windowResolution` property with the new width and height.\n\nThe `cloneComponent` method is used to clone a `ScreenComponent`. It creates a new `ScreenComponent` and sets its properties to the same values as the original component.\n\nThe `onRemoveComponent` method is called when a `ScreenComponent` is removed from an entity. It calls the `onRemove` method of the component.\n\nThe `processDrawOrderSyncQueue` method is used to process the draw order sync queue. It loops through the queue and calls each callback.\n\nThe `queueDrawOrderSync` method is used to queue up a draw order sync. It adds a new item to the `_drawOrderSyncQueue` with a callback and a scope. If the queue is empty, it adds an event listener to the `prerender` event to process the queue.\n\nOverall, the `ScreenComponentSystem` class is an important part of the PlayCanvas engine project. It manages the creation and initialization of `ScreenComponent`s, which are used to render UI elements on the screen. It also provides methods for syncing the draw order of UI elements.\n## Questions: \n 1. What is the purpose of the `ScreenComponentSystem` class?\n    \n    The `ScreenComponentSystem` class manages the creation of `ScreenComponent`s and handles their initialization, updates, and removal.\n\n2. What events is the `ScreenComponentSystem` class listening to?\n    \n    The `ScreenComponentSystem` class is listening to the `resizecanvas` event on the graphics device and the `update` event on the systems.\n\n3. What is the purpose of the `queueDrawOrderSync` method?\n    \n    The `queueDrawOrderSync` method queues up a draw order sync for a `ScreenComponent` and attaches an event listener to process the sync queue before the next frame is rendered.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/screen/system.md"}}],["316",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/script/data.js)\n\nThe code above defines a class called `ScriptComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to a script component, which is a component that allows developers to attach custom scripts to entities in the game world.\n\nThe `constructor` method initializes a property called `enabled` to `true`. This property determines whether the script component is enabled or disabled. When a script component is disabled, its associated script will not be executed.\n\nDevelopers can use this class to create instances of script components and set their `enabled` property as needed. For example, if a developer wants to create a script component that is initially disabled, they can create a new instance of `ScriptComponentData` and set its `enabled` property to `false`:\n\n```\nconst myScriptComponent = new ScriptComponentData();\nmyScriptComponent.enabled = false;\n```\n\nThis class is likely used in conjunction with other classes and components in the PlayCanvas engine to create complex game logic and behavior. By allowing developers to attach custom scripts to entities, the engine provides a high degree of flexibility and customization for game development.\n## Questions: \n 1. **What is the purpose of this class?** \nA smart developer might want to know what functionality this class provides and how it fits into the overall architecture of the PlayCanvas engine. Based on the name and code, it appears to be a data structure for storing information about a script component.\n\n2. **What is the significance of the `enabled` property?** \nA smart developer might want to know how the `enabled` property is used and what effect it has on the behavior of the script component. Based on the code, it appears that the `enabled` property determines whether the script component is active or not.\n\n3. **How is this class used in the PlayCanvas engine?** \nA smart developer might want to know where this class is used and how it is instantiated. Based on the `export` statement, it appears that this class is intended to be used by other modules within the PlayCanvas engine. The developer would need to look at other parts of the codebase to see how it is used.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/script/data.md"}}],["317",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/script/system.js)\n\nThe `ScriptComponentSystem` class is a system that allows scripts to be attached to an entity and executed. It is a subclass of `ComponentSystem` and is part of the PlayCanvas engine project. \n\nThe `ScriptComponentSystem` class has a constructor that initializes the system and sets up event listeners for various system events. It also defines two arrays, `_components` and `_enabledComponents`, which hold all the script components and enabled script components respectively. The `initializeComponentData` method is called when a new script component is added to an entity. It sets the execution order of the component and adds it to the `_components` array. If the component is enabled, it is also added to the `_enabledComponents` array. The `cloneComponent` method is called when an entity is cloned. It clones the script components of the original entity and adds them to the cloned entity. \n\nThe `ScriptComponentSystem` class also defines several methods that are called during different stages of the application lifecycle. The `_onInitialize` method is called during the `initialize` event and initializes the attributes of all components and calls the `onInitialize` method of all enabled components. The `_onPostInitialize` method is called during the `postInitialize` event and calls the `onPostInitialize` method of all enabled components. The `_onUpdate` method is called during the `update` event and calls the `onUpdate` method of all enabled components. The `_onPostUpdate` method is called during the `postUpdate` event and calls the `onPostUpdate` method of all enabled components. \n\nThe `ScriptComponentSystem` class also defines several helper methods that are used to add and remove components from the `_enabledComponents` array. \n\nOverall, the `ScriptComponentSystem` class is an important part of the PlayCanvas engine project as it allows scripts to be attached to entities and executed. It provides a way for developers to add custom behavior to their entities and create complex game logic.\n## Questions: \n 1. What is the purpose of the `ScriptComponentSystem` class?\n- The `ScriptComponentSystem` class allows scripts to be attached to an Entity and executed.\n\n2. What is the significance of the `executionOrderCounter` variable?\n- The `executionOrderCounter` variable is an ever-increasing integer used as the execution order of new script components. It is used instead of the order of the script component in the components array because if components are removed from the array, the execution order for all subsequent script components in the array would have to be re-calculated every time, which would be slow.\n\n3. What is the purpose of the `cloneComponent` method?\n- The `cloneComponent` method clones a script component from one entity to another. It creates a new component with the same scripts and attributes as the original component and adds it to the new entity.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/script/system.md"}}],["318",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/script-legacy/component.js)\n\nThe code defines a class called `ScriptLegacyComponent` that extends the `Component` class. This class is used to manage scripts attached to an entity in the PlayCanvas engine. \n\nThe `ScriptLegacyComponent` class has several methods that are used to load, update, and manage scripts. The `onSetScripts` method is called when the scripts attached to an entity are changed. This method first checks if the scripts have already been loaded. If they have, it updates the script attributes and initializes the script component. If not, it loads the scripts asynchronously using the resource loader. \n\nThe `_loadFromCache` method is used to load scripts synchronously from the cache. If a script is not found in the cache, it returns false and loads all scripts with the resource loader. The `_loadScripts` method is used to load scripts asynchronously using the resource loader. \n\nThe `onEnable` method is called when the component is enabled. If the scripts have been loaded and the system is not preloading, it initializes and enables the script component. The `onDisable` method is called when the component is disabled. It disables the script component. \n\nThe `send` method is deprecated and will be removed soon. It is used to send messages between scripts. \n\nOverall, the `ScriptLegacyComponent` class is an important part of the PlayCanvas engine as it manages scripts attached to entities. It provides methods for loading, updating, and managing scripts, making it easier for developers to work with scripts in their projects. \n\nExample usage:\n\n```\n// create an entity\nconst entity = new pc.Entity();\n\n// add a script component to the entity\nentity.addComponent('script', {\n    scripts: [\n        { url: 'path/to/script1.js' },\n        { url: 'path/to/script2.js' }\n    ]\n});\n\n// get the script component\nconst scriptComponent = entity.script;\n\n// check if the scripts have been loaded\nif (scriptComponent.areScriptsLoaded) {\n    // initialize the script component\n    scriptComponent.system._initializeScriptComponent(scriptComponent);\n}\n```\n## Questions: \n 1. What is the purpose of the `ScriptLegacyComponent` class?\n- The `ScriptLegacyComponent` class is a subclass of the `Component` class and is used to manage scripts attached to an entity in the PlayCanvas engine.\n\n2. What is the `send` method used for?\n- The `send` method is deprecated and will be removed soon. It is used to call a function on a script instance attached to the entity.\n\n3. What is the difference between `onEnable` and `onDisable` methods?\n- The `onEnable` method is called when the component is enabled and is used to initialize and enable the script component. The `onDisable` method is called when the component is disabled and is used to disable and destroy the script component.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/script-legacy/component.md"}}],["319",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/script-legacy/data.js)\n\nThe code defines a class called `ScriptLegacyComponentData` which is used to store data related to scripts in the PlayCanvas engine. The class has a constructor that initializes several properties. \n\nThe `scripts` property is an array that stores the names of all the scripts attached to an entity. The `enabled` property is a boolean that determines whether the scripts are currently enabled or not. \n\nThe `instances` and `_instances` properties are objects that store instances of the scripts. The `instances` object is used to store instances of the scripts that are currently running, while the `_instances` object is used to store instances of the scripts that have been disabled. \n\nThe `runInTools` property is a boolean that determines whether the scripts should be run in the PlayCanvas editor or not. The `attributes` property is an object that stores the attributes of the scripts. The `initialized` and `postInitialized` properties are booleans that determine whether the scripts have been initialized and post-initialized respectively. \n\nThe `areScriptsLoaded` property is a boolean that determines whether the scripts have been loaded or not. \n\nThis class is likely used in the larger PlayCanvas engine project to manage scripts attached to entities. It provides a way to store information about the scripts, including their names, instances, and attributes. This information can be used to enable or disable scripts, run them in the editor, and initialize them. \n\nExample usage:\n\n```javascript\n// create a new instance of the ScriptLegacyComponentData class\nconst scriptData = new ScriptLegacyComponentData();\n\n// add a script to the scripts array\nscriptData.scripts.push('myScript');\n\n// disable the scripts\nscriptData.enabled = false;\n\n// set the runInTools property to true\nscriptData.runInTools = true;\n\n// set an attribute for the script\nscriptData.attributes.myAttribute = 'myValue';\n```\n## Questions: \n 1. **What is the purpose of this class?**\n    \n    This class is a data structure for storing information related to legacy scripts in a PlayCanvas component.\n\n2. **What are the differences between the serialized and non-serialized properties?**\n    \n    The `scripts` and `enabled` properties are serialized, meaning they are saved and loaded with the component. The `instances`, `_instances`, `runInTools`, `attributes`, `initialized`, `postInitialized`, and `areScriptsLoaded` properties are not serialized and are used for internal purposes.\n\n3. **What is the significance of the `runInTools` property?**\n    \n    The `runInTools` property is a boolean flag that determines whether the scripts associated with this component should be executed in the PlayCanvas editor tools.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/script-legacy/data.md"}}],["320",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/scroll-view/constants.js)\n\nThis code defines several constants related to scrolling behavior and scrollbar visibility. These constants are likely used throughout the PlayCanvas engine to provide a consistent way of referring to these different modes and options.\n\nThe first three constants (`SCROLL_MODE_CLAMP`, `SCROLL_MODE_BOUNCE`, and `SCROLL_MODE_INFINITE`) define different modes of scrolling behavior. `SCROLL_MODE_CLAMP` indicates that content should not scroll beyond its bounds, while `SCROLL_MODE_BOUNCE` allows content to scroll past its bounds and then gently bounce back. `SCROLL_MODE_INFINITE` indicates that content can scroll forever.\n\nThe next two constants (`SCROLLBAR_VISIBILITY_SHOW_ALWAYS` and `SCROLLBAR_VISIBILITY_SHOW_WHEN_REQUIRED`) define different options for when the scrollbar should be visible. `SCROLLBAR_VISIBILITY_SHOW_ALWAYS` indicates that the scrollbar should always be visible, while `SCROLLBAR_VISIBILITY_SHOW_WHEN_REQUIRED` indicates that the scrollbar should only be visible when the content exceeds the size of the viewport.\n\nThese constants are likely used throughout the PlayCanvas engine to provide a consistent way of referring to these different modes and options. For example, a component that allows scrolling may use these constants to allow the user to select the desired scrolling behavior and scrollbar visibility. \n\nHere is an example of how these constants might be used in a PlayCanvas script:\n\n```\nimport { SCROLL_MODE_CLAMP, SCROLLBAR_VISIBILITY_SHOW_ALWAYS } from 'playcanvas-engine';\n\n// Create a new scrolling component with clamped scrolling and always-visible scrollbar\nconst scrollingComponent = new ScrollingComponent({\n  scrollMode: SCROLL_MODE_CLAMP,\n  scrollbarVisibility: SCROLLBAR_VISIBILITY_SHOW_ALWAYS\n});\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines constants for different scroll modes and scrollbar visibility options.\n\n2. How can these constants be used in the PlayCanvas engine?\n- These constants can be used as parameters for functions or properties in the PlayCanvas engine that involve scrolling or scrollbar visibility.\n\n3. Are there any other scroll modes or scrollbar visibility options available in the PlayCanvas engine?\n- It is unclear from this code whether there are additional scroll modes or scrollbar visibility options available in the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/scroll-view/constants.md"}}],["321",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/scroll-view/data.js)\n\nThe code above defines a class called `ScrollViewComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to a scroll view component, which is a UI element that allows users to scroll through a larger content area. \n\nThe `constructor` method initializes the `enabled` property to `true`, indicating that the scroll view component is enabled by default. This property can be modified later to disable the component if needed. \n\nThis class can be used in conjunction with other classes and methods in the PlayCanvas engine to create and manipulate scroll view components. For example, a developer could create a new instance of `ScrollViewComponentData` and pass it as an argument to a method that creates a new scroll view component. \n\n```\nimport { ScrollViewComponentData } from 'playcanvas-engine';\n\nconst scrollViewData = new ScrollViewComponentData();\ncreateScrollView(scrollViewData);\n```\n\nOverall, this code provides a simple and reusable way to store data related to scroll view components in the PlayCanvas engine project.\n## Questions: \n 1. **What is the purpose of the ScrollViewComponentData class?** \n   \n   The ScrollViewComponentData class is likely used to store data related to a scroll view component in the PlayCanvas engine.\n\n2. **What does the `enabled` property do?**\n   \n   The `enabled` property is a boolean value that determines whether the scroll view component is enabled or disabled.\n\n3. **How is this code used in the PlayCanvas engine?**\n   \n   This code is likely used as a data structure for the ScrollViewComponent in the PlayCanvas engine, allowing developers to enable or disable the component as needed.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/scroll-view/data.md"}}],["322",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/scroll-view/system.js)\n\nThe code defines a `ScrollViewComponentSystem` class that manages the creation of `ScrollViewComponent`s. This class extends the `ComponentSystem` class, which is a base class for all component systems in the PlayCanvas engine. \n\nThe `ScrollViewComponent` is a UI component that provides a scrollable view of content within a fixed-size viewport. The `ScrollViewComponentSystem` class is responsible for creating and managing instances of this component. \n\nThe `_schema` constant defines the properties that can be set on a `ScrollViewComponent`. These properties include `enabled`, `horizontal`, `vertical`, `scrollMode`, `bounceAmount`, `friction`, `dragThreshold`, `useMouseWheel`, `mouseWheelSensitivity`, `horizontalScrollbarVisibility`, `verticalScrollbarVisibility`, `viewportEntity`, `contentEntity`, `horizontalScrollbarEntity`, and `verticalScrollbarEntity`. \n\nThe `DEFAULT_DRAG_THRESHOLD` constant sets the default value for the `dragThreshold` property. \n\nThe `ScrollViewComponentSystem` constructor sets the `id`, `ComponentType`, `DataType`, and `schema` properties of the class. It also registers an event listener for the `beforeremove` event and the `update` event of the `app.systems` object. \n\nThe `initializeComponentData` method initializes the data for a `ScrollViewComponent`. It sets default values for the `dragThreshold`, `useMouseWheel`, and `mouseWheelSensitivity` properties if they are not already defined. \n\nThe `onUpdate` method is called every frame and updates all `ScrollViewComponent`s that are enabled and attached to enabled entities. \n\nThe `_onRemoveComponent` method is called when a `ScrollViewComponent` is removed from an entity. \n\nThe `destroy` method removes the `update` event listener. \n\nFinally, the `Component._buildAccessors` method is called to create getter and setter methods for the properties defined in the `_schema` constant. \n\nOverall, the `ScrollViewComponentSystem` class provides a way to create and manage `ScrollViewComponent`s in the PlayCanvas engine. Developers can use this component to create scrollable views of content within their applications.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file manages the creation of ScrollViewComponent instances in the PlayCanvas engine.\n\n2. What is the significance of the _schema variable?\n- The _schema variable defines the properties and their types for ScrollViewComponent instances.\n\n3. What does the onUpdate method do?\n- The onUpdate method updates all enabled ScrollViewComponent instances in the store by calling their respective onUpdate methods.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/scroll-view/system.md"}}],["323",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/scrollbar/component.js)\n\nThe `ScrollbarComponent` is a class that enables a group of entities to behave like a draggable scrollbar. It is part of the PlayCanvas engine project and is used to create scrollbars in 2D and 3D applications. \n\nThe `ScrollbarComponent` has several properties that can be set to customize its behavior. The `orientation` property determines whether the scrollbar moves horizontally or vertically. It can be set to `ORIENTATION_HORIZONTAL` or `ORIENTATION_VERTICAL`, and it defaults to `ORIENTATION_HORIZONTAL`. The `value` property determines the current position value of the scrollbar, in the range 0 to 1, and it defaults to 0. The `handleSize` property determines the size of the handle relative to the size of the track, in the range 0 to 1. For a vertical scrollbar, a value of 1 means that the handle will take up the full height of the track. The `handleEntity` property determines the entity to be used as the scrollbar handle. This entity must have a `Scrollbar` component.\n\nThe `ScrollbarComponent` class extends the `Component` class and has several methods that are used to handle events and update the scrollbar's position and size. The `_onHandleElementGain()` method is called when the scrollbar handle is added to the scene. It creates a new `ElementDragHelper` object that is used to handle drag events on the scrollbar handle. The `_onHandleElementLose()` method is called when the scrollbar handle is removed from the scene. It destroys the `ElementDragHelper` object. The `_onHandleDrag()` method is called when the scrollbar handle is dragged. It updates the scrollbar's value based on the position of the handle.\n\nThe `_onSetValue()`, `_onSetHandleSize()`, and `_onSetOrientation()` methods are called when the `value`, `handleSize`, and `orientation` properties are set, respectively. They update the scrollbar's position and size based on the new values. The `_updateHandlePositionAndSize()` method is called to update the position and size of the scrollbar handle based on the current value and handle size. The `_handlePositionToScrollValue()` and `_scrollValueToHandlePosition()` methods are used to convert between the position of the scrollbar handle and the scrollbar's value.\n\nThe `onEnable()`, `onDisable()`, and `onRemove()` methods are called when the scrollbar is enabled, disabled, or removed from the scene, respectively. They enable or disable the scrollbar handle's drag events and destroy the `ElementDragHelper` object.\n\nOverall, the `ScrollbarComponent` class provides a flexible and customizable way to create scrollbars in PlayCanvas applications.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a `ScrollbarComponent` class that enables a group of entities to behave like a draggable scrollbar.\n\n2. What properties can be set on a `ScrollbarComponent` instance?\n- A `ScrollbarComponent` instance can have properties such as `orientation`, `value`, `handleSize`, and `handleEntity`.\n\n3. What events can be fired by a `ScrollbarComponent` instance?\n- A `ScrollbarComponent` instance can fire a `set:value` event whenever the scroll value changes.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/scrollbar/component.md"}}],["324",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/scrollbar/data.js)\n\nThe code above defines a class called `ScrollbarComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to a scrollbar component, which can be used to create a scrollbar UI element in a PlayCanvas application.\n\nThe `ScrollbarComponentData` class has a constructor method that sets the `enabled` property to `true` by default. This property determines whether the scrollbar component is enabled or disabled in the application. If `enabled` is set to `false`, the scrollbar will not be visible or functional.\n\nThis class can be used in conjunction with other classes and methods in the PlayCanvas engine to create a fully functional scrollbar UI element. For example, a developer could use the `ScrollbarComponentData` class to create a new instance of a scrollbar component, set its properties (such as `enabled`), and then attach it to a UI element in the application using the `addComponent` method.\n\nHere is an example of how the `ScrollbarComponentData` class might be used in a PlayCanvas application:\n\n```\n// create a new entity to hold the scrollbar component\nconst entity = new pc.Entity();\n\n// create a new instance of the ScrollbarComponentData class\nconst scrollbarData = new ScrollbarComponentData();\n\n// set the enabled property to false\nscrollbarData.enabled = false;\n\n// add the scrollbar component to the entity\nentity.addComponent('scrollbar', {\n    data: scrollbarData\n});\n```\n\nIn this example, a new entity is created to hold the scrollbar component. A new instance of the `ScrollbarComponentData` class is created and its `enabled` property is set to `false`. Finally, the scrollbar component is added to the entity using the `addComponent` method, with the `data` parameter set to the `ScrollbarComponentData` instance.\n\nOverall, the `ScrollbarComponentData` class is a small but important part of the PlayCanvas engine project, providing a way to store data related to scrollbar UI elements in a PlayCanvas application.\n## Questions: \n 1. **What is the purpose of the ScrollbarComponentData class?** \nThe ScrollbarComponentData class is likely used to store data related to a scrollbar component in the PlayCanvas engine.\n\n2. **What does the `enabled` property do?** \nThe `enabled` property is a boolean value that determines whether the scrollbar component is enabled or disabled.\n\n3. **How is this code used within the PlayCanvas engine?** \nThis code is likely used as part of the PlayCanvas engine's implementation of scrollbar components, but more information would be needed to determine exactly how it fits into the engine's architecture.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/scrollbar/data.md"}}],["325",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/scrollbar/system.js)\n\nThe code defines a `ScrollbarComponentSystem` class that manages the creation of `ScrollbarComponent`s. This class extends the `ComponentSystem` class, which is a base class for all component systems in the PlayCanvas engine. \n\nThe `ScrollbarComponentSystem` class has a constructor that takes an instance of the `AppBase` class as a parameter. It sets the `id` property of the class to `'scrollbar'`, which is used to identify the component system. It also sets the `ComponentType` property to `ScrollbarComponent` and the `DataType` property to `ScrollbarComponentData`. These properties define the component and data types that the system manages. \n\nThe `_schema` constant is an array of objects that define the properties of the `ScrollbarComponent`. Each object has a `name` property that specifies the name of the property, and a `type` property that specifies the data type of the property. The `ScrollbarComponentSystem` class uses this schema to initialize the component data when a new component is created. \n\nThe `initializeComponentData` method initializes the data of a `ScrollbarComponent` instance. It takes a `component` parameter that is the component instance to initialize, a `data` parameter that is the data to initialize the component with, and a `properties` parameter that is an array of property names to initialize. The method calls the `initializeComponentData` method of the base `ComponentSystem` class, passing in the `component`, `data`, and `_schema` parameters. \n\nThe `_onRemoveComponent` method is called when a component is removed from an entity. It takes an `entity` parameter that is the entity the component is being removed from, and a `component` parameter that is the component being removed. The method calls the `onRemove` method of the `component` parameter. \n\nThe `ScrollbarComponentSystem` class also has a static method called `_buildAccessors` that is used to create getter and setter methods for the properties of the `ScrollbarComponent`. The method takes a `prototype` parameter that is the prototype of the `ScrollbarComponent` class, and a `_schema` parameter that is the schema of the `ScrollbarComponent`. \n\nOverall, the `ScrollbarComponentSystem` class is used to manage the creation and initialization of `ScrollbarComponent`s in the PlayCanvas engine. It provides methods for initializing component data and handling component removal. Developers can use this class to create and manage scrollbar components in their PlayCanvas projects.\n## Questions: \n 1. What is the purpose of this code file?\n    \n    This code file manages the creation of ScrollbarComponents in the PlayCanvas engine.\n\n2. What is the inheritance hierarchy for the ScrollbarComponentSystem class?\n    \n    The ScrollbarComponentSystem class inherits from the ComponentSystem class.\n\n3. What is the purpose of the _onRemoveComponent method?\n    \n    The _onRemoveComponent method is called when a component is removed from an entity and it calls the onRemove method of the component.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/scrollbar/system.md"}}],["326",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/sound/data.js)\n\nThe code above defines a class called `SoundComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to sound components in the engine. \n\nThe `constructor` method initializes a property called `enabled` to `true`. This property is used to determine whether or not a sound component is currently enabled or disabled. \n\nThis class can be used in conjunction with other parts of the PlayCanvas engine to create and manage sound components. For example, a sound component may be attached to a game object in the engine, and the `enabled` property of the `SoundComponentData` class can be used to control whether or not that sound component is currently playing. \n\nHere is an example of how this class might be used in the PlayCanvas engine:\n\n```\nimport { SoundComponentData } from 'playcanvas-engine';\n\n// create a new sound component data object\nconst soundData = new SoundComponentData();\n\n// disable the sound component\nsoundData.enabled = false;\n\n// check if the sound component is enabled\nif (soundData.enabled) {\n    // play the sound\n} else {\n    // do not play the sound\n}\n```\n\nIn this example, a new `SoundComponentData` object is created and its `enabled` property is set to `false`. Later in the code, the `enabled` property is checked to determine whether or not to play a sound. If the property is `true`, the sound will play, but if it is `false`, the sound will not play. \n\nOverall, the `SoundComponentData` class is an important part of the PlayCanvas engine's sound system, allowing developers to create and manage sound components in their projects.\n## Questions: \n 1. **What is the purpose of the SoundComponentData class?** \nThe SoundComponentData class is likely used to store data related to sound components in the PlayCanvas engine.\n\n2. **What does the `enabled` property do?** \nThe `enabled` property is a boolean value that determines whether the sound component is enabled or disabled.\n\n3. **How is this code used within the PlayCanvas engine?** \nIt is unclear how this code is used within the PlayCanvas engine without additional context. It may be used as a base class for other sound-related classes or components.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/sound/data.md"}}],["327",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/sound/system.js)\n\nThe code defines a class called `SoundComponentSystem` that manages the creation of `SoundComponent`s. This class extends the `ComponentSystem` class, which is a base class for all component systems in the PlayCanvas engine. \n\nThe `SoundComponentSystem` class has a constructor that takes an instance of the `AppBase` class as an argument. It sets the `id` property of the class to `'sound'`, and sets the `ComponentType` and `DataType` properties to `SoundComponent` and `SoundComponentData`, respectively. It also sets the `schema` property to `['enabled']`. \n\nThe `SoundComponentSystem` class has a `manager` property that gets and sets the sound manager. The sound manager is an instance of the `SoundManager` class, which is responsible for loading and playing sounds. The `SoundComponentSystem` class also has a `volume` property that gets and sets the volume for the entire sound system. \n\nThe `SoundComponentSystem` class has a `context` property that gets the `AudioContext` currently used by the sound manager. If the device does not support the Web Audio API, this property returns `null`. \n\nThe `SoundComponentSystem` class has an `onUpdate` method that is called every frame. This method updates the position of the sound if it is a 3D sound. \n\nThe `SoundComponentSystem` class has an `onBeforeRemove` method that is called before a sound component is removed. This method stops non-overlapping sounds and calls the `onRemove` method of the component. \n\nThe `SoundComponentSystem` class has a `destroy` method that removes the `onUpdate` event listener. \n\nOverall, the `SoundComponentSystem` class is responsible for managing the creation and removal of sound components, and for updating the position of 3D sounds. It also provides a way to get and set the volume of the entire sound system, and to get the `AudioContext` used by the sound manager. \n\nExample usage:\n\n```javascript\nconst app = new pc.Application();\nconst soundComponentSystem = new pc.SoundComponentSystem(app);\n\n// Set the volume of the sound system\nsoundComponentSystem.volume = 0.5;\n\n// Get the AudioContext used by the sound manager\nconst context = soundComponentSystem.context;\n```\n## Questions: \n 1. What is the purpose of this code file?\n- This code file manages the creation of SoundComponents and sets the volume for the entire Sound system.\n\n2. What is the significance of the `app.soundManager` property?\n- The `app.soundManager` property is used to get and set the sound manager for the SoundComponentSystem.\n\n3. What is the purpose of the `onUpdate` method?\n- The `onUpdate` method updates the position of the slot if this is a 3d sound and the component is enabled and positional.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/sound/system.md"}}],["328",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/sprite/constants.js)\n\nThis code defines two constants, `SPRITETYPE_SIMPLE` and `SPRITETYPE_ANIMATED`, which are used to identify two different types of `SpriteComponent` in the PlayCanvas engine. \n\nA `SpriteComponent` is a component that can be attached to an entity in the engine to render a 2D sprite. The `SPRITETYPE_SIMPLE` constant is used to identify a `SpriteComponent` that displays a single frame from a sprite asset. This type of `SpriteComponent` is useful for rendering static images, such as icons or logos. \n\nThe `SPRITETYPE_ANIMATED` constant is used to identify a `SpriteComponent` that renders sprite animations. This type of `SpriteComponent` is useful for rendering animated sprites, such as characters or objects that move or change over time. \n\nThese constants are likely used throughout the PlayCanvas engine to identify and differentiate between different types of `SpriteComponent`s. For example, they may be used in the engine's rendering system to determine how to render a particular `SpriteComponent`. \n\nHere is an example of how these constants might be used in code:\n\n```\nconst spriteType = SPRITETYPE_ANIMATED;\n\nif (spriteType === SPRITETYPE_SIMPLE) {\n  // render a simple sprite component\n} else if (spriteType === SPRITETYPE_ANIMATED) {\n  // render an animated sprite component\n}\n```\n\nOverall, this code is a small but important part of the PlayCanvas engine's functionality, allowing developers to easily identify and work with different types of `SpriteComponent`s.\n## Questions: \n 1. What is a `SpriteComponent` and how is it used in the PlayCanvas engine?\n   - A `SpriteComponent` is a component that displays a single frame or animation from a sprite asset. It can be used to render 2D graphics in the PlayCanvas engine.\n\n2. What is the difference between `SPRITETYPE_SIMPLE` and `SPRITETYPE_ANIMATED`?\n   - `SPRITETYPE_SIMPLE` is used for displaying a single frame from a sprite asset, while `SPRITETYPE_ANIMATED` is used for rendering sprite animations.\n\n3. Are there any other sprite types available in the PlayCanvas engine?\n   - The code provided only defines two sprite types (`SPRITETYPE_SIMPLE` and `SPRITETYPE_ANIMATED`), so it is unclear if there are any other sprite types available in the PlayCanvas engine. Further investigation may be necessary.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/sprite/constants.md"}}],["329",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/sprite/data.js)\n\nThe code above defines a class called `SpriteComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to sprite components, which are used to render 2D images in the engine.\n\nThe `constructor` method initializes a property called `enabled` to `true`. This property is used to determine whether or not the sprite component is currently active and should be rendered. By default, all sprite components are enabled when they are created.\n\nDevelopers working on the PlayCanvas engine project can use this class to create and manage sprite components in their games or applications. For example, they might create a new sprite component like this:\n\n```\nimport { SpriteComponentData } from 'playcanvas';\n\nconst spriteData = new SpriteComponentData();\n```\n\nThey can then set various properties on the `spriteData` object to customize the appearance and behavior of the sprite. For example, they might set the `enabled` property to `false` to temporarily hide the sprite:\n\n```\nspriteData.enabled = false;\n```\n\nOverall, the `SpriteComponentData` class is a simple but important part of the PlayCanvas engine's 2D rendering system. By providing a standardized way to store and manipulate sprite data, it helps developers create high-quality 2D graphics for their games and applications.\n## Questions: \n 1. **What is the purpose of the SpriteComponentData class?** \n    The SpriteComponentData class is likely used to store data related to sprite components in the PlayCanvas engine.\n\n2. **What does the `enabled` property do?** \n    The `enabled` property is a boolean value that determines whether the sprite component is enabled or disabled.\n\n3. **How is the SpriteComponentData class used in the PlayCanvas engine?** \n    It is unclear from this code snippet alone how the SpriteComponentData class is used in the PlayCanvas engine. Further investigation into other files and documentation may be necessary to determine its usage.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/sprite/data.md"}}],["330",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/sprite/system.js)\n\nThe `SpriteComponentSystem` class is responsible for managing the creation of `SpriteComponent`s in the PlayCanvas engine. It extends the `ComponentSystem` class and provides functionality for creating, updating, and destroying `SpriteComponent`s.\n\nThe `SpriteComponentSystem` class has several properties that are used to create and manage `SpriteComponent`s. These include the `defaultTexture`, `defaultMaterial`, `default9SlicedMaterialSlicedMode`, and `default9SlicedMaterialTiledMode`. These properties are used to create default textures and materials that are used by `SpriteComponent`s when no other texture or material is specified.\n\nThe `SpriteComponentSystem` class also has methods for initializing component data, cloning components, and updating components. The `initializeComponentData` method is used to initialize the data for a new `SpriteComponent`. The `cloneComponent` method is used to clone an existing `SpriteComponent`. The `onUpdate` method is called every frame to update the state of all `SpriteComponent`s.\n\nThe `SpriteComponentSystem` class is used in the larger PlayCanvas engine project to provide a way to create and manage 2D sprites. It is used by game developers to create and manage sprites in their games. The `SpriteComponent` class is used to represent a sprite and provides functionality for rendering, animating, and interacting with the sprite. The `SpriteComponentSystem` class is used to manage the creation and updating of `SpriteComponent`s, and provides default textures and materials that can be used by `SpriteComponent`s when no other texture or material is specified.\n\nExample usage:\n\n```javascript\nimport { SpriteComponentSystem } from 'playcanvas';\n\n// create a new sprite component system\nconst spriteSystem = new SpriteComponentSystem(app);\n\n// create a new sprite component\nconst spriteComponent = spriteSystem.addComponent(entity, {\n    spriteAsset: spriteAsset,\n    width: 100,\n    height: 100\n});\n\n// update the sprite component\nspriteComponent.width = 200;\nspriteComponent.height = 200;\n```\n## Questions: \n 1. What is the purpose of this code file?\n- This code file manages the creation of SpriteComponents in the PlayCanvas engine.\n\n2. What is the default material used by sprites?\n- The default material used by sprites is a StandardMaterial with a white emissive texture that can be tinted with an emissive color.\n\n3. What does the onUpdate method do?\n- The onUpdate method advances the current clip of enabled SpriteComponents by a given delta time.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/sprite/system.md"}}],["331",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/system.js)\n\nThe code defines a class called `ComponentSystem` which is used to manage and update components of a particular type. The class extends the `EventHandler` class, which allows it to handle events. The `ComponentSystem` class has several methods that can be used to add, remove, and clone components. \n\nWhen a new instance of the `ComponentSystem` class is created, it takes an instance of the `AppBase` class as a parameter. The `store` property is an object that stores all `ComponentData` objects. The `schema` property is an array that contains property descriptors for the component. \n\nThe `addComponent` method is used to create a new component and component data instances and attach them to an entity. The method takes an entity and an optional data object as parameters. The `ComponentType` and `DataType` properties are used to create new instances of the component and component data. The `initializeComponentData` method is called to initialize the component data in the store. The method takes the component, data, and properties as parameters. The `onEnable` method is called if the component is enabled and the entity is enabled. Finally, the `add` event is fired with the entity and component as parameters.\n\nThe `removeComponent` method is used to remove a component from an entity and delete the associated component data. The method takes an entity as a parameter. The `beforeremove` event is fired with the entity and component as parameters before the component is removed. Finally, the `remove` event is fired with the entity and record data as parameters.\n\nThe `cloneComponent` method is used to create a clone of a component. The method takes an entity and a clone entity as parameters. The `addComponent` method is called with the clone entity and the data from the original entity.\n\nThe `initializeComponentData` method is called during the `addComponent` method to initialize the component data in the store. The method takes the component, data, and properties as parameters. The method loops through the properties and initializes them with the data or the default value from the component data.\n\nThe `getPropertiesOfType` method is used to search the component schema for properties that match the specified type. The method takes a type as a parameter and returns an array of property descriptors matching the specified type.\n\nThe `convertValue` function is used to convert raw data into an instance of the specified type. The function takes a value and a type as parameters. The function checks the type and returns a new instance of the specified type with the values from the raw data.\n\nOverall, the `ComponentSystem` class is an important part of the PlayCanvas engine project as it manages and updates components of a particular type. The class can be used to add, remove, and clone components, as well as search for properties that match a specified type.\n## Questions: \n 1. What is the purpose of the `ComponentSystem` class?\n- The `ComponentSystem` class contains the logic and functionality to update all components of a particular type.\n\n2. What parameters does the `addComponent` method take and what does it return?\n- The `addComponent` method takes an `entity` parameter of type `Entity` and an optional `data` parameter of type `object`. It returns a `Component` of the type defined by the component system.\n\n3. What is the purpose of the `convertValue` function?\n- The `convertValue` function is used to convert raw data into an instance of a specified type. It handles conversion for types such as `rgb`, `vec2`, `vec3`, `vec4`, and `entity`.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/system.md"}}],["332",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/zone/component.js)\n\nThe `ZoneComponent` class is a component of the PlayCanvas engine that allows developers to define an area in world space of a certain size. This component can be used in various ways, such as affecting audio reverb when an `AudioListenerComponent` is within the zone, or creating a culling system with portals between zones to hide whole indoor sections for performance reasons. Zones are building blocks and meant to be used in many different ways.\n\nThis class extends the `Component` class and has several events that can be fired when the component becomes enabled or disabled, changes state, or is removed from an entity. The `size` property of the `ZoneComponent` class is a `Vec3` object that represents the size of the axis-aligned box of the zone.\n\nThe `ZoneComponent` class has several methods that are used to check the state of the component and fire events accordingly. The `_checkState()` method checks if the component is enabled and if the entity it is attached to is enabled. If the state of the component has changed, the `enable` and `state` events are fired. The `_onSetEnabled()` method is called when the `enabled` property of the component is set and calls the `_checkState()` method. The `onEnable()` and `onDisable()` methods are called when the component is enabled or disabled, respectively, and call the `_checkState()` method. The `_onBeforeRemove()` method is called when the component is removed from an entity and fires the `remove` event.\n\nHere is an example of how to create a new `ZoneComponent` instance:\n\n```javascript\nimport { ZoneComponent } from 'path/to/zone/component.js';\n\nconst zone = new ZoneComponent(system, entity);\nzone.size = [10, 10, 10];\n```\n\nIn this example, a new `ZoneComponent` instance is created and the `size` property is set to a `Vec3` object with the values `[10, 10, 10]`. This component can now be used to define an area in world space of a certain size and can be used in various ways, such as affecting audio reverb or creating a culling system with portals between zones.\n## Questions: \n 1. What is the purpose of the `ZoneComponent` in the PlayCanvas engine?\n- The `ZoneComponent` allows developers to define an area in world space of a certain size, which can be used for various purposes such as affecting audio reverb or creating a culling system with portals between zones to hide indoor sections for performance reasons.\n\n2. What events can be fired by the `ZoneComponent`?\n- The `ZoneComponent` can fire events for when it becomes enabled, disabled, changes state, or is removed from an entity.\n\n3. How does the `size` property of the `ZoneComponent` work?\n- The `size` property of the `ZoneComponent` is a `Vec3` object that represents the size of the axis-aligned box of the zone. It can be set either by passing in a `Vec3` object or an array of at least 3 values representing the x, y, and z dimensions.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/zone/component.md"}}],["333",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/zone/data.js)\n\nThe code above defines a class called `ZoneComponentData` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to store data related to a zone component, which is a feature that allows developers to define areas in a game or application that have specific properties or behaviors. \n\nThe `ZoneComponentData` class has a constructor method that sets the `enabled` property to `true` by default. This property determines whether the zone component is active or not. If it is set to `false`, the zone will not have any effect on the game or application. \n\nDevelopers can use this class to create instances of zone components and set their properties as needed. For example, they might create a zone component that slows down time when the player enters a certain area. They could do this by creating a new instance of `ZoneComponentData`, setting the `enabled` property to `true`, and adding additional properties to define the size and shape of the zone, as well as the specific behavior that should occur when the player enters it. \n\nHere is an example of how this class might be used in a larger project:\n\n```\nimport { ZoneComponentData } from 'playcanvas-engine';\n\nconst slowDownZone = new ZoneComponentData();\nslowDownZone.enabled = true;\nslowDownZone.radius = 10;\nslowDownZone.behavior = 'slowDownTime';\n\n// Add the zone component to a game object\nconst player = new GameObject();\nplayer.addComponent('zone', slowDownZone);\n```\n\nIn this example, we create a new instance of `ZoneComponentData` called `slowDownZone` and set its `enabled` property to `true`. We also add additional properties to define the size and behavior of the zone. Finally, we add the zone component to a game object called `player` using the `addComponent` method. This will cause the zone to have an effect on the player when they enter its radius. \n\nOverall, the `ZoneComponentData` class is a useful tool for developers who want to create complex behaviors and interactions in their games or applications. By defining zones with specific properties and behaviors, they can create immersive and engaging experiences for their users.\n## Questions: \n 1. **What is the purpose of the ZoneComponentData class?** \nThe ZoneComponentData class is likely a data structure used to store information about a zone component in the PlayCanvas engine.\n\n2. **What does the `enabled` property do?** \nThe `enabled` property is a boolean value that determines whether the zone component is enabled or disabled.\n\n3. **How is this code used within the PlayCanvas engine?** \nThis code is likely used as part of the PlayCanvas engine's zone component system, which allows developers to define areas within a scene that affect gameplay or other aspects of the game.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/zone/data.md"}}],["334",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/components/zone/system.js)\n\nThe code defines a class called `ZoneComponentSystem` which is responsible for creating and managing instances of `ZoneComponent`. The `ZoneComponent` is a component that defines a zone in 3D space. The purpose of this system is to provide an interface for creating and managing these components.\n\nThe `ZoneComponentSystem` extends the `ComponentSystem` class, which is a base class for all component systems in the PlayCanvas engine. It defines a constructor that takes an instance of the `AppBase` class as an argument. The `AppBase` class represents the application that is using the PlayCanvas engine.\n\nThe `ZoneComponentSystem` class defines a few properties and methods. The `id` property is a string that identifies the system. The `ComponentType` property is a reference to the `ZoneComponent` class. The `DataType` property is a reference to the `ZoneComponentData` class. The `schema` property is an array of strings that define the properties of the component.\n\nThe `initializeComponentData` method is called when a new `ZoneComponent` is created. It takes three arguments: the component instance, the data object, and the properties object. It sets the `enabled` property of the component to `true` if it is not already defined in the data object. It also sets the `size` property of the component if it is defined in the data object.\n\nThe `cloneComponent` method is called when a new entity is created that is a clone of an existing entity. It takes two arguments: the original entity and the clone entity. It creates a new `ZoneComponent` for the clone entity and sets its `size` property to the `size` property of the original entity's `ZoneComponent`.\n\nThe `_onBeforeRemove` method is called when a component is about to be removed from an entity. It calls the `_onBeforeRemove` method of the component.\n\nThe `Component._buildAccessors` method is called to create getter and setter methods for the properties defined in the `schema` array.\n\nOverall, the `ZoneComponentSystem` class provides an interface for creating and managing `ZoneComponent` instances. It defines methods for initializing component data, cloning components, and handling component removal. This class is used in the larger PlayCanvas engine project to provide a way to define zones in 3D space. An example of how this class might be used is to create a new `ZoneComponent` for an entity that represents a room in a game. The `size` property of the `ZoneComponent` could be set to the dimensions of the room to define the boundaries of the zone.\n## Questions: \n 1. What is the purpose of the `ZoneComponentSystem` class?\n- The `ZoneComponentSystem` class creates and manages instances of `ZoneComponent`.\n\n2. What is the inheritance hierarchy of the `ZoneComponentSystem` class?\n- The `ZoneComponentSystem` class extends the `ComponentSystem` class.\n\n3. What is the purpose of the `_onBeforeRemove` method?\n- The `_onBeforeRemove` method is called before a component is removed from an entity and is used to perform any necessary cleanup or actions.","metadata":{"source":".autodoc/docs/markdown/src/framework/components/zone/system.md"}}],["335",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/constants.js)\n\nThis code defines several constants that are used to control the behavior of the canvas element in the PlayCanvas engine. The canvas element is used to render graphics and animations in the browser.\n\nThe first three constants, `FILLMODE_NONE`, `FILLMODE_FILL_WINDOW`, and `FILLMODE_KEEP_ASPECT`, control how the canvas element is resized when the window is resized. `FILLMODE_NONE` means that the canvas size will not change when the window is resized. `FILLMODE_FILL_WINDOW` means that the canvas size will be changed to fill the entire window. `FILLMODE_KEEP_ASPECT` means that the canvas size will be changed to fill the window as best it can while maintaining the same aspect ratio.\n\nThe last two constants, `RESOLUTION_AUTO` and `RESOLUTION_FIXED`, control how the canvas resolution is changed when the canvas size is changed. `RESOLUTION_AUTO` means that the canvas resolution will be changed to match the canvas size. `RESOLUTION_FIXED` means that the canvas resolution will remain the same and the output will be scaled to fit the canvas.\n\nThese constants can be used in the PlayCanvas engine to control the behavior of the canvas element. For example, if the developer wants the canvas to always fill the entire window, they can set the fill mode to `FILLMODE_FILL_WINDOW`. If the developer wants the canvas to maintain a certain aspect ratio, they can set the fill mode to `FILLMODE_KEEP_ASPECT`. If the developer wants the canvas to have a fixed resolution, they can set the resolution mode to `RESOLUTION_FIXED`.\n\nHere is an example of how these constants can be used in the PlayCanvas engine:\n\n```javascript\n// Set the fill mode to FILL_WINDOW\napp.setCanvasFillMode(pc.FILLMODE_FILL_WINDOW);\n\n// Set the resolution mode to FIXED\napp.setCanvasResolution(pc.RESOLUTION_FIXED);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines constants for different fill modes and resolutions for a canvas element in a web application.\n\n2. How can these constants be used in the PlayCanvas engine?\n- These constants can be used as options for configuring the behavior of the canvas element when it is resized or its resolution is changed.\n\n3. Are there any other constants or functions related to canvas management in the PlayCanvas engine?\n- It is not clear from this code alone whether there are other related constants or functions in the PlayCanvas engine. Further investigation or documentation may be necessary to determine this.","metadata":{"source":".autodoc/docs/markdown/src/framework/constants.md"}}],["336",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/font/constants.js)\n\nThis code defines two constants, `FONT_MSDF` and `FONT_BITMAP`, which are used to specify the type of font to be used in the PlayCanvas engine project. \n\nThe `FONT_MSDF` constant stands for \"multi-channel signed distance field\" and is a technique used to render high-quality text in real-time applications. This technique involves generating a texture that stores the distance from each pixel to the nearest edge of the text glyph. This texture is then used to render the text, resulting in sharp, high-quality edges. \n\nThe `FONT_BITMAP` constant, on the other hand, refers to a traditional bitmap font, which is a font that is stored as a bitmap image. Bitmap fonts are typically lower quality than MSDF fonts, but they are simpler to generate and can be more efficient in certain situations. \n\nBy defining these constants, the PlayCanvas engine allows developers to easily specify the type of font to be used in their projects. For example, a developer might use the following code to load an MSDF font:\n\n```\nconst fontAsset = new pc.Asset('myFont', 'font', {\n    url: 'path/to/my/font.msdf',\n    type: 'font',\n});\n```\n\nAlternatively, they could load a bitmap font using the following code:\n\n```\nconst fontAsset = new pc.Asset('myFont', 'font', {\n    url: 'path/to/my/font.png',\n    type: 'font',\n});\n```\n\nOverall, this code is a small but important part of the PlayCanvas engine, as it allows developers to easily specify the type of font to be used in their projects, which can have a significant impact on the quality and performance of their applications.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code exports two constants, `FONT_MSDF` and `FONT_BITMAP`, which likely have some relevance to font rendering within the PlayCanvas engine.\n\n2. **What is the difference between `FONT_MSDF` and `FONT_BITMAP`?**\\\nWithout further context or documentation, it is unclear what distinguishes these two font types and when one might be preferred over the other.\n\n3. **How are these constants used within the PlayCanvas engine?**\\\nIt is not clear from this code snippet how or where these constants are utilized within the PlayCanvas engine, leaving room for further investigation and documentation.","metadata":{"source":".autodoc/docs/markdown/src/framework/font/constants.md"}}],["337",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/font/font.js)\n\nThe code defines a class called `Font` that represents the resource of a font asset. The purpose of this class is to provide a way to load and store font data and textures. \n\nThe constructor of the `Font` class takes two parameters: an array of textures and an object containing font data. The `textures` parameter is an array of texture objects that represent the font textures. The `data` parameter is an object that contains information about the font, such as its type and intensity.\n\nThe `Font` class has a property called `type` that is set to the value of `data.type` if it exists, otherwise it is set to a default value of `FONT_MSDF`. `FONT_MSDF` is a constant that is imported from another file called `constants.js`.\n\nThe `Font` class also has a property called `em` that is set to a value of 1. This property is not used in the code and its purpose is unclear.\n\nThe `Font` class has a property called `textures` that is set to the value of the `textures` parameter. This property is an array of texture objects that represent the font textures.\n\nThe `Font` class has a property called `intensity` that is set to a value of 0.0. This property is used to control the intensity of the font.\n\nThe `Font` class has a private property called `_data` that is initially set to `null`. This property is used to store the font data passed in the `data` parameter. The `Font` class has a getter and a setter for the `data` property. The setter sets the value of `_data` to the value passed in the `value` parameter. If the value is `null`, the function returns immediately. If the `intensity` property is defined in the font data, the `intensity` property of the `Font` class is set to the value of the `intensity` property in the font data. If the `info` property is not defined in the font data, it is set to an empty object. If the `version` property is not defined in the font data or is less than 2, the `maps` property of the `info` property is set to an array containing an object with the `width` and `height` properties set to the values of the `width` and `height` properties in the `info` property. If the `chars` property is defined in the font data, the `map` property of each character in the `chars` property is set to 0.\n\nThe `Font` class is exported so that it can be used in other parts of the PlayCanvas engine project. For example, it could be used in a text rendering system to load and store font data and textures. An example of how to create a new `Font` instance is shown below:\n\n```\nimport { Texture } from '../../platform/graphics/texture.js';\nimport { Font } from './font.js';\n\nconst textures = [new Texture(), new Texture()];\nconst data = { type: 'msdf', intensity: 0.5, info: { width: 512, height: 512 } };\nconst font = new Font(textures, data);\n```\n## Questions: \n 1. What is the purpose of the `FONT_MSDF` constant imported at the beginning of the file?\n- `FONT_MSDF` is a constant that represents the type of font used in the `Font` class. It is used as a fallback value if the `type` property is not defined in the `data` parameter passed to the constructor.\n\n2. What is the significance of the `intensity` property in the `Font` class?\n- The `intensity` property is a number that represents the intensity of the font. It is set to 0.0 by default, but can be changed by setting the `intensity` property of the `data` parameter passed to the constructor.\n\n3. Why is there a check for version 2 of the font data in the `set data` method?\n- The check for version 2 of the font data is to ensure that the font data is in the correct format. If the font data is not in version 2 format, the method migrates the data to version 2 by adding a `maps` property to the `info` property and setting the `map` property of each character in the `chars` property to 0.","metadata":{"source":".autodoc/docs/markdown/src/framework/font/font.md"}}],["338",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/globals.js)\n\nThe code above defines two functions, `getApplication()` and `setApplication()`, and exports them for use in other parts of the PlayCanvas engine project. \n\nThe `getApplication()` function simply returns the value of a variable called `currentApplication`. This variable is initially undefined, but can be set to an instance of the PlayCanvas application using the `setApplication()` function. \n\nThe `setApplication()` function takes an argument `app`, which is expected to be an instance of the PlayCanvas application. It sets the value of `currentApplication` to this instance, and then calls a method from the `GraphicsDeviceAccess` module to set the graphics device for the application. \n\nOverall, these functions provide a way for other parts of the PlayCanvas engine to access the current application instance and its graphics device. For example, if another module needs to render graphics, it can call `getApplication()` to get the current application instance, and then access its graphics device through the `graphicsDevice` property. \n\nHere is an example of how these functions might be used in another module:\n\n```\nimport { getApplication } from \"./application.js\";\n\nconst app = getApplication();\nconst graphicsDevice = app.graphicsDevice;\n\n// Use the graphics device to render something...\n```\n\nBy using the `getApplication()` function, this module can access the current application instance without needing to know how it is stored or managed internally. This helps to keep the code modular and maintainable.\n## Questions: \n 1. **What is the purpose of the `GraphicsDeviceAccess` import?**\\\n   The `GraphicsDeviceAccess` import is used to access the graphics device of the current application.\n\n2. **What is the significance of the `currentApplication` variable?**\\\n   The `currentApplication` variable is used to store the current application instance.\n\n3. **What does the `setApplication` function do?**\\\n   The `setApplication` function sets the current application instance and also sets the graphics device of the application using the `GraphicsDeviceAccess` module.","metadata":{"source":".autodoc/docs/markdown/src/framework/globals.md"}}],["339",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/graphics/picker.js)\n\nThe code defines a `Picker` class that is used to select mesh instances from screen coordinates. The `Picker` object is created with a specified width and height, and a render target is created internally. The `prepare` method is used to prime the pick buffer with a rendering of the specified models from the point of view of the supplied camera. Once the pick buffer has been prepared, `getSelection` can be called multiple times on the same `Picker` object to return an array of mesh instances that are in the selection. The `resize` method is used to set the resolution of the pick buffer.\n\nThe `Picker` class imports various modules from the PlayCanvas engine, including `Color`, `GraphicsDevice`, `RenderTarget`, `Texture`, `DebugGraphics`, `Camera`, `Command`, `Layer`, `LayerComposition`, `getApplication`, `Entity`, `Debug`, and `BlendState`. The `Picker` class also defines a `tempSet` constant that is used to store the selected mesh instances.\n\nThe `Picker` class has a `renderTarget` property that is used to store the render target used by the picker internally. The `renderTarget` property is set to `null` by default, and is set to a new `RenderTarget` object when the `allocateRenderTarget` method is called. The `releaseRenderTarget` method is used to unset the render target from the camera and destroy the render target.\n\nThe `Picker` class has an `initLayerComposition` method that is used to create a new layer composition with the layer and camera. The `layer` property is used to store the layer that is used to render all meshes for picking. The `cameraEntity` property is used to store the camera entity that is used to render the scene. The `mapping` property is used to store the mapping table from ids to mesh instances. The `clearDepthCommand` property is used to simulate layer clearing, which is required due to storing meshes from multiple layers on a single layer.\n\nThe `Picker` class has a `getSelection` method that is used to return the list of mesh instances selected by the specified rectangle in the previously prepared pick buffer. The `x`, `y`, `width`, and `height` parameters are used to specify the rectangle using top-left coordinate system. The `pixels` array is used to store the pixel data of the pick buffer. The `mapping` array is used to store the mapping table from ids to mesh instances. The `selection` array is used to store the selected mesh instances.\n\nThe `Picker` class has a `prepare` method that is used to prepare the pick buffer with a rendering of the specified models from the point of view of the supplied camera. The `camera` parameter is used to specify the camera component used to render the scene. The `scene` parameter is used to specify the scene containing the pickable mesh instances. The `layers` parameter is used to specify the layers from which objects will be picked. If not supplied, all layers of the specified camera will be used.\n\nThe `Picker` class has an `updateCamera` method that is used to update the rendering camera. The `srcCamera` parameter is used to specify the camera component used to render the scene.\n\nThe `Picker` class has a `resize` method that is used to set the resolution of the pick buffer. The `width` and `height` parameters are used to specify the width and height of the pick buffer in pixels.\n## Questions: \n 1. What is the purpose of the `Picker` class?\n    \n    The `Picker` class is used to select mesh instances from screen coordinates in a 3D scene.\n\n2. How does the `Picker` class prepare the pick buffer for selection?\n    \n    The `Picker` class populates a layer with meshes and depth clear commands, updates the rendering camera, and renders the layer using the `app.renderComposition` method.\n\n3. What is the trade-off between pick buffer resolution and selection accuracy?\n    \n    The lower the resolution of the pick buffer, the less accurate the selection results returned by `Picker.getSelection`. However, smaller pick buffers will yield greater performance.","metadata":{"source":".autodoc/docs/markdown/src/framework/graphics/picker.md"}}],["340",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/anim-clip.js)\n\nThe code defines a resource handler for loading animation clip resources in the PlayCanvas engine. The `AnimClipHandler` class implements the `ResourceHandler` interface and has a `handlerType` property set to \"animclip\". \n\nThe `load` method is responsible for loading the animation clip resource from the specified URL. It takes a URL and a callback function as parameters. If the URL is a string, it is converted to an object with `load` and `original` properties. The method then makes an HTTP GET request to the URL using the `http.get` method from the `http` module. The `options` object passed to `http.get` specifies whether to retry the request and the maximum number of retries. If the URL starts with \"blob:\", the response type is set to JSON. If the request is successful, the callback function is called with the response data. If there is an error, the callback function is called with an error message.\n\nThe `open` method is responsible for parsing the animation clip data and returning an `AnimTrack` object. It takes a URL and the data returned by the `load` method as parameters. The method extracts the name, duration, inputs, outputs, and curves properties from the data object and creates new `AnimData` and `AnimCurve` objects from them. It then creates a new `AnimTrack` object with the extracted data and returns it.\n\nThe `patch` method is empty and does not do anything.\n\nOverall, this code provides a way to load and parse animation clip resources in the PlayCanvas engine. It can be used by other parts of the engine that need to work with animation clips, such as the animation system or the asset manager. An example of how this code might be used is shown below:\n\n```javascript\nconst app = new pc.Application();\nconst asset = new pc.Asset('my-anim-clip', 'animclip', {\n    url: 'path/to/my-anim-clip.json'\n});\napp.assets.add(asset);\napp.assets.load(asset, function (err, asset) {\n    if (!err) {\n        const animClip = asset.resource;\n        // use the animClip object\n    } else {\n        console.error(err);\n    }\n});\n```\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains the implementation of a resource handler used for loading AnimClip resources in the PlayCanvas engine.\n\n2. What is the role of the `load` function in this code?\n- The `load` function is responsible for loading the AnimClip resource from a specified URL and calling the provided callback function with the loaded data or an error message.\n\n3. What is the purpose of the `open` function in this code?\n- The `open` function is responsible for parsing the loaded data and creating an AnimTrack object that can be used by the engine to play the animation.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/anim-clip.md"}}],["341",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/anim-state-graph.js)\n\nThe code defines a resource handler for loading `AnimStateGraph` resources in the PlayCanvas engine project. A resource handler is responsible for loading and processing a specific type of resource. In this case, the `AnimStateGraphHandler` class implements the `ResourceHandler` interface and defines methods for loading and opening `AnimStateGraph` resources.\n\nThe `load` method takes a URL and a callback function as parameters. It uses the `http.get` method to load the resource from the specified URL and passes the response data to the callback function. If an error occurs during loading, the callback function is called with an error message.\n\nThe `open` method takes a URL and the loaded data as parameters and returns a new `AnimStateGraph` object created from the data. This method is called after the resource has been loaded to create a usable object from the raw data.\n\nThe `patch` method is not implemented and does nothing. It is called after the resource has been loaded and opened, and can be used to modify the asset or other related assets.\n\nThe `AnimStateGraphHandler` class is exported for use in other parts of the PlayCanvas engine project. It can be used to load and process `AnimStateGraph` resources in a consistent and standardized way. For example, a developer could use this resource handler to load an `AnimStateGraph` resource and use it to control the animation of a 3D model in a PlayCanvas scene.\n\nExample usage:\n\n```\nimport { AnimStateGraphHandler } from 'path/to/AnimStateGraphHandler.js';\n\nconst handler = new AnimStateGraphHandler();\n\nhandler.load('path/to/animstategraph.json', (err, data) => {\n  if (err) {\n    console.error(err);\n  } else {\n    const animStateGraph = handler.open('path/to/animstategraph.json', data);\n    // use animStateGraph to control animation\n  }\n});\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines a resource handler class for loading `AnimStateGraph` resources in the PlayCanvas engine.\n\n2. What is the `handlerType` property used for?\n    \n    The `handlerType` property is used to specify the type of resource that this handler can handle, which in this case is `\"animstategraph\"`.\n\n3. What is the purpose of the `patch` method?\n    \n    The `patch` method is currently empty and does not have any functionality. It is likely a placeholder for future functionality to modify or update loaded assets.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/anim-state-graph.md"}}],["342",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/animation.js)\n\nThe `AnimationHandler` class is a resource handler used for loading animation resources in the PlayCanvas engine. It implements the `ResourceHandler` interface and has a `handlerType` property set to `\"animation\"`. \n\nThe `load` method is responsible for loading the animation resource. It takes a URL, a callback function, and an optional asset object as parameters. If the URL is a string, it is converted to an object with `load` and `original` properties. The method then sets the options for the HTTP request, including the response type, depending on the URL's extension. If the URL starts with `\"blob:\"` or `\"data:\"` and has a `.glb` extension, the response type is set to `Http.ResponseType.ARRAY_BUFFER`. Otherwise, it is set to `Http.ResponseType.JSON`. The method then sends an HTTP GET request to the URL using the `http.get` method and passes the options and a callback function as parameters. If there is an error, the callback function is called with an error message. Otherwise, the result is parsed immediately. If the URL has a `.glb` extension, the `GlbParser` is used to parse the response and create an array of `Animation` objects. If the asset object has a `data` property with an `events` property, the `AnimEvents` class is used to create an `events` property for each `Animation` object. If the URL has a different extension, the `_parseAnimationV3` or `_parseAnimationV4` method is called to parse the response and create an `Animation` object. The parsed `Animation` object is then passed to the callback function.\n\nThe `open` method is responsible for opening the animation resource. It takes a URL, data, and an asset object as parameters and returns the data.\n\nThe `patch` method is responsible for patching the animation resource. It takes an asset object and an assets object as parameters and does nothing.\n\nThe `_parseAnimationV3` and `_parseAnimationV4` methods are responsible for parsing the animation data and creating an `Animation` object. They take the animation data as a parameter and return an `Animation` object. The `_parseAnimationV3` method is used for animation data with a version of 3, while the `_parseAnimationV4` method is used for animation data with a version of 4. They both create a new `Animation` object, set its name and duration properties, create a new `Node` object for each node in the animation data, set its name property, create a new `Key` object for each key in the node data, set its time, position, rotation, and scale properties, and add it to the node's `_keys` array. They then add the node to the `Animation` object and return it.\n\nOverall, the `AnimationHandler` class is an important part of the PlayCanvas engine's animation system. It allows for the loading and parsing of animation resources, which can then be used to animate entities in the scene.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains the implementation of a resource handler for loading Animation resources in the PlayCanvas engine.\n\n2. What external dependencies does this code have?\n- This code imports several modules from the PlayCanvas engine, including path, Quat, Vec3, http, Animation, Key, Node, and AnimEvents. It also imports the GlbParser module from the parsers directory.\n\n3. What is the format of the Animation resource that this handler can load?\n- This handler can load Animation resources in either JSON or binary GLB format. If the resource is a GLB file, the handler will use the GlbParser module to parse the binary data and extract the animations.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/animation.md"}}],["343",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/audio.js)\n\nThe code defines an AudioHandler class that is used to load audio resources for the PlayCanvas engine. The class implements the ResourceHandler interface and is responsible for loading Sound resources. The handlerType property of the class is set to \"audio\" to indicate that it handles audio resources.\n\nThe constructor of the AudioHandler class takes an instance of the AppBase class as an argument. The soundManager property of the AppBase class is used to manage the audio resources. The constructor sets the maxRetries property to 0.\n\nThe load method of the AudioHandler class is used to load an audio resource. It takes a URL and a callback function as arguments. If the URL is a string, it is converted to an object with load and original properties. The load property contains the URL to load, and the original property contains the original URL. The load method checks if the audio format is supported by calling the _isSupported method. If the format is supported, the _createSound method is called to create a Sound resource. If the format is not supported, the error callback is called with an error message.\n\nThe open method of the AudioHandler class is a no-op method that returns the data argument.\n\nThe patch method of the AudioHandler class is a no-op method that takes an asset and assets as arguments.\n\nThe _createSound method of the AudioHandler class is a private method that is used to create a Sound resource. The method takes a URL, a success callback function, and an error callback function as arguments. The method checks if the browser supports the AudioContext API by calling the hasAudioContext method. If the API is supported, the method uses the http.get method to load the audio data as an array buffer. The decodeAudioData method of the AudioContext class is used to decode the audio data and create a Sound resource. If the API is not supported, the method creates an Audio element and sets its source to the URL. The canplaythrough event is used to determine when the audio is ready to be played. Once the audio is ready, the success callback is called with the audio element.\n\nOverall, the AudioHandler class is an important part of the PlayCanvas engine that is used to load audio resources. It provides a consistent interface for loading audio resources and handles the differences between browsers that support the AudioContext API and those that do not. Developers can use the AudioHandler class to load audio resources in their PlayCanvas projects by creating an instance of the class and passing it to the asset registry. For example:\n\n```\nconst audioHandler = new AudioHandler(app);\napp.assets.loadFromUrl('my-audio.ogg', 'audio', audioHandler, function (err, asset) {\n    if (!err) {\n        const sound = asset.resource;\n        sound.play();\n    }\n});\n```\n## Questions: \n 1. What is the purpose of the `AudioHandler` class?\n- The `AudioHandler` class is a resource handler used for loading audio resources, specifically `Sound` resources.\n\n2. What is the `_createSound` method used for?\n- The `_createSound` method is used to load an audio asset using an AudioContext by URL and calls success or error with the created resource or error respectively.\n\n3. What is the purpose of the `ie` variable?\n- The `ie` variable is used to check if the user is running Internet Explorer (IE) and returns the version number if it is IE 10 or older or IE 11, otherwise it returns false.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/audio.md"}}],["344",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/basis.js)\n\nThe code is a part of the PlayCanvas engine project and provides functionality for transcoding Basis Universal texture files to compressed texture formats that are supported by the user's device. The code is split into several parts, each with a specific purpose.\n\nThe `getCompressionFormats` function returns an object that lists the supported compression formats of the user's device. This information is used later to determine which compression format to use when transcoding the Basis file.\n\nThe `prepareWorkerModules` function downloads the Basis code and compiles the WebAssembly module for use in workers. It also checks if the user's device supports WebAssembly and sends the compiled module to the callback function. If the device does not support WebAssembly, it falls back to using an array buffer.\n\nThe `BasisQueue` class manages a queue of transcode jobs and clients ready to run them. It enqueues jobs and clients and handles responses from the workers.\n\nThe `BasisClient` class is a client interface to a Basis transcoder instance running on a web worker. It runs jobs and enqueues itself while a job is running if it is an eager client. Otherwise, it only enqueues itself once the current job has finished running.\n\nThe `basisInitialize` function initializes the Basis transcode worker. It takes a configuration object as an argument and sets default values if any of the URLs are not specified. It also sets the number of workers to use for transcoding, the priority order of texture compression formats, and the maximum number of http load retry attempts. It then calls the `prepareWorkerModules` function to prepare the worker modules.\n\nThe `basisTranscode` function enqueues a blob of Basis data for transcoding. It takes the user's device, the URL of the Basis file, the file data to transcode, a callback function to receive the transcode result, and an options structure as arguments. It then enqueues the job and returns true if the Basis worker was initialized and false otherwise.\n\nOverall, this code provides a way to transcode Basis Universal texture files to compressed texture formats that are supported by the user's device. It uses web workers to perform the transcoding and manages a queue of jobs and clients to ensure efficient use of resources.\n## Questions: \n 1. What is the purpose of the `prepareWorkerModules` function?\n- The `prepareWorkerModules` function downloads the basis code and compiles the wasm module for use in workers.\n\n2. What is the `BasisQueue` class responsible for?\n- The `BasisQueue` class is responsible for managing a queue of transcode jobs and clients ready to run them.\n\n3. What is the purpose of the `getCompressionFormats` function?\n- The `getCompressionFormats` function returns a list of the device's supported compression formats.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/basis.md"}}],["345",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/binary.js)\n\nThe code defines a class called `BinaryHandler` that is responsible for handling binary resources in the PlayCanvas engine project. The purpose of this class is to provide a way to load binary resources such as images, audio files, and other binary data from a server and make them available for use in the project.\n\nThe `BinaryHandler` class has three methods: `load`, `open`, and `patch`. The `load` method is responsible for loading the binary resource from the server. It takes a URL and a callback function as parameters. If the URL is a string, it is converted to an object with two properties: `load` and `original`. The `load` property contains the URL to load the binary resource from, while the `original` property contains the original URL passed to the method. The `http.get` method is then called to load the binary resource from the server. The `responseType` property is set to `ARRAY_BUFFER` to indicate that the response should be treated as binary data. If the `maxRetries` property is greater than 0, the request will be retried if it fails. If the request is successful, the `callback` function is called with the binary data as the second parameter. If the request fails, the `callback` function is called with an error message.\n\nThe `open` method is responsible for opening the binary resource. It takes a URL and the binary data as parameters and returns the binary data. This method is not used in this class, but it is provided for compatibility with other resource handlers.\n\nThe `patch` method is responsible for patching the asset. It takes an asset and an assets object as parameters. This method is not used in this class, but it is provided for compatibility with other resource handlers.\n\nThe `BinaryHandler` class is exported so that it can be used in other parts of the PlayCanvas engine project. For example, it can be used in a script that needs to load a binary resource such as an image or an audio file. Here is an example of how the `BinaryHandler` class can be used:\n\n```\nimport { BinaryHandler } from 'path/to/BinaryHandler.js';\n\nconst binaryHandler = new BinaryHandler();\n\nbinaryHandler.load('path/to/image.png', function (err, data) {\n    if (!err) {\n        const image = new Image();\n        image.src = URL.createObjectURL(new Blob([data], { type: 'image/png' }));\n        document.body.appendChild(image);\n    } else {\n        console.error(err);\n    }\n});\n```\n\nIn this example, the `BinaryHandler` class is imported and a new instance of the class is created. The `load` method is then called with the URL of the image to load and a callback function. If the image is loaded successfully, a new `Image` element is created and the binary data is converted to a `Blob` object and used to create a URL for the image. The URL is then set as the `src` attribute of the `Image` element and the element is appended to the `body` of the document. If the image fails to load, an error message is logged to the console.\n## Questions: \n 1. What is the purpose of the `BinaryHandler` class?\n    \n    The `BinaryHandler` class is a resource handler for binary files.\n\n2. What is the `load` method used for?\n    \n    The `load` method is used to load a binary resource from a given URL and returns the response in an array buffer.\n\n3. What is the `patch` method used for?\n    \n    The `patch` method is an empty method that is not currently used for anything in this implementation of the `BinaryHandler` class.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/binary.md"}}],["346",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/bundle.js)\n\nThe code defines a class called `BundleHandler` that implements the `ResourceHandler` interface. The purpose of this class is to load and handle bundle assets. \n\nThe `BundleHandler` constructor takes an instance of `AppBase` as an argument and initializes some properties. The `load` method is used to load a bundle asset from a given URL. It first checks if the URL is a string and converts it to an object with `load` and `original` properties if necessary. It then uses the `http.get` method to make a request to the URL and retrieve the asset data as an array buffer. If the request is successful, the `_untar` method is called to extract the files from the archive and pass them to the callback function. If there is an error, the callback function is called with an error message.\n\nThe `_untar` method is responsible for extracting the files from the archive. It first checks if web workers are available and creates a new `UntarWorker` instance if necessary. It then calls the `untar` method on the worker to extract the files and pass them to the callback function. If web workers are not available, it creates a new `Untar` instance and calls the `untar` method on it to extract the files.\n\nThe `open` method is used to create a new `Bundle` instance from the data returned by the `load` method. The `patch` method is not implemented and does nothing.\n\nOverall, the `BundleHandler` class provides a way to load and handle bundle assets in the PlayCanvas engine. It uses the `http` module to make requests and the `Untar` and `UntarWorker` classes to extract files from the archive. The class can be used by other parts of the engine that need to load and work with bundle assets. For example, a game level may be stored as a bundle asset and loaded using the `BundleHandler` class.\n## Questions: \n 1. What is the purpose of this code and what does it do?\n- This code defines a class called `BundleHandler` that implements a `ResourceHandler` interface and is responsible for loading bundle assets. It uses `http` to make a request to load the bundle, and then untars the response using a web worker if available, or in the main thread if not.\n\n2. What other classes or modules does this code depend on?\n- This code imports several modules from the `core/platform.js` and `platform/net/http.js` files, as well as the `Bundle` and `Untar` classes from a `bundle.js` file and an `untar.js` file respectively. It also depends on an `AppBase` class.\n\n3. What is the significance of the `handlerType` property and how is it used?\n- The `handlerType` property is a string that specifies the type of resource that this handler is responsible for. It is used to match the handler to the appropriate resource when loading or patching assets.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/bundle.md"}}],["347",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/container.js)\n\n# PlayCanvas Engine: ContainerHandler.js\n\nThe `ContainerHandler.js` file is a module that contains the `ContainerHandler` class and the `ContainerResource` class. The `ContainerHandler` class is responsible for loading files that contain multiple resources, such as glTF files that can contain textures, models, and animations. The `ContainerResource` class is a container for a list of animations, textures, materials, renders, and a model.\n\nThe `ContainerHandler` class implements the `ResourceHandler` interface and has three methods: `load`, `open`, and `patch`. The `load` method loads the resource from the specified URL and calls the callback function when the resource is loaded or an error occurs. The `open` method parses the resource data and returns the parsed data. The `patch` method patches the asset with the specified data.\n\nThe `ContainerResource` class has four properties: `renders`, `materials`, `textures`, and `animations`. These properties are arrays of `Asset` objects that represent the resources contained in the container. The class also has three methods: `instantiateModelEntity`, `instantiateRenderEntity`, and `getMaterialVariants`. The `instantiateModelEntity` method instantiates an entity with a model component based on the container's model. The `instantiateRenderEntity` method instantiates an entity with a render component based on the container's model. The `getMaterialVariants` method queries the list of available material variants.\n\nThe `ContainerResource` class also has two methods that apply material variants to the container's entities: `applyMaterialVariant` and `applyMaterialVariantInstances`. The `applyMaterialVariant` method applies a material variant to an entity hierarchy, while the `applyMaterialVariantInstances` method applies a material variant to a set of mesh instances.\n\nOverall, the `ContainerHandler.js` file provides a way to load and parse files that contain multiple resources and provides a container for these resources. The `ContainerResource` class provides methods to instantiate entities with model and render components and apply material variants to these entities.\n## Questions: \n 1. What is the purpose of the `ContainerResource` class?\n- The `ContainerResource` class is a container for a list of animations, textures, materials, renders, and a model. It provides methods for instantiating entities with model or render components, querying available material variants, and applying material variants to entities or mesh instances.\n\n2. What types of resources can be loaded using the `ContainerHandler` class?\n- The `ContainerHandler` class can load files that contain multiple resources, such as glTF files that can contain textures, models, and animations. It supports various resource types, including global, node, light, camera, animation, material, image, texture, buffer, and bufferView.\n\n3. What is the purpose of the `maxRetries` property in the `ContainerHandler` class?\n- The `maxRetries` property sets the maximum number of times the parser should retry loading a resource if it fails. It applies to both the `GlbParser` and any other parsers registered with the `parsers` property of the `ContainerHandler` instance.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/container.md"}}],["348",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/css.js)\n\nThe code defines a class called `CssHandler` that handles loading and processing of CSS resources. This class is a part of the PlayCanvas engine project. \n\nThe `CssHandler` class has three methods: `load()`, `open()`, and `patch()`. The `load()` method is responsible for loading the CSS resource from the specified URL. It takes two arguments: `url` and `callback`. The `url` argument can be either a string or an object with two properties: `load` and `original`. The `load` property contains the URL of the CSS resource to be loaded, while the `original` property contains the original URL. The `callback` argument is a function that is called when the resource is loaded. If the resource is loaded successfully, the `callback` function is called with two arguments: `null` and the response object. If there is an error loading the resource, the `callback` function is called with an error message.\n\nThe `open()` method is responsible for processing the loaded CSS resource. It takes two arguments: `url` and `data`. The `url` argument is the URL of the CSS resource, while the `data` argument is the loaded CSS data. This method simply returns the loaded CSS data.\n\nThe `patch()` method is not implemented and does nothing.\n\nThe `CssHandler` class is exported so that it can be used by other parts of the PlayCanvas engine project. It is likely that this class is used by other classes or modules in the project that need to load and process CSS resources. For example, a module that renders UI elements may use the `CssHandler` class to load and apply CSS styles to those elements. \n\nHere is an example of how the `CssHandler` class can be used:\n\n```\nimport { CssHandler } from 'path/to/css-handler.js';\n\nconst cssHandler = new CssHandler();\n\ncssHandler.load('path/to/styles.css', (err, response) => {\n  if (!err) {\n    const cssData = cssHandler.open('path/to/styles.css', response);\n    // apply cssData to UI elements\n  } else {\n    console.error(err);\n  }\n});\n```\n## Questions: \n 1. What is the purpose of the `http` import from `../../platform/net/http.js`?\n- A smart developer might ask what functionality the `http` module provides and how it is used in this code. \n\n2. What is the significance of the `handlerType` property and how is it used?\n- A smart developer might ask why the `handlerType` property is set to `\"css\"` and how it is used within the PlayCanvas engine.\n\n3. What is the purpose of the `open` and `patch` methods, and how are they used in the PlayCanvas engine?\n- A smart developer might ask how the `open` and `patch` methods are used in the PlayCanvas engine and what functionality they provide.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/css.md"}}],["349",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/cubemap.js)\n\nThe `CubemapHandler` class is a resource handler used for loading cubemap textures. It implements the `ResourceHandler` interface and is responsible for loading, opening, patching, and updating cubemap assets. \n\nWhen a cubemap asset is loaded, the `loadAssets` method is called. This method initializes the asset structures for tracking load requests and retrieves the list of dependent asset ids for the cubemap. The dependent assets include the prefiltered cubemap and the six faces of the cubemap. The method then processes each dependent asset by checking if it is already loaded, registering for load and error events if it is not loaded, or creating a new asset if it is a URL or file object. Once all dependent assets are loaded, the `update` method is called to update the cubemap resources with the newly loaded assets. \n\nThe `update` method first extracts the prelit data from the prefiltered cubemap asset and creates new cubemap resources for each level of the prelit cubemap. It then extracts the cubemap level data from the six face texture assets and creates a new cubemap resource for the faces. The method checks if any of the resources have changed and sets the new resources, firing change events. Finally, the method destroys the old cubemap resources that are no longer needed and unloads the old assets that have been replaced. \n\nThe `CubemapHandler` class is used in the larger PlayCanvas engine project to handle the loading and updating of cubemap textures. It is used by the `AssetRegistry` to manage cubemap assets and by the `SceneLoader` to load cubemap textures for use in scenes. \n\nExample usage:\n\n```\nimport { CubemapHandler } from 'playcanvas';\n\n// create a new app\nconst app = new pc.Application();\n\n// create a new cubemap asset\nconst cubemapAsset = new pc.Asset('myCubemap', 'cubemap', {\n    url: 'path/to/cubemap'\n});\n\n// create a new cubemap handler\nconst cubemapHandler = new CubemapHandler(app);\n\n// load the cubemap asset\ncubemapHandler.loadAssets(cubemapAsset, function (err, resources) {\n    if (!err) {\n        // cubemap resources are now available\n        const cubemap = new pc.Texture(app.graphicsDevice, {\n            cubemap: true,\n            rgbm: true,\n            fixCubemapSeams: true,\n            ...resources\n        });\n    }\n});\n\n// add the cubemap asset to the asset registry\napp.assets.add(cubemapAsset);\n```\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains the implementation of a resource handler used for loading cubemap textures in the PlayCanvas engine.\n\n2. What dependencies does this code file have?\n- This code file imports constants and the Texture class from the graphics module, as well as the Asset class from the asset module. It also depends on the AppBase class and the loader and assets properties of the app instance passed to its constructor.\n\n3. What is the main functionality of the CubemapHandler class?\n- The CubemapHandler class is responsible for loading and updating cubemap texture resources. It implements the ResourceHandler interface and provides methods for loading, opening, and patching assets, as well as for getting the list of dependent asset ids and comparing asset ids. It also defines a cmpArrays method for comparing arrays and a resolveId method for converting string ids to integers.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/cubemap.md"}}],["350",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/folder.js)\n\nThe `FolderHandler` class is a resource handler for folders in the PlayCanvas engine project. It has two methods: `load` and `open`.\n\nThe `load` method takes in a URL and a callback function. It does not perform any actual loading of data, but simply calls the callback function with `null` values for both parameters. This is because folders do not contain any data that needs to be loaded, but rather serve as containers for other resources.\n\nThe `open` method takes in a URL and data. It simply returns the data that was passed in. This method is used to retrieve the contents of a folder and return it to the caller.\n\nThe `handlerType` property is a string that specifies the type of resource that this handler is responsible for. In this case, it is set to \"folder\".\n\nThis class can be used in the larger PlayCanvas engine project to handle the loading and opening of folders. For example, if a user wants to load the contents of a folder, they can use this class to do so. Here is an example of how this class might be used:\n\n```\nimport { FolderHandler } from 'path/to/FolderHandler.js';\n\nconst folderUrl = 'path/to/folder';\nconst folderHandler = new FolderHandler();\n\nfolderHandler.load(folderUrl, (err, data) => {\n  if (err) {\n    console.error(`Error loading folder: ${err}`);\n  } else {\n    const folderContents = folderHandler.open(folderUrl, data);\n    console.log(`Folder contents: ${folderContents}`);\n  }\n});\n```\n\nIn this example, we import the `FolderHandler` class and create a new instance of it. We then specify the URL of the folder we want to load and call the `load` method with a callback function. If there are no errors, we call the `open` method with the same URL and the data that was returned from the `load` method. This will give us the contents of the folder, which we can then use as needed.\n## Questions: \n 1. **What is the purpose of the `FolderHandler` class?** \nThe `FolderHandler` class is a resource handler that handles folders.\n\n2. **What does the `load` method do?** \nThe `load` method takes a URL and a callback function as parameters and calls the callback with `null` values for both parameters.\n\n3. **What does the `open` method do?** \nThe `open` method takes a URL and data as parameters and returns the data.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/folder.md"}}],["351",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/font.js)\n\nThe code defines a `FontHandler` class that implements the `ResourceHandler` interface for loading font resources in a PlayCanvas engine project. The `FontHandler` class is responsible for loading font resources, upgrading font data schema, and patching font assets.\n\nThe `upgradeDataSchema` function is used to convert font data schema from version 1 and 2 to version 3. It checks the version of the font data and upgrades it if necessary. The function converts the character codes to letters and maps them to the corresponding characters. It also sets the version of the font data to 3.\n\nThe `FontHandler` class has a `load` method that loads font resources. If the font resource is a JSON file, it loads the JSON data and the texture of the same name. It then upgrades the asset data schema and loads the textures. If the font resource is not a JSON file, it upgrades the asset data schema and loads the textures. The `_loadTextures` method is used to load the textures of the font resource. It loads the textures and uploads them to the GPU.\n\nThe `open` method is used to create a new `Font` instance from the loaded font resource. If the font resource has both data and textures, it creates a new `Font` instance with the textures and data. If the font resource has only textures, it creates a new `Font` instance with the textures and null data.\n\nThe `patch` method is used to patch font assets. It gets the font data block from the asset and assigns it to the font resource if it is not already set. It also upgrades the asset data schema if necessary.\n\nThe `FontHandler` class can be used in a PlayCanvas engine project to load font resources and create `Font` instances. For example, to load a font resource, you can use the following code:\n\n```\nconst app = new pc.Application();\nconst fontHandler = new pc.FontHandler(app);\n\nfontHandler.load('fonts/myfont.json', function (err, font) {\n    if (!err) {\n        const fontAsset = new pc.Asset('myfont', 'font', {\n            url: 'fonts/myfont.json'\n        });\n        fontAsset.resource = font;\n        app.assets.add(fontAsset);\n        app.assets.load(fontAsset);\n    }\n});\n```\n\nThis code creates a new `FontHandler` instance and uses it to load a font resource. It then creates a new `Font` asset and adds it to the application assets. Finally, it loads the font asset.\n## Questions: \n 1. What is the purpose of the `upgradeDataSchema` function?\n- The `upgradeDataSchema` function is used to convert font data from version 1 or 2 to version 3 schema.\n\n2. What is the `FontHandler` class used for?\n- The `FontHandler` class is a resource handler used for loading `Font` resources.\n\n3. What is the purpose of the `loadTextures` method in the `FontHandler` class?\n- The `loadTextures` method is used to load the textures associated with a font resource. It takes in a URL and font data, and returns an array of textures.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/font.md"}}],["352",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/handler.js)\n\nThe code defines an interface called `ResourceHandler` that is used by the `ResourceLoader` to load and open resources. The `ResourceHandler` interface has three methods: `load`, `open`, and `patch`. \n\nThe `load` method is used to load a resource from a remote URL. It takes a URL or an object containing the URL and an original URL, a callback function to be called when the resource is loaded or an error occurs, and an optional asset. The callback function takes an error message and the raw data that has been loaded. If the resource fails to load, the error message will be non-null. If the resource is successfully loaded, the raw data will be passed to the `open` method.\n\nThe `open` method is used to convert the raw resource data into a resource instance. It takes the URL of the resource, the raw data passed by the `load` method, and an optional asset. It returns the parsed resource data. For example, if the raw data is a 3D model format JSON, the `open` method will return a `Model` instance.\n\nThe `patch` method is an optional function that can be used to perform any operations on a resource that requires a dependency on its asset data or any other asset data. It takes an asset and an asset registry as parameters.\n\nThis interface is used by the `ResourceLoader` to load and open resources. The `ResourceLoader` uses different `ResourceHandler` implementations for different types of resources. For example, there might be a `TextureHandler` that implements the `ResourceHandler` interface for loading and opening textures, and a `ModelHandler` that implements the `ResourceHandler` interface for loading and opening 3D models. \n\nHere is an example of how the `ResourceHandler` interface might be used in the larger project:\n\n```javascript\nimport { ResourceLoader } from 'playcanvas';\n\n// create a resource loader\nconst loader = new ResourceLoader();\n\n// register a texture handler\nloader.addHandler('.png', new TextureHandler());\n\n// register a model handler\nloader.addHandler('.json', new ModelHandler());\n\n// load a texture\nloader.load('textures/texture.png', (err, texture) => {\n    if (err) {\n        console.error(err);\n    } else {\n        // use the texture\n    }\n});\n\n// load a model\nloader.load('models/model.json', (err, model) => {\n    if (err) {\n        console.error(err);\n    } else {\n        // use the model\n    }\n});\n```\n## Questions: \n 1. What is the purpose of the `ResourceHandler` interface?\n    \n    The `ResourceHandler` interface is used by the `ResourceLoader` to load and open resources from a remote URL and convert raw resource data into a resource instance.\n\n2. What parameters does the `load` function take and what is its purpose?\n    \n    The `load` function takes a `url` parameter which can be either the URL of the resource to load or a structure containing the load and original URL, a `callback` parameter which is used when the resource is loaded or an error occurs, and an optional `asset` parameter that is passed by the `ResourceLoader`. The purpose of the `load` function is to load a resource from a remote URL and return the raw resource data or an error using the callback.\n\n3. What is the purpose of the `open` function and what parameters does it take?\n    \n    The `open` function is used to convert raw resource data into a resource instance. It takes a `url` parameter which is the URL of the resource to open, a `data` parameter which is the raw resource data passed by the `ResourceHandler#load` callback, and an optional `asset` parameter that is passed by the `ResourceLoader`. The function returns the parsed resource data.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/handler.md"}}],["353",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/hierarchy.js)\n\nThe code defines a class called `HierarchyHandler` that is responsible for handling resources of type \"hierarchy\". The class has two methods: `load` and `open`. \n\nThe `load` method takes a URL and a callback function as parameters. It uses the `SceneUtils.load` method to load the scene data from the specified URL and passes the loaded data to the callback function. The `maxRetries` property is used to specify the maximum number of times the `load` method should retry loading the scene data if it fails. \n\nThe `open` method takes a URL and scene data as parameters. It sets a flag to prevent script initialization until the entire scene is open. It then creates a new `SceneParser` object and uses it to parse the scene data. The parsed scene data is returned as a parent object. Finally, the flag is reset to enable script initialization. \n\nThe `HierarchyHandler` class is likely used in the larger PlayCanvas engine project to handle loading and parsing of scene data. It provides a standardized way of handling scene data and allows other parts of the engine to easily load and manipulate scene data. \n\nExample usage:\n\n```\nconst app = new pc.Application();\nconst handler = new HierarchyHandler(app);\n\n// load scene data from URL\nhandler.load('https://example.com/scene.json', function(data) {\n  // open scene data\n  const parent = handler.open('https://example.com/scene.json', data);\n  // manipulate scene data\n  // ...\n});\n```\n## Questions: \n 1. What is the purpose of the `SceneParser` and `SceneUtils` imports?\n- The `SceneParser` is used to parse scene data, while `SceneUtils` is used to load scene data with retries.\n\n2. What is the significance of the `handlerType` property?\n- The `handlerType` property specifies the type of resource that the `HierarchyHandler` handles.\n\n3. What does the `open` method do?\n- The `open` method initializes the `SceneParser` to parse the scene data, disables script initialization until the entire scene is open, and then returns the parent entity of the scene.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/hierarchy.md"}}],["354",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/html.js)\n\nThe code defines a class called `HtmlHandler` that is responsible for handling HTML resources. The class has three methods: `load`, `open`, and `patch`. \n\nThe `load` method is used to load an HTML resource from a given URL. It takes two parameters: `url` and `callback`. The `url` parameter can be either a string or an object that contains two properties: `load` and `original`. The `load` property is the URL to load the HTML resource from, while the `original` property is the original URL that was passed to the method. The `callback` parameter is a function that is called when the resource has been loaded. If the resource is loaded successfully, the `callback` function is called with two parameters: `null` and the loaded resource. If there is an error loading the resource, the `callback` function is called with an error message.\n\nThe `open` method is used to open an HTML resource. It takes two parameters: `url` and `data`. The `url` parameter is the URL of the resource, while the `data` parameter is the data that was loaded by the `load` method. The `open` method simply returns the `data` parameter.\n\nThe `patch` method is not implemented and does nothing.\n\nThe `HtmlHandler` class is exported so that it can be used by other parts of the PlayCanvas engine. It is likely that this class is used by other classes or modules in the engine to handle HTML resources. For example, a module that loads and displays HTML pages in a game might use the `HtmlHandler` class to load and handle the HTML resources. \n\nHere is an example of how the `load` method might be used:\n\n```\nconst htmlHandler = new HtmlHandler();\nconst url = 'https://example.com/index.html';\n\nhtmlHandler.load(url, function (err, response) {\n    if (!err) {\n        // Display the loaded HTML resource\n        console.log(response);\n    } else {\n        console.error(err);\n    }\n});\n```\n## Questions: \n 1. What is the purpose of the `http` import from `../../platform/net/http.js`?\n- The `http` import is used to make an HTTP GET request to load an HTML resource.\n\n2. What is the `open` method used for in the `HtmlHandler` class?\n- The `open` method is not used in this implementation of the `HtmlHandler` class.\n\n3. What is the `patch` method used for in the `HtmlHandler` class?\n- The `patch` method is not used in this implementation of the `HtmlHandler` class.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/html.md"}}],["355",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/json.js)\n\nThe code defines a class called `JsonHandler` that is responsible for handling JSON resources in the PlayCanvas engine project. The class has three methods: `constructor`, `load`, `open`, and `patch`. \n\nThe `constructor` method initializes the `maxRetries` property to 0. This property is used to determine the number of times the resource should be retried in case of an error during loading.\n\nThe `load` method is responsible for loading the JSON resource from the specified URL. It takes two parameters: `url` and `callback`. The `url` parameter can be either a string or an object that contains two properties: `load` and `original`. The `load` property contains the URL of the resource to be loaded, while the `original` property contains the original URL of the resource. If the `url` parameter is a string, it is converted to an object with `load` and `original` properties set to the same value.\n\nThe method then checks if the URL starts with `blob:`. If it does, it sets the `responseType` property of the `options` object to `Http.ResponseType.JSON`. This is necessary because blob URLs require a specific response type to be set in order to be parsed as JSON.\n\nFinally, the method calls the `http.get` function to load the resource from the specified URL. If the resource is loaded successfully, the `callback` function is called with the loaded resource as the second parameter. If there is an error during loading, the `callback` function is called with an error message as the first parameter.\n\nThe `open` method is responsible for opening the loaded JSON resource. It takes two parameters: `url` and `data`. The `url` parameter is the URL of the resource, while the `data` parameter is the loaded resource. The method simply returns the `data` parameter, indicating that the resource has been opened.\n\nThe `patch` method is responsible for patching the loaded JSON resource. It takes two parameters: `asset` and `assets`. The `asset` parameter is the asset to be patched, while the `assets` parameter is an array of assets. The method does not do anything and is empty.\n\nOverall, the `JsonHandler` class is an important part of the PlayCanvas engine project as it provides a way to handle JSON resources. It can be used to load and open JSON files, and can be extended to patch the loaded resources if necessary. Here is an example of how to use the `JsonHandler` class:\n\n```\nimport { JsonHandler } from 'path/to/json-handler.js';\n\nconst handler = new JsonHandler();\n\nhandler.load('path/to/json/file.json', function (err, data) {\n    if (!err) {\n        const openedData = handler.open('path/to/json/file.json', data);\n        console.log(openedData);\n    } else {\n        console.error(err);\n    }\n});\n```\n## Questions: \n 1. What is the purpose of the `JsonHandler` class?\n    \n    The `JsonHandler` class is a resource handler for JSON files.\n\n2. What is the significance of the `handlerType` property?\n    \n    The `handlerType` property is a string that specifies the type of resource that the `JsonHandler` class handles.\n\n3. What is the purpose of the `load` method and what parameters does it take?\n    \n    The `load` method is used to load a JSON resource from a URL and takes a URL string or object and a callback function as parameters. It also has options for retrying failed requests and setting the response type for blob URLs.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/json.md"}}],["356",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/loader.js)\n\n# ResourceLoader Class\n\nThe `ResourceLoader` class is responsible for loading resource data, potentially from remote sources, and caching the resource on load to prevent multiple requests. It also adds `ResourceHandlers` to handle different types of resources.\n\n## Constructor\n\nThe constructor takes an `app` parameter, which is an instance of `AppBase`. It initializes the `_handlers`, `_requests`, `_cache`, and `_app` properties.\n\n```javascript\nconstructor(app) {\n    this._handlers = {};\n    this._requests = {};\n    this._cache = {};\n    this._app = app;\n}\n```\n\n## Methods\n\n### addHandler(type, handler)\n\nThis method adds a `ResourceHandler` for a resource type. The handler should support at least `load()` and `open()`. Handlers can optionally support `patch(asset, assets)` to handle dependencies on other assets.\n\n```javascript\naddHandler(type, handler) {\n    this._handlers[type] = handler;\n    handler._loader = this;\n}\n```\n\n### removeHandler(type)\n\nThis method removes a `ResourceHandler` for a resource type.\n\n```javascript\nremoveHandler(type) {\n    delete this._handlers[type];\n}\n```\n\n### getHandler(type)\n\nThis method gets a `ResourceHandler` for a resource type.\n\n```javascript\ngetHandler(type) {\n    return this._handlers[type];\n}\n```\n\n### load(url, type, callback, asset)\n\nThis method makes a request for a resource from a remote URL. It parses the returned data using the handler for the specified type. When loaded and parsed, it uses the callback to return an instance of the resource.\n\n```javascript\nload(url, type, callback, asset) {\n    const handler = this._handlers[type];\n    if (!handler) {\n        const err = `No resource handler for asset type: '${type}' when loading [${url}]`;\n        Debug.errorOnce(err);\n        callback(err);\n        return;\n    }\n\n    // handle requests with null file\n    if (!url) {\n        this._loadNull(handler, callback, asset);\n        return;\n    }\n\n    const key = url + type;\n\n    if (this._cache[key] !== undefined) {\n        // in cache\n        callback(null, this._cache[key]);\n    } else if (this._requests[key]) {\n        // existing request\n        this._requests[key].push(callback);\n    } else {\n        // new request\n        this._requests[key] = [callback];\n\n        const self = this;\n\n        const handleLoad = function (err, urlObj) {\n            if (err) {\n                self._onFailure(key, err);\n                return;\n            }\n\n            handler.load(urlObj, function (err, data, extra) {\n                // make sure key exists because loader\n                // might have been destroyed by now\n                if (!self._requests[key]) {\n                    return;\n                }\n\n                if (err) {\n                    self._onFailure(key, err);\n                    return;\n                }\n\n                try {\n                    self._onSuccess(key, handler.open(urlObj.original, data, asset), extra);\n                } catch (e) {\n                    self._onFailure(key, e);\n                }\n            }, asset);\n        };\n\n        const normalizedUrl = url.split('?')[0];\n        if (this._app.enableBundles && this._app.bundles.hasUrl(normalizedUrl)) {\n            if (!this._app.bundles.canLoadUrl(normalizedUrl)) {\n                handleLoad(`Bundle for ${url} not loaded yet`);\n                return;\n            }\n\n            this._app.bundles.loadUrl(normalizedUrl, function (err, fileUrlFromBundle) {\n                handleLoad(err, {\n                    load: fileUrlFromBundle,\n                    original: normalizedUrl\n                });\n            });\n        } else {\n            handleLoad(null, {\n                load: url,\n                original: asset && asset.file.filename || url\n            });\n        }\n    }\n}\n```\n\n### open(type, data)\n\nThis method converts raw resource data into a resource instance. For example, it takes 3D model format JSON and returns a `Model`.\n\n```javascript\nopen(type, data) {\n    const handler = this._handlers[type];\n    if (!handler) {\n        console.warn('No resource handler found for: ' + type);\n        return data;\n    }\n\n    return handler.open(null, data);\n}\n```\n\n### patch(asset, assets)\n\nThis method performs any operations on a resource that require a dependency on its asset data or any other asset data.\n\n```javascript\npatch(asset, assets) {\n    const handler = this._handlers[asset.type];\n    if (!handler)  {\n        console.warn('No resource handler found for: ' + asset.type);\n        return;\n    }\n\n    if (handler.patch) {\n        handler.patch(asset, assets);\n    }\n}\n```\n\n### clearCache(url, type)\n\nThis method removes a resource from the cache.\n\n```javascript\nclearCache(url, type) {\n    delete this._cache[url + type];\n}\n```\n\n### getFromCache(url, type)\n\nThis method checks the cache for a resource from a URL. If present, it returns the cached value.\n\n```javascript\ngetFromCache(url, type) {\n    if (this._cache[url + type]) {\n        return this._cache[url + type];\n    }\n    return undefined;\n}\n```\n\n### enableRetry(maxRetries)\n\nThis method enables retrying of failed requests when loading assets. The `maxRetries` parameter is the maximum number of times to retry loading an asset. It defaults to 5.\n\n```javascript\nenableRetry(maxRetries = 5) {\n    maxRetries = Math.max(0, maxRetries) || 0;\n\n    for (const key in this._handlers) {\n        this._handlers[key].maxRetries = maxRetries;\n    }\n}\n```\n\n### disableRetry()\n\nThis method disables retrying of failed requests when loading assets.\n\n```javascript\ndisableRetry() {\n    for (const key in this._handlers) {\n        this._handlers[key].maxRetries = 0;\n    }\n}\n```\n\n### destroy()\n\nThis method destroys the resource loader.\n\n```javascript\ndestroy() {\n    this._handlers = {};\n    this._requests = {};\n    this._cache = {};\n}\n```\n\n## Callbacks\n\n### ResourceLoaderCallback\n\nThis callback is used by `ResourceLoader.load()` when a resource is loaded (or an error occurs). It takes two parameters:\n\n- `err`: The error message in the case where the load fails.\n- `resource`: The resource that has been successfully loaded.\n\n```javascript\n/**\n * Callback used by {@link ResourceLoader#load} when a resource is loaded (or an error occurs).\n *\n * @callback ResourceLoaderCallback\n * @param {string|null} err - The error message in the case where the load fails.\n * @param {*} [resource] - The resource that has been successfully loaded.\n */\n```\n## Questions: \n 1. What is the purpose of the `ResourceLoader` class?\n- The `ResourceLoader` class is used to load resource data, potentially from remote sources, and caches the resource on load to prevent multiple requests. It also allows for the addition and removal of `ResourceHandlers` to handle different types of resources.\n\n2. What is the `load` method used for?\n- The `load` method is used to make a request for a resource from a remote URL, parse the returned data using the handler for the specified type, and return an instance of the resource using the provided callback.\n\n3. What is the purpose of the `patch` method?\n- The `patch` method is used to perform any operations on a resource that require a dependency on its asset data or any other asset data. It takes an `Asset` object and an `AssetRegistry` object as parameters.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/loader.md"}}],["357",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/material.js)\n\nThe code defines a MaterialHandler class that is responsible for loading and handling Material resources in the PlayCanvas engine. The MaterialHandler class implements the ResourceHandler interface and provides methods for loading, opening, and patching Material resources. \n\nThe load method is responsible for loading Material resources from a URL. It uses the http module to make a GET request to the specified URL and returns the response data. If an error occurs during the request, it returns an error message.\n\nThe open method is responsible for parsing the data returned by the load method and creating a new Material instance. It uses the JsonStandardMaterialParser class to parse the data and create a new Material instance.\n\nThe patch method is responsible for patching the Material resource with any changes made to the asset. It updates the Material name property with the asset name and binds and assigns any assets required by the Material.\n\nThe MaterialHandler class also defines several helper methods for handling textures and cubemaps. The _assignTexture method assigns a texture to a Material resource, while the _assignCubemap method assigns a cubemap to a Material resource. The _assignPlaceholderTexture method assigns a placeholder texture to a Material resource while waiting for the actual texture to load. The _getPlaceholderTexture method returns the correct placeholder texture for a given texture parameter.\n\nThe MaterialHandler class also defines a constant object called PLACEHOLDER_MAP that maps texture parameter names to placeholder texture names. This object is used by the _getPlaceholderTexture and _assignPlaceholderTexture methods.\n\nOverall, the MaterialHandler class is an important part of the PlayCanvas engine as it provides a way to load and handle Material resources. It is used by other parts of the engine to create and manage Materials for use in the scene.\n## Questions: \n 1. What is the purpose of the `MaterialHandler` class?\n- The `MaterialHandler` class is a resource handler used for loading `Material` resources.\n\n2. What is the `PLACEHOLDER_MAP` object used for?\n- The `PLACEHOLDER_MAP` object is used to map texture parameter names to placeholder texture names.\n\n3. What is the `_bindAndAssignAssets` method used for?\n- The `_bindAndAssignAssets` method is used to bind and assign assets to a material, including texture and cubemap assets.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/material.md"}}],["358",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/model.js)\n\nThe `ModelHandler` class is a resource handler used for loading 3D models in the PlayCanvas engine. It implements the `ResourceHandler` interface and is responsible for loading and parsing model data from various sources. \n\nThe `ModelHandler` constructor takes an instance of `AppBase` as an argument and initializes the `_device` property with the graphics device of the app. It also initializes the `_parsers` array with two parsers: `JsonModelParser` and `GlbModelParser`. These parsers are used to parse JSON and GLB model data respectively. The `addParser` method can be used to add additional parsers to the `_parsers` array.\n\nThe `load` method is used to load model data from a given URL. It takes a URL and a callback function as arguments. If the URL is a string, it is converted to an object with `load` and `original` properties. The `http.get` method is then used to fetch the model data from the URL. The `options` object passed to `http.get` specifies the response type based on the file extension of the URL. If the URL starts with `blob:` or `data:`, the response type is set to either `ARRAY_BUFFER` or `JSON` depending on the file extension. Once the model data is fetched, the `_parsers` array is iterated over to find the appropriate parser for the data. The `decider` function of each parser is called with the URL and response data as arguments. If a parser's `decider` function returns `true`, the parser's `parse` method is called with the response data and a callback function as arguments. The callback function is called with the parsed model data or an error if parsing fails.\n\nThe `open` method is used to process the parsed model data. It takes a URL and the parsed data as arguments and returns the data as-is.\n\nThe `patch` method is used to patch the loaded model data with additional data. It takes an `asset` object and an `assets` object as arguments. The `asset` object contains the loaded model data and the `assets` object contains all the assets loaded by the app. The `patch` method is used to map materials to mesh instances in the loaded model data. If a material is not found in the `assets` object, it is loaded and added to the `assets` object. If a material is removed from the `assets` object, the corresponding mesh instance is set to use the default material.\n\nThe `addParser` method is used to add a parser to the `_parsers` array. It takes a parser object and a `decider` function as arguments. The `decider` function is used to decide if the parser should be used to parse the model data. It takes a URL and data as arguments and returns `true` if the parser should be used.\n\nOverall, the `ModelHandler` class is an important part of the PlayCanvas engine as it provides a way to load and parse 3D model data from various sources. It can be extended with additional parsers to support new model formats.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file is a resource handler used for loading Model resources in the PlayCanvas engine.\n\n2. What types of parsers are added to this ModelHandler instance?\n- Two types of parsers are added to this ModelHandler instance: JsonModelParser and GlbModelParser.\n\n3. What is the purpose of the `patch` method in this ModelHandler class?\n- The `patch` method is used to modify the loaded asset by replacing its meshInstance material with a new one based on the data mapping provided in the asset.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/model.md"}}],["359",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/render.js)\n\nThe code defines a `RenderHandler` class that implements the `ResourceHandler` interface. This handler is used for loading `Render` resources in the PlayCanvas engine project. The `RenderHandler` class has three methods: `load`, `open`, and `patch`.\n\nThe `load` method is empty and does not perform any action. The `open` method creates a new `Render` object and returns it. The `patch` method is called when a new `Render` asset is added to the registry. It checks if the `Render` asset has a `containerAsset` property. If it does, it retrieves the container asset from the registry. If the container asset is not found, it registers a callback to be called when the container asset is added to the registry. If the container asset is found, it calls the `onContainerAssetAdded` function with the container asset as an argument.\n\nThe `onContainerAssetAdded` function is called when a container asset is added to the registry. It removes any existing event listeners for the container asset and registers new ones. It then checks if the container asset has a resource. If it does not, it loads the container asset. If it does, it calls the `onContainerAssetLoaded` function with the container asset as an argument.\n\nThe `onContainerAssetLoaded` function is called when a container asset is loaded. It retrieves the `Render` asset and the container resource. It then retrieves the `Render` object from the container resource using the `renderIndex` property of the `Render` asset. If the `Render` object is found, it sets the `meshes` property of the `Render` asset's resource to the `meshes` property of the `Render` object's resource.\n\nThe `onContainerAssetRemoved` function is called when a container asset is removed from the registry. It removes the event listener for the container asset and destroys the `Render` asset's resource if it exists.\n\nOverall, this code defines a `RenderHandler` class that is used for loading `Render` resources in the PlayCanvas engine project. It provides methods for loading, opening, and patching `Render` assets. The `patch` method retrieves the container asset and calls the `onContainerAssetAdded` function. The `onContainerAssetAdded` function registers event listeners for the container asset and loads it if it is not loaded. The `onContainerAssetLoaded` function retrieves the `Render` object from the container resource and sets the `meshes` property of the `Render` asset's resource. The `onContainerAssetRemoved` function removes the event listener for the container asset and destroys the `Render` asset's resource.\n## Questions: \n 1. What is the purpose of the `onContainerAssetLoaded` function?\n   - The `onContainerAssetLoaded` function is called when a container asset is loaded and it sets the meshes of the render asset's resource to the meshes of the loaded container resource's render at the specified index.\n\n2. What is the `RenderHandler` class used for?\n   - The `RenderHandler` class is a resource handler used for loading `Render` resources and it implements the `ResourceHandler` interface.\n\n3. What happens in the `patch` method of the `RenderHandler` class?\n   - The `patch` method of the `RenderHandler` class checks if the render asset has a container asset and if it does, it either calls `onContainerAssetAdded` with the container asset or adds a listener to the registry for when the container asset is added.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/render.md"}}],["360",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/scene-settings.js)\n\nThe `SceneSettingsHandler` class is a part of the PlayCanvas engine project and is responsible for handling scene settings. The purpose of this class is to provide a way to load and open scene settings for a given URL. \n\nThe class imports `SceneUtils` from another file called `scene-utils.js`. The `SceneUtils` module is used to load the scene data from the given URL. The `load` method of the `SceneSettingsHandler` class takes in a URL and a callback function. It then calls the `load` method of the `SceneUtils` module with the given URL and the maximum number of retries allowed. If the scene data is successfully loaded, the callback function is called with the loaded data as an argument.\n\nThe `open` method of the `SceneSettingsHandler` class takes in a URL and data. It then returns the `settings` property of the given data. This method is used to open the scene settings for a given URL.\n\nThe `constructor` method of the `SceneSettingsHandler` class takes in an `app` object and sets it as a property of the class. It also sets the `maxRetries` property to 0.\n\nOverall, the `SceneSettingsHandler` class provides a way to load and open scene settings for a given URL. It is a useful tool for developers working on the PlayCanvas engine project who need to access and modify scene settings. \n\nExample usage:\n\n```\nimport { SceneSettingsHandler } from './scene-settings-handler.js';\n\nconst app = new pc.Application();\nconst sceneSettingsHandler = new SceneSettingsHandler(app);\n\nsceneSettingsHandler.load('https://example.com/scene.json', (data) => {\n  const settings = sceneSettingsHandler.open('https://example.com/scene.json', data);\n  console.log(settings);\n});\n```\n## Questions: \n 1. What is the purpose of the SceneUtils module being imported?\n    - The SceneUtils module is being used to load a scene from a given URL with a specified number of retries.\n\n2. What is the significance of the maxRetries property being set to 0 in the constructor?\n    - The maxRetries property determines the number of times the scene will attempt to load before failing. Setting it to 0 means that it will not retry if it fails on the first attempt.\n\n3. What does the open method do and what is its expected return value?\n    - The open method takes a URL and data as parameters and returns the settings data from the loaded scene.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/scene-settings.md"}}],["361",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/scene-utils.js)\n\nThe code defines a module called `SceneUtils` that provides a single function called `load`. The purpose of this function is to load a scene JSON file from a URL and invoke a callback function when the file has been loaded. The function takes three arguments: `url`, `maxRetries`, and `callback`.\n\nThe `url` argument is a string that represents the URL of the scene JSON file to be loaded. If the `url` argument is not a string, the function assumes that it is an object with a `load` property that contains the URL of the scene JSON file to be loaded and an `original` property that contains the original URL passed to the function.\n\nThe `maxRetries` argument is a number that represents the maximum number of times the function should retry loading the scene JSON file if the initial attempt fails. If `maxRetries` is greater than 0, the function will retry loading the file until it succeeds or until the maximum number of retries has been reached.\n\nThe `callback` argument is a function that will be invoked when the scene JSON file has been loaded. The function takes two arguments: `err` and `response`. If the file was loaded successfully, `err` will be `null` and `response` will contain the loaded JSON data. If the file failed to load, `err` will contain an error message and `response` will be `undefined`.\n\nThe function uses the `http` module from the PlayCanvas engine to load the scene JSON file. It calls the `http.get` function with the URL of the file and an options object that specifies whether to retry loading the file and how many times to retry. If the file is loaded successfully, the function invokes the callback function with `err` set to `null` and `response` set to the loaded JSON data. If the file fails to load, the function constructs an error message and invokes the callback function with `err` set to the error message and `response` set to `undefined`.\n\nThis function is useful for loading scene JSON files in PlayCanvas projects. It provides a simple interface for loading files and handling errors, and it can be used in conjunction with other PlayCanvas modules to create complex scenes and applications. Here is an example of how to use the `SceneUtils.load` function:\n\n```\nimport { SceneUtils } from 'path/to/SceneUtils.js';\n\nSceneUtils.load('path/to/scene.json', 3, function (err, response) {\n    if (!err) {\n        // Use the loaded JSON data to create a scene\n    } else {\n        console.error(err);\n    }\n});\n```\n\nThis code loads the scene JSON file located at `'path/to/scene.json'` with a maximum of 3 retries. If the file is loaded successfully, the callback function creates a scene using the loaded JSON data. If the file fails to load, the callback function logs an error message to the console.\n## Questions: \n 1. What is the purpose of the `http` import and where is it coming from?\n   - The `http` import is coming from a file located at `../../platform/net/http.js` and is used to make HTTP requests.\n2. What is the expected format of the scene JSON file that is being loaded?\n   - The code does not provide information on the expected format of the scene JSON file.\n3. What happens if the `maxRetries` parameter is set to 0?\n   - If the `maxRetries` parameter is set to 0, the `retry` option in the `http.get` call will be `false` and no retries will be attempted if the initial request fails.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/scene-utils.md"}}],["362",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/scene.js)\n\nThe code defines a resource handler for loading and opening scene resources in the PlayCanvas engine project. The `SceneHandler` class implements the `ResourceHandler` interface and has two main methods: `load` and `open`. \n\nThe `load` method takes a URL and a callback function as parameters and uses the `SceneUtils` module to load the scene data from the specified URL. The `maxRetries` property is used to specify the maximum number of retries in case of a failed load. \n\nThe `open` method takes a URL and the scene data as parameters and is responsible for parsing and opening the scene. It first sets the `preloading` property of the `script` system to `true` to prevent script initialization until the entire scene is open. It then creates a new `SceneParser` instance and uses it to parse the scene data. The parsed data is set as the root of the scene and the scene settings are applied using the `_app.applySceneSettings` method. Finally, the `preloading` property is set back to `false` to re-enable script initialization. The method returns the opened scene.\n\nThe `SceneHandler` class is used as a resource handler for scene resources in the PlayCanvas engine project. It provides a way to load and open scene data and is used by other parts of the engine to manage scenes. For example, a game developer could use this handler to load and open different levels of a game as scenes. \n\nExample usage:\n\n```\nimport { SceneHandler } from './scene-handler.js';\n\n// create a new instance of the SceneHandler class\nconst sceneHandler = new SceneHandler(app);\n\n// load a scene from a URL\nsceneHandler.load('https://example.com/scene.json', (err, data) => {\n  if (err) {\n    console.error(err);\n  } else {\n    // open the scene\n    const scene = sceneHandler.open('https://example.com/scene.json', data);\n    // use the scene in the game\n    app.scene = scene;\n  }\n});\n```\n## Questions: \n 1. What is the purpose of the `SceneHandler` class?\n    \n    The `SceneHandler` class is a resource handler used for loading and opening `Scene` resources in the PlayCanvas engine.\n\n2. What is the `load` method used for?\n    \n    The `load` method is used to load a `Scene` resource using `SceneUtils.load` with a specified URL and callback function.\n\n3. What is the `patch` method used for?\n    \n    The `patch` method is an empty method that does not have any functionality and is not used in the `SceneHandler` class.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/scene.md"}}],["363",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/script.js)\n\nThe `ScriptHandler` class is a resource handler for loading JavaScript files dynamically in the PlayCanvas engine project. It is responsible for loading two types of JavaScript files: PlayCanvas scripts that contain calls to `createScript`, and regular JavaScript files such as third-party libraries. \n\nThe class implements the `ResourceHandler` interface, which requires the implementation of four methods: `load`, `open`, `patch`, and `loadScript`. The `load` method is called when a resource is requested to be loaded. It takes a URL and a callback function as parameters. If the URL is a string, it is converted to an object with `load` and `original` properties for consistency. The method then calls the `_loadScript` method to load the script. If the script is loaded successfully, the method returns the resource to the callback function. \n\nThe `open` method is called when a resource is requested to be opened. It takes a URL and data as parameters and returns the data. The `patch` method is called when a resource is requested to be patched. It takes an asset and assets as parameters and does nothing. \n\nThe `_loadScript` method is a private method that loads the script by creating a new `script` element and appending it to the `head` of the document. The method sets the `async` attribute of the `script` element to `false` to force scripts to execute in order. It also sets an `onload` event listener to fire when the script is loaded successfully. If the script fails to load, an error message is returned. \n\nThe `ScriptHandler` class also has a constructor that takes an `app` parameter and initializes the `_app`, `_scripts`, and `_cache` properties. The `_app` property is set to the running `AppBase`. The `_scripts` property is an object that stores the PlayCanvas scripts indexed by URL. The `_cache` property is an object that stores the `script` elements indexed by URL. \n\nOverall, the `ScriptHandler` class is an important part of the PlayCanvas engine project as it allows for the dynamic loading of JavaScript files, including PlayCanvas scripts and third-party libraries. It provides a consistent way to load resources and ensures that scripts are executed in order. Developers can use this class to load scripts and other resources as needed in their PlayCanvas projects. \n\nExample usage:\n\n```javascript\nimport { ScriptHandler } from 'playcanvas-engine';\n\nconst app = new pc.Application();\nconst scriptHandler = new ScriptHandler(app);\n\nscriptHandler.load('path/to/script.js', (err, resource) => {\n  if (!err) {\n    console.log(resource);\n  } else {\n    console.error(err);\n  }\n});\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines a resource handler class called `ScriptHandler` that is responsible for dynamically loading JavaScript files in a PlayCanvas engine project. It can load both PlayCanvas scripts and regular JavaScript files.\n\n2. What is the `ResourceHandler` interface and how does `ScriptHandler` implement it?\n    \n    The `ResourceHandler` interface is not defined in this code, but it is referenced in a `@typedef` annotation. It is likely an interface that defines methods for loading, opening, and patching resources. `ScriptHandler` implements this interface by defining these methods (`load`, `open`, and `patch`) and providing their implementations.\n\n3. What is the purpose of the `ScriptTypes` module and how is it used in this code?\n    \n    The `ScriptTypes` module is imported in this code and is used to keep track of PlayCanvas script types. It is used to determine the type of a loaded script and to store it in the `_scripts` object. It is also used to create an object that maps script type names to their corresponding types, which is passed to the `callback` function when a script is loaded.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/script.md"}}],["364",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/shader.js)\n\nThe code defines a class called `ShaderHandler` that is responsible for handling shader resources in the PlayCanvas engine project. The `ShaderHandler` class has three methods: `load()`, `open()`, and `patch()`. \n\nThe `load()` method is used to load a shader resource from a given URL. It takes two parameters: `url` and `callback`. The `url` parameter can be either a string or an object that contains two properties: `load` and `original`. The `load` property is the URL from which the shader resource should be loaded, and the `original` property is the original URL that was passed to the `load()` method. The `callback` parameter is a function that is called when the shader resource has been loaded. If the resource is loaded successfully, the `callback` function is called with two parameters: `null` and the loaded resource. If there is an error loading the resource, the `callback` function is called with an error message.\n\nThe `open()` method is used to open a shader resource that has already been loaded. It takes two parameters: `url` and `data`. The `url` parameter is the URL of the shader resource, and the `data` parameter is the loaded resource. The `open()` method simply returns the `data` parameter.\n\nThe `patch()` method is not implemented and does nothing. It takes two parameters: `asset` and `assets`. The `asset` parameter is the shader resource that needs to be patched, and the `assets` parameter is an object that contains all the loaded assets.\n\nThe `ShaderHandler` class has a property called `handlerType` that is set to `\"shader\"`. This property specifies the type of resource that the `ShaderHandler` class handles.\n\nThe `ShaderHandler` class is exported from the module, so it can be used in other parts of the PlayCanvas engine project. For example, it can be used by the asset registry to load and handle shader resources. Here is an example of how the `ShaderHandler` class can be used:\n\n```\nimport { AssetRegistry } from 'playcanvas';\n\nconst registry = new AssetRegistry();\n\nregistry.addHandler('shader', new ShaderHandler());\n\nregistry.loadFromUrl('path/to/shader.json', function (err, asset) {\n    if (!err) {\n        const shader = asset.resource;\n        // use the loaded shader resource\n    } else {\n        console.error(err);\n    }\n});\n```\n\nIn this example, a new `AssetRegistry` instance is created, and a new `ShaderHandler` instance is added to the registry for handling shader resources. Then, the `loadFromUrl()` method is called to load a shader resource from a URL. When the resource is loaded, the `callback` function is called with an `Asset` object that contains the loaded resource. The loaded shader resource can then be accessed through the `resource` property of the `Asset` object.\n## Questions: \n 1. What is the purpose of the `http` import from `../../platform/net/http.js`?\n- The `http` import is used to make an HTTP GET request to load a shader resource.\n\n2. What is the significance of the `handlerType` property in the `ShaderHandler` class?\n- The `handlerType` property specifies the type of resource that the `ShaderHandler` class handles, which is \"shader\".\n\n3. What is the purpose of the `patch` method in the `ShaderHandler` class?\n- The `patch` method is currently empty and does not have a defined purpose in the `ShaderHandler` class. It may be intended for future use or as a placeholder for potential functionality.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/shader.md"}}],["365",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/sprite.js)\n\nThe code defines a `SpriteHandler` class that is used to handle the loading and processing of `Sprite` resources in the PlayCanvas engine. The `SpriteHandler` class implements the `ResourceHandler` interface, which defines the methods that must be implemented to handle a specific type of resource. \n\nThe `SpriteHandler` class has several methods that are used to load, open, and patch sprite resources. The `load` method is used to load the sprite resource from a URL. If the URL is a JSON file, the method uses the `http.get` method to retrieve the file and passes the response to the callback function. The `open` method is used to create a new `Sprite` object and store the data temporarily if the URL is present. The `patch` method is used to set the sprite data and update the sprite atlas. \n\nThe `SpriteHandler` class also has two helper functions, `onTextureAtlasLoaded` and `onTextureAtlasAdded`, which are used to handle the loading and adding of texture atlas assets. These functions are called with the scope of the sprite asset and are used to set the sprite atlas when the texture atlas asset is loaded.\n\nOverall, the `SpriteHandler` class is an important part of the PlayCanvas engine as it handles the loading and processing of sprite resources. It provides a way to create and update sprite objects and set their properties, such as the render mode, pixels per unit, and frame keys. The `SpriteHandler` class also handles the loading of texture atlas assets, which are used to define the texture regions for the sprite frames.\n## Questions: \n 1. What is the purpose of the `onTextureAtlasLoaded` function?\n- The `onTextureAtlasLoaded` function is called when a texture atlas is loaded and sets the atlas resource on the sprite asset.\n\n2. What is the `SpriteHandler` class used for?\n- The `SpriteHandler` class is a resource handler used for loading `Sprite` resources.\n\n3. What happens in the `_updateAtlas` method?\n- The `_updateAtlas` method loads the texture atlas asset and sets the atlas resource on the sprite asset. If the atlas asset is not yet loaded, it registers event listeners to load the asset and set the atlas resource when it is loaded.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/sprite.md"}}],["366",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/template.js)\n\nThe code defines a class called `TemplateHandler` that is responsible for handling templates in the PlayCanvas engine project. The `TemplateHandler` class has two methods: `load` and `open`. \n\nThe `load` method takes a URL and a callback function as parameters. It first checks if the URL is a string and if so, converts it to an object with two properties: `load` and `original`. The `load` property is the URL to be loaded, and the `original` property is the original URL passed to the method. \n\nThe method then sets some options for the HTTP request, including whether to retry the request and how many times to retry. It then makes an HTTP GET request to the specified URL using the `http.get` method from the PlayCanvas engine's `http` module. If the request is successful, the method calls the callback function with the response data. If the request fails, the method calls the callback function with an error message.\n\nThe `open` method takes a URL and data as parameters and returns a new `Template` object. The `Template` object is defined in another module in the PlayCanvas engine project and is responsible for rendering HTML templates.\n\nOverall, the `TemplateHandler` class provides a way to load and render HTML templates in the PlayCanvas engine project. It can be used by other modules in the project that need to display dynamic content using HTML templates. For example, a game developer could use the `TemplateHandler` class to load and render a template for a game's user interface. \n\nExample usage:\n\n```\nimport { TemplateHandler } from './template-handler.js';\n\nconst app = ... // create a PlayCanvas app object\n\nconst templateHandler = new TemplateHandler(app);\n\ntemplateHandler.load('/templates/my-template.html', function (err, response) {\n    if (err) {\n        console.error(err);\n    } else {\n        const template = templateHandler.open('/templates/my-template.html', response);\n        // use the template to render dynamic content\n    }\n});\n```\n## Questions: \n 1. What is the purpose of the `http` import from `../../platform/net/http.js`?\n- A smart developer might ask what functionality the `http` module provides and how it is used in this code. \n\n2. What is the `Template` class that is being imported from `../template.js`?\n- A smart developer might ask what methods and properties the `Template` class has and how it is used in this code.\n\n3. What is the significance of the `handlerType` property being set to `\"template\"`?\n- A smart developer might ask how the `handlerType` property is used in the PlayCanvas engine and what other values it can take.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/template.md"}}],["367",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/text.js)\n\nThe code defines a class called `TextHandler` that is responsible for handling text resources in the PlayCanvas engine project. The `TextHandler` class has three methods: `load`, `open`, and `patch`. \n\nThe `load` method is used to load a text resource from a given URL. It takes two parameters: `url` and `callback`. The `url` parameter can be either a string or an object that contains two properties: `load` and `original`. The `load` property is the URL from which the text resource should be loaded, and the `original` property is the original URL that was passed to the `load` method. The `callback` parameter is a function that is called when the text resource has been loaded. If the resource is loaded successfully, the `callback` function is called with two parameters: `null` and the loaded resource. If there is an error loading the resource, the `callback` function is called with an error message.\n\nThe `open` method is used to open a text resource that has already been loaded. It takes two parameters: `url` and `data`. The `url` parameter is the URL of the text resource, and the `data` parameter is the loaded resource. The `open` method simply returns the loaded resource.\n\nThe `patch` method is not implemented and does nothing.\n\nThe `TextHandler` class has a property called `handlerType` that is set to `\"text\"`. This property indicates the type of resource that the `TextHandler` class handles.\n\nThe `TextHandler` class imports the `http` module from the `platform/net/http.js` file. The `http` module is used to make HTTP requests to load the text resource.\n\nOverall, the `TextHandler` class is a simple implementation of a resource handler for text resources. It provides methods for loading and opening text resources, and can be used in the larger PlayCanvas engine project to handle text resources. An example usage of the `TextHandler` class might look like this:\n\n```\nimport { TextHandler } from 'path/to/TextHandler.js';\n\nconst textHandler = new TextHandler();\n\ntextHandler.load('path/to/text/resource.txt', (err, text) => {\n  if (!err) {\n    console.log(text);\n  } else {\n    console.error(err);\n  }\n});\n```\n## Questions: \n 1. What is the purpose of the `http` import from `../../platform/net/http.js`?\n- A smart developer might ask what functionality the `http` module provides and how it is used in this code. \n\n2. What is the significance of the `handlerType` property and how is it used?\n- A smart developer might ask why the `handlerType` property is set to `\"text\"` and how it is used in the PlayCanvas engine.\n\n3. What is the purpose of the `open` and `patch` methods and when are they called?\n- A smart developer might ask what the `open` and `patch` methods do and when they are called in the PlayCanvas engine's resource loading process.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/text.md"}}],["368",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/texture-atlas.js)\n\nThe code defines a `TextureAtlasHandler` class that is used to handle the loading and processing of texture atlas resources. A texture atlas is a collection of smaller images (called frames) that are combined into a larger image to reduce the number of texture lookups required by a game engine. \n\nThe `TextureAtlasHandler` class implements the `ResourceHandler` interface, which defines two methods: `load` and `open`. The `load` method is responsible for loading the texture atlas data from a URL, while the `open` method is responsible for creating a new `TextureAtlas` resource from the loaded data.\n\nThe `TextureAtlasHandler` class also defines two dictionaries: `JSON_ADDRESS_MODE` and `JSON_FILTER_MODE`. These dictionaries map string values to constants that are used to configure the texture atlas texture.\n\nThe `TextureAtlasHandler` class uses the `http` module to load the texture atlas data from a URL. If the URL ends with `.json`, the handler loads the JSON data and then loads the texture of the same name. Otherwise, it delegates the loading to the `texture` resource handler.\n\nOnce the texture atlas data is loaded, the `open` method creates a new `TextureAtlas` resource and sets its `texture` property to the loaded texture. If the texture atlas data contains frame information, the `open` method also sets the `frames` property of the `TextureAtlas` resource.\n\nThe `TextureAtlasHandler` class also defines a `_onAssetChange` method that is used to update the `TextureAtlas` resource when its associated asset changes. This method is called when the `change` event is fired on the asset.\n\nOverall, the `TextureAtlasHandler` class is an important part of the PlayCanvas engine that is used to load and process texture atlas resources. It provides a convenient way to manage texture atlases and reduce the number of texture lookups required by a game engine.\n## Questions: \n 1. What is the purpose of the `TextureAtlasHandler` class?\n    \n    The `TextureAtlasHandler` class is a resource handler used for loading and creating `TextureAtlas` resources.\n\n2. What is the role of the `load` method in the `TextureAtlasHandler` class?\n    \n    The `load` method is responsible for loading the texture atlas texture using the texture resource loader. If supplied with a JSON file URL, it loads the JSON data and then loads the texture of the same name.\n\n3. What is the purpose of the `patch` method in the `TextureAtlasHandler` class?\n    \n    The `patch` method is used to update the texture atlas resource with the asset data. It sets the frames, passes texture data, and listens for changes to the asset.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/texture-atlas.md"}}],["369",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/texture.js)\n\nThe code defines a TextureHandler class that is responsible for loading and parsing 2D and 3D texture assets. It imports several constants and classes from other modules, including parsers for different texture formats. The TextureHandler class implements the ResourceHandler interface and has a load method that takes a URL and a callback function as arguments. The method determines the appropriate parser to use based on the file extension of the URL and calls the parser's load method to load the texture asset. The TextureHandler class also has an open method that takes a URL, raw data, and an asset as arguments. The method determines the appropriate parser to use based on the file extension of the URL and calls the parser's open method to parse the raw data and create a Texture object. The Texture object is returned by the open method.\n\nThe code also defines a TextureParser interface that has two methods: load and open. The load method takes a URL, a callback function, and an optional asset as arguments. The method is responsible for loading the texture asset from the remote URL and calling the callback function with the raw resource data or an error. The open method takes a URL, raw data, and a graphics device as arguments. The method is responsible for parsing the raw data and creating a Texture object.\n\nThe code defines several constants that map JSON values to their corresponding constants used in the engine. These constants are used to set properties of the Texture object, such as minFilter, magFilter, addressU, addressV, mipmaps, anisotropy, flipY, and type.\n\nThe code also defines a function called _completePartialMipmapChain that is used to generate missing levels of mip data for a texture that has more than one level of mip data specified but not the full mip chain. This is done to overcome an issue where some devices ignore further updates to the mip data after invoking gl.generateMipmap on the texture. The function only resamples RGBA8 and RGBAFloat32 data.\n\nOverall, the TextureHandler class and TextureParser interface provide a way to load and parse texture assets in different formats and create Texture objects that can be used in the engine. The code demonstrates how to use different parsers for different file formats and how to set properties of the Texture object based on JSON values.\n## Questions: \n 1. What is the purpose of the `TextureParser` interface and its methods?\n- The `TextureParser` interface is used to handle the loading and opening of texture assets. It has two methods: `load` and `open`. The `load` method loads the texture from a remote URL and returns the raw resource data or an error using a callback. The `open` method converts the raw resource data into a resource instance, such as a `Texture`.\n\n2. What is the purpose of the `_completePartialMipmapChain` function?\n- The `_completePartialMipmapChain` function is used to generate missing levels of mip data for a texture that has more than one level of mip data specified but not the full mip chain. This is done to overcome an issue where certain devices ignore further updates to the mip data after invoking `gl.generateMipmap` on the texture.\n\n3. What is the purpose of the `patch` method in the `TextureHandler` class?\n- The `patch` method is used to modify a `Texture` asset after it has been loaded and opened. It can be used to set various properties of the texture, such as its min/mag filter, address mode, mipmaps, anisotropy, and type, based on the data in the asset.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/texture.md"}}],["370",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/handlers/untar.js)\n\nThe code defines the Untar and UntarWorker classes, which are used to extract files from a tar archive. The Untar class is used to extract files from a tar archive in the form of an ArrayBuffer. The UntarWorker class is a wrapper around the Untar class that uses a Web Worker to extract files from a tar archive. \n\nThe Untar class has a private constructor that takes an ArrayBuffer as a parameter. The constructor creates a DataView object from the ArrayBuffer and initializes some private variables. The untar() method is used to extract the files from the tar archive. It returns an array of file descriptors in the format {name, start, size, url}. The name property is the name of the file, the start property is the offset of the file data in the ArrayBuffer, the size property is the size of the file data in bytes, and the url property is a URL that can be used to access the file data. \n\nThe UntarWorker class has a constructor that takes an optional filenamePrefix parameter. The constructor creates an instance of the Untar class and a Web Worker. The untar() method is used to extract the files from the tar archive using the Web Worker. It takes an ArrayBuffer as a parameter and a callback function that is called when the extraction is complete. The callback function has two parameters: an error message (if any) and an array of file descriptors. The hasPendingRequests() method is used to check if there are any pending requests to extract files from tar archives. The destroy() method is used to terminate the Web Worker and free up any resources used by the UntarWorker instance. \n\nThe code also defines the PaxHeader class, which is used to parse PAX headers in the tar archive. The PaxHeader class has a constructor that takes an array of fields as a parameter. The applyHeader() method is used to apply the PAX header to a file descriptor. \n\nThe code uses the TextDecoder interface to decode the data in the tar archive. If the TextDecoder interface is not available, the code logs a warning message and returns an empty array. \n\nThe code also defines the UntarScope function, which is used as the code that ends up in a Web Worker. The function initializes some private variables and defines some private methods. The function also defines the onmessage handler for the Web Worker. \n\nThe code uses a Blob URL to create a Web Worker. The URL is created by converting the UntarScope function to a string and adding the onmessage handler for the Web Worker. The URL is used to create a new instance of the Web Worker in the UntarWorker constructor. \n\nOverall, the code provides a way to extract files from a tar archive using a Web Worker. The Untar class can be used to extract files from a tar archive in the main thread, while the UntarWorker class can be used to extract files from a tar archive in a separate thread using a Web Worker.\n## Questions: \n 1. What is the purpose of the `Untar` variable and why is it declared outside the `UntarScope` function?\n- The `Untar` variable is used to store the `UntarInternal` constructor function. It is declared outside the `UntarScope` function so that a `return` statement does not need to be added to the function. This is necessary because the `UntarScope` function is used as the code that ends up in a Web Worker.\n2. What is the purpose of the `PaxHeader` function and how is it used?\n- The `PaxHeader` function is used to parse PAX headers in a tar archive. It takes a buffer, start index, and length as arguments and returns a `PaxHeader` object. The `applyHeader` method of the `PaxHeader` object is used to apply the header fields to a file descriptor object.\n3. What is the purpose of the `UntarWorker` class and how is it used?\n- The `UntarWorker` class is used to untar a tar archive using a Web Worker. It takes an array buffer and a callback function as arguments and returns an array of file descriptors. The `hasPendingRequests` method is used to check if there are any pending requests to untar array buffers, and the `destroy` method is used to terminate the internal Web Worker.","metadata":{"source":".autodoc/docs/markdown/src/framework/handlers/untar.md"}}],["371",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/i18n/constants.js)\n\nThis code defines two constants related to localization in the PlayCanvas engine project. The first constant, `DEFAULT_LOCALE`, sets the default locale to 'en-US'. This means that if no specific locale is specified, the code will default to using the English language with US regional settings.\n\nThe second constant, `DEFAULT_LOCALE_FALLBACKS`, defines a set of fallback locales for specific languages. For example, if the desired locale is 'en-AS' (English language with American Samoa regional settings), but the project only has 'en-US' and 'en-GB' locales available, the code will fallback to using 'en-US'. If 'en-US' is also not available, the code will pick the first locale that satisfies the language, which in this case would be 'en-GB'.\n\nThis constant is useful for ensuring that the project can still display content in the user's preferred language, even if a specific regional setting is not available. It also allows for more flexibility in the localization process, as developers can focus on creating translations for the most commonly used regional settings and rely on fallbacks for less common ones.\n\nHere is an example of how this constant could be used in the PlayCanvas engine project:\n\n```javascript\n// get the user's preferred language and regional settings\nconst userLocale = navigator.language;\n\n// check if the desired locale is available, otherwise fallback to a default\nconst localeToUse = DEFAULT_LOCALE_FALLBACKS[userLocale] || DEFAULT_LOCALE;\n\n// load the appropriate localization data for the chosen locale\nloadLocalizationData(localeToUse);\n```\n\nIn this example, the code first gets the user's preferred language and regional settings using the `navigator.language` property. It then checks if the desired locale is available in the project's localization data, using the `DEFAULT_LOCALE_FALLBACKS` constant to determine the fallback locale if necessary. Finally, it loads the appropriate localization data for the chosen locale using a `loadLocalizationData` function (not shown).\n## Questions: \n 1. What is the purpose of the `DEFAULT_LOCALE` constant?\n   - The `DEFAULT_LOCALE` constant is used as a fallback locale if a specific locale is not found.\n\n2. What is the structure of the `DEFAULT_LOCALE_FALLBACKS` object?\n   - The `DEFAULT_LOCALE_FALLBACKS` object maps language codes to their corresponding fallback locales.\n\n3. How are fallback locales chosen if a specific locale is not found?\n   - If a specific locale is not found, the fallback locale is chosen based on the language code. If a fallback for the specific language code is not found, the first locale that satisfies the language is chosen.","metadata":{"source":".autodoc/docs/markdown/src/framework/i18n/constants.md"}}],["372",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/i18n/i18n-parser.js)\n\nThe `I18nParser` class is responsible for parsing internationalization (i18n) data and validating its structure. The `parse` method takes in a `data` object and returns an array of `messages`. The `data` object is expected to have a `header` field with a `version` number and a `data` field that is an array of objects containing `info` and `messages` fields. The `info` field is an object that contains a `locale` string, which specifies the language of the messages. The `messages` field is an object that contains key-value pairs of message IDs and their translations.\n\nBefore parsing the data, the `_validate` method is called to ensure that the data has the correct structure. If any of the required fields are missing or have an invalid type, an error is thrown. This method is only called if the `_DEBUG` flag is set, which suggests that it is intended for development purposes only.\n\nThe `I18nParser` class is likely used in the larger PlayCanvas engine project to support internationalization of game content. Game developers can use this class to parse i18n data files and retrieve translated messages for different languages. The `parse` method could be called by other parts of the engine to retrieve the parsed messages and use them to display text in the game. For example:\n\n```\nconst i18nData = {\n  header: {\n    version: 1\n  },\n  data: [\n    {\n      info: {\n        locale: 'en-US'\n      },\n      messages: {\n        greeting: 'Hello, world!',\n        farewell: 'Goodbye, world!'\n      }\n    },\n    {\n      info: {\n        locale: 'fr-FR'\n      },\n      messages: {\n        greeting: 'Bonjour le monde!',\n        farewell: 'Au revoir le monde!'\n      }\n    }\n  ]\n};\n\nconst parser = new I18nParser();\nconst messages = parser.parse(i18nData);\n\nconsole.log(messages[0].greeting); // 'Hello, world!'\nconsole.log(messages[1].greeting); // 'Bonjour le monde!'\n```\n## Questions: \n 1. What is the purpose of this code?\n    - This code defines a class called `I18nParser` which has a method called `parse` that takes in data and returns a specific property of that data.\n\n2. What is the expected format of the data that is passed into the `parse` method?\n    - The data passed into the `parse` method should have a `header` field with a `version` property that is a number 1, a `data` field that is an array, and each element of the `data` array should have an `info` field with a `locale` property that is a string and a `messages` field.\n\n3. What happens if the data passed into the `parse` method does not meet the expected format?\n    - If the data passed into the `parse` method does not meet the expected format, the `_validate` method is called and it throws various errors depending on which fields are missing or have incorrect types.","metadata":{"source":".autodoc/docs/markdown/src/framework/i18n/i18n-parser.md"}}],["373",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/i18n/utils.js)\n\nThe code defines a set of functions that are used to handle localization and pluralization in the PlayCanvas engine. The code imports two constants from another file, `DEFAULT_LOCALE` and `DEFAULT_LOCALE_FALLBACKS`. The `DEFAULT_LOCALE` constant is used as a fallback locale when a desired locale is not available. The `DEFAULT_LOCALE_FALLBACKS` constant is an object that maps locales to their fallback locales.\n\nThe `PLURALS` object is a map that associates a locale with a function that returns the plural index based on the CLDR rules. The `definePluralFn` function is a helper function that takes an array of locales and a function that returns the plural index and adds the locales to the `PLURALS` object. The `getLang` function takes a locale and returns the language portion of the locale. The `replaceLang` function takes a locale and a desired language and returns a new locale with the desired language and the same region and variant as the original locale.\n\nThe `findAvailableLocale` function takes a desired locale and an object that maps available locales to `true` and returns the best available locale. If the desired locale is available, it is returned. Otherwise, the function tries to find a fallback locale. If a fallback locale is found, it is returned. If no fallback locale is found, the `DEFAULT_LOCALE` is returned.\n\nThe code defines plural functions for several languages. The plural functions take a number and return the plural index based on the CLDR rules. The plural index is used to select the appropriate plural form for a localized string. The `getPluralFn` function takes a language and returns the plural function for that language. If no plural function is defined for the language, the default plural function is returned.\n\nThe code exports four functions: `replaceLang`, `getLang`, `getPluralFn`, and `findAvailableLocale`. These functions are used to handle localization and pluralization in the PlayCanvas engine. The `replaceLang` function is used to change the language of a locale. The `getLang` function is used to get the language portion of a locale. The `getPluralFn` function is used to get the plural function for a language. The `findAvailableLocale` function is used to find the best available locale for a desired locale.\n## Questions: \n 1. What is the purpose of the `PLURALS` object?\n- The `PLURALS` object maps locales to functions that return the plural index based on the CLDR rules.\n\n2. What is the purpose of the `definePluralFn` function?\n- The `definePluralFn` function is a helper function that defines the plural function for an array of locales.\n\n3. What is the purpose of the `getPluralFn` function?\n- The `getPluralFn` function returns the function that converts to plural for a given language.","metadata":{"source":".autodoc/docs/markdown/src/framework/i18n/utils.md"}}],["374",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/lightmapper/bake-light-ambient.js)\n\nThe code defines a class called `BakeLightAmbient` which represents an ambient light used for baking in the PlayCanvas engine. The class extends another class called `BakeLight` and imports several modules including `Vec3`, `random`, `Color`, `Entity`, and `SHADOW_PCF3`. \n\nWhen an instance of `BakeLightAmbient` is created, it creates a new `Entity` object with a `light` component. The `light` component is set up with various properties such as `type`, `affectDynamic`, `affectLightmapped`, `bake`, `bakeNumSamples`, `castShadows`, `normalOffsetBias`, `shadowBias`, `shadowDistance`, `shadowResolution`, `shadowType`, `color`, `intensity`, and `bakeDir`. These properties determine how the ambient light will be rendered and baked into the scene. \n\nThe `BakeLightAmbient` class also has two methods: `numVirtualLights` and `prepareVirtualLight`. The `numVirtualLights` method returns the number of virtual lights used for baking the ambient light. The `prepareVirtualLight` method prepares a virtual light for baking by setting its direction and intensity. It uses the `random.spherePointDeterministic` method to generate a random point on a sphere and sets the direction of the virtual light to point towards that point. It then calculates the intensity of the virtual light based on the sphere part used and the number of virtual lights. \n\nOverall, this code is used to create and configure an ambient light for baking in the PlayCanvas engine. The `BakeLightAmbient` class can be used in conjunction with other classes and modules in the engine to create complex lighting setups for 3D scenes. For example, it could be used to create a realistic outdoor environment with dynamic lighting and shadows.\n## Questions: \n 1. What is the purpose of the `BakeLightAmbient` class?\n- The `BakeLightAmbient` class represents an ambient light used for baking, either as a cubemap or a constant light source.\n\n2. What is the significance of the `numVirtualLights` property and the `prepareVirtualLight` method?\n- The `numVirtualLights` property returns the number of virtual lights used for baking, while the `prepareVirtualLight` method prepares a virtual light for baking by setting its direction and intensity based on the scene's settings.\n\n3. What are the default values for the `light` component added to the `lightEntity` in the constructor?\n- The default values for the `light` component include a directional light type, dynamic light affecting enabled, lightmapping disabled, shadow casting enabled, and a white color with an intensity of 1.","metadata":{"source":".autodoc/docs/markdown/src/framework/lightmapper/bake-light-ambient.md"}}],["375",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/lightmapper/bake-light-simple.js)\n\nThe code defines a class called `BakeLightSimple` which extends another class called `BakeLight`. This class represents a type of light that can be used for baking lighting information in a 3D scene. The `BakeLightSimple` class is used to represent directional, omni, or spot lights.\n\nThe class has two methods: `numVirtualLights` and `prepareVirtualLight`. The `numVirtualLights` method returns the number of virtual lights that should be used for baking. For directional lights, this is determined by the `bakeNumSamples` property of the light. For other types of lights, only one virtual light is used.\n\nThe `prepareVirtualLight` method is called for each virtual light and is responsible for preparing the light for baking. It takes two arguments: `index`, which is the index of the virtual light being prepared, and `numVirtualLights`, which is the total number of virtual lights being used.\n\nThe method first sets the light's rotation to its original rotation. If the light is a directional light and `index` is greater than 0, it applies a random adjustment to the light's facing direction. This is done by calculating a spread angle based on the light's `bakeArea` property, and then rotating the light's node by a random angle within that spread angle. Finally, the method updates the light's transform and adjusts its intensity based on the number of virtual lights being used.\n\nThis class is likely used in the larger PlayCanvas engine project to provide a way to bake lighting information for a 3D scene. By defining different types of `BakeLight` classes, the engine can support different types of lights and provide more accurate lighting information for the scene. Developers can use this functionality to improve the visual quality of their games or applications. \n\nExample usage:\n\n```\nimport { BakeLightSimple } from './bake-light-simple.js';\n\n// create a directional light for baking\nconst light = new pc.Entity();\nlight.addComponent('light', {\n    type: pc.LIGHTTYPE_DIRECTIONAL,\n    bakeNumSamples: 4,\n    bakeArea: 10\n});\n\n// create a BakeLightSimple instance for the light\nconst bakeLight = new BakeLightSimple(light, 1.0, scene);\n\n// prepare the virtual lights for baking\nfor (let i = 0; i < bakeLight.numVirtualLights; i++) {\n    bakeLight.prepareVirtualLight(i, bakeLight.numVirtualLights);\n}\n\n// bake the lighting information for the scene\nscene.lightmapper.bake();\n```\n## Questions: \n 1. What is the purpose of the `BakeLightSimple` class and how does it relate to the `BakeLight` class?\n- The `BakeLightSimple` class is a subclass of `BakeLight` and represents a directional, omni, or spot type of light used for baking. \n\n2. What is the significance of the `numVirtualLights` method and how does it determine the number of samples for a directional light?\n- The `numVirtualLights` method returns the number of virtual lights to use for baking. For directional lights, the number of samples is determined by the `bakeNumSamples` property of the light.\n\n3. What is the purpose of the `prepareVirtualLight` method and what transformations does it apply to the light?\n- The `prepareVirtualLight` method prepares a virtual light for baking by setting its rotation to the original rotation, applying a random adjustment to the directional light facing (if applicable), updating its transform, and dividing its intensity by the number of virtual lights.","metadata":{"source":".autodoc/docs/markdown/src/framework/lightmapper/bake-light-simple.md"}}],["376",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/lightmapper/bake-light.js)\n\nThe code defines a helper class called `BakeLight` that is used to store and manipulate light properties during a baking process. The class takes in a `scene` and a `light` object as parameters. The `scene` parameter is used to access the scene where the light is located, while the `light` parameter is the light object whose properties are being manipulated.\n\nThe `constructor` method initializes the `scene` and `light` properties of the class. It also stores the original properties of the light object using the `store` method. Additionally, it sets the `numCascades` property of the light object to 1 and calculates the bounds of the light object if it is not of type `LIGHTTYPE_DIRECTIONAL`.\n\nThe `store` method stores the original properties of the light object in the class instance. The `restore` method restores the original properties of the light object from the class instance.\n\nThe `startBake` method enables the light object and destroys any shadow map it might have. It also calls the `beginFrame` method of the light object.\n\nThe `endBake` method disables the light object and returns its shadow map to a cache if it exists.\n\nThis class is used in the PlayCanvas engine to manipulate light properties during a baking process. It is particularly useful for baking lightmaps, where the properties of the lights in the scene need to be temporarily modified to generate the lightmap. An example usage of this class is shown below:\n\n```javascript\nconst bakeLight = new BakeLight(scene, light);\nbakeLight.startBake();\n// modify light properties as needed\nbakeLight.endBake(shadowMapCache);\nbakeLight.restore();\n```\n\nIn this example, a new `BakeLight` instance is created with the `scene` and `light` parameters. The `startBake` method is called to enable the light and destroy its shadow map. The light properties can then be modified as needed. After the baking process is complete, the `endBake` method is called to disable the light and return its shadow map to the cache. Finally, the `restore` method is called to restore the original properties of the light object.\n## Questions: \n 1. What is the purpose of the `BakeLight` class?\n- The `BakeLight` class is a helper class that stores all lights including their original state and provides methods to start and end baking.\n\n2. What is the significance of the `tempSphere` variable?\n- The `tempSphere` variable is used to store a temporary `BoundingSphere` object that is used to calculate the bounds for non-directional lights.\n\n3. What does the `store` method do?\n- The `store` method stores the original properties of the light, such as its mask, shadow update mode, enabled state, intensity, rotation, and number of cascades, so that they can be restored later.","metadata":{"source":".autodoc/docs/markdown/src/framework/lightmapper/bake-light.md"}}],["377",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/lightmapper/bake-mesh-node.js)\n\nThe code defines a helper class called `BakeMeshNode` that wraps a node in the PlayCanvas engine along with its associated mesh instances. The purpose of this class is to provide a convenient way to perform baking operations on the mesh instances of a node. \n\nWhen an instance of the `BakeMeshNode` class is created, it takes in a `node` parameter and an optional `meshInstances` parameter. If `meshInstances` is not provided, the class will use the mesh instances associated with the node's `render` or `model` component. The class then stores the original properties of the component and the mesh instances.\n\nThe `BakeMeshNode` class also has a `bounds` property that represents the world space axis-aligned bounding box (AABB) for all the mesh instances associated with the node. This is useful for determining the size and position of the mesh instances in world space.\n\nAdditionally, the class has a `renderTargets` property that is an array of render targets with attached color buffers for each render pass. This is useful for rendering the mesh instances to a texture for baking purposes.\n\nThe `store` method of the `BakeMeshNode` class is used to store the original `castShadows` property of the component. This property determines whether the mesh instances cast shadows in the scene.\n\nThe `restore` method of the `BakeMeshNode` class is used to restore the original `castShadows` property of the component after a baking operation has been performed.\n\nOverall, the `BakeMeshNode` class provides a convenient way to perform baking operations on the mesh instances of a node in the PlayCanvas engine. It encapsulates the necessary properties and methods needed for baking and provides a clean interface for performing these operations. \n\nExample usage:\n\n```javascript\n// create a new BakeMeshNode instance\nconst bakeNode = new BakeMeshNode(myNode);\n\n// perform a baking operation on the mesh instances\n// ...\n\n// restore the original castShadows property\nbakeNode.restore();\n```\n## Questions: \n 1. What is the purpose of the `BakeMeshNode` class?\n    \n    The `BakeMeshNode` class is a helper class that wraps a node and its meshInstances, and provides methods to store and restore the original component properties.\n\n2. What is the significance of the `meshInstances` parameter in the constructor?\n\n    The `meshInstances` parameter is an optional parameter that allows the developer to specify the meshInstances to be used for the `BakeMeshNode` instance. If not provided, the meshInstances of the node's `render` or `model` component will be used.\n\n3. What is the purpose of the `renderTargets` property?\n\n    The `renderTargets` property is an array that holds a render target with an attached color buffer for each render pass. This is used in the baking process to store the results of each render pass.","metadata":{"source":".autodoc/docs/markdown/src/framework/lightmapper/bake-mesh-node.md"}}],["378",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/lightmapper/lightmap-filters.js)\n\nThe code defines a helper class called `LightmapFilters` that is used by the PlayCanvas engine's lightmapper to wrap the functionality of dilate and denoise shaders. The class is imported into the file from other modules in the PlayCanvas engine project. \n\nThe `LightmapFilters` class constructor takes a `device` parameter, which is an object representing the graphics device used by the engine. It creates a shader program for dilating and a constant texture source. The `setSourceTexture` method sets the texture source for the shader program. The `prepare` method takes the texture width and height as parameters and sets the inverse texture size. \n\nThe `prepareDenoise` method is used to prepare the denoise shader program. It takes two parameters, `filterRange` and `filterSmoothness`, which are used to calculate the sigma values for the bilateral denoise shader. If the shader program has not been created yet, it creates the shader program and resolves the constant sigmas, kernel, and bZnorm. The `evaluateDenoiseUniforms` method is used to calculate the kernel and bZnorm values based on the filter range and smoothness. \n\nThe `LightmapFilters` class is used by the PlayCanvas engine's lightmapper to apply dilate and denoise filters to lightmaps. The `LightmapFilters` class is not meant to be used directly by developers using the PlayCanvas engine, but rather as a helper class for the engine's lightmapper. \n\nExample usage:\n\n```\nconst lightmapFilters = new LightmapFilters(device);\nlightmapFilters.setSourceTexture(lightmapTexture);\nlightmapFilters.prepare(lightmapTexture.width, lightmapTexture.height);\nlightmapFilters.prepareDenoise(0.5, 0.5);\n```\n## Questions: \n 1. What is the purpose of the `LightmapFilters` class?\n- The `LightmapFilters` class is a helper class used by the lightmapper, which wraps the functionality of dilate and denoise shaders.\n\n2. What is the significance of the `DENOISE_FILTER_SIZE` constant?\n- The `DENOISE_FILTER_SIZE` constant represents the size of the kernel and needs to match the constant in the shader.\n\n3. Why is the `shaderDenoise` property set to null initially?\n- The `shaderDenoise` property is set to null initially because the denoise shader is optional and gets created only when needed.","metadata":{"source":".autodoc/docs/markdown/src/framework/lightmapper/lightmap-filters.md"}}],["379",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/draco-decoder.js)\n\nThe code defines a JobQueue class that manages a set of web workers and enqueues jobs on them. The workers are used to decode Draco compressed meshes. The JobQueue class keeps track of the workers and their workload to balance the workload as much as possible. Workers have a maximum of two jobs assigned at any one time. \n\nThe `initializeWorkers` function initializes the workers and creates a JobQueue instance. It takes an optional configuration object that specifies the URLs of the glue script and the wasm module, the number of workers to use for decoding, and whether to wait for the first decode request before initializing workers. If the configuration object is not provided, the function uses default values. \n\nThe `dracoInitialize` function initializes the Draco mesh decoder. It takes the same configuration object as `initializeWorkers`, but also has an additional `lazyInit` property that, if set to true, delays the initialization of workers until the first decode request is received. Otherwise, workers are initialized immediately. \n\nThe `dracoDecode` function enqueues a buffer for decoding. It takes an ArrayBuffer and a callback function to receive the decoded result. It returns true if the draco worker was initialized and false otherwise. \n\nHere's an example of how to use the `dracoInitialize` and `dracoDecode` functions:\n\n```\n// initialize the Draco mesh decoder with default configuration\ndracoInitialize();\n\n// decode a Draco compressed mesh\nconst buffer = ... // ArrayBuffer containing the compressed mesh data\ndracoDecode(buffer, (error, result) => {\n    if (error) {\n        console.error(error);\n    } else {\n        const indices = result.indices;\n        const vertices = result.vertices;\n        // use the decoded mesh data\n    }\n});\n```\n\nOverall, this code provides a way to decode Draco compressed meshes using web workers to offload the decoding work from the main thread and improve performance. The JobQueue class ensures that the workload is balanced across the available workers to maximize efficiency.\n## Questions: \n 1. What is the purpose of the `JobQueue` class?\n- The `JobQueue` class keeps track of a set of web workers and enqueues jobs on them. It balances the workload as much as possible by assigning a maximum of 2 jobs to each worker at any one time.\n\n2. What is the purpose of the `initializeWorkers` function?\n- The `initializeWorkers` function creates a set of web workers and initializes the `JobQueue` with them. It takes a configuration object as an optional parameter, which specifies the URLs of the glue script and wasm module, the number of workers to use, and whether to wait for the first decode request before initializing workers.\n\n3. What is the purpose of the `dracoDecode` function?\n- The `dracoDecode` function enqueues a buffer for decoding by the web workers in the `JobQueue`. It takes a buffer and a callback function as parameters, and returns `true` if the `JobQueue` was initialized and `false` otherwise.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/draco-decoder.md"}}],["380",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/draco-worker.js)\n\nThe `DracoWorker` function is a Web Worker that decodes Draco compressed meshes. It loads the Draco WebAssembly module and provides a `decodeMesh` function that accepts a `DataView` containing the compressed mesh data and returns an object containing the decoded mesh vertices and indices. The `decodeMesh` function uses the Draco decoder to decode the mesh data and generates normals if they are missing. The decoded mesh data is interleaved and packed into an `ArrayBuffer` that is returned as part of the result object.\n\nThe `DracoWorker` function uses the following constants and functions:\n\n- `POSITION_ATTRIBUTE` and `NORMAL_ATTRIBUTE` are constants that represent the attribute types for position and normal data.\n- `loadWasm` is a function that loads the Draco WebAssembly module and returns a promise that resolves to the module instance.\n- `wrap` is a function that wraps a typed array with a new typed array of a different data type.\n- `componentSizeInBytes` is a function that returns the size in bytes of a component of a given data type.\n- `attributeSizeInBytes` is a function that returns the size in bytes of an attribute.\n- `attributeOrder` is an object that maps attribute types to indices used for sorting attributes.\n- `generateNormals` is a function that generates normals for a mesh given its vertices and indices.\n- `decodeMesh` is a function that decodes a Draco compressed mesh and returns an object containing the decoded mesh vertices and indices.\n- `decode` is a function that decodes a mesh data object and posts a message containing the decoded mesh data back to the main thread.\n\nThe `DracoWorker` function exports the `DracoWorker` class, which can be used to create a new instance of the worker. The `DracoWorker` class provides a `decodeMesh` method that can be used to decode a compressed mesh. The `decodeMesh` method accepts a `DataView` containing the compressed mesh data and returns a promise that resolves to an object containing the decoded mesh vertices and indices. The `DracoWorker` class uses the `Worker` API to create a new Web Worker and communicate with it using messages.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a worker function called `DracoWorker` that can decode Draco-encoded meshes and generate normals for them.\n\n2. What external dependencies does this code have?\n- This code depends on the Draco library, which is loaded as a WebAssembly module and a JavaScript file.\n\n3. What is the format of the data that this code can decode?\n- This code can decode data that represents a triangular mesh in the Draco format.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/draco-worker.md"}}],["381",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/glb-container-resource.js)\n\nThe code defines a class called `GlbContainerResource` which is a container resource returned by the `GlbParser`. This class implements the `ContainerResource` interface. The purpose of this class is to create assets from the parsed GLB data structures. The assets created include render assets, material assets, and animation assets. The class also creates a model from the parsed GLB data structures. The model is created only when needed. The class provides methods to instantiate model entities and render entities. The instantiated model entity is an `Entity` with a `model` component. The instantiated render entity is an `Entity` with a `render` component. The class also provides methods to get material variants and apply material variants to entities and mesh instances. The class has a `destroy` method that unloads and destroys the assets created by the class.\n\nThe `GlbContainerResource` class imports several classes from the `PlayCanvas engine` project. These classes include `Debug`, `MeshInstance`, `Model`, `MorphInstance`, `SkinInstance`, `SkinInstanceCache`, `Entity`, and `Asset`.\n\nThe `GlbContainerResource` class has a constructor that takes four parameters: `data`, `asset`, `assets`, and `defaultMaterial`. The `data` parameter is the parsed GLB data structures. The `asset` parameter is an `Asset` object. The `assets` parameter is an `AssetRegistry` object. The `defaultMaterial` parameter is the default material to use when a material is not specified.\n\nThe `GlbContainerResource` class has a `createAsset` static method that creates an `Asset` object. The method takes four parameters: `assetName`, `type`, `resource`, and `index`. The `assetName` parameter is the name of the asset. The `type` parameter is the type of the asset. The `resource` parameter is the resource of the asset. The `index` parameter is the index of the asset.\n\nThe `GlbContainerResource` class has a `createSceneHierarchy` static method that creates a single hierarchy from an array of nodes. The method takes two parameters: `sceneNodes` and `nodeType`. The `sceneNodes` parameter is an array of nodes. The `nodeType` parameter is the type of the node.\n\nThe `GlbContainerResource` class has a `createModel` static method that creates a `Model` object from the parsed GLB data structures. The method takes two parameters: `glb` and `defaultMaterial`. The `glb` parameter is the parsed GLB data structures. The `defaultMaterial` parameter is the default material to use when a material is not specified.\n\nThe `GlbContainerResource` class has an `instantiateModelEntity` method that creates an `Entity` with a `model` component. The method takes one parameter: `options`. The `options` parameter is an object that contains options for the `model` component.\n\nThe `GlbContainerResource` class has an `instantiateRenderEntity` method that creates an `Entity` with a `render` component. The method takes one parameter: `options`. The `options` parameter is an object that contains options for the `render` component.\n\nThe `GlbContainerResource` class has a `getMaterialVariants` method that returns an array of material variants.\n\nThe `GlbContainerResource` class has an `applyMaterialVariant` method that applies a material variant to an entity. The method takes two parameters: `entity` and `name`. The `entity` parameter is the entity to apply the material variant to. The `name` parameter is the name of the material variant.\n\nThe `GlbContainerResource` class has an `applyMaterialVariantInstances` method that applies a material variant to mesh instances. The method takes two parameters: `instances` and `name`. The `instances` parameter is an array of mesh instances. The `name` parameter is the name of the material variant.\n\nThe `GlbContainerResource` class has a `_applyMaterialVariant` method that applies a material variant to instances. The method takes two parameters: `variant` and `instances`. The `variant` parameter is the material variant. The `instances` parameter is an array of instances.\n\nThe `GlbContainerResource` class has a `destroy` method that unloads and destroys the assets created by the class. The method does not take any parameters.\n## Questions: \n 1. What is the purpose of the `GlbContainerResource` class?\n- The `GlbContainerResource` class is a container resource returned by the GlbParser that implements the ContainerResource interface. It creates assets for renders, materials, and animations, and provides methods to instantiate model and render entities.\n\n2. What is the significance of the `createAsset` function?\n- The `createAsset` function is used to create a new asset with a given type, resource, and index. It adds the asset to the `assets` object and returns the new asset.\n\n3. What is the purpose of the `applyMaterialVariant` function?\n- The `applyMaterialVariant` function applies a material variant to a given entity by finding all of its render components and updating their mesh instances with the appropriate material. It uses the `_applyMaterialVariant` function internally to apply the variant to the mesh instances.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/glb-container-resource.md"}}],["382",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/glb-model.js)\n\nThe code above defines a class called `GlbModelParser` that is used to parse GLB (Binary glTF) files. The class takes in two parameters: `device` and `defaultMaterial`. `device` is an object that represents the graphics device used to render the model, while `defaultMaterial` is the material used to render the model if no other material is specified.\n\nThe `parse` method of the `GlbModelParser` class is used to parse the GLB file. It takes in two parameters: `data` and `callback`. `data` is the binary data of the GLB file, while `callback` is a function that is called when the parsing is complete. The callback function takes in two parameters: `err` and `model`. If an error occurs during parsing, `err` will contain the error message, otherwise `model` will contain the parsed model.\n\nThe `parse` method uses the `GlbParser` class to parse the GLB file. The `GlbParser` class is imported from the `glb-parser.js` file. The `parse` method of the `GlbParser` class takes in five parameters: `filename`, `data`, `device`, `defaultMaterial`, and a callback function. `filename` is the name of the GLB file, `data` is the binary data of the GLB file, `device` is the graphics device used to render the model, `defaultMaterial` is the material used to render the model if no other material is specified, and the callback function is called when the parsing is complete.\n\nOnce the GLB file is parsed, the `createModel` method of the `GlbContainerResource` class is used to create a model from the parsed data. The `GlbContainerResource` class is imported from the `glb-container-resource.js` file. The `createModel` method takes in two parameters: `data` and `defaultMaterial`. `data` is the parsed data of the GLB file, while `defaultMaterial` is the material used to render the model if no other material is specified.\n\nFinally, the parsed data is destroyed and the `model` is passed to the `callback` function.\n\nThis code is used in the PlayCanvas engine project to parse GLB files and create models from the parsed data. The `GlbModelParser` class can be used by other classes or scripts in the project to parse GLB files and create models. For example, a script that loads a GLB file and displays it in the scene can use the `GlbModelParser` class to parse the file and create a model that can be added to the scene. \n\nExample usage:\n\n```\nimport { GlbModelParser } from './glb-model-parser.js';\n\nconst device = // get graphics device\nconst defaultMaterial = // create default material\n\nconst parser = new GlbModelParser(device, defaultMaterial);\n\nconst url = 'path/to/model.glb';\nfetch(url)\n  .then(response => response.arrayBuffer())\n  .then(data => {\n    parser.parse(data, (err, model) => {\n      if (err) {\n        console.error(err);\n      } else {\n        // add model to scene\n      }\n    });\n  });\n```\n## Questions: \n 1. What is the purpose of the GlbContainerResource and GlbParser classes being imported at the beginning of the file?\n- The GlbContainerResource and GlbParser classes are imported to be used in the GlbModelParser class for parsing and creating models from GLB data.\n\n2. What is the significance of the constructor parameters device and defaultMaterial in the GlbModelParser class?\n- The device parameter is used to specify the graphics device to be used for rendering, while the defaultMaterial parameter is used to specify the material to be used for rendering if no other material is specified.\n\n3. What is the purpose of the parse method in the GlbModelParser class?\n- The parse method is used to parse GLB data and create a model from it using the GlbContainerResource and GlbParser classes. The resulting model is then passed to the callback function.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/glb-model.md"}}],["383",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/material/json-standard-material.js)\n\nThe code defines a class called `JsonStandardMaterialParser` that is responsible for converting incoming JSON data into a `StandardMaterial` object. The `StandardMaterial` is a class that represents a material that can be applied to a 3D model in the PlayCanvas engine. The `JsonStandardMaterialParser` class is not meant to be used directly by developers, but rather is used internally by the engine to parse material data that is loaded from a server or other external source.\n\nThe `JsonStandardMaterialParser` class has a `parse` method that takes an input object and returns a `StandardMaterial` object. The `parse` method first calls the `migrate` method to convert any properties that are out of date or from old versions into the current version. It then calls the `_validate` method to check for invalid properties. Finally, it creates a new `StandardMaterial` object and initializes its properties from the input data using the `initialize` method.\n\nThe `initialize` method takes a `StandardMaterial` object and a data object and initializes the material properties from the input data. It first checks if the data has been validated and validates it if necessary. It then initializes the material values from the input data. The method loops through each key in the data object and checks the type of the value. If the type is `vec2`, it creates a new `Vec2` object and assigns it to the material property. If the type is `rgb`, it creates a new `Color` object and assigns it to the material property. If the type is `texture`, it checks if the value is an instance of `Texture` and assigns it to the material property if it is. If the value is not an instance of `Texture`, it sets the material property to `null`. If the material already has a texture assigned but the data contains a valid asset id (which means the asset isn't yet loaded), it leaves the current texture (probably a placeholder) until the asset is loaded. If the type is `cubemap`, it checks if the value is an instance of `Texture` and assigns it to the material property if it is. If the value is not an instance of `Texture`, it sets the material property to `null`. If the key is `cubeMap` and the value is falsy, it clears the prefiltered data. If the material already has a texture assigned but the data contains a valid asset id (which means the asset isn't yet loaded), it leaves the current texture (probably a placeholder) until the asset is loaded. If the type is `boundingbox`, it creates a new `BoundingBox` object and assigns it to the material property. If the type is a number, boolean, or enum, it assigns the value to the material property. Finally, it calls the `update` method on the material to update its state.\n\nThe `migrate` method converts any properties that are out of date or from old versions into the current version. It replaces the old `shader` property with the new `shadingModel` property. It also makes the property names conform to JavaScript style. It then loops through a list of properties that have been renamed in `StandardMaterial` but may still exist in data in old format. If an old property name exists without a new one, it moves the property into the new name and deletes the old one. Finally, it removes any properties that may exist in input data but are now ignored.\n\nThe `_validate` method checks for invalid properties. If the data has not been validated, it creates a new `StandardMaterialValidator` object and calls its `validate` method to validate the data.\n\nOverall, the `JsonStandardMaterialParser` class is an important part of the PlayCanvas engine that allows developers to load material data from external sources and convert it into a `StandardMaterial` object that can be applied to 3D models.\n## Questions: \n 1. What is the purpose of the `JsonStandardMaterialParser` class?\n- The `JsonStandardMaterialParser` class is used to convert incoming JSON data into a `StandardMaterial`.\n\n2. What types of properties can be initialized in the `initialize` method of the `JsonStandardMaterialParser` class?\n- The `initialize` method can initialize properties of type `vec2`, `rgb`, `texture`, `cubemap`, `boundingbox`, number, boolean, and enum types.\n\n3. What is the purpose of the `_validate` method in the `JsonStandardMaterialParser` class?\n- The `_validate` method is used to check for invalid properties in the input data and validate the data if it has not already been validated.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/material/json-standard-material.md"}}],["384",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/scene.js)\n\nThe `SceneParser` class is responsible for parsing scene data and creating entities in the PlayCanvas engine. The `parse` method takes in scene data and returns the root entity of the scene hierarchy. The `SceneParser` class is used in the larger project to load and instantiate scenes.\n\nThe `SceneParser` constructor takes in an `app` object and a boolean `isTemplate` flag. The `app` object is an instance of the PlayCanvas application and is used to create entities. The `isTemplate` flag is used to indicate whether the scene being parsed is a template or not.\n\nThe `parse` method first creates an empty object `entities` to store the entities created during parsing. It then loops through the entities in the scene data and creates an entity for each one using the `_createEntity` method. The `_createEntity` method creates a new `Entity` object, sets its name, position, rotation, and scale, and adds any tags or labels. If the scene being parsed is a template, the `template` property of the entity is set to `true`.\n\nAfter all the entities have been created, the `parse` method loops through them again and adds them to the scene hierarchy using the `addChild` method. It then calls the `_openComponentData` method to create and add components to the entities.\n\nThe `_openComponentData` method loops through the systems in the PlayCanvas engine and creates components for each entity using the `addComponent` method. It then recursively calls itself on each child entity to add components to them as well.\n\nThe `_setPosRotScale` method is used to set the position, rotation, and scale of an entity. If the scene data is compressed, it uses the `CompressUtils.setCompressedPRS` method to set the position, rotation, and scale. Otherwise, it sets them using the `setLocalPosition`, `setLocalEulerAngles`, and `setLocalScale` methods.\n\nOverall, the `SceneParser` class is an important part of the PlayCanvas engine that is used to load and instantiate scenes. It creates entities, adds them to the scene hierarchy, and creates and adds components to them.\n## Questions: \n 1. What is the purpose of the `SceneParser` class?\n- The `SceneParser` class is responsible for parsing scene data and creating entities with their components and hierarchy.\n\n2. What is the significance of the `compressed` variable in the `parse` method?\n- The `compressed` variable is used to check if the scene data is in a compressed format and if so, it is decompressed before being parsed.\n\n3. What is the purpose of the `_openComponentData` method?\n- The `_openComponentData` method is responsible for creating components for an entity in a specific order and adding its children to the entity's hierarchy.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/scene.md"}}],["385",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/texture/basis.js)\n\nThis code defines a parser for basis files, which are a type of texture file format. The `BasisParser` class implements the `TextureParser` interface and is used to load and parse basis files into textures that can be used in the PlayCanvas engine.\n\nThe `BasisParser` constructor takes in a `registry` and a `device` object. The `device` object is used to create a new texture instance from the parsed basis file. The `maxRetries` property is set to 0 by default and determines the maximum number of times the parser will attempt to load the file.\n\nThe `load` method is used to load the basis file from a given URL. It takes in a `url` string, a `callback` function, and an `asset` object. The `Asset.fetchArrayBuffer` method is used to fetch the file as an array buffer. If there is an error, the `callback` function is called with the error message. If the file is successfully fetched, the `transcode` function is called with the file data. The `transcode` function uses the `basisTranscode` function to transcode the basis file data into a texture that can be used by the engine. If the `basisTranscode` function is not found, an error message is passed to the `callback` function.\n\nThe `open` method is used to create a new texture instance from the parsed basis file data. It takes in a `url` string, the `data` object returned from the `basisTranscode` function, and the `device` object. The `Texture` constructor is used to create a new texture instance with the given parameters. The `addressU` and `addressV` properties are set to either `ADDRESS_CLAMP_TO_EDGE` or `ADDRESS_REPEAT` depending on whether the texture is a cubemap or not. The `width`, `height`, `format`, `cubemap`, and `levels` properties are set to the corresponding values from the `data` object. Finally, the `upload` method is called on the texture instance to upload the texture to the GPU.\n\nOverall, this code provides a way to parse basis files into textures that can be used in the PlayCanvas engine. It can be used by other parts of the engine that require texture loading and parsing functionality. For example, it may be used by the asset loader to load and parse texture assets.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains a parser for basis files used for loading textures in the PlayCanvas engine.\n\n2. What is the role of the `maxRetries` property in the `load` method?\n- The `maxRetries` property determines the maximum number of times the `Asset.fetchArrayBuffer` method will retry fetching the asset in case of a network error.\n\n3. What is the purpose of the `open` method and what does it return?\n- The `open` method creates a new texture instance based on the data provided and returns it. It also sets the texture's properties such as name, addressU, addressV, width, height, format, cubemap, and levels.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/texture/basis.md"}}],["386",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/texture/dds.js)\n\nThe code defines a class called `DdsParser` which is a legacy texture parser for DDS files. The class implements the `TextureParser` interface and is used to parse DDS files and create a `Texture` object that can be used in the PlayCanvas engine. \n\nThe `DdsParser` class has two methods: `load` and `open`. The `load` method is used to fetch the DDS file as an array buffer and pass it to the `open` method. The `open` method is responsible for parsing the DDS file and creating a `Texture` object. \n\nThe `open` method first reads the DDS file header to extract information such as the width, height, number of mipmaps, pixel format, and whether the texture is a cubemap. It then checks the pixel format to determine whether the texture is compressed or not. If the texture is compressed, it checks the FourCC code to determine the compression format (DXT1, DXT5, ETC1, PVRTC2, PVRTC4). If the texture is not compressed, it checks the bits per pixel to determine the pixel format (RGBA8). \n\nIf the pixel format is unsupported, the method logs an error and creates an empty texture. Otherwise, it creates a `Texture` object with the specified width, height, format, and mipmaps. It then reads the texture data from the DDS file and stores it in the `Texture` object. If the texture is a cubemap, it stores the data for each face separately. Finally, it uploads the texture data to the GPU and returns the `Texture` object. \n\nThis class is used in the PlayCanvas engine to parse DDS files and create `Texture` objects that can be used in the engine. Developers can use this class to load DDS files and create textures for their projects. For example, the following code can be used to load a DDS file and create a texture:\n\n```\nconst asset = new pc.Asset('texture', 'texture', { url: 'texture.dds' });\nasset.ready((texture) => {\n    // use the texture\n});\nasset.resource = new pc.DdsParser().open(asset.getFileUrl(), asset.resource, device);\n```\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains a legacy texture parser for dds files in the PlayCanvas engine.\n\n2. What types of pixel formats are supported by this parser?\n- This parser supports various pixel formats including DXT1, DXT5, ETC1, PVRTC, RGB8, RGBA8, RGBA16f, and RGBA32f.\n\n3. How does this parser handle unsupported pixel formats?\n- If the pixel format is unsupported, the parser will create an empty texture instead and output an error message using the Debug.error() method.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/texture/dds.md"}}],["387",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/texture/hdr.js)\n\nThe code defines a class called `HdrParser` that implements the `TextureParser` interface. This class is responsible for parsing HDR files and returning a `Texture` object that can be used in the PlayCanvas engine. \n\nThe `HdrParser` class has three methods: `constructor`, `load`, and `open`. The `constructor` method initializes the `maxRetries` property to 0. The `load` method fetches the HDR file using the `Asset.fetchArrayBuffer` method and passes the result to the `parse` method. The `open` method takes the parsed data and creates a new `Texture` object with the appropriate properties. \n\nThe `parse` method reads the HDR file and extracts the header variables, resolution specifier, and pixel data. It then creates a new object with the width, height, and pixel data and returns it. If any errors occur during parsing, `null` is returned instead. \n\nThe `_readPixels` method is a helper method used by `parse` to read the pixel data from the HDR file. It first checks if the file is RLE-encoded and allocates a buffer for the pixel data. It then reads each scanline and stores the pixel data in the buffer. If any errors occur during reading, `null` is returned instead. \n\nThe `_readPixelsFlat` method is another helper method used by `_readPixels` to read the pixel data if the file is not RLE-encoded. It simply returns the remaining bytes in the `ReadStream` object as a `Uint8Array`. \n\nOverall, this code is an essential part of the PlayCanvas engine's texture loading system. It allows HDR files to be loaded and used as textures in the engine. Developers can use this class to create custom texture loaders for other file formats by implementing the `TextureParser` interface. \n\nExample usage:\n\n```javascript\nimport { HdrParser } from 'path/to/hdr-parser.js';\n\nconst parser = new HdrParser();\nconst url = 'path/to/texture.hdr';\n\nparser.load(url, (err, data) => {\n  if (err) {\n    console.error(err);\n    return;\n  }\n\n  const texture = parser.open(url, data, device);\n  // use texture in PlayCanvas engine\n});\n```\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains a texture parser for HDR files in the PlayCanvas engine.\n\n2. What is the format of the texture created by this parser?\n- The texture created by this parser has a format of PIXELFORMAT_RGBA8 and a type of TEXTURETYPE_RGBE.\n\n3. What is the algorithm used to parse the HDR file?\n- The algorithm used to parse the HDR file involves reading the header variables, checking for the FORMAT variable, reading the resolution specifier, and then reading the pixel data. The pixel data can be either flat or run-length encoded (RLE).","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/texture/hdr.md"}}],["388",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/texture/img.js)\n\nThe code defines a class called `ImgParser` that is responsible for parsing browser-supported image formats and loading them as textures. This class implements the `TextureParser` interface and is used in the larger PlayCanvas engine project to handle image assets.\n\nThe `ImgParser` class has a constructor that takes in a `registry` and a `device`. The `registry` parameter is used to determine whether to set the `crossOrigin` property to `'anonymous'` or `null`. The `device` parameter is used to check if the browser supports the `ImageBitmap` interface.\n\nThe `load` method of the `ImgParser` class is used to load an image asset. It takes in a `url`, a `callback`, and an optional `asset` parameter. If the `asset` parameter has a `file` property with `contents`, the method checks if the browser supports the `ImageBitmap` interface. If it does, the method calls the `_loadImageBitmapFromData` method to load the image. Otherwise, it creates a new `url` object using `URL.createObjectURL` and a `Blob` object containing the `contents` of the `asset` file. The method then calls the `_loadImage` method to load the image.\n\nThe `_loadImage` method is responsible for loading an image using the `Image` constructor. It takes in a `url`, an `originalUrl`, a `crossOrigin`, and a `callback`. If `crossOrigin` is truthy, it sets the `crossOrigin` property of the `Image` object to `crossOrigin`. The method then sets up an `onload` callback that calls the `callback` with the loaded `Image` object. If the image fails to load, the method retries loading the image a few times before calling the `callback` with an error.\n\nThe `_loadImageBitmap` method is responsible for loading an image using the `http` module and the `createImageBitmap` method. It takes in a `url`, an `originalUrl`, a `crossOrigin`, and a `callback`. The method makes an HTTP GET request to the `url` using the `http.get` method and passes in options to cache the response, set the response type to `'blob'`, and retry the request if it fails. If the request succeeds, the method calls the `createImageBitmap` method to create an `ImageBitmap` object from the `Blob` response and passes it to the `callback`. If the request fails, the method calls the `callback` with an error.\n\nThe `_loadImageBitmapFromData` method is responsible for loading an image from data using the `createImageBitmap` method. It takes in `data` and a `callback`. The method creates a new `Blob` object from the `data` and passes it to the `createImageBitmap` method to create an `ImageBitmap` object. The method then passes the `ImageBitmap` object to the `callback`.\n\nThe `open` method of the `ImgParser` class is used to open an image asset. It takes in a `url`, `data`, and a `device`. The method creates a new `Texture` object using the `Texture` constructor and sets its properties using the `data` parameter. The method then sets the source of the `Texture` object to `data` and returns the `Texture` object.\n\nOverall, the `ImgParser` class is a crucial part of the PlayCanvas engine project as it handles the loading and parsing of image assets. It provides a way to load images as textures and supports cross-origin loading and retrying failed requests.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains a parser for browser-supported image formats used in the PlayCanvas engine.\n\n2. What is the significance of the `crossOrigin` property in the `ImgParser` class?\n- The `crossOrigin` property is used to determine whether to set the `crossOrigin` attribute of an image element to \"anonymous\" or not, which affects how cookies are sent in some browsers.\n\n3. What is the purpose of the `_loadImageBitmap` method in the `ImgParser` class?\n- The `_loadImageBitmap` method is used to load an image as an `ImageBitmap` object, which can be more efficient than loading it as an `Image` element in some cases.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/texture/img.md"}}],["389",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/texture/ktx.js)\n\nThe code defines a parser for KTX files, which are used to store compressed and uncompressed textures. The parser is implemented as a class called `KtxParser` that implements the `TextureParser` interface. The `load` method of the parser is used to fetch the KTX file as an array buffer, while the `open` method is used to parse the data and create a new `Texture` object.\n\nThe `parse` method of the parser is responsible for parsing the KTX file data. It first checks the magic bits to ensure that the file is a valid KTX file. It then unpacks the header information, which includes the texture format, width, height, number of faces, and number of mipmap levels. The parser only supports a subset of pixel formats, which are defined in the `KNOWN_FORMATS` object. If the format is not supported, the parser returns null.\n\nThe parser then creates a new `Texture` object with the parsed information and uploads the texture data to the GPU. The `Texture` object is returned to the caller.\n\nThe `createContainer` function is used to create a new typed array that represents the texture data for a given pixel format. The function takes the pixel format, buffer, byte offset, and byte size as arguments and returns a new typed array.\n\nOverall, the `KtxParser` class provides a way to parse KTX files and create `Texture` objects from them. This is useful for loading textures in a game engine or other graphics application. The parser supports a subset of pixel formats and does not support volume textures or texture arrays.\n## Questions: \n 1. What is the purpose of this code?\n- This code is a texture parser for KTX files in the PlayCanvas engine.\n\n2. What formats of textures does this parser support?\n- This parser supports both compressed and uncompressed formats of textures, including DXT1, DXT3, DXT5, ETC1, ETC2_RGB, ETC2_RGBA, PVRTC_4BPP_RGB_1, PVRTC_2BPP_RGB_1, PVRTC_4BPP_RGBA_1, PVRTC_2BPP_RGBA_1, RGB8, RGBA8, SRGB, SRGBA, 111110F, RGB16F, and RGBA16F.\n\n3. What is the purpose of the `createContainer` function?\n- The `createContainer` function creates a new typed array to hold the texture data based on the pixel format of the texture. If the pixel format is `PIXELFORMAT_111110F`, it creates a `Uint32Array`, otherwise it creates a `Uint8Array`.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/texture/ktx.md"}}],["390",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/parsers/texture/ktx2.js)\n\nThe code defines a class called `Ktx2Parser` which is a texture parser for KTX2 files. The class implements the `TextureParser` interface and is used to parse KTX2 files and create textures from them. \n\nThe `Ktx2Parser` class has three methods: `load`, `open`, and `parse`. The `load` method is used to fetch the KTX2 file and parse it. It takes a URL, a callback function, and an asset object as parameters. The `open` method is used to create a new texture object from the parsed data. It takes a URL, data, and a device object as parameters. The `parse` method is used to parse the KTX2 file. It takes an array buffer, a URL, a callback function, and an asset object as parameters.\n\nThe `Ktx2Parser` class uses the `ReadStream` class to read the KTX2 file data. It checks the magic header bits to ensure that the file is a valid KTX2 file. It then unpacks the header, index, levels, and data format descriptor. The `parse` method checks if the file is supercompressed or not. If it is supercompressed, it uses the `basisTranscode` function to transcode the data into a format that can be used by the device. If it is not supercompressed, it logs an error message saying that the pixel format is unsupported.\n\nThe `Ktx2Parser` class is used in the larger PlayCanvas engine project to parse KTX2 files and create textures from them. It is used by other classes and functions that require textures, such as the `Material` class and the `Model` class. \n\nExample usage:\n\n```\nimport { Ktx2Parser } from 'playcanvas-engine';\n\nconst parser = new Ktx2Parser(registry, device);\n\nparser.load('path/to/texture.ktx2', (err, texture) => {\n    if (err) {\n        console.error(err);\n    } else {\n        console.log(texture);\n    }\n}, asset);\n```\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains a Texture parser for ktx2 files in the PlayCanvas engine.\n\n2. What is the format of the texture that this parser can handle?\n- This parser can handle ktx2 files.\n\n3. What is the purpose of the `KHRConstants` object?\n- The `KHRConstants` object contains constants for the KHR_DF_MODEL_ETC1S and KHR_DF_MODEL_UASTC models.","metadata":{"source":".autodoc/docs/markdown/src/framework/parsers/texture/ktx2.md"}}],["391",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/scene-registry-item.js)\n\nThe code defines a class called `SceneRegistryItem` which represents an item to be stored in the `SceneRegistry`. The `SceneRegistry` is likely a data structure used to manage scenes in the PlayCanvas engine. \n\nThe `SceneRegistryItem` class has a constructor that takes two parameters: `name` and `url`. These parameters represent the name and URL of the scene file, respectively. The constructor initializes the `name` and `url` properties of the instance with the provided values. It also initializes the `data` property to `null`, indicating that the scene data has not yet been loaded. The `_loading` property is initialized to `false`, indicating that the scene data is not currently being loaded. Finally, the `_onLoadedCallbacks` property is initialized to an empty array, which will be used to store callbacks that should be executed when the scene data has finished loading.\n\nThe `SceneRegistryItem` class has two getter methods: `loaded` and `loading`. The `loaded` getter returns `true` if the `data` property is not `null`, indicating that the scene data has been loaded. The `loading` getter returns the value of the `_loading` property, indicating whether the scene data is currently being loaded.\n\nThis class is likely used in the larger PlayCanvas engine project to manage scenes. When a new scene is loaded, a new `SceneRegistryItem` instance is created with the name and URL of the scene file. This instance is then added to the `SceneRegistry`, which likely keeps track of all the scenes in the project. The `loaded` and `loading` getters can be used to determine the status of a particular scene's data. For example, if a script needs to access a particular scene's data, it can check the `loaded` property of the corresponding `SceneRegistryItem` instance. If the `loaded` property is `false`, the script can register a callback function with the `_onLoadedCallbacks` array. When the scene data finishes loading, the `SceneRegistryItem` instance will execute all the registered callbacks.\n## Questions: \n 1. What is the purpose of this code and how does it fit into the PlayCanvas engine? \n- This code defines a class called `SceneRegistryItem` which represents an item to be stored in the `SceneRegistry`. It contains properties for the name and URL of a scene file, as well as methods for checking if the scene data has loaded or is still loading.\n\n2. What parameters are required to create a new instance of `SceneRegistryItem`? \n- Two parameters are required: `name` (a string representing the name of the scene) and `url` (a string representing the URL of the scene file).\n\n3. What is the purpose of the `_onLoadedCallbacks` property and how is it used? \n- The `_onLoadedCallbacks` property is an array that stores callback functions to be executed when the scene data has finished loading. These callbacks can be added to the array using the `addLoadedCallback` method, and are executed when the `load` method is called and the scene data has finished loading.","metadata":{"source":".autodoc/docs/markdown/src/framework/scene-registry-item.md"}}],["392",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/script/constants.js)\n\nThis code defines a set of constants that represent different stages in the lifecycle of a script in the PlayCanvas engine. \n\nThe `SCRIPT_INITIALIZE` constant represents the initialization stage of a script, which is called once when the script is first loaded. This is where any necessary setup or initialization code can be executed.\n\nThe `SCRIPT_POST_INITIALIZE` constant represents the post-initialization stage of a script, which is called after all scripts have been initialized. This is where any additional setup or initialization code that depends on other scripts can be executed.\n\nThe `SCRIPT_UPDATE` constant represents the update stage of a script, which is called every frame. This is where any code that needs to be executed every frame, such as game logic or animation updates, can be executed.\n\nThe `SCRIPT_POST_UPDATE` constant represents the post-update stage of a script, which is called after all scripts have been updated. This is where any additional code that depends on the state of other scripts can be executed.\n\nThe `SCRIPT_SWAP` constant represents the swap stage of a script, which is called when a script is swapped out for another script. This is where any cleanup or teardown code can be executed.\n\nThese constants can be used by script developers to define functions that will be called at each stage of the script lifecycle. For example, a script might define an `initialize` function that is called during the initialization stage, like this:\n\n```\nclass MyScript extends pc.ScriptType {\n    initialize() {\n        // Initialization code goes here\n    }\n}\n```\n\nBy using these constants and defining functions for each stage of the script lifecycle, script developers can ensure that their code is executed at the appropriate times and in the correct order. This can help to avoid bugs and ensure that the game or application behaves as expected.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code exports constants related to script events in the PlayCanvas engine, such as initialization, updating, and swapping.\n\n2. **How are these constants used in the PlayCanvas engine?**\\\nThese constants are likely used as event names that trigger specific functions or behaviors within the engine's scripting system.\n\n3. **Are there any other script events that are not included in these constants?**\\\nIt's possible that there are other script events in the PlayCanvas engine that are not included in these constants, as this code only exports a specific set of events. A developer may need to consult the engine's documentation or source code to determine if there are additional events available.","metadata":{"source":".autodoc/docs/markdown/src/framework/script/constants.md"}}],["393",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/script/script-registry.js)\n\nThe `ScriptRegistry` class is a container for all `ScriptType`s that are available to the PlayCanvas engine application. It is an extension of the `EventHandler` class, which allows it to handle events. The purpose of this class is to manage the scripts that are used in the application and provide a way to add, remove, and get scripts from the registry.\n\nWhen an instance of the `ScriptRegistry` class is created, it takes an `AppBase` object as a parameter. This object is the application to which the registry is attached. The `_scripts` and `_list` properties are initialized as empty objects and arrays, respectively.\n\nThe `add` method is used to add a `ScriptType` to the registry. If the script already exists in the registry, and the new script has a `swap` method defined, it will perform code hot swapping automatically in an asynchronous manner. The `add` method returns `true` if the script is added for the first time or `false` if the script already exists.\n\nThe `remove` method is used to remove a `ScriptType` from the registry. It takes the name or type of the script as a parameter and returns `true` if the script is removed or `false` if it is not in the registry.\n\nThe `get` method is used to get a `ScriptType` by name. It takes the name of the script as a parameter and returns the script type if it exists in the registry or `null` otherwise.\n\nThe `has` method is used to check if a `ScriptType` with the specified name is in the registry. It takes the name or type of the script as a parameter and returns `true` if the script is in the registry.\n\nThe `list` method is used to get a list of all `ScriptType`s from the registry. It returns an array of all `ScriptType`s in the registry.\n\nOverall, the `ScriptRegistry` class provides a way to manage the scripts used in the PlayCanvas engine application. It allows for easy addition, removal, and retrieval of scripts from the registry. It also provides a way to check if a script is in the registry and get a list of all scripts in the registry.\n## Questions: \n 1. What is the purpose of the `ScriptRegistry` class?\n    \n    The `ScriptRegistry` class is a container for all `ScriptType`s that are available to the PlayCanvas application. It allows PlayCanvas scripts to access the Script Registry from inside the application with `AppBase#scripts`.\n\n2. What methods are available in the `ScriptRegistry` class?\n    \n    The `ScriptRegistry` class has several methods available, including `add`, `remove`, `get`, `has`, and `list`. These methods allow developers to add, remove, get, check for, and list `ScriptType`s in the registry.\n\n3. How does the `add` method work in the `ScriptRegistry` class?\n    \n    The `add` method in the `ScriptRegistry` class adds a `ScriptType` to the registry. If a script already exists in the registry and the new script has a `swap` method defined, it will perform code hot swapping automatically in an async manner. The method returns `true` if the script was added for the first time or `false` if the script already exists.","metadata":{"source":".autodoc/docs/markdown/src/framework/script/script-registry.md"}}],["394",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/script/script-type.js)\n\nThe code defines a class called `ScriptType` that represents the type of a script in the PlayCanvas engine. The class extends the `EventHandler` class and has several methods and properties that are used to manage the behavior of a script instance. \n\nThe `ScriptType` class has several methods that are executed by the engine on instances of this type, such as `initialize`, `postInitialize`, `update`, `postUpdate`, and `swap`. The `initialize` and `postInitialize` methods are called (if defined) when a script is about to run for the first time. The `postInitialize` method will run after all `initialize` methods are executed in the same tick or enabling chain of actions. The `update` and `postUpdate` methods are called (if defined) for enabled (running state) scripts on each tick. The `swap` method is called when a `ScriptType` that already exists in the registry gets redefined. If the new `ScriptType` has a `swap` method in its prototype, then it will be executed to perform hot-reload at runtime.\n\nThe `ScriptType` class has several events that can be fired during the lifecycle of a script instance. These events include `enable`, `disable`, `state`, `destroy`, `attr`, `attr:[name]`, and `error`. The `enable` event is fired when a script instance becomes enabled, the `disable` event is fired when a script instance becomes disabled, and the `state` event is fired when a script instance changes state to enabled or disabled. The `destroy` event is fired when a script instance is destroyed and removed from the component. The `attr` event is fired when any script attribute has been changed, and the `attr:[name]` event is fired when a specific script attribute has been changed. The `error` event is fired when a script instance had an exception, and the script instance will be automatically disabled.\n\nThe `ScriptType` class has a static property called `attributes` that is used to define attributes for script types. The `attributes` property is an instance of the `ScriptAttributes` class, which provides an interface to define attributes for script types. The `ScriptType` class also has a static method called `extend` that is used to extend the `ScriptType` prototype with a list of methods.\n\nOverall, the `ScriptType` class is an important part of the PlayCanvas engine that provides a way to define and manage the behavior of script instances. It allows developers to define custom scripts and attach them to entities in the engine, and provides a set of methods and events that can be used to manage the behavior of these scripts.\n## Questions: \n 1. What is the purpose of the `ScriptType` class and how is it used in the PlayCanvas engine?\n   \n   The `ScriptType` class represents the type of a script in the PlayCanvas engine and is used to define the behavior of entities in the game. It is extended using its JavaScript prototype and has a list of methods that will be executed by the engine on instances of this type, such as `initialize`, `postInitialize`, `update`, `postUpdate`, and `swap`.\n\n2. What is the purpose of the `attributes` property in the `ScriptType` class?\n   \n   The `attributes` property is used to define the attributes for a script type in the PlayCanvas engine. It is an interface to the `ScriptAttributes` class and allows developers to specify the type, title, placeholder, and default value for each attribute.\n\n3. What is the purpose of the `enabled` property in the `ScriptType` class?\n   \n   The `enabled` property is used to determine whether an instance of a script type is in a running state or not. When disabled, no update methods will be called on each tick, and initialize and postInitialize methods will run once when the script instance is in an `enabled` state during app tick.","metadata":{"source":".autodoc/docs/markdown/src/framework/script/script-type.md"}}],["395",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/script/script-types.js)\n\nThe code defines a class called `ScriptTypes` which is used to manage the types of scripts used in the PlayCanvas engine project. The class has a static property called `_types` which is an array that holds the different script types. \n\nThe `push` method is used to add new script types to the `_types` array. It takes two parameters: `Type` and `isLegacy`. `Type` is the type of script being added and `isLegacy` is a boolean value that indicates whether the script is a legacy script or not. If `isLegacy` is true and there are already scripts in the `_types` array, an assertion error is thrown. Otherwise, the new script type is added to the end of the `_types` array.\n\nThis class is likely used in other parts of the PlayCanvas engine project to manage the different types of scripts that can be used in the engine. For example, when a new script is added to a project, the `push` method may be called to add the new script type to the `_types` array. Other parts of the engine may then use the `_types` array to determine which scripts are available and how they should be handled.\n\nHere is an example of how the `push` method might be used:\n\n```\nimport { ScriptTypes } from 'playcanvas-engine';\n\nclass MyScript {\n    // ...\n}\n\nScriptTypes.push(MyScript, false);\n```\n\nIn this example, a new script type called `MyScript` is defined and then added to the `_types` array using the `push` method. The second parameter is set to `false` because `MyScript` is not a legacy script.\n## Questions: \n 1. What is the purpose of the `ScriptTypes` class?\n    - The `ScriptTypes` class is used to manage an array of script types.\n\n2. What is the significance of the `isLegacy` parameter in the `push` method?\n    - The `isLegacy` parameter is used to determine if the script being pushed is a legacy script or not. If it is a legacy script and there are already scripts in the array, an error message is logged.\n\n3. What is the purpose of the `export` statement at the end of the code?\n    - The `export` statement is used to make the `ScriptTypes` class available for use in other files or modules.","metadata":{"source":".autodoc/docs/markdown/src/framework/script/script-types.md"}}],["396",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/script/script.js)\n\nThe code defines functions that allow developers to create and register custom scripts for the PlayCanvas engine. The `createScript` function creates a new `ScriptType` class that is registered to the `ScriptRegistry` using a unique name. The `ScriptType` class is used to define custom logic using JavaScript that can be used to create interaction for entities. The function returns the class type (constructor function) that the developer can further extend by adding attributes and prototype methods. If there is an error, the function returns null.\n\nThe `registerScript` function registers an existing class type as a `ScriptType` to the `ScriptRegistry`. This is useful when defining an ES6 script class that extends `ScriptType`. The function takes an existing class type (constructor function) and registers it to the `ScriptRegistry` using a unique name. The function also adds the class to the `scripts` registry and pushes it to the `ScriptTypes` array.\n\nThe `getReservedScriptNames` function returns a set of reserved script names that cannot be used when creating a new script. These names include `system`, `entity`, `create`, `destroy`, `swap`, `move`, `data`, `scripts`, `_scripts`, `_scriptsIndex`, `_scriptsData`, `enabled`, `_oldState`, `onEnable`, `onDisable`, `onPostStateChange`, `_onSetEnabled`, `_checkState`, `_onBeforeRemove`, `_onInitializeAttributes`, `_onInitialize`, `_onPostInitialize`, `_onUpdate`, `_onPostUpdate`, `_callbacks`, `has`, `get`, `on`, `off`, `fire`, `once`, and `hasEvent`.\n\nThe `reservedAttributes` object is used by the editor and is deprecated. It is migrated to `ScriptAttributes.reservedNames` and should be deleted.\n\nThe code provides an example of how to use the `createScript` function to create a new script called `Turning`. The `attributes` object is used to define a `speed` attribute that is available in the Editor UI. The `update` function is defined to rotate the entity around the y-axis based on the `speed` attribute.\n\nOverall, these functions provide a way for developers to create and register custom scripts for the PlayCanvas engine. This allows for greater flexibility and customization of the engine to suit the needs of different projects.\n## Questions: \n 1. What is the purpose of the `createScript` function?\n   \n   The `createScript` function is used to create and register a new `ScriptType` class, which is used to define custom logic using JavaScript that is used to create interaction for entities.\n\n2. What is the difference between `createScript` and `registerScript` functions?\n   \n   The `createScript` function is used to create and register a new `ScriptType` class, while the `registerScript` function is used to register an existing class type as a `ScriptType` to the `ScriptRegistry`.\n\n3. What is the purpose of the `reservedScriptNames` and `reservedAttributes` constants?\n   \n   The `reservedScriptNames` constant is a set of reserved names that cannot be used as script names, while the `reservedAttributes` constant is a mapping of reserved attribute names that are used in the Editor UI.","metadata":{"source":".autodoc/docs/markdown/src/framework/script/script.md"}}],["397",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/script.js)\n\nThe code is a module that defines the `script` namespace, which is used to create and manage scripts in the PlayCanvas engine. The module imports the `events` module from the `core` directory and the `getApplication` function and `ScriptTypes` enum from the `globals` and `script` modules, respectively.\n\nThe `script` namespace has several methods that are used to create and manage scripts. The `create` method is used to create a script resource object. The method takes two parameters: the name of the script object and a callback function that returns the type of the script resource to be instanced for each entity. The callback function is passed an `AppBase` object, which is used to access entities and components. The `ScriptType` returned by the callback function is stored with the script name and pushed onto the loading stack. The `create` method fires a `created` event with the script name and callback function as parameters.\n\nThe `attribute` method is used to create a script attribute for the current script. The method takes four parameters: the name of the attribute, the type of the attribute, the default value of the attribute, and optional parameters for the attribute. The `attribute` method only works when parsing the script.\n\nThe `createLoadingScreen` method is used to handle the creation of the loading screen of the application. The method takes a callback function that can set up and tear down a customized loading screen. The callback function is passed an `AppBase` object, which is used to show a loading screen, progress bar, etc.\n\nThe module also defines a `CreateScreenCallback` and `CreateScriptCallback` callback functions that are used by the `createLoadingScreen` and `create` methods, respectively.\n\nThe module exports the `script` namespace and sets the `legacy` property of the `script` namespace to `false` by default. The `legacy` property can be set to `true` to enable legacy script loading.\n\nOverall, this module provides a way to create and manage scripts in the PlayCanvas engine. The `script` namespace can be used to create script resources, script attributes, and loading screens. The `create` method is particularly useful for creating custom scripts that can be attached to entities in the engine.\n## Questions: \n 1. What is the purpose of the `script.create` function?\n- The `script.create` function is used to create a script resource object that will be instantiated when attached to entities. It takes a name and a callback function that returns the type of the script resource to be instanced for each entity.\n\n2. What is the `script.attribute` function used for?\n- The `script.attribute` function is used to create a script attribute for the current script. Script attributes can be accessed inside or outside a script instance and can be edited from the Attribute Editor of the PlayCanvas Editor like normal components.\n\n3. What is the purpose of the `script.createLoadingScreen` function?\n- The `script.createLoadingScreen` function is used to handle the creation of the loading screen of the application. A script can subscribe to the events of an `AppBase` to show a loading screen, progress bar, etc. by setting the project's loading screen script to the script that calls this method.","metadata":{"source":".autodoc/docs/markdown/src/framework/script.md"}}],["398",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/stats.js)\n\nThe code defines a class called `ApplicationStats` that records performance-related statistics for the PlayCanvas engine application. The purpose of this class is to provide developers with information about the performance of their application, which can be used to optimize it.\n\nThe `ApplicationStats` class has several properties that store different types of performance-related statistics. The `frame` property stores statistics related to the rendering of frames, such as frames per second (fps), time to render a frame (ms), and time to update the scene (updateTime). It also stores statistics related to other aspects of rendering, such as the number of triangles, shaders, and materials used.\n\nThe `drawCalls` property stores statistics related to the number of draw calls made during rendering. Draw calls are used to render objects on the screen, and the number of draw calls can have a significant impact on performance. The `misc` property stores other miscellaneous statistics, such as the time it takes to create render targets.\n\nThe `particles` property stores statistics related to particle systems, such as the number of updates per frame and the time it takes to update particles.\n\nThe `shaders` property stores statistics related to the number of shaders used by the graphics device, and the `vram` property stores statistics related to the amount of video memory used by the graphics device.\n\nThe `getApplication` function is imported from `globals.js` and is used to get the current application instance. The `scene`, `lightmapper`, and `batcher` properties are getters that return statistics related to the scene, lightmapping, and batching, respectively.\n\nOverall, the `ApplicationStats` class provides developers with a way to monitor the performance of their PlayCanvas engine application and identify areas that need optimization. For example, if the number of draw calls is high, developers can try to reduce it by using batching or other optimization techniques.\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n- This code defines a class called `ApplicationStats` that records performance-related statistics for the PlayCanvas engine. It is used to monitor and optimize the engine's performance.\n\n2. What parameters does the `constructor` function of the `ApplicationStats` class take?\n- The `constructor` function takes a single parameter called `device`, which is an instance of the `GraphicsDevice` class defined in another file.\n\n3. What other statistics are being tracked besides performance-related ones?\n- The code also tracks statistics related to draw calls, memory usage, particles, shaders, and various other miscellaneous metrics.","metadata":{"source":".autodoc/docs/markdown/src/framework/stats.md"}}],["399",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/template.js)\n\nThe code defines a class called `Template` that is used to create a resource template from raw database data. The purpose of this class is to allow for the creation of multiple instances of a template, each with its own unique properties and values. \n\nThe `Template` class has a constructor that takes two parameters: an instance of `AppBase` and an object containing asset data from the database. The `AppBase` instance is used to access the application's resources and functionality, while the asset data is used to define the template's properties and values.\n\nThe `Template` class has a single public method called `instantiate()`. This method creates an instance of the template by cloning the root entity of the template. If the template has not been parsed yet, the `_parseTemplate()` method is called to parse the template data and create the root entity.\n\nThe `_parseTemplate()` method creates a new instance of the `SceneParser` class, passing in the `AppBase` instance and a boolean value indicating whether or not to load scripts. The `SceneParser` class is responsible for parsing scene data and creating entities from it. The `_parseTemplate()` method then calls the `parse()` method of the `SceneParser` instance, passing in the template data. The `parse()` method returns the root entity of the parsed scene, which is then stored in the `_templateRoot` property of the `Template` instance.\n\nOverall, the `Template` class provides a way to create and manage resource templates in the PlayCanvas engine. It can be used to create multiple instances of a template with different properties and values, making it a useful tool for game development. \n\nExample usage:\n\n```\nconst app = new AppBase();\nconst templateData = {\n    // asset data from the database\n};\nconst template = new Template(app, templateData);\nconst instance1 = template.instantiate();\nconst instance2 = template.instantiate();\n```\n## Questions: \n 1. What is the purpose of the `SceneParser` import and how is it used in this code?\n   - The `SceneParser` is imported from the `./parsers/scene.js` file and is used to parse the `data` object passed to the `Template` constructor in the `_parseTemplate()` method.\n2. What is the expected format of the `data` object passed to the `Template` constructor?\n   - The format of the `data` object is not specified in this code and would need to be documented elsewhere in the PlayCanvas engine documentation.\n3. What is the significance of the `true` argument passed to the `SceneParser` constructor?\n   - The `true` argument passed to the `SceneParser` constructor indicates that the parser should create a hierarchy of entities from the parsed data.","metadata":{"source":".autodoc/docs/markdown/src/framework/template.md"}}],["400",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/constants.js)\n\nThis file contains a set of constants that define different types of XR sessions, reference spaces, target rays, hand types, trackable types, and depth sensing preferences. These constants are used throughout the PlayCanvas engine to provide a standardized way of referring to these different types.\n\nThe XR session types include XRTYPE_INLINE, XRTYPE_VR, and XRTYPE_AR, which represent inline sessions, immersive VR sessions, and immersive AR sessions, respectively. These types are used to specify the type of session that should be created when initializing an XR session.\n\nThe reference spaces include XRSPACE_VIEWER, XRSPACE_LOCAL, XRSPACE_LOCALFLOOR, XRSPACE_BOUNDEDFLOOR, and XRSPACE_UNBOUNDED. These represent different types of tracking spaces that can be used in an XR session. For example, XRSPACE_LOCAL represents a tracking space with a native origin near the viewer at the time of creation, while XRSPACE_UNBOUNDED represents a tracking space where the user is expected to move freely around their environment.\n\nThe target rays include XRTARGETRAY_GAZE, XRTARGETRAY_SCREEN, and XRTARGETRAY_POINTER, which represent different types of input sources for the XR session. For example, XRTARGETRAY_GAZE indicates that the target ray will originate at the viewer and follow the direction it is facing.\n\nThe hand types include XRHAND_NONE, XRHAND_LEFT, and XRHAND_RIGHT, which represent different types of input sources for the XR session. For example, XRHAND_LEFT indicates that the input source is meant to be held in the left hand.\n\nThe trackable types include XRTRACKABLE_POINT, XRTRACKABLE_PLANE, and XRTRACKABLE_MESH, which represent different types of objects that can be tracked in an XR session. For example, XRTRACKABLE_PLANE indicates that the hit test results will be computed based on the planes detected by the underlying Augmented Reality system.\n\nFinally, the depth sensing preferences include XRDEPTHSENSINGUSAGE_CPU, XRDEPTHSENSINGUSAGE_GPU, XRDEPTHSENSINGFORMAT_L8A8, and XRDEPTHSENSINGFORMAT_F32, which represent different preferences for how depth sensing should be performed in an XR session. For example, XRDEPTHSENSINGUSAGE_CPU indicates that depth sensing preferred usage is CPU.\n\nOverall, this file provides a standardized set of constants that can be used throughout the PlayCanvas engine to refer to different types of XR sessions, reference spaces, target rays, hand types, trackable types, and depth sensing preferences. This helps to ensure consistency and clarity throughout the codebase.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines constants for different types of XR sessions, reference spaces, target rays, hand types, trackable types, and depth sensing preferences.\n\n2. What are some examples of use cases for these constants?\n- These constants can be used in XR-related code to specify the type of session, reference space, target ray, hand type, trackable type, or depth sensing preference that is being used.\n\n3. Are there any limitations or dependencies associated with these constants?\n- The code does not provide any information on limitations or dependencies associated with these constants. It is possible that certain constants may only be available on certain platforms or with certain devices, but this is not specified in the code.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/constants.md"}}],["401",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-dom-overlay.js)\n\nThe code defines a class called `XrDomOverlay` that provides the ability to use DOM elements as an overlay in a WebXR AR session. It requires that the root DOM element is provided for session start. This way, input source select events are first tested against DOM Elements and then propagated down to the XR Session. If this propagation is not desirable, use the `beforexrselect` event on a DOM element and the `preventDefault` function to stop propagation.\n\nThe class has a constructor that takes a `manager` parameter, which is a WebXR Manager. It has a private `_manager` property that is set to the `manager` parameter. It also has a private `_supported` property that is set to `true` if the browser is supported and the `window.XRDOMOverlayState` is available. It has a private `_root` property that is set to `null`.\n\nThe class has a `supported` getter that returns the value of the `_supported` property. It has an `available` getter that returns `true` if the `_supported` property is `true`, the `_manager.active` property is `true`, and the `_manager._session.domOverlayState` property is not `null`.\n\nThe class has a `state` getter that returns the state of the DOM Overlay, which defines how the root DOM element is rendered. Possible options are `screen`, `floating`, and `head-locked`. It returns `null` if the `_supported` property is `false`, the `_manager.active` property is `false`, or the `_manager._session.domOverlayState` property is `null`.\n\nThe class has a `root` setter that sets the `_root` property to the value passed in if the `_supported` property is `true` and the `_manager.active` property is `false`. It has a `root` getter that returns the `_root` property.\n\nTo use the `XrDomOverlay` class, first create an instance of the `XrDomOverlay` class by passing in a `manager` parameter. Then set the `root` property to the root DOM element. Finally, start the WebXR session using the `start` method of the WebXR Manager. Here is an example:\n\n```javascript\nconst xrManager = new XrManager();\nconst xrDomOverlay = new XrDomOverlay(xrManager);\nxrDomOverlay.root = document.getElementById('root');\nxrManager.start(camera, pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR);\n```\n\nTo disable input source firing `select` event when some descendant element of DOM overlay root is touched/clicked, use the `beforexrselect` event on a DOM element and the `preventDefault` function to stop propagation. Here is an example:\n\n```javascript\nsomeElement.addEventListener('beforexrselect', function (evt) {\n    evt.preventDefault();\n});\n```\n## Questions: \n 1. What is the purpose of the XrDomOverlay class?\n- The XrDomOverlay class provides the ability to use DOM elements as an overlay in a WebXR AR session, allowing input source select events to be first tested against DOM elements and then propagated down to the XR Session.\n\n2. How can a developer disable input source firing 'select' event when some descendant element of DOM overlay root is touched/clicked?\n- A developer can use the 'beforexrselect' event on a DOM element and the 'preventDefault' function to stop propagation.\n\n3. What are the possible options for the state of the DOM Overlay?\n- The possible options for the state of the DOM Overlay are 'screen', 'floating', and 'head-locked'.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-dom-overlay.md"}}],["402",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-finger.js)\n\nThe code defines a class called `XrFinger` which represents a finger in a hand. The class has private properties `_index`, `_hand`, `_joints`, and `_tip`. `_index` is a number representing the index of the finger, `_hand` is an instance of `XrHand` representing the hand that the finger belongs to, `_joints` is an array of `XrJoint` instances representing the joints in the finger, and `_tip` is an instance of `XrJoint` representing the tip of the finger.\n\nThe constructor of the class takes two arguments, `index` and `hand`, and initializes the `_index` and `_hand` properties with them. It also adds the instance of `XrFinger` to the `_fingers` array of the `_hand` instance.\n\nThe class has four getter methods, `index`, `hand`, `joints`, and `tip`, which return the values of the corresponding private properties. The `index` getter returns the index of the finger, the `hand` getter returns the instance of `XrHand` representing the hand that the finger belongs to, the `joints` getter returns an array of `XrJoint` instances representing the joints in the finger, and the `tip` getter returns an instance of `XrJoint` representing the tip of the finger.\n\nThis class is likely used in the larger project to represent fingers in a hand for use in XR (extended reality) applications. The `XrFinger` instances can be used to track the movement and position of fingers in a hand, which can be used for various purposes such as hand gestures and interactions with virtual objects. An example of how this class could be used is shown below:\n\n```\nconst hand = new XrHand();\nconst indexFinger = new XrFinger(1, hand);\nconst joints = indexFinger.joints;\nconst tip = indexFinger.tip;\n```\n\nIn this example, a new instance of `XrHand` is created, and then a new instance of `XrFinger` representing the index finger is created and added to the hand. The `joints` and `tip` properties of the `XrFinger` instance are then accessed and stored in variables for further use.\n## Questions: \n 1. What is the purpose of the XrFinger class?\n- The XrFinger class represents a finger with related joints and index.\n\n2. What are the parameters of the constructor function?\n- The constructor function takes in two parameters: index (number) and hand (XrHand).\n\n3. What is the purpose of the get tip() method?\n- The get tip() method returns the tip of a finger or null if not available.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-finger.md"}}],["403",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-hand.js)\n\nThe code defines a class called `XrHand` that represents a hand with fingers and joints in a WebXR environment. The class extends the `EventHandler` class, which allows it to fire and listen to events. The `XrHand` class has several private properties, including `_manager`, `_inputSource`, `_tracking`, `_fingers`, `_joints`, `_jointsById`, `_tips`, and `_wrist`. \n\nThe constructor of the `XrHand` class takes an `XrInputSource` object as an argument and initializes the private properties. It also creates `XrJoint` and `XrFinger` objects based on the `fingerJointIds` array, which contains the IDs of the joints for each finger. The `XrJoint` and `XrFinger` objects are stored in the `_joints` and `_fingers` arrays, respectively. The `_jointsById` object is used to store the `XrJoint` objects by their IDs.\n\nThe `update` method of the `XrHand` class is called every frame and updates the position and orientation of the joints based on the current frame. It also calculates the position and direction of the hand ray, which is used for raycasting. The method also emulates squeeze events by checking if all four fingers are closed.\n\nThe `getJointById` method returns an `XrJoint` object based on its ID. The `fingers`, `joints`, `tips`, `wrist`, and `tracking` properties return the fingers, joints, fingertips, wrist, and tracking status of the hand, respectively.\n\nOverall, the `XrHand` class provides a way to represent a hand with fingers and joints in a WebXR environment and provides methods to access and manipulate the hand's properties. It can be used in conjunction with other classes in the PlayCanvas engine to create interactive WebXR experiences.\n## Questions: \n 1. What is the purpose of the `fingerJointIds` array?\n- The `fingerJointIds` array is used to store the IDs of the joints for each finger of a hand.\n\n2. What is the purpose of the `update` method?\n- The `update` method is used to update the state of the `XrHand` object based on the current frame of the XR session.\n\n3. What is the purpose of the `getJointById` method?\n- The `getJointById` method is used to retrieve a specific joint of the hand based on its ID, as defined in the WebXR Hand Input specification.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-hand.md"}}],["404",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-image-tracking.js)\n\nThe `XrImageTracking` class is a part of the PlayCanvas engine project and provides the ability to track real-world images by providing image samples and their estimated sizes. This class extends the `EventHandler` class and is used to track images in an AR session. It is used in conjunction with the `XrTrackedImage` class, which contains tracking information for a single image.\n\nThe `XrImageTracking` class has a constructor that takes a `manager` parameter, which is an instance of the `XrManager` class. The constructor sets up event listeners for the `start` and `end` events of the `manager` object. When the `start` event is fired, the `_onSessionStart` method is called, which retrieves the tracked image scores and sets the `_available` property to true. When the `end` event is fired, the `_onSessionEnd` method is called, which sets the `_available` property to false and resets the tracking information for all images.\n\nThe `XrImageTracking` class has an `add` method that takes an `image` parameter and a `width` parameter. The `image` parameter is an image that matches a real-world image as closely as possible, and the `width` parameter is the width of the image in the real world. The method creates a new `XrTrackedImage` object with the provided parameters and adds it to the `_images` array. If image tracking is not supported or the XR manager is not active, the method returns null.\n\nThe `XrImageTracking` class also has a `remove` method that takes a `trackedImage` parameter. The method removes the `trackedImage` object from the `_images` array and destroys it.\n\nThe `XrImageTracking` class has a `prepareImages` method that takes a `callback` parameter. The method prepares all images in the `_images` array as image bitmaps and calls the `callback` function when all images have been prepared.\n\nThe `XrImageTracking` class has an `update` method that takes a `frame` parameter. The method updates the tracking information for all images in the `_images` array based on the results of the `getImageTrackingResults` method of the `frame` object. If an image is being tracked and is no longer visible, the method fires an `untracked` event. If an image is not being tracked and becomes visible, the method fires a `tracked` event.\n\nThe `XrImageTracking` class has three properties: `supported`, `available`, and `images`. The `supported` property is a boolean that indicates whether image tracking is supported. The `available` property is a boolean that indicates whether image tracking is available. The `images` property is an array of `XrTrackedImage` objects that contain tracking information for all images being tracked.\n\nOverall, the `XrImageTracking` class provides a way to track real-world images in an AR session and is an important part of the PlayCanvas engine project.\n## Questions: \n 1. What is the purpose of this code?\n- This code provides the ability to track real world images by provided image samples and their estimated sizes.\n\n2. What is the format of the image that can be added for image tracking?\n- The image can be in the format of HTMLCanvasElement, HTMLImageElement, SVGImageElement, HTMLVideoElement, Blob, ImageData, or ImageBitmap.\n\n3. When can an image be removed from image tracking?\n- An image can be removed from image tracking only before an AR session is started.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-image-tracking.md"}}],["405",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-input.js)\n\nThe `XrInput` class is a part of the PlayCanvas engine project and provides access to input sources for WebXR. It extends the `EventHandler` class and is used to handle input events such as select, squeeze, and their respective start and end events. \n\nThe class contains a constructor that takes a `manager` parameter, which is an instance of the `XrManager` class. The constructor sets up event listeners for the `start` and `end` events of the `manager` object. When the `start` event is fired, the `_onSessionStart` method is called, which sets up event listeners for the `select`, `selectstart`, `selectend`, `squeeze`, `squeezestart`, and `squeezeend` events. It also adds input sources to the `_inputSources` array. When the `end` event is fired, the `_onSessionEnd` method is called, which removes all input sources from the `_inputSources` array.\n\nThe class also has several private methods, including `_onInputSourcesChange`, `_getByInputSource`, `_addInputSource`, and `_removeInputSource`. The `_onInputSourcesChange` method is called when the `inputsourceschange` event is fired and adds or removes input sources from the `_inputSources` array. The `_getByInputSource` method searches for an input source that matches the given WebXR input source. The `_addInputSource` method adds an input source to the `_inputSources` array, and the `_removeInputSource` method removes an input source from the `_inputSources` array.\n\nThe `XrInput` class also has several events that can be listened to, including `add`, `remove`, `select`, `selectstart`, `selectend`, `squeeze`, `squeezestart`, and `squeezeend`. These events are fired when input sources are added or removed, or when input events occur.\n\nOverall, the `XrInput` class is an important part of the PlayCanvas engine project and provides a way to handle input events for WebXR. Developers can use this class to create interactive WebXR experiences that respond to user input. For example, they can listen to the `select` event to detect when a user has selected an object in a scene and respond accordingly.\n## Questions: \n 1. What is the purpose of this code?\n- This code provides access to input sources for WebXR and handles events related to input sources.\n\n2. What events can be fired by this code?\n- This code can fire events such as 'add', 'remove', 'select', 'selectstart', 'selectend', 'squeeze', 'squeezestart', and 'squeezeend' related to input sources.\n\n3. What is the relationship between this code and the PlayCanvas engine?\n- This code is a part of the PlayCanvas engine and provides functionality related to input sources for WebXR.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-input.md"}}],["406",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-joint.js)\n\nThe code defines a class called `XrJoint` that represents a joint of a finger in a WebXR hand input system. The class has several private and public properties and methods that allow for getting the position and rotation of a joint in world space, as well as other information about the joint.\n\nThe `XrJoint` class constructor takes four parameters: `index`, `id`, `hand`, and `finger`. `index` is the index of the joint within a finger, `id` is the ID of the joint based on WebXR Hand Input Specs, `hand` is the hand that the joint relates to, and `finger` is the finger that the joint is related to. `finger` can be null in the case of the wrist joint.\n\nThe `XrJoint` class has several private properties, including `_index`, `_id`, `_hand`, `_finger`, `_wrist`, `_tip`, `_radius`, `_localTransform`, `_worldTransform`, `_localPosition`, `_localRotation`, `_position`, `_rotation`, and `_dirtyLocal`. `_index` is the index of the joint within a finger, `_id` is the ID of the joint based on WebXR Hand Input Specs, `_hand` is the hand that the joint relates to, `_finger` is the finger that the joint is related to, `_wrist` is a boolean that is true if the joint is a wrist, `_tip` is a boolean that is true if the joint is a tip of a finger, `_radius` is the radius of the joint, `_localTransform` is a `Mat4` object that represents the local transform of the joint, `_worldTransform` is a `Mat4` object that represents the world transform of the joint, `_localPosition` is a `Vec3` object that represents the local position of the joint, `_localRotation` is a `Quat` object that represents the local rotation of the joint, `_position` is a `Vec3` object that represents the world space position of the joint, `_rotation` is a `Quat` object that represents the world space rotation of the joint, and `_dirtyLocal` is a boolean that is true if the local transform of the joint has changed.\n\nThe `XrJoint` class has several public methods, including `getPosition()`, `getRotation()`, `index`, `hand`, `finger`, `wrist`, `tip`, and `radius`. `getPosition()` returns the world space position of the joint as a `Vec3` object. `getRotation()` returns the world space rotation of the joint as a `Quat` object. `index` returns the index of the joint within a finger. `hand` returns the hand that the joint relates to. `finger` returns the finger that the joint is related to. `wrist` returns true if the joint is a wrist. `tip` returns true if the joint is a tip of a finger. `radius` returns the radius of the joint, which is a distance from the joint to the edge of the skin.\n\nThe code also defines a constant called `tipJointIds` that is an array of IDs of the tip joints of the fingers. The IDs are based on WebXR Hand Input Specs. The constant is only defined if the code is running in a browser and the `window.XRHand` object is available. The code also defines an object called `tipJointIdsIndex` that is used to quickly check if a joint ID is a tip joint ID.\n\nOverall, the `XrJoint` class provides a way to represent and manipulate joints of a finger in a WebXR hand input system. It allows for getting the position and rotation of a joint in world space, as well as other information about the joint. The `XrJoint` class is likely used in conjunction with other classes in the PlayCanvas engine to provide a complete WebXR hand input system.\n## Questions: \n 1. What is the purpose of the `tipJointIds` array and how is it used in the code?\n- The `tipJointIds` array is used to store the IDs of the tip joints of each finger in a hand. It is used to determine whether a joint is a tip joint or not in the `XrJoint` class constructor.\n\n2. What is the `_dirtyLocal` property used for in the `XrJoint` class?\n- The `_dirtyLocal` property is used to determine whether the local transform of a joint needs to be updated or not. It is set to `true` whenever the joint is updated, and set to `false` whenever the local transform is computed.\n\n3. What is the purpose of the `getPosition` and `getRotation` methods in the `XrJoint` class?\n- The `getPosition` method returns the world space position of a joint, while the `getRotation` method returns the world space rotation of a joint. These methods are used to get the position and rotation of a joint in the world space, which can be useful for various applications such as rendering or physics simulations.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-joint.md"}}],["407",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-light-estimation.js)\n\nThe code defines a class called `XrLightEstimation` that provides illumination data from the real world, which is estimated by the underlying AR system. It provides a reflection Cube Map, that represents the reflection estimation from the viewer position. A more simplified approximation of light is provided by L2 Spherical Harmonics data. And the most simple level of light estimation is the most prominent directional light, its rotation, intensity, and color.\n\nThe class extends the `EventHandler` class and has several private and public properties. The private properties include `_manager`, `_supported`, `_available`, `_lightProbeRequested`, `_lightProbe`, `_intensity`, `_rotation`, `_color`, and `_sphericalHarmonics`. The public properties include `supported`, `available`, `intensity`, `color`, `rotation`, and `sphericalHarmonics`.\n\nThe class has several methods including `_onSessionStart`, `_onSessionEnd`, `start`, `end`, and `update`. The `_onSessionStart` method is called when the XR session starts and checks if light estimation is supported. The `_onSessionEnd` method is called when the XR session ends and resets the `_supported`, `_available`, `_lightProbeRequested`, and `_lightProbe` properties.\n\nThe `start` method starts the estimation of illumination data. If it fails to start estimation, an `error` event is fired. The `end` method ends the estimation of illumination data. The `update` method updates the `_intensity`, `_color`, `_rotation`, and `_sphericalHarmonics` properties based on the light estimate from the AR system.\n\nThe `supported` property returns true if Light Estimation is supported. The `available` property returns true if estimated light information is available. The `intensity` property returns the intensity of what is estimated to be the most prominent directional light. The `color` property returns the color of what is estimated to be the most prominent directional light. The `rotation` property returns the rotation of what is estimated to be the most prominent directional light. The `sphericalHarmonics` property returns the spherical harmonics coefficients of what is estimated to be the most prominent directional light.\n\nThis class is used in the PlayCanvas engine to provide illumination data from the real world in AR applications. It can be used to adjust the lighting in the scene to match the real-world lighting conditions.\n## Questions: \n 1. What is the purpose of the `XrLightEstimation` class?\n- The `XrLightEstimation` class provides illumination data from the real world, which is estimated by the underlying AR system. It provides a reflection Cube Map, that represents the reflection estimation from the viewer position. A more simplified approximation of light is provided by L2 Spherical Harmonics data. And the most simple level of light estimation is the most prominent directional light, its rotation, intensity and color.\n\n2. What events can be fired by the `XrLightEstimation` class?\n- The `XrLightEstimation` class can fire two events: `available` when light estimation data becomes available, and `error` when light estimation has failed to start.\n\n3. What are the properties that can be accessed from the `XrLightEstimation` class?\n- The `XrLightEstimation` class has several properties that can be accessed, including `supported` which is true if Light Estimation is supported, `available` which is true if estimated light information is available, `intensity` which is the intensity of what is estimated to be the most prominent directional light, `color` which is the color of what is estimated to be the most prominent directional light, and `rotation` which is the rotation of what is estimated to be the most prominent directional light.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-light-estimation.md"}}],["408",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-plane-detection.js)\n\nThe code defines a class called `XrPlaneDetection` that provides the ability to detect real-world surfaces based on estimations of the underlying AR system. The class extends the `EventHandler` class and has several properties and methods that allow developers to interact with the plane detection system.\n\nThe `XrPlaneDetection` class has a constructor that takes a `manager` parameter, which is an instance of the `XrManager` class. The constructor sets up event listeners for the `end` event on the `manager` object. When the `end` event is fired, the `_onSessionEnd` method is called, which destroys all the detected planes and clears the `_planesIndex` map.\n\nThe `XrPlaneDetection` class has several events that can be listened to, including `available`, `unavailable`, `add`, and `remove`. The `available` event is fired when plane detection becomes available, and the `unavailable` event is fired when plane detection becomes unavailable. The `add` event is fired when a new plane is detected, and the `remove` event is fired when a plane is removed.\n\nThe `XrPlaneDetection` class has a `supported` property that returns `true` if plane detection is supported and `false` otherwise. It also has an `available` property that returns `true` if plane detection is available and `false` otherwise. The `planes` property returns an array of `XrPlane` instances that contain individual plane information, or `null` if plane detection is not available.\n\nThe `XrPlaneDetection` class has an `update` method that takes an `XRFrame` object as a parameter. The method updates the detected planes by iterating through the indexed planes and removing any planes that are no longer detected. It then iterates through the detected planes and creates new `XrPlane` instances for any planes that are not already indexed. If a plane is already indexed, it updates the existing `XrPlane` instance.\n\nThe `XrPlaneDetection` class can be used in conjunction with the `XrManager` class to start a session with plane detection enabled. For example:\n\n```javascript\napp.xr.start(camera, pc.XRTYPE_VR, pc.XRSPACE_LOCALFLOOR, {\n    planeDetection: true\n});\n```\n\nDevelopers can also listen to the `add` event to be notified when a new plane is detected. For example:\n\n```javascript\napp.xr.planeDetection.on('add', function (plane) {\n    // new plane is added\n});\n```\n\nOverall, the `XrPlaneDetection` class provides a way for developers to detect real-world surfaces in an AR environment and interact with the detected planes.\n## Questions: \n 1. What is the purpose of this code?\n- This code provides the ability to detect real world surfaces based on estimations of the underlying AR system using Plane Detection.\n\n2. What events can be fired by XrPlaneDetection?\n- XrPlaneDetection can fire the 'available', 'unavailable', 'add', and 'remove' events.\n\n3. What is the structure of the XrPlaneDetection class?\n- The XrPlaneDetection class extends the EventHandler class and has private properties for the manager, supported, available, planesIndex, and planes. It also has methods for updating the planes, checking if plane detection is supported and available, and getting the planes.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-plane-detection.md"}}],["409",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-plane.js)\n\nThe code defines a class called `XrPlane` that represents a detected plane in a WebXR scene. The class extends the `EventHandler` class, which allows it to emit events when certain actions occur. The `XrPlane` class has several private properties, including an ID, a reference to the plane detection system, a reference to the instantiated XRPlane object, the time the plane was last changed, the plane's orientation, and the plane's position and rotation represented by `Vec3` and `Quat` objects, respectively.\n\nThe constructor for the `XrPlane` class takes two arguments: a reference to the plane detection system and an XRPlane object. The constructor sets the private properties of the class based on the values of the arguments passed in. The `update` method is called every frame and updates the position and rotation of the plane based on the current frame's pose. If the plane has changed since the last frame, the `change` event is fired.\n\nThe `XrPlane` class has several public methods and properties. The `getPosition` and `getRotation` methods return the world space position and rotation of the plane, respectively. The `id` property returns the unique identifier of the plane. The `orientation` property returns the orientation of the plane (horizontal or vertical) or null if the orientation is anything else. The `points` property returns an array of `DOMPointReadOnly` objects that define the local points of the plane's polygon.\n\nThe `XrPlane` class is used in the larger PlayCanvas engine project to represent detected planes in a WebXR scene. Developers can use the `XrPlane` class to get the position and rotation of a detected plane and to draw lines between the points of the plane's polygon. The `XrPlane` class emits events when the plane is removed or changed, allowing developers to respond to these events in their code.\n## Questions: \n 1. What is the purpose of the `XrPlane` class and what does it provide?\n- The `XrPlane` class is a detected plane instance that provides position, rotation, and polygon points. It is subject to change during its lifetime.\n\n2. What events can be fired by an `XrPlane` instance and how are they used?\n- An `XrPlane` instance can fire a `remove` event when it is removed and a `change` event when its attributes such as orientation and/or points have been changed. These events can be used to perform actions when the plane is removed or changed.\n\n3. How can the position, rotation, and polygon points of an `XrPlane` instance be accessed?\n- The position and rotation can be accessed using the `getPosition()` and `getRotation()` methods respectively, which return `Vec3` and `Quat` objects. The polygon points can be accessed using the `points` property, which returns an array of `DOMPointReadOnly` objects.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-plane.md"}}],["410",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/framework/xr/xr-tracked-image.js)\n\nThe code defines a class called `XrTrackedImage` which is used to represent a tracked image in an AR (Augmented Reality) system. The class extends the `EventHandler` class which allows it to handle events. The class has several private and public properties and methods that are used to manage the state of the tracked image.\n\nThe `XrTrackedImage` class has a constructor that takes two parameters: an image and a width. The image parameter is an HTML element that represents the image being tracked. The width parameter is the width of the image in meters. The constructor sets the `_image` and `_width` properties of the class to the values passed in as parameters.\n\nThe class has several private properties that are used to manage the state of the tracked image. These properties include `_bitmap`, `_measuredWidth`, `_trackable`, `_tracking`, `_emulated`, `_pose`, `_position`, and `_rotation`. These properties are used to store information about the tracking state of the image, its position and rotation in the real world, and other related information.\n\nThe class has several public properties and methods that can be used to interact with the tracked image. These include the `image`, `width`, `trackable`, `tracking`, and `emulated` properties, and the `getPosition()` and `getRotation()` methods. The `image` property returns the image being tracked, while the `width` property gets or sets the width of the image in meters. The `trackable` property returns a boolean value indicating whether the image is trackable or not. The `tracking` property returns a boolean value indicating whether the image is currently being tracked or not. The `emulated` property returns a boolean value indicating whether the image was recently tracked but is not currently being actively tracked.\n\nThe `getPosition()` and `getRotation()` methods return the position and rotation of the tracked image respectively. These methods can be used to update the position and rotation of an entity in the AR system to match the position and rotation of the tracked image.\n\nOverall, the `XrTrackedImage` class is an important part of the PlayCanvas engine's AR system. It provides a way to represent and manage tracked images in an AR system, and allows developers to interact with these images in a meaningful way.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `XrTrackedImage` which represents a tracked image in an AR session. It contains information about the tracking state, position, and rotation of the image.\n\n2. What parameters are required to create an instance of `XrTrackedImage`?\n- An instance of `XrTrackedImage` requires an image that matches the real world image as closely as possible and a width (in meters) of the image in the real world.\n\n3. What methods are available to get the position and rotation of a tracked image?\n- The `getPosition()` and `getRotation()` methods can be used to get the position and rotation of a tracked image, respectively.","metadata":{"source":".autodoc/docs/markdown/src/framework/xr/xr-tracked-image.md"}}],["411",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/audio/capabilities.js)\n\nThe code above is a function called `hasAudioContext` that checks whether the device supports the Web Audio API. The Web Audio API is a powerful tool for creating and manipulating audio in web applications. This function is important because it allows the PlayCanvas engine to determine whether it can use the Web Audio API to create and manipulate audio in the application.\n\nThe function returns a boolean value of `true` if the Web Audio API is supported and `false` otherwise. The `!!` operator is used to convert the result of the `typeof` checks to a boolean value. If either `AudioContext` or `webkitAudioContext` is defined, then the function returns `true`.\n\nThis function is marked with the `@ignore` JSDoc tag, which means that it is not intended to be used directly by developers using the PlayCanvas engine. Instead, it is likely used internally by other functions or modules within the engine.\n\nHere is an example of how this function might be used within the PlayCanvas engine:\n\n```javascript\nif (hasAudioContext()) {\n  // Use the Web Audio API to create and manipulate audio\n} else {\n  // Use a fallback method for creating and manipulating audio\n}\n```\n\nIn this example, the `hasAudioContext` function is used to determine whether the Web Audio API can be used to create and manipulate audio. If it returns `true`, then the Web Audio API is used. If it returns `false`, then a fallback method is used instead.\n\nOverall, the `hasAudioContext` function is a small but important part of the PlayCanvas engine's audio system. It allows the engine to determine whether it can use the Web Audio API to create and manipulate audio, which is a key feature of the engine.\n## Questions: \n 1. What is the purpose of this function?\n   - This function checks whether the device supports the Web Audio API and returns a boolean value indicating its support.\n\n2. What are the possible return values of this function?\n   - The function can return either true or false, depending on whether the device supports the Web Audio API.\n\n3. Why is the \"@ignore\" tag used in the JSDoc comment?\n   - The \"@ignore\" tag is used to indicate that this function should not be included in the generated documentation, as it is an internal helper function and not intended for external use.","metadata":{"source":".autodoc/docs/markdown/src/platform/audio/capabilities.md"}}],["412",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/audio/channel.js)\n\nThe code defines a class called `Channel` that represents a channel for playing audio. The `Channel` class is used by the `SoundManager` to play sounds. The `Channel` class is responsible for creating and managing an audio source, setting the volume, pitch, and loop properties of the audio, and starting, pausing, and stopping playback of the audio.\n\nThe `Channel` class has a constructor that takes a `SoundManager` instance, a `Sound` instance, and an optional options object. The options object can contain the volume, pitch, and loop properties of the audio. If the `SoundManager` has an audio context, the `Channel` class creates a gain node and connects it to the audio context. If the `SoundManager` does not have an audio context, the `Channel` class creates an audio source and connects it to the `SoundManager`'s audio element.\n\nThe `Channel` class has methods for getting and setting the volume, pitch, and loop properties of the audio. The `Channel` class also has methods for starting, pausing, and stopping playback of the audio. The `Channel` class has methods for handling the `SoundManager`'s `volumechange`, `suspend`, and `resume` events.\n\nThe `Channel` class has a private method called `_createSource` that creates an audio source and connects it to the gain node or the `SoundManager`'s audio element. The `_createSource` method also sets the `onended` property of the audio source to pause the audio when it ends if the audio is not set to loop.\n\nOverall, the `Channel` class provides a way to play audio with the `SoundManager` and control the volume, pitch, and loop properties of the audio. The `Channel` class is used internally by the `SoundManager` and is not intended to be used directly by developers.\n## Questions: \n 1. What is the purpose of the `Channel` class?\n   \n   The `Channel` class is responsible for creating and managing audio channels for playback of sounds. It is usually created internally by the `SoundManager` and developers usually won't have to create Channels manually.\n\n2. What options can be passed to the `Channel` constructor?\n   \n   The `Channel` constructor can be passed an optional `options` object which can contain the following properties: `volume` (number between 0 and 1), `pitch` (relative pitch, default of 1), and `loop` (boolean indicating whether the sound should loop when it reaches the end or not).\n\n3. What is the purpose of the `hasAudioContext` function?\n   \n   The `hasAudioContext` function is used to check whether the browser supports the Web Audio API. It is used to determine whether to create a `gain` node for controlling the volume of the audio or to use the `volume` property of the audio element.","metadata":{"source":".autodoc/docs/markdown/src/platform/audio/channel.md"}}],["413",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/audio/channel3d.js)\n\nThe code defines a class called `Channel3d` which extends another class called `Channel`. The purpose of this class is to handle 3D audio playback. It is used in the larger PlayCanvas engine project to provide spatial audio for games and other interactive applications.\n\nThe `Channel3d` class has several methods for setting and getting properties related to the audio playback. For example, `setPosition` sets the position of the audio source in 3D space, `setVelocity` sets the velocity of the audio source, and `setMaxDistance` sets the maximum distance at which the audio can be heard. These methods are used to control the spatial properties of the audio playback.\n\nThe `Channel3d` class also has a method called `_createSource` which creates a new buffer source and connects it to the correct audio nodes. This method is called internally when a new `Channel3d` instance is created.\n\nThe code also includes a fall off function which is used to calculate the volume of the audio based on its distance from the listener. This function is used when the Web Audio API is not available, and is based on the same function used by the Web Audio API. The function takes several parameters including the position of the listener and the position of the audio source, and returns a value between 0 and 1 which is used to adjust the volume of the audio.\n\nOverall, the `Channel3d` class provides a way to play 3D audio in a PlayCanvas project. It can be used to create immersive audio experiences for games and other interactive applications.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `Channel3d` which is a subclass of `Channel`. It is used for 3D audio playback and handles the positioning and velocity of the audio source.\n\n2. What is the difference between `hasAudioContext()` and `DISTANCE_EXPONENTIAL`, `DISTANCE_INVERSE`, `DISTANCE_LINEAR`?\n- `hasAudioContext()` is a function that checks if the Web Audio API is available in the current browser. `DISTANCE_EXPONENTIAL`, `DISTANCE_INVERSE`, and `DISTANCE_LINEAR` are constants used to specify the type of distance model used for audio playback.\n\n3. What is the purpose of the `fallOff` function?\n- The `fallOff` function is used to calculate the volume of the audio source based on its distance from the listener. It takes in the positions of the listener and the audio source, as well as other parameters such as the minimum and maximum distances and the rolloff factor, and returns a value between 0 and 1 representing the volume of the audio source.","metadata":{"source":".autodoc/docs/markdown/src/platform/audio/channel3d.md"}}],["414",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/audio/constants.js)\n\nThis code defines three constants that represent different distance models: linear, inverse, and exponential. These distance models are used in various parts of the PlayCanvas engine to calculate the distance between two points in 3D space. \n\nThe linear distance model calculates the distance between two points as the absolute difference between their coordinates. This is the simplest distance model and assumes that the distance between two points is a straight line.\n\nThe inverse distance model calculates the distance between two points as the reciprocal of the distance between them. This means that the closer two points are, the larger the distance value will be. This model is useful when you want to give more weight to objects that are closer to the camera.\n\nThe exponential distance model calculates the distance between two points as a function of their distance raised to a power. This model is useful when you want to give more weight to objects that are very close or very far away from the camera.\n\nDevelopers can use these constants in their code to specify which distance model they want to use in a particular calculation. For example, if a developer wants to calculate the distance between two points using the linear distance model, they can use the `DISTANCE_LINEAR` constant in their code:\n\n```\nconst distance = calculateDistance(pointA, pointB, DISTANCE_LINEAR);\n```\n\nOverall, this code provides a simple and flexible way for developers to specify different distance models in their code, which can be used in various parts of the PlayCanvas engine.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code defines three constants that represent different distance models: linear, inverse, and exponential.\n\n2. **How are these constants used in the PlayCanvas engine?**\\\nWithout further context, it is unclear how these constants are used in the PlayCanvas engine. It is possible that they are used as options for calculating distances between objects or entities.\n\n3. **Are there any other distance models available in the PlayCanvas engine?**\\\nBased on this code alone, it is unclear if there are any other distance models available in the PlayCanvas engine. It is possible that these three models are the only ones available, or there may be additional models defined elsewhere in the codebase.","metadata":{"source":".autodoc/docs/markdown/src/platform/audio/constants.md"}}],["415",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/bind-group-format.js)\n\nThe code defines three classes: `BindBufferFormat`, `BindTextureFormat`, and `BindGroupFormat`. These classes are used to define the format of a bind group, which is a collection of resources (buffers and textures) that can be bound to a shader. \n\n`BindBufferFormat` is a simple class that defines the name and visibility of a buffer. `BindTextureFormat` is a more complex class that defines the name, visibility, texture dimension, and sample type of a texture. The `textureDimensionInfo` object maps texture dimension constants to their string representations. `BindGroupFormat` is the main class that defines the format of a bind group. It takes an instance of `GraphicsDevice`, an array of `BindBufferFormat` objects, and an array of `BindTextureFormat` objects as arguments. It creates a map that maps buffer format names to indices, and another map that maps texture format names to slot indices. It also creates a scope ID for each texture format. \n\nThe `getTexture` method of `BindGroupFormat` returns the `BindTextureFormat` object for a given texture name. The `getShaderDeclarationTextures` method returns a string that contains the shader declarations for all the textures in the bind group. The `loseContext` method is not implemented.\n\nThis code is used in the PlayCanvas engine to define the format of a bind group. A bind group is created by passing an instance of `BindGroupFormat` to the `createBindGroup` method of `GraphicsDevice`. The resulting bind group can then be bound to a shader using the `setBindGroup` method of `CommandEncoder`. \n\nExample usage:\n\n```\nimport { GraphicsDevice } from './graphics-device.js';\nimport { BindGroupFormat } from './bind-group-format.js';\n\nconst device = new GraphicsDevice();\nconst bufferFormat = new BindBufferFormat('buffer', 'SHADERSTAGE_VERTEX');\nconst textureFormat = new BindTextureFormat('texture', 'SHADERSTAGE_FRAGMENT');\nconst bindGroupFormat = new BindGroupFormat(device, [bufferFormat], [textureFormat]);\nconst bindGroup = device.createBindGroup(bindGroupFormat);\n```\n## Questions: \n 1. What is the purpose of the `BindGroupFormat` class?\n- The `BindGroupFormat` class is used to define the format of a bind group, which is a collection of resources (buffers and textures) that can be bound to a shader program.\n\n2. What is the `getTexture` method used for?\n- The `getTexture` method is used to retrieve the format of a texture with a specified name from the `BindGroupFormat` instance.\n\n3. What is the purpose of the `getShaderDeclarationTextures` method?\n- The `getShaderDeclarationTextures` method is used to generate shader code that declares the textures in the `BindGroupFormat` instance, which can be used to bind the textures to a shader program.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/bind-group-format.md"}}],["416",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/bind-group.js)\n\nThe code defines a class called `BindGroup` which represents a collection of `UniformBuffer` and `Texture` instances that can be bound to a GPU for rendering. This class is not intended to be used directly by developers, hence the `@ignore` tag in the JSDoc comment. \n\nThe constructor of the `BindGroup` class takes three parameters: `graphicsDevice`, `format`, and `defaultUniformBuffer`. `graphicsDevice` is an instance of the `GraphicsDevice` class which is used to manage the uniform buffer. `format` is an instance of the `BindGroupFormat` class which specifies the format of the bind group. `defaultUniformBuffer` is an optional parameter that represents the default uniform buffer for the bind group. \n\nThe `BindGroup` class has several methods. The `setUniformBuffer` method assigns a uniform buffer to a slot with a given name. The `setTexture` method assigns a texture to a named slot. The `update` method applies any changes made to the bind group's properties. The `destroy` method frees resources associated with the bind group. \n\nThe `BindGroup` class is used in the larger PlayCanvas engine project to manage the binding of uniform buffers and textures to the GPU for rendering. Developers can use the `UniformBuffer` and `Texture` classes to create instances of these objects and then use the `BindGroup` class to bind them to the GPU. \n\nHere is an example of how the `BindGroup` class might be used in the PlayCanvas engine project:\n\n```javascript\nconst graphicsDevice = new GraphicsDevice();\nconst uniformBuffer = new UniformBuffer();\nconst texture = new Texture();\nconst bindGroupFormat = new BindGroupFormat();\nconst bindGroup = new BindGroup(graphicsDevice, bindGroupFormat, uniformBuffer);\n\nbindGroup.setTexture('diffuseMap', texture);\nbindGroup.setUniformBuffer('modelMatrix', uniformBuffer);\nbindGroup.update();\n```\n## Questions: \n 1. What is the purpose of the `BindGroup` class?\n    \n    The `BindGroup` class represents a collection of uniform buffers and textures that can be bound to the GPU for rendering.\n\n2. What is the significance of the `dirty` property?\n    \n    The `dirty` property indicates whether any changes have been made to the bind group's properties since the last time it was updated. If it is `true`, the changes will be applied when `update()` is called.\n\n3. What is the purpose of the `Debug` class and its methods used in this code?\n    \n    The `Debug` class is used for logging and debugging purposes. The `trace()` and `assert()` methods are used to log information about the allocation of a new bind group and to check that a uniform buffer or texture is being set to a valid slot, respectively.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/bind-group.md"}}],["417",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/blend-state.js)\n\nThe code defines a class called `BlendState` that represents a descriptor for how the output of a fragment shader is blended and written into a render target. The class provides methods for setting and getting various blend state parameters such as blending factors, blending operations, and color write masks. \n\nThe `BlendState` class has a constructor that takes several optional parameters for configuring the blend state. These parameters include `blend`, `colorOp`, `colorSrcFactor`, `colorDstFactor`, `alphaOp`, `alphaSrcFactor`, `alphaDstFactor`, `redWrite`, `greenWrite`, `blueWrite`, and `alphaWrite`. The `blend` parameter enables or disables blending, while the `colorOp`, `colorSrcFactor`, and `colorDstFactor` parameters configure the color blending operation, source color blending factor, and destination color blending factor, respectively. Similarly, the `alphaOp`, `alphaSrcFactor`, and `alphaDstFactor` parameters configure the alpha blending operation, source alpha blending factor, and destination alpha blending factor, respectively. The `redWrite`, `greenWrite`, `blueWrite`, and `alphaWrite` parameters control whether the corresponding color channels are written to the render target.\n\nThe `BlendState` class also provides methods for setting and getting the blend state parameters. These methods include `setColorBlend`, `setAlphaBlend`, `setColorWrite`, `setRedWrite`, `setGreenWrite`, `setBlueWrite`, and `setAlphaWrite`. The `setColorBlend` and `setAlphaBlend` methods set the color and alpha blending parameters, respectively, while the `setColorWrite`, `setRedWrite`, `setGreenWrite`, `setBlueWrite`, and `setAlphaWrite` methods set the color write masks. The class also provides getter methods for retrieving the blend state parameters.\n\nThe `BlendState` class uses bit packing to store the blend state parameters in a single integer value. The `target0` property of the class stores the packed blend state parameters. The class uses the `BitPacking` class to pack and unpack the blend state parameters. The `BitPacking` class provides methods for setting and getting individual bits and ranges of bits in an integer value.\n\nThe `BlendState` class provides a `copy` method for copying the contents of one blend state to another, and a `clone` method for creating a new blend state that is a copy of an existing blend state. The class also provides a `key` property that returns the packed blend state parameters as a single integer value, and an `equals` method for comparing two blend states for equality.\n\nThe `BlendState` class defines two static properties: `DEFAULT` and `NOWRITE`. The `DEFAULT` property is a default blend state that has blending disabled and writes to all color channels. The `NOWRITE` property is a blend state that does not write to any color channels.\n\nOverall, the `BlendState` class provides a convenient way to configure and manage blend states for use in materials and graphics devices. By using bit packing, the class is able to store all the blend state parameters in a single integer value, which makes it efficient to pass blend states between different parts of the engine.\n## Questions: \n 1. What is the purpose of the `BitPacking` import and how is it used in this code?\n   \n   `BitPacking` is used to pack and unpack bitfields in a number. It is used in this code to store multiple blend state parameters in a single number for efficient storage and retrieval.\n\n2. What are the default values for the `BlendState` constructor parameters?\n   \n   The default values for the `BlendState` constructor parameters are as follows:\n   - `blend`: `false`\n   - `colorOp`: `BLENDEQUATION_ADD`\n   - `colorSrcFactor`: `BLENDMODE_ONE`\n   - `colorDstFactor`: `BLENDMODE_ZERO`\n   - `alphaOp`: same as `colorOp`\n   - `alphaSrcFactor`: same as `colorSrcFactor`\n   - `alphaDstFactor`: same as `colorDstFactor`\n   - `redWrite`: `true`\n   - `greenWrite`: `true`\n   - `blueWrite`: `true`\n   - `alphaWrite`: `true`\n\n3. What is the purpose of the `allWrite` getter in the `BlendState` class?\n   \n   The `allWrite` getter returns a number with all 4 bits representing the color write mask, for fast comparison with other blend states.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/blend-state.md"}}],["418",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/debug-graphics.js)\n\nThe `DebugGraphics` class is an internal graphics debug system used for GPU markers and similar debugging functionality. This class is only executed in the debug build and is stripped out in other builds. \n\nThe purpose of this class is to provide a way to track and debug GPU markers during rendering. It contains four methods: `clearGpuMarkers()`, `pushGpuMarker(device, name)`, `popGpuMarker(device)`, and `toString()`. \n\nThe `clearGpuMarkers()` method clears the internal stack of GPU markers. It should be called at the start of each frame to prevent the array from growing if there are exceptions during rendering. \n\nThe `pushGpuMarker(device, name)` method pushes a GPU marker to the stack on the device. It takes in a `GraphicsDevice` object and a `name` string as parameters. The `name` parameter represents the name of the marker being pushed to the stack. \n\nThe `popGpuMarker(device)` method pops a GPU marker from the stack on the device. It takes in a `GraphicsDevice` object as a parameter. If the `markers` array is not empty, the last element is removed from the array. Then, the `popMarker()` method is called on the `device` object. \n\nThe `toString()` method converts the current markers into a single string format. It returns a string representation of the current markers, with each marker separated by a `|` character. \n\nThis class can be used in the larger project to debug GPU markers during rendering. For example, a developer could use the `pushGpuMarker()` method to push a marker to the stack before rendering a specific object. Then, they could use the `popGpuMarker()` method to pop the marker from the stack after rendering the object. If there are any issues during rendering, the `toString()` method could be used to get a string representation of the current markers and help identify the issue. \n\nOverall, the `DebugGraphics` class provides a useful debugging tool for tracking GPU markers during rendering in the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of this code and when does it execute?\n- This code is an internal graphics debug system for GPU markers and similar, and it only executes in the debug build.\n\n2. What is the significance of the `markers` array and when is it cleared?\n- The `markers` array represents a stack of GPU markers, and it is cleared at the start of each frame to prevent the array from growing if there are exceptions during rendering.\n\n3. What is the purpose of the `toString` method and what does it return?\n- The `toString` method converts the current markers into a single string format, and it returns a string representation of the current markers joined by a pipe character.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/debug-graphics.md"}}],["419",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/depth-state.js)\n\nThe code defines a class called `DepthState` that represents the depth state of a fragment in the rendering pipeline. The depth state is used to determine whether a fragment should be drawn based on its depth value compared to the depth value in the depth buffer. The class has two properties: `func` and `write`. \n\nThe `func` property controls how the depth of the fragment is compared against the current depth in the depth buffer. It can take one of the following values: `FUNC_NEVER`, `FUNC_LESS`, `FUNC_EQUAL`, `FUNC_LESSEQUAL`, `FUNC_GREATER`, `FUNC_NOTEQUAL`, `FUNC_GREATEREQUAL`, and `FUNC_ALWAYS`. The `write` property controls whether the depth value of the fragment should be written to the depth buffer. \n\nThe class has a constructor that takes two optional parameters: `func` and `write`. If these parameters are not provided, the default values are `FUNC_LESSEQUAL` and `true`, respectively. The class also has a `test` property that is a setter/getter for the `func` property. If `test` is set to `true`, the `func` property is set to `FUNC_LESSEQUAL`, otherwise it is set to `FUNC_ALWAYS`. \n\nThe class has a `copy` method that takes another `DepthState` object and copies its data to the current object. It also has a `clone` method that returns a new `DepthState` object that is a copy of the current object. The class has a `key` property that returns the bitfield representing the depth state. The class also has two static properties: `DEFAULT`, `NODEPTH`, and `WRITEDEPTH`. `DEFAULT` is a default depth state that has the depth testing function set to `FUNC_LESSEQUAL` and depth writes enabled. `NODEPTH` is a depth state that always passes the fragment but does not write depth to the depth buffer. `WRITEDEPTH` is a depth state that always passes the fragment and writes depth to the depth buffer.\n\nThe purpose of this class is to provide a way to define the depth state of a fragment in the rendering pipeline. It can be used to create multiple depth states and assign them to materials or graphics devices as needed. By default, the depth state is set to `FUNC_LESSEQUAL` and depth writes are enabled. However, the depth state can be modified by setting the `func` and `write` properties or the `test` property. The `copy` and `clone` methods can be used to copy depth states between objects. The static properties provide some commonly used depth states that can be used as defaults.\n## Questions: \n 1. What is the purpose of the `DepthState` class?\n    \n    The `DepthState` class is a descriptor that defines how the depth value of the fragment is used by the rendering pipeline. It can be set on a material using `Material#depthState`, or in some cases on the graphics device using `GraphicsDevice#setDepthState`.\n\n2. What is the `test` property used for?\n    \n    The `test` property is used to determine whether a shader fragment is only written to the current render target if it passes the depth test. If `test` is set to `false`, it is written regardless of what is in the depth buffer. Note that when depth testing is disabled, writes to the depth buffer are also disabled.\n\n3. What are the possible values for the `func` property?\n    \n    The `func` property controls how the depth of the fragment is compared against the current depth contained in the depth buffer. It can be set to one of the following values:\n    \n    - `FUNC_NEVER`: don't draw\n    - `FUNC_LESS`: draw if new depth < depth buffer\n    - `FUNC_EQUAL`: draw if new depth == depth buffer\n    - `FUNC_LESSEQUAL`: draw if new depth <= depth buffer\n    - `FUNC_GREATER`: draw if new depth > depth buffer\n    - `FUNC_NOTEQUAL`: draw if new depth != depth buffer\n    - `FUNC_GREATEREQUAL`: draw if new depth >= depth buffer\n    - `FUNC_ALWAYS`: always draw","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/depth-state.md"}}],["420",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/device-cache.js)\n\n# DeviceCache Class\n\nThe `DeviceCache` class is a cache that stores shared resources associated with a device. The resources are removed from the cache when the device is destroyed. This class is used internally in the PlayCanvas engine project.\n\n## Properties\n\n### _cache\n\nThis is a private property that stores the resource for each `GraphicsDevice`. It is a `Map` object that maps a `GraphicsDevice` to a resource.\n\n```javascript\n_cache = new Map();\n```\n\n## Methods\n\n### get(device, onCreate)\n\nThis method returns the resources for the supplied `GraphicsDevice`. If the resource does not exist in the cache, it is created using the `onCreate` function and added to the cache. The `onCreate` function is a callback function that is called when the resource needs to be created.\n\n```javascript\nget(device, onCreate) {\n    if (!this._cache.has(device)) {\n        this._cache.set(device, onCreate());\n\n        // when the device is destroyed, destroy and remove its entry\n        device.on('destroy', () => {\n            this.remove(device);\n        });\n\n        // when the context is lost, call optional loseContext on its entry\n        device.on('devicelost', () => {\n            this._cache.get(device)?.loseContext?.(device);\n        });\n    }\n\n    return this._cache.get(device);\n}\n```\n\n### remove(device)\n\nThis method destroys and removes the content of the cache associated with the `GraphicsDevice`.\n\n```javascript\nremove(device) {\n    this._cache.get(device)?.destroy?.(device);\n    this._cache.delete(device);\n}\n```\n\n## Usage\n\nThe `DeviceCache` class is used to store shared resources associated with a `GraphicsDevice`. It is used internally in the PlayCanvas engine project to manage resources such as textures, shaders, and buffers. When a resource is needed, the `get` method is called with the `GraphicsDevice` as the argument. If the resource does not exist in the cache, it is created using the `onCreate` function and added to the cache. When the `GraphicsDevice` is destroyed, the resource is automatically removed from the cache. \n\n```javascript\nconst deviceCache = new DeviceCache();\n\nconst device = new GraphicsDevice();\n\nconst texture = deviceCache.get(device, () => {\n    // create texture\n    return new Texture(device, {\n        width: 256,\n        height: 256,\n        format: PIXELFORMAT_R8_G8_B8_A8\n    });\n});\n\n// use texture\n\ndevice.destroy();\n\n// texture is automatically removed from cache\n```\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n- This code defines a class called `DeviceCache` that stores shared resources associated with a graphics device. It is used to cache resources and remove them when the device is destroyed.\n\n2. What is the `_cache` property and how is it used?\n- The `_cache` property is a `Map` that stores the resource for each `GraphicsDevice`. It is used to retrieve and store resources associated with a device.\n\n3. What happens when a device is destroyed or its context is lost?\n- When a device is destroyed, its entry in the cache is removed. When its context is lost, the optional `loseContext` function is called on its entry.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/device-cache.md"}}],["421",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/graphics-device-access.js)\n\nThe code above defines a static class called `GraphicsDeviceAccess` that provides access to the current `GraphicsDevice`. The purpose of this class is to preserve backwards API compatibility, as it is not recommended to access the graphics device directly in normal situations. Instead, a device should be passed in to classes that need it.\n\nThe class has two static methods: `set` and `get`. The `set` method takes a `graphicsDevice` parameter and sets the `_graphicsDevice` property of the class to that value. The `_graphicsDevice` property is static, meaning it is shared across all instances of the class. This allows the graphics device to be accessed from anywhere in the codebase.\n\nThe `get` method simply returns the `_graphicsDevice` property. This allows other classes to access the graphics device without needing to create a new instance of the `GraphicsDeviceAccess` class.\n\nAn example of how this class may be used in the larger project is in the initialization of other classes that require access to the graphics device. For example, a `Mesh` class may require a graphics device to create and render the mesh. Instead of creating a new graphics device instance within the `Mesh` class, it can simply access the current graphics device through the `GraphicsDeviceAccess` class.\n\n```\nimport { GraphicsDeviceAccess } from 'playcanvas-engine';\n\nclass Mesh {\n  constructor() {\n    this.graphicsDevice = GraphicsDeviceAccess.get();\n    // use graphicsDevice to create and render mesh\n  }\n}\n```\n\nOverall, the `GraphicsDeviceAccess` class provides a convenient way to access the current graphics device without needing to pass it around as a parameter to every class that requires it.\n## Questions: \n 1. What is the purpose of the GraphicsDeviceAccess class?\n    \n    The purpose of the GraphicsDeviceAccess class is to provide access to the current GraphicsDevice, but it should not be used to access the graphics device normally, and is provided only as a way of obtaining it to preserve backwards API compatibility.\n\n2. Why is it recommended to pass in a device to classes that need it instead of using GraphicsDeviceAccess?\n    \n    It is recommended to pass in a device to classes that need it instead of using GraphicsDeviceAccess because GraphicsDeviceAccess is only provided to preserve backwards API compatibility and should not be used to access the graphics device normally.\n\n3. What is the significance of the \"ignore\" tag in the class documentation?\n    \n    The \"ignore\" tag in the class documentation indicates that the class should be ignored and not used in normal situations.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/graphics-device-access.md"}}],["422",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/graphics-device-create.js)\n\nThe code defines a function called `createGraphicsDevice` that creates a graphics device for rendering graphics on a canvas element. The function takes two parameters: a canvas element and an options object. The options object is optional and can contain the following properties:\n\n- `deviceTypes`: An array of device types to use for rendering. The default value is an empty array. If the specified array does not contain `DEVICETYPE_WEBGL2` or `DEVICETYPE_WEBGL1`, those are internally added to its end in this order. Typically, you'd only specify `DEVICETYPE_WEBGPU`, or leave it empty.\n- `antialias`: A boolean that indicates whether or not to perform anti-aliasing if possible. The default value is `true`.\n- `glslangUrl`: An url to glslang script, required if `DEVICETYPE_WEBGPU` type is added to deviceTypes array. Not used for `DEVICETYPE_WEBGL` device type creation.\n- `twgslUrl`: An url to twgsl script, required if `glslangUrl` was specified.\n\nThe function returns a Promise object representing the created graphics device. The function first sets the default value of `antialias` to `true` if it is not specified in the options object. It then checks if `deviceTypes` is specified in the options object. If not, it defaults to an empty array. If `DEVICETYPE_WEBGL2` or `DEVICETYPE_WEBGL1` is not included in the `deviceTypes` array, they are added to the end of the array in that order.\n\nThe function then iterates over the `deviceTypes` array and tries to create a graphics device for each device type. If the device type is `DEVICETYPE_WEBGPU` and the browser supports WebGPU, it creates a `WebgpuGraphicsDevice` object and initializes it using the `initWebGpu` method. If the device type is not `DEVICETYPE_WEBGPU`, it creates a `WebglGraphicsDevice` object and returns a Promise object representing the created graphics device.\n\nIf the function fails to create a graphics device, it throws an error and returns a rejected Promise object.\n\nThis function is a part of the PlayCanvas engine project and is used to create a graphics device for rendering graphics on a canvas element. It provides a flexible way to specify the device types to use for rendering and allows for fallbacks if the preferred device type is not available. The function also provides options for anti-aliasing and specifying URLs for glslang and twgsl scripts.\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code creates a graphics device for a canvas element using either WebGL or WebGPU, with options for device type, anti-aliasing, and glslangUrl.\n\n2. What are the possible values for the `deviceTypes` option?\n    \n    The `deviceTypes` option is an array of constants that can include `DEVICETYPE_WEBGL2`, `DEVICETYPE_WEBGL1`, and `DEVICETYPE_WEBGPU`.\n\n3. What happens if the requested device types are not available?\n    \n    If the requested device types are not available, the code will throw an error and reject the Promise with a message indicating that it failed to allocate a graphics device.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/graphics-device-create.md"}}],["423",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/index-buffer.js)\n\nThe code defines a class called `IndexBuffer` which represents an index buffer used to store index values into a vertex buffer. Indexed graphical primitives can utilize less memory than unindexed primitives if vertices are shared. The class is used to create and manage index buffers in the PlayCanvas engine.\n\nThe constructor of the `IndexBuffer` class takes four parameters: `graphicsDevice`, `format`, `numIndices`, `usage`, and `initialData`. `graphicsDevice` is an instance of the `GraphicsDevice` class used to manage the index buffer. `format` specifies the type of each index to be stored in the index buffer and can be one of `INDEXFORMAT_UINT8`, `INDEXFORMAT_UINT16`, or `INDEXFORMAT_UINT32`. `numIndices` specifies the number of indices to be stored in the index buffer. `usage` specifies the usage type of the vertex buffer and can be one of `BUFFER_DYNAMIC`, `BUFFER_STATIC`, or `BUFFER_STREAM`. `initialData` is optional and specifies the initial data to be stored in the index buffer. If left unspecified, the index buffer will be initialized to zeros.\n\nThe `IndexBuffer` class has several methods. `destroy()` frees resources associated with the index buffer. `getFormat()` returns the data format of the index buffer. `getNumIndices()` returns the number of indices stored in the index buffer. `lock()` gives access to the block of memory that stores the buffer's indices. `unlock()` signals that the block of memory returned by a call to the `lock()` function is ready to be given to the graphics hardware. `setData(data)` sets preallocated data on the index buffer. `writeData(data, count)` copies the specified number of elements from `data` into the index buffer. `readData(data)` copies index data from the index buffer into the provided data array.\n\nThe `IndexBuffer` class is used to create an index buffer and set it on a `Mesh` object. The following code creates an index buffer holding 3 16-bit indices and marks the buffer as static, hinting that the buffer will never be modified:\n\n```\nvar indices = new UInt16Array([0, 1, 2]);\nvar indexBuffer = new pc.IndexBuffer(graphicsDevice,\n                                      pc.INDEXFORMAT_UINT16,\n                                      3,\n                                      pc.BUFFER_STATIC,\n                                      indices);\n```\n\nOverall, the `IndexBuffer` class is an important part of the PlayCanvas engine as it allows for efficient storage and retrieval of index values into a vertex buffer.\n## Questions: \n 1. What is the purpose of the `IndexBuffer` class?\n- The `IndexBuffer` class stores index values into a `VertexBuffer` and is used for indexed graphical primitives.\n\n2. What are the different usage types of the vertex buffer?\n- The different usage types of the vertex buffer are `BUFFER_DYNAMIC`, `BUFFER_STATIC`, and `BUFFER_STREAM`.\n\n3. How does the `adjustVramSizeTracking` method work?\n- The `adjustVramSizeTracking` method updates the VRAM size tracking by adding the size of the index buffer to the current VRAM size.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/index-buffer.md"}}],["424",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/render-pass.js)\n\nThe code defines two classes, `ColorAttachmentOps` and `DepthStencilAttachmentOps`, and a third class `RenderPass` that represents a node in the frame graph and encapsulates a system that renders to a render target using an execution callback. \n\nThe `ColorAttachmentOps` class defines properties that describe how a color attachment should be handled during rendering. These properties include `clearValue`, which is a color used to clear the color attachment when the clear is enabled; `clear`, which is a boolean value that indicates whether the attachment should be cleared before rendering; `store`, which is a boolean value that indicates whether the attachment needs to be stored after the render pass; `resolve`, which is a boolean value that indicates whether the attachment needs to be resolved; and `mipmaps`, which is a boolean value that indicates whether the attachment needs to have mipmaps generated.\n\nThe `DepthStencilAttachmentOps` class defines properties that describe how a depth/stencil attachment should be handled during rendering. These properties include `clearDepthValue`, which is a depth value used to clear the depth attachment when the clear is enabled; `clearStencilValue`, which is a stencil value used to clear the stencil attachment when the clear is enabled; `clearDepth`, which is a boolean value that indicates whether the depth attachment should be cleared before rendering; `clearStencil`, which is a boolean value that indicates whether the stencil attachment should be cleared before rendering; `storeDepth`, which is a boolean value that indicates whether the depth attachment needs to be stored after the render pass; and `storeStencil`, which is a boolean value that indicates whether the stencil attachment needs to be stored after the render pass.\n\nThe `RenderPass` class represents a node in the frame graph and encapsulates a system that renders to a render target using an execution callback. It has properties such as `name`, `renderTarget`, `samples`, `colorOps`, `depthStencilOps`, `requiresCubemaps`, `fullSizeClearRect`, `execute`, `before`, and `after`. The `name` property is a string that represents the name of the render pass. The `renderTarget` property is an instance of the `RenderTarget` class that represents the render target to render into (output). The `samples` property is a number that represents the number of samples. The `colorOps` property is an instance of the `ColorAttachmentOps` class that describes how the color attachment should be handled during rendering. The `depthStencilOps` property is an instance of the `DepthStencilAttachmentOps` class that describes how the depth/stencil attachment should be handled during rendering. The `requiresCubemaps` property is a boolean value that indicates whether the pass might use dynamically rendered cubemaps. The `fullSizeClearRect` property is a boolean value that indicates whether the render pass uses the full viewport/scissor for rendering into the render target. The `execute` property is a custom function that is called to render the pass. The `before` property is a custom function that is called before the pass has started. The `after` property is a custom function that is called after the pass has finished.\n\nThe `RenderPass` class has methods such as `init`, `setClearColor`, `setClearDepth`, and `setClearStencil`. The `init` method initializes the render pass with the given render target. The `setClearColor` method marks the render pass as clearing the full color buffer. The `setClearDepth` method marks the render pass as clearing the full depth buffer. The `setClearStencil` method marks the render pass as clearing the full stencil buffer. The `render` method renders the render pass.\n\nOverall, this code provides a way to define how color and depth/stencil attachments should be handled during rendering, and defines a class that represents a node in the frame graph and encapsulates a system that renders to a render target using an execution callback. This class can be used in the larger project to define how rendering should be done for different parts of the scene.\n## Questions: \n 1. What is the purpose of the `RenderPass` class?\n- The `RenderPass` class represents a node in the frame graph and encapsulates a system which renders to a render target using an execution callback.\n\n2. What are the different properties of `ColorAttachmentOps` and `DepthStencilAttachmentOps` classes?\n- `ColorAttachmentOps` class has properties such as `clearValue`, `clear`, `store`, `resolve`, and `mipmaps` which are used to clear and store color attachments after rendering.\n- `DepthStencilAttachmentOps` class has properties such as `clearDepthValue`, `clearStencilValue`, `clearDepth`, `clearStencil`, `storeDepth`, and `storeStencil` which are used to clear and store depth and stencil attachments after rendering.\n\n3. What is the purpose of the `log` method in the `RenderPass` class?\n- The `log` method is used to log information about the render pass, such as the render target, color, depth, and stencil operations, and the number of samples. It is used for debugging purposes and is only called if the `TRACEID_RENDER_PASS` or `TRACEID_RENDER_PASS_DETAIL` tracing flags are set.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/render-pass.md"}}],["425",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/render-target.js)\n\nThe code defines a class called `RenderTarget` which represents a rectangular rendering surface. The purpose of this class is to create and manage a render target that can be used to render graphics to a texture. The class provides a constructor that takes an options object as an argument. The options object can contain the following properties:\n\n- `autoResolve`: A boolean that specifies whether to automatically resolve the render target after rendering to it. Defaults to true.\n- `colorBuffer`: A texture that the render target will treat as a rendering surface.\n- `depth`: A boolean that specifies whether to create a depth buffer. Defaults to true.\n- `depthBuffer`: A texture that the render target will treat as a depth/stencil surface. If set, the `depth` and `stencil` properties are ignored. Texture must have `PIXELFORMAT_DEPTH` or `PIXELFORMAT_DEPTHSTENCIL` format.\n- `face`: A number that specifies which face of a cubemap to render to. Can be one of the following: `CUBEFACE_POSX`, `CUBEFACE_NEGX`, `CUBEFACE_POSY`, `CUBEFACE_NEGY`, `CUBEFACE_POSZ`, `CUBEFACE_NEGZ`. Defaults to `CUBEFACE_POSX`.\n- `flipY`: A boolean that specifies whether to flip the image in Y. Default is false.\n- `name`: A string that specifies the name of the render target.\n- `samples`: A number that specifies the number of hardware anti-aliasing samples. Default is 1.\n- `stencil`: A boolean that specifies whether to include stencil in the depth buffer. Defaults to false.\n\nThe class provides methods to initialize, destroy, and copy the render target. The `init()` method initializes the resources associated with the render target. The `destroy()` method frees resources associated with the render target. The `copy()` method copies color and/or depth contents of a source render target to this one. Formats, sizes, and anti-aliasing samples must match. Depth buffer can only be copied on WebGL 2.0.\n\nThe class also provides getters for various properties of the render target, such as `samples`, `depth`, `stencil`, `colorBuffer`, `depthBuffer`, `face`, `width`, and `height`.\n\nOverall, the `RenderTarget` class is an important part of the PlayCanvas engine project as it allows developers to create and manage render targets for rendering graphics to textures.\n## Questions: \n 1. What is the purpose of the `RenderTarget` class?\n    \n    The `RenderTarget` class represents a rectangular rendering surface and is used to create a new instance of a render target with a color buffer or a depth buffer.\n\n2. What are the optional arguments that can be passed to the `RenderTarget` constructor?\n    \n    The optional arguments that can be passed to the `RenderTarget` constructor include `autoResolve`, `colorBuffer`, `depth`, `depthBuffer`, `face`, `flipY`, `name`, `samples`, and `stencil`.\n\n3. What is the purpose of the `resolve` method in the `RenderTarget` class?\n    \n    The `resolve` method is used to resolve the anti-aliased render target if samples > 1. It averages all samples and creates a simple texture with one color per pixel. The `color` and `depth` parameters can be used to specify whether to resolve the color buffer and/or depth buffer.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/render-target.md"}}],["426",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/scope-id.js)\n\nThe code defines a class called `ScopeId` which represents the scope of a variable. The purpose of this class is to provide a way to store and retrieve the value of a variable within a specific scope. \n\nThe `ScopeId` class has a constructor that takes a `name` parameter which represents the name of the variable. The `name` parameter is stored as a property of the class instance. The class also has a `value` property which is initially set to `null`. Additionally, the class has a `versionObject` property which is an instance of the `VersionedObject` class. \n\nThe `ScopeId` class has two methods: `setValue` and `getValue`. The `setValue` method takes a `value` parameter and sets the `value` property of the class instance to the provided value. The `versionObject` property is also incremented to indicate that the value has been updated. The `getValue` method returns the current value of the `value` property.\n\nThe `ScopeId` class also has a `toJSON` method which is used to prevent the `value` property from being included when the class instance is serialized to JSON. This is because the `value` property is not needed when serializing a uniform buffer format which internally stores the scope.\n\nOverall, the `ScopeId` class provides a way to store and retrieve the value of a variable within a specific scope. This class can be used in the larger PlayCanvas engine project to manage variables and their values within different scopes. For example, it could be used to store the position of an object within a specific scene or to store the health of a character within a specific level. \n\nExample usage:\n\n```\nconst scopeId = new ScopeId('position');\nscopeId.setValue({ x: 0, y: 0, z: 0 });\nconst position = scopeId.getValue(); // { x: 0, y: 0, z: 0 }\n```\n## Questions: \n 1. What is the purpose of the `VersionedObject` import?\n- The `VersionedObject` is used to create a version object for the `ScopeId` instance.\n\n2. What is the purpose of the `setValue` method?\n- The `setValue` method is used to set the value of the `ScopeId` instance and increment its revision.\n\n3. Why is the `toJSON` method implemented in this class?\n- The `toJSON` method is implemented to prevent the `value` property from being stored when stringifying a uniform buffer format, which internally stores the scope.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/scope-id.md"}}],["427",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/scope-space.js)\n\nThe code defines a class called `ScopeSpace` which represents a scope for variables. The purpose of this class is to create a new scope and manage variables within that scope. The class has a constructor that takes a `name` parameter and sets it as the name of the scope. It also creates a new `Map` object called `variables` which will be used to store the variables in the scope.\n\nThe `resolve` method is used to get or create a variable in the scope. It takes a `name` parameter which is the name of the variable. If the variable does not exist in the `variables` map, a new `ScopeId` object is created with the `name` parameter and added to the `variables` map. The `ScopeId` object is then returned. If the variable already exists in the `variables` map, the existing `ScopeId` object is returned.\n\nThe `removeValue` method is used to clear the value for any uniform with a matching value. It takes a `value` parameter which is the value to clear. This method is marked with `@ignore` which means it is not intended to be used outside of the class.\n\nThis class is likely used in the larger PlayCanvas engine project to manage variables in different scopes. For example, if there are multiple shaders in the project, each shader may have its own scope for variables. The `ScopeSpace` class can be used to create and manage these scopes. The `resolve` method can be used to get or create a variable in a specific scope, and the `removeValue` method can be used to clear the value for any uniform with a matching value. \n\nHere is an example of how the `ScopeSpace` class can be used:\n\n```\nimport { ScopeSpace } from 'playcanvas-engine';\n\n// create a new scope for variables\nconst shaderScope = new ScopeSpace('shader');\n\n// get or create a variable in the scope\nconst color = shaderScope.resolve('color');\n\n// set the value of the variable\ncolor.value = [1, 0, 0];\n\n// clear the value for any uniform with a matching value\nshaderScope.removeValue([1, 0, 0]);\n```\n## Questions: \n 1. What is the purpose of the `ScopeId` import?\n   - The `ScopeId` import is used in the `ScopeSpace` class to create a new instance of `ScopeId` for each variable in the scope.\n\n2. What is the `resolve` method used for?\n   - The `resolve` method is used to get or create a variable in the scope and returns the corresponding `ScopeId` instance.\n\n3. What is the purpose of the `removeValue` method?\n   - The `removeValue` method is used to clear the value for any uniform with a matching value, which is used to remove deleted textures.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/scope-space.md"}}],["428",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader-chunks/frag/gles2.js)\n\nThe code above is a GLSL shader code that defines several macros and fallbacks for texture sampling and shadow mapping. The purpose of this code is to provide a unified interface for texture and shadow map sampling across different graphics APIs, such as WebGL and WebGPU.\n\nThe first macro defined in the code is `texture2DBias`, which is simply an alias for the built-in `texture2D` function. This macro is used to provide a consistent name for texture sampling across different APIs.\n\nThe next set of macros are `SHADOWMAP_PASS` and `SHADOWMAP_ACCEPT`, which are used to pass and accept shadow maps as function parameters. In WebGL, shadow maps can be passed as is, but in WebGPU, they need to be wrapped in a sampler object. These macros provide a way to abstract away this difference between the two APIs.\n\nSimilarly, the `TEXTURE_PASS` and `TEXTURE_ACCEPT` macros are used to pass and accept regular textures as function parameters.\n\nThe code then defines a set of fallback macros for texture sampling instructions that are not supported by all graphics APIs. For example, the `texture2DLodEXT` macro is defined as an alias for the built-in `texture2D` function, since not all APIs support the `lod` parameter for texture sampling.\n\nFinally, the code defines a macro for shadow map sampling called `textureShadow`. This macro uses the `texture2DGradEXT` function to sample a shadow map with a given resolution and UV coordinates, and returns the result as a float value.\n\nOverall, this code provides a set of macros and fallbacks that can be used to write GLSL shaders that work across different graphics APIs. By abstracting away the differences between these APIs, developers can write more portable and maintainable shader code. Here is an example of how these macros can be used in a shader:\n\n```\nuniform SHADOWMAP_ACCEPT(shadowMap);\nuniform TEXTURE_ACCEPT(diffuseMap);\n\nvoid main() {\n    vec4 diffuseColor = texture2DBias(diffuseMap, vUv);\n    float shadow = textureShadow(shadowMap, vShadowUv);\n    gl_FragColor = diffuseColor * shadow;\n}\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines several macros related to texture and shadow map handling, including fallbacks for lod instructions.\n\n2. What is the difference between SHADOWMAP_PASS and SHADOWMAP_ACCEPT?\n- SHADOWMAP_PASS is used to pass a shadow map or texture as a function parameter, while SHADOWMAP_ACCEPT is used to accept a sampler2D as a function parameter.\n\n3. What is the purpose of the SUPPORTS_TEXLOD conditional?\n- The SUPPORTS_TEXLOD conditional is used to define fallbacks for lod instructions if the system does not support them.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader-chunks/frag/gles2.md"}}],["429",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader-chunks/frag/gles3.js)\n\nThe code above is a GLSL shader code that defines a set of macros and functions that are used to simplify the process of writing shaders for the PlayCanvas engine. The purpose of this code is to provide a set of pre-defined functions and macros that can be used to sample textures, shadows, and other data in a more efficient and optimized way.\n\nThe code defines a set of macros that are used to alias certain GLSL functions and variables. For example, the `texture2D` macro is used to alias the `texture` function, and the `gl_FragColor` variable is aliased as `pc_fragColor`. This makes it easier to write shaders that are compatible with different versions of GLSL and different graphics APIs.\n\nThe code also defines a set of macros and functions that are used to sample shadows and textures in a more efficient way. For example, the `textureShadow` function is used to sample shadows using the `textureGrad` function, which removes derivatives in dynamic loops. This is done to improve performance and reduce the time it takes to compile the shader.\n\nThe code also defines macros that are used to pass and accept shadow maps and textures as function parameters. This is done to ensure compatibility with different graphics APIs, such as WebGPU.\n\nOverall, this code is an important part of the PlayCanvas engine, as it provides a set of pre-defined functions and macros that can be used to simplify the process of writing shaders. By using these pre-defined functions and macros, developers can write more efficient and optimized shaders that are compatible with different versions of GLSL and different graphics APIs.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines various texture-related functions and macros for use in shaders, including sampling shadows and passing/accepting shadow maps and textures.\n\n2. What is the significance of the \"glsl\" tag at the beginning of the code?\n- The \"glsl\" tag indicates that this code is written in GLSL (OpenGL Shading Language), a C-like language used to write shaders for graphics processing units (GPUs).\n\n3. What is the purpose of the \"#define GL2\" line?\n- The \"#define GL2\" line defines a macro that can be used to conditionally compile code for a specific version of OpenGL (in this case, OpenGL 2.0).","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader-chunks/frag/gles3.md"}}],["430",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader-chunks/frag/shared.js)\n\nThe code provided is a GLSL shader code that contains two functions: `getGrabScreenPos` and `getImageEffectUV`. These functions are used to convert coordinates in different ways to sample textures in the PlayCanvas engine.\n\nThe `getGrabScreenPos` function takes a `vec4` parameter `clipPos`, which represents a position in clip space. The function first divides the `xy` components of `clipPos` by its `w` component, which normalizes the position to a range of [-1, 1]. It then scales and translates the normalized position to a range of [0, 1] by multiplying it with `0.5` and adding `0.5`. Finally, the function returns the resulting `uv` coordinates.\n\nThe purpose of this function is to convert a position in clip space to texture coordinates that can be used to sample scene grab textures. Scene grab textures are textures that capture the current state of the scene, which can be used for various effects like reflections and refractions. By converting the clip space position to texture coordinates, the function allows the scene grab textures to be sampled at the correct location.\n\nHere is an example usage of `getGrabScreenPos`:\n\n```glsl\nvec4 clipPos = gl_Position;\nvec2 uv = getGrabScreenPos(clipPos);\nvec4 sceneColor = texture(sceneGrabTexture, uv);\n```\n\nThe `getImageEffectUV` function takes a `vec2` parameter `uv`, which represents a position in texture coordinates. The function first flips the `y` component of `uv` if the code is compiled for WebGPU. It then returns the resulting `uv` coordinates.\n\nThe purpose of this function is to convert texture coordinates to sample image effect textures. Image effect textures are textures that are rendered without the forward renderer, which does the flip in the projection matrix. By flipping the `y` component of `uv`, the function corrects for the difference in coordinate systems between the forward renderer and the image effect renderer.\n\nHere is an example usage of `getImageEffectUV`:\n\n```glsl\nvec2 uv = textureCoord.xy;\nuv = getImageEffectUV(uv);\nvec4 color = texture(imageEffectTexture, uv);\n```\n\nOverall, these functions are important for correctly sampling textures in the PlayCanvas engine. They allow for the correct conversion of coordinates between different coordinate systems, which is necessary for rendering accurate and visually appealing graphics.\n## Questions: \n 1. What is the purpose of this code?\n    - This code provides two functions for converting coordinates to sample textures in the PlayCanvas engine.\n\n2. What is the input and output of the `getGrabScreenPos` function?\n    - The input is a `vec4` representing clip space position, and the output is a `vec2` representing texture coordinates.\n\n3. What is the difference between `getGrabScreenPos` and `getImageEffectUV` functions?\n    - `getGrabScreenPos` converts clip space position to texture coordinates for scene grab textures, while `getImageEffectUV` converts UV coordinates to sample image effect textures. The latter is rendered without the forward renderer which does the flip in the projection matrix.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader-chunks/frag/shared.md"}}],["431",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader-chunks/frag/webgpu.js)\n\nThe code provided is a GLSL shader code that defines several macros for texture sampling. These macros are used to simplify the process of sampling textures in the PlayCanvas engine project. \n\nThe first line of the code exports the code as a default module. The code then defines several macros that can be used to sample textures. These macros include `texture2D`, `texture2DBias`, `texture2DLodEXT`, `textureCube`, `textureCubeLodEXT`, and `textureShadow`. \n\nThe `texture2D` macro is used to sample a 2D texture. It takes two parameters: the texture resource and the texture coordinates. The `texture2DBias` macro is similar to `texture2D`, but it also takes a bias value as a third parameter. The `texture2DLodEXT` macro is used to sample a 2D texture with a specific level of detail. It takes three parameters: the texture resource, the texture coordinates, and the level of detail. \n\nThe `textureCube` macro is used to sample a cube texture. It takes two parameters: the texture resource and the texture coordinates. The `textureCubeLodEXT` macro is similar to `textureCube`, but it also takes a level of detail as a third parameter. The `textureShadow` macro is used to sample a shadow map. It takes two parameters: the texture resource and the texture coordinates. \n\nThe code also defines several other macros that are not currently implemented, including `texture2DProj`, `texture2DProjLodEXT`, `texture2DGradEXT`, `texture2DProjGradEXT`, and `textureCubeGradEXT`. \n\nFinally, the code defines several other macros that are used to pass and accept shadow maps and textures as function parameters. These macros include `SHADOWMAP_PASS`, `SHADOWMAP_ACCEPT`, `TEXTURE_PASS`, and `TEXTURE_ACCEPT`. \n\nOverall, this code provides a set of macros that simplify the process of sampling textures in the PlayCanvas engine project. These macros can be used in shaders to sample textures with different levels of detail and to pass and accept textures and shadow maps as function parameters.\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines several macros for texture sampling and shadow map handling in GLSL shaders, as well as enabling certain extensions and defining some preprocessor directives.\n\n2. What is the significance of the `GL_EXT_samplerless_texture_functions` extension?\n    \n    This extension provides support for texture sampling without the need for a separate sampler object, which can simplify shader code and reduce memory usage.\n\n3. What other texture sampling macros are planned for implementation?\n    \n    The code includes commented-out definitions for several other texture sampling macros, such as `texture2DProj` and `textureCubeGradEXT`, which may be implemented in the future.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader-chunks/frag/webgpu.md"}}],["432",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader-chunks/vert/gles3.js)\n\nThe code above is a GLSL shader code that defines several preprocessor directives. These directives are used to modify the behavior of the shader compiler and to customize the shader code for specific use cases. \n\nThe first directive, `#define attribute in`, defines a new macro that replaces the keyword `attribute` with the keyword `in`. This is useful because some versions of GLSL use the `in` keyword instead of `attribute` to define vertex attributes. By defining this macro, the shader code can be written in a more flexible way that is compatible with different versions of GLSL.\n\nThe second directive, `#define varying out`, defines a new macro that replaces the keyword `varying` with the keyword `out`. This is similar to the first directive, but it is used to define output variables that are interpolated across the surface of a primitive. By defining this macro, the shader code can be written in a more consistent way that is compatible with different versions of GLSL.\n\nThe third directive, `#define texture2D texture`, defines a new macro that replaces the function `texture2D` with the function `texture`. This is useful because some versions of GLSL use the `texture` function instead of `texture2D` to sample a texture. By defining this macro, the shader code can be written in a more flexible way that is compatible with different versions of GLSL.\n\nThe fourth directive, `#define GL2`, defines a new macro that is used to indicate that the shader code is written for WebGL 2. This is important because WebGL 2 supports a different version of GLSL than WebGL 1. By defining this macro, the shader code can be written in a way that takes advantage of the new features and capabilities of WebGL 2.\n\nThe fifth directive, `#define VERTEXSHADER`, defines a new macro that is used to indicate that the shader code is a vertex shader. This is important because GLSL supports both vertex shaders and fragment shaders, which have different inputs and outputs. By defining this macro, the shader code can be written in a way that is specific to vertex shaders.\n\nOverall, this code is a useful tool for customizing GLSL shader code for different versions of GLSL and different use cases. It can be used in the larger PlayCanvas engine project to create more flexible and efficient shaders that take advantage of the latest features of WebGL 2. Here is an example of how this code might be used in a larger shader program:\n\n```\n// Vertex shader code\nimport glsl from 'glslify';\n\nconst vertexShader = glsl`\n  #define attribute in\n  #define varying out\n  #define texture2D texture\n  #define GL2\n  #define VERTEXSHADER\n\n  // Vertex shader code goes here\n`;\n\n// Fragment shader code\nimport glsl from 'glslify';\n\nconst fragmentShader = glsl`\n  #define varying in\n  #define texture2D texture\n  #define GL2\n  #define FRAGMENTSHADER\n\n  // Fragment shader code goes here\n`;\n\n// Create a new shader program\nconst shaderProgram = new pc.Shader(device, {\n  attributes: {\n    aPosition: pc.SEMANTIC_POSITION,\n    aNormal: pc.SEMANTIC_NORMAL,\n    aUv0: pc.SEMANTIC_TEXCOORD0,\n  },\n  vshader: vertexShader,\n  fshader: fragmentShader,\n});\n```\n## Questions: \n 1. What is the purpose of this code?\n   - This code is defining preprocessor directives for a GLSL vertex shader.\n\n2. What is the significance of the \"#define GL2\" directive?\n   - The \"#define GL2\" directive indicates that the shader is written for the WebGL 2.0 graphics API.\n\n3. How is this code used in the PlayCanvas engine?\n   - This code is likely used as part of the engine's shader compilation process, where it is combined with other shader code to produce a complete shader program.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader-chunks/vert/gles3.md"}}],["433",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader-chunks/vert/webgpu.js)\n\nThe code above is a GLSL shader code that defines some preprocessor macros. These macros are used to simplify the process of writing GLSL shaders for the PlayCanvas engine. \n\nThe first macro, `texture2D`, takes two arguments: a texture resource and a UV coordinate. It returns the color value of the texture at the given UV coordinate. This macro is used to sample textures in the shader code. \n\nThe second macro, `GL2`, defines a preprocessor macro that is used to indicate that the shader code is written for OpenGL 2.0. This macro is used to ensure that the shader code is compatible with the OpenGL 2.0 rendering pipeline. \n\nThe third macro, `WEBGPU`, defines a preprocessor macro that is used to indicate that the shader code is written for the WebGPU API. This macro is used to ensure that the shader code is compatible with the WebGPU rendering pipeline. \n\nThe fourth macro, `VERTEXSHADER`, defines a preprocessor macro that is used to indicate that the shader code is a vertex shader. This macro is used to differentiate between vertex shaders and fragment shaders in the PlayCanvas engine. \n\nOverall, these macros are used to simplify the process of writing GLSL shaders for the PlayCanvas engine. They provide a convenient way to sample textures, ensure compatibility with different rendering pipelines, and differentiate between vertex and fragment shaders. \n\nExample usage of the `texture2D` macro:\n\n```\nvec4 color = texture2D(diffuseMap, vUv);\n```\n\nThis code samples the `diffuseMap` texture at the UV coordinate `vUv` and returns the color value of the texture at that coordinate.\n## Questions: \n 1. What is the purpose of the `texture2D` function defined in this code?\n   - The `texture2D` function is a macro that simplifies the process of sampling a texture in GLSL by taking in a resource and UV coordinates as arguments.\n\n2. What do the `GL2` and `WEBGPU` defines do?\n   - The `GL2` and `WEBGPU` defines are used to specify the graphics API that the code is targeting. In this case, it is targeting both OpenGL 2.0 and the WebGPU API.\n\n3. What is the significance of the `VERTEXSHADER` define?\n   - The `VERTEXSHADER` define is used to indicate that the code is defining a vertex shader. This is important because it tells the compiler how to interpret the code and how to integrate it into the overall rendering pipeline.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader-chunks/vert/webgpu.md"}}],["434",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader-processor-options.js)\n\nThe code defines a class called `ShaderProcessorOptions` that provides options for processing shaders to add support for bind groups and uniform buffers. The class has three properties: `uniformFormats`, `bindGroupFormats`, and `vertexFormat`. The `uniformFormats` property is an array of `UniformBufferFormat` objects, the `bindGroupFormats` property is an array of `BindGroupFormat` objects, and the `vertexFormat` property is a `VertexFormat` object. \n\nThe constructor of the class takes three optional parameters: `viewUniformFormat`, `viewBindGroupFormat`, and `vertexFormat`. The `viewUniformFormat` and `viewBindGroupFormat` parameters are used to construct a sparse array of uniform and bind group formats, respectively. The `vertexFormat` parameter is used to set the `vertexFormat` property of the class.\n\nThe class provides three methods: `hasUniform`, `hasTexture`, and `generateKey`. The `hasUniform` method takes a uniform name as a parameter and returns `true` if the uniform exists in any of the uniform formats, `false` otherwise. The `hasTexture` method takes a texture uniform name as a parameter and returns `true` if the texture uniform exists in any of the bind group formats, `false` otherwise. The `generateKey` method generates a unique key representing the processing options.\n\nThis class is likely used in the larger PlayCanvas engine project to provide options for processing shaders to add support for bind groups and uniform buffers. The `ShaderProcessorOptions` class is used in conjunction with other classes in the project to process shaders and generate unique keys representing the processing options. \n\nExample usage of the `ShaderProcessorOptions` class:\n\n```\nimport { ShaderProcessorOptions } from \"playcanvas-engine\";\n\nconst uniformFormat = new UniformBufferFormat();\nconst bindGroupFormat = new BindGroupFormat();\nconst vertexFormat = new VertexFormat();\n\nconst options = new ShaderProcessorOptions(uniformFormat, bindGroupFormat, vertexFormat);\n\nconst hasUniform = options.hasUniform(\"u_color\");\nconst hasTexture = options.hasTexture(\"texture_diffuse\");\n\nconst key = options.generateKey();\n```\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n- This code defines a class called `ShaderProcessorOptions` that is used to process shaders for uniform buffer and bind group support. It is likely used in the rendering pipeline of the PlayCanvas engine.\n\n2. What parameters does the constructor of `ShaderProcessorOptions` take and how are they used?\n- The constructor takes three optional parameters: `viewUniformFormat`, `viewBindGroupFormat`, and `vertexFormat`. These parameters are used to construct a sparse array of uniform and bind group formats, and to set the vertex format.\n\n3. What is the purpose of the `generateKey` method and how is it used?\n- The `generateKey` method generates a unique key representing the processing options of the shader. This key is likely used to cache shader programs and avoid redundant shader processing.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader-processor-options.md"}}],["435",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader-utils.js)\n\nThe code provided is a module that exports a class called `ShaderUtils`. This class provides utility functions for shader creation. The purpose of this class is to create a shader definition that can be used to create a shader program. \n\nThe `createDefinition` method is the main method of this class. It takes two arguments: a `device` object and an `options` object. The `device` object is an instance of the `GraphicsDevice` class, which is used to determine the version of the shader code to generate. The `options` object is used to pass optional arguments to the method. \n\nThe `createDefinition` method generates a shader definition object that contains the following properties: \n\n- `name`: A string that represents the name of the shader.\n- `attributes`: An object that maps attribute names to semantic values.\n- `vshader`: A string that represents the vertex shader code.\n- `fshader`: A string that represents the fragment shader code.\n- `useTransformFeedback`: A boolean that indicates whether to use transform feedback.\n\nThe `createDefinition` method generates the vertex and fragment shader code by concatenating several strings together. These strings are generated by calling other methods of the `ShaderUtils` class. \n\nThe `versionCode` method generates a string that represents the version of the shader code to generate. This method checks the `device` object to determine whether to generate WebGL 1.0 or WebGL 2.0 shader code. \n\nThe `precisionCode` method generates a string that represents the precision of the shader code to generate. This method checks the `device` object to determine the maximum precision supported by the device. \n\nThe `gl1Extensions` method generates a string that represents the extensions used by the shader code. This method checks the `device` object to determine which extensions are supported by the device. \n\nThe `dummyFragmentCode` method generates a string that represents a dummy fragment shader code. This method is used as a fallback in case the `options` object does not contain a `fragmentCode` property. \n\nThe `collectAttributes` method extracts the attributes specified in a vertex shader. This method returns an object that maps attribute names to semantic values. \n\nOverall, the `ShaderUtils` class is an important part of the PlayCanvas engine project. It provides a convenient way to generate shader definitions that can be used to create shader programs.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file provides utility functions for shader creation.\n\n2. What are the different shader chunks included in this file?\n- This file includes shader chunks for gles2, gles3, and webgpu for both vertex and fragment shaders, as well as a shared fragment shader chunk.\n\n3. What is the purpose of the `_attrib2Semantic` object?\n- The `_attrib2Semantic` object maps attribute names to semantic values used in the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader-utils.md"}}],["436",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/shader.js)\n\nThe code defines a class called `Shader` that represents a program responsible for rendering graphical primitives on a device's graphics processor. The shader is generated from a shader definition, which specifies the code for processing vertices and fragments processed by the GPU. The language of the code is GLSL (or more specifically ESSL, the OpenGL ES Shading Language). The shader definition also describes how the PlayCanvas engine should map vertex buffer elements onto the attributes specified in the vertex shader code.\n\nThe `Shader` class has a constructor that takes a `graphicsDevice` object and a `definition` object as parameters. The `definition` object contains the name of the shader, the mapping of vertex shader attribute names to semantics SEMANTIC_*, vertex shader source (GLSL code), fragment shader source (GLSL code), and other optional parameters. The constructor initializes the `id`, `device`, `definition`, and `name` properties of the `Shader` instance. It also preprocesses the shader sources using the `Preprocessor` class and initializes the `ready` and `failed` properties to `false`.\n\nThe `Shader` class has a `destroy` method that frees resources associated with the shader. It also has `loseContext` and `restoreContext` methods that are called when the WebGL context is lost or restored, respectively.\n\nThe `Shader` class is used in the PlayCanvas engine to create and manage shaders for rendering 3D graphics. Developers can create custom shaders by defining the vertex and fragment shader sources and specifying the mapping of vertex buffer elements to shader attributes. The `Shader` class provides a way to preprocess the shader sources and manage the lifecycle of the shader. \n\nExample usage:\n\n```\nconst shaderDefinition = {\n    attributes: {\n        aPosition: pc.SEMANTIC_POSITION\n    },\n    vshader: [\n        \"attribute vec3 aPosition;\",\n        \"\",\n        \"void main(void)\",\n        \"{\",\n        \"    gl_Position = vec4(aPosition, 1.0);\",\n        \"}\"\n    ].join(\"\\n\"),\n    fshader: [\n        \"precision \" + graphicsDevice.precision + \" float;\",\n        \"\",\n        \"void main(void)\",\n        \"{\",\n        \"    gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0);\",\n        \"}\"\n    ].join(\"\\n\")\n};\n\nconst shader = new pc.Shader(graphicsDevice, shaderDefinition);\n```\n## Questions: \n 1. What is the purpose of the `Shader` class?\n- The `Shader` class is responsible for rendering graphical primitives on a device's graphics processor. It is generated from a shader definition that specifies the code for processing vertices and fragments processed by the GPU.\n\n2. What parameters are required to create a new instance of the `Shader` class?\n- A new instance of the `Shader` class requires a `graphicsDevice` parameter used to manage the shader, and a `definition` object parameter that specifies the shader definition from which to build the shader. The `definition` object should include a `vshader` parameter for the vertex shader source (GLSL code) and an optional `fshader` parameter for the fragment shader source (GLSL code).\n\n3. What is the purpose of the `init()` method in the `Shader` class?\n- The `init()` method initializes a shader back to its default state by setting the `ready` and `failed` properties to `false`. This method is called internally by the `constructor` method.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/shader.md"}}],["437",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/transform-feedback.js)\n\nThe code defines a class called `TransformFeedback` that allows the user to configure and use the transform feedback feature in WebGL2. The transform feedback feature allows the user to capture the output of a vertex shader and use it as input for another shader, effectively allowing the user to chain multiple vertex shaders together. \n\nThe `TransformFeedback` class has a constructor that takes an input vertex buffer and an optional usage type for the output vertex buffer. The input vertex buffer can be any `VertexBuffer`, either manually created or from a `Mesh`. The output vertex buffer is created internally by the `TransformFeedback` object and can be accessed using the `outputBuffer` property. The usage type of the output vertex buffer can be one of `BUFFER_STATIC`, `BUFFER_DYNAMIC`, `BUFFER_STREAM`, or `BUFFER_GPUDYNAMIC`, with `BUFFER_GPUDYNAMIC` being the default and recommended option for continuous update.\n\nThe `TransformFeedback` class also has a `process` method that takes a vertex shader and runs it on the input buffer, writing the results into the output buffer. The input and output buffers can be optionally swapped, which is useful for continuous buffer processing. The `process` method takes care of setting up the graphics device, setting the input and output buffers, setting the shader, and drawing the vertices. \n\nThe `TransformFeedback` class also has a `destroy` method that destroys the output buffer.\n\nThe `TransformFeedback` class has a static method called `createShader` that takes a graphics device, vertex shader code, and a name, and returns a shader that is ready to be used with the `process` method. The vertex shader code should contain output variables starting with \"out_\".\n\nThe code also imports several other classes and constants from other files in the project, including `Debug`, `BUFFER_GPUDYNAMIC`, `PRIMITIVE_POINTS`, `VertexBuffer`, `DebugGraphics`, `Shader`, and `ShaderUtils`. \n\nThe code includes two code examples, one for the shader asset and one for the script asset. The shader asset example shows how to define the outputs in the vertex shader and how to assign values to these outputs. The script asset example shows how to create a `TransformFeedback` object, how to create a shader using `TransformFeedback.createShader`, and how to run the shader using `tf.process(shader)`. The script asset example also shows how to create a `MeshInstance` and a `pc.Entity` and add them to the scene. \n\nOverall, the `TransformFeedback` class provides a way to use the transform feedback feature in WebGL2 and chain multiple vertex shaders together. It is a useful tool for creating complex vertex transformations and animations.\n## Questions: \n 1. What is the purpose of the TransformFeedback object?\n- The TransformFeedback object allows for the configuration and use of the transform feedback feature in WebGL2, which allows for the output of a vertex shader to be written to a buffer for further processing.\n\n2. What are the steps to use the TransformFeedback object?\n- First, check that you're on WebGL2. Then, define the outputs in your vertex shader, create the shader, create/acquire the input vertex buffer, create the TransformFeedback object with the input buffer, and run the shader with the TransformFeedback object.\n\n3. What is the recommended usage type for the output vertex buffer?\n- The recommended usage type for the output vertex buffer is BUFFER_GPUDYNAMIC, which is suitable for continuous updates.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/transform-feedback.md"}}],["438",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/uniform-buffer-format.js)\n\nThe code defines two classes, `UniformFormat` and `UniformBufferFormat`, which are used to describe the layout of data inside a uniform buffer. A uniform buffer is a block of memory that contains a collection of uniform variables that can be accessed by a shader program. The `UniformFormat` class stores information about an individual uniform, such as its name, type, byte size, and offset. The `UniformBufferFormat` class stores a collection of `UniformFormat` objects and calculates the byte size and offset of each uniform in the buffer.\n\nThe `UniformFormat` class has a constructor that takes a name, type, and count as arguments. The `name` argument is the name of the uniform, the `type` argument is the type of the uniform (e.g. float, vec3, mat4), and the `count` argument is the number of elements in an array uniform. The class also has a `calculateOffset` method that calculates the offset of the uniform in the buffer based on the std140 rules. The std140 rules specify how data should be aligned in a uniform buffer to ensure that it can be efficiently accessed by a shader program.\n\nThe `UniformBufferFormat` class has a constructor that takes a graphics device and an array of `UniformFormat` objects as arguments. The constructor calculates the byte size and offset of each uniform in the buffer using the `calculateOffset` method of the `UniformFormat` class. The class also has a `get` method that returns the `UniformFormat` object for a given uniform name. The `getShaderDeclaration` method returns a string that contains the GLSL code for declaring the uniform buffer in a shader program.\n\nOverall, these classes are used to define the layout of data inside a uniform buffer, which is an important concept in computer graphics programming. The `UniformBufferFormat` class is used by the PlayCanvas engine to create uniform buffers that can be used by shader programs to efficiently access data. The `UniformFormat` class is used internally by the `UniformBufferFormat` class to store information about individual uniforms.\n## Questions: \n 1. What is the purpose of the `UniformBufferFormat` class?\n- The `UniformBufferFormat` class defines the layout of data inside a uniform buffer.\n\n2. What is the purpose of the `calculateOffset` method in the `UniformFormat` class?\n- The `calculateOffset` method calculates the offset of the uniform in an array of 32bit values based on the std140 rules.\n\n3. What is the purpose of the `getShaderDeclaration` method in the `UniformBufferFormat` class?\n- The `getShaderDeclaration` method returns a shader declaration for the uniform buffer format, including the layout, set, binding, and type of each uniform.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/uniform-buffer-format.md"}}],["439",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/uniform-buffer.js)\n\nThe code defines a class called `UniformBuffer` that represents a GPU memory buffer storing uniforms. The class has a constructor that takes a `graphicsDevice` and a `format` as arguments. The `graphicsDevice` is used to manage the uniform buffer, while the `format` specifies the format of the uniform buffer. The class also has a `destroy()` method that frees resources associated with the uniform buffer.\n\nThe `UniformBuffer` class has a `setUniform()` method that assigns a value to the uniform specified by its format. This method is the fast version of assigning a value to a uniform, avoiding any lookups. The class also has a `set()` method that assigns a value to the uniform specified by name. The `update()` method sets new values for the uniforms and uploads the new data.\n\nThe code also defines an array called `_updateFunctions` that contains functions for updating the uniform buffer for different types of uniforms. The functions are only implemented for types for which the default array to buffer copy does not work, or could be slower. The types include `UNIFORMTYPE_INT`, `UNIFORMTYPE_FLOAT`, `UNIFORMTYPE_VEC2`, `UNIFORMTYPE_VEC3`, `UNIFORMTYPE_VEC4`, `UNIFORMTYPE_IVEC2`, `UNIFORMTYPE_IVEC3`, `UNIFORMTYPE_IVEC4`, `UNIFORMTYPE_FLOATARRAY`, `UNIFORMTYPE_VEC2ARRAY`, `UNIFORMTYPE_VEC3ARRAY`, `UNIFORMTYPE_MAT2`, and `UNIFORMTYPE_MAT3`. \n\nEach function takes a `uniformBuffer`, a `value`, and an `offset` as arguments. The `uniformBuffer` is the uniform buffer to update, the `value` is the value to assign to the uniform, and the `offset` is the offset in the buffer to assign the value to. The functions update the uniform buffer with the value for the specified type of uniform.\n\nThe code also imports `Debug` and `constants.js` modules. The `Debug` module is used for debugging purposes, while the `constants.js` module contains constants used in the code.\n\nOverall, the `UniformBuffer` class is an important part of the PlayCanvas engine project as it represents a GPU memory buffer storing uniforms. The class provides methods for assigning values to uniforms and updating the uniform buffer. The `_updateFunctions` array provides functions for updating the uniform buffer for different types of uniforms.\n## Questions: \n 1. What is the purpose of the `_updateFunctions` array?\n- The `_updateFunctions` array contains functions that are used to set values in a uniform buffer for specific uniform types that cannot be set using the default array to buffer copy.\n\n2. What is the `UniformBuffer` class used for?\n- The `UniformBuffer` class represents a GPU memory buffer that stores uniforms and provides methods for setting and updating uniform values.\n\n3. What is the purpose of the `loseContext` method in the `UniformBuffer` class?\n- The `loseContext` method is called when the rendering context is lost and releases all context-related resources associated with the uniform buffer.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/uniform-buffer.md"}}],["440",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/version.js)\n\nThe code defines a class called `Version` which is used to represent a version number. The class has three methods: `equals`, `copy`, and `reset`. \n\nThe `constructor` method initializes two instance variables: `globalId` and `revision`. These variables are set to 0 by default. \n\nThe `equals` method takes another `Version` object as an argument and returns `true` if the `globalId` and `revision` of the two objects are equal. Otherwise, it returns `false`. This method can be used to compare two `Version` objects to see if they represent the same version. \n\nThe `copy` method takes another `Version` object as an argument and copies its `globalId` and `revision` values to the current object. This method can be used to make a copy of a `Version` object. \n\nThe `reset` method sets the `globalId` and `revision` values of the current object to 0. This method can be used to reset a `Version` object to its default values. \n\nThe `Version` class is exported using the `export` keyword, which means it can be imported and used in other files. For example, if another file needs to keep track of the version number of a game or application, it can import the `Version` class and create a new instance of it. \n\n```javascript\nimport { Version } from 'playcanvas-engine';\n\nconst version = new Version();\nconsole.log(version.globalId); // 0\nconsole.log(version.revision); // 0\n\nversion.globalId = 1;\nversion.revision = 2;\n\nconst otherVersion = new Version();\notherVersion.copy(version);\n\nconsole.log(otherVersion.equals(version)); // true\n\nversion.reset();\n\nconsole.log(version.globalId); // 0\nconsole.log(version.revision); // 0\n``` \n\nIn summary, the `Version` class provides a simple way to represent and compare version numbers. It can be used in various parts of the PlayCanvas engine project to keep track of version numbers for different components.\n## Questions: \n 1. **What is the purpose of the Version class?**\n    \n    The Version class is used to store and manipulate version information, specifically a global ID and revision number.\n\n2. **What does the equals() method do?**\n    \n    The equals() method compares the globalId and revision properties of the current Version object with those of another Version object passed as an argument, and returns true if they are equal.\n\n3. **What is the significance of the export statement at the end of the code?**\n    \n    The export statement makes the Version class available for use in other modules or files that import it using the import statement.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/version.md"}}],["441",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/versioned-object.js)\n\nThe code above defines a class called `VersionedObject` and exports it for use in other parts of the PlayCanvas engine project. The purpose of this class is to provide a way to track changes made to objects in the engine. \n\nWhen an instance of `VersionedObject` is created, a unique ID is assigned to it by incrementing a global counter called `idCounter`. A new `Version` object is also created and assigned to the `version` property of the instance. The `Version` class is likely defined in the `version.js` file that is imported at the top of this file. \n\nThe `Version` class likely has properties that keep track of the revision number and the global ID of the object. The `increment` method of `VersionedObject` is used to increment the revision number of the object's version. This method can be called whenever a change is made to the object, allowing other parts of the engine to know that the object has been modified. \n\nThis class can be used in other parts of the PlayCanvas engine project to keep track of changes made to objects. For example, if an object is modified in a physics simulation, the `increment` method can be called to update the object's version. Other parts of the engine that rely on the state of the object can then check the object's version to see if it has been modified since the last time they accessed it. \n\nHere is an example of how this class might be used in another part of the PlayCanvas engine project:\n\n```javascript\nimport { VersionedObject } from './versioned-object.js';\n\nclass PhysicsObject extends VersionedObject {\n    constructor() {\n        super();\n        this.position = { x: 0, y: 0, z: 0 };\n    }\n\n    setPosition(x, y, z) {\n        this.position.x = x;\n        this.position.y = y;\n        this.position.z = z;\n        this.increment();\n    }\n}\n\nexport { PhysicsObject };\n```\n\nIn this example, a new class called `PhysicsObject` is defined that extends `VersionedObject`. This class represents an object in a physics simulation and has a `position` property that can be set using the `setPosition` method. When `setPosition` is called, the `increment` method of `VersionedObject` is also called to update the object's version. Other parts of the engine that rely on the state of the `PhysicsObject` can then check its version to see if it has been modified.\n## Questions: \n 1. **What is the purpose of the `Version` import?**\n    \n    The `Version` import is used to create a new version for each `VersionedObject` instance.\n\n2. **What is the significance of the `idCounter` variable?**\n    \n    The `idCounter` variable is used to assign a unique ID to each `VersionedObject` instance.\n\n3. **What does the `increment` method do?**\n    \n    The `increment` method increases the revision number of the `VersionedObject` instance.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/versioned-object.md"}}],["442",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/vertex-buffer.js)\n\nThe code defines a class called `VertexBuffer` that represents a mechanism for specifying vertex data to the graphics hardware. The class takes in a `GraphicsDevice` object, a `VertexFormat` object, the number of vertices that the buffer will hold, and optional parameters for the usage type of the buffer and initial data. \n\nThe constructor initializes the object with the given parameters and creates a vertex buffer implementation using the `createVertexBufferImpl` method of the `GraphicsDevice` object. It also calculates the size of the buffer and allocates storage for it. If initial data is provided, it sets the data using the `setData` method, otherwise it creates a new `ArrayBuffer` for storage. \n\nThe class provides methods for getting the format, usage, and number of vertices of the buffer, as well as locking and unlocking the buffer's memory for reading and writing. The `setData` method allows for copying data into the buffer's memory. \n\nThe `destroy` method frees resources associated with the buffer and removes it from the list of buffers in the `GraphicsDevice` object. The `adjustVramSizeTracking` method is used to track the size of the buffer in VRAM. The `loseContext` method is called when the rendering context is lost and releases all context-related resources. \n\nOverall, the `VertexBuffer` class is an important component of the PlayCanvas engine that allows for efficient management of vertex data for rendering. It can be used in conjunction with other classes and methods in the engine to create and manipulate 3D graphics. \n\nExample usage:\n\n```\nconst graphicsDevice = new GraphicsDevice();\nconst vertexFormat = new VertexFormat();\nconst numVertices = 100;\nconst usage = BUFFER_STATIC;\nconst initialData = new ArrayBuffer(1000);\n\nconst vertexBuffer = new VertexBuffer(graphicsDevice, vertexFormat, numVertices, usage, initialData);\n\n// Get the format of the buffer\nconst format = vertexBuffer.getFormat();\n\n// Lock the buffer's memory for writing\nconst bufferData = vertexBuffer.lock();\n\n// Write data to the buffer\n// ...\n\n// Unlock the buffer's memory\nvertexBuffer.unlock();\n\n// Destroy the buffer\nvertexBuffer.destroy();\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines a class called `VertexBuffer` that represents a mechanism for specifying vertex data to the graphics hardware in a PlayCanvas engine project.\n\n2. What parameters are required to create a new instance of the `VertexBuffer` class?\n    \n    A new instance of the `VertexBuffer` class requires a `graphicsDevice` object, a `format` object representing the vertex format, and the `numVertices` parameter representing the number of vertices that the buffer will hold. Optional parameters include `usage` and `initialData`.\n\n3. What is the purpose of the `lock()` and `unlock()` methods?\n    \n    The `lock()` method returns a mapped memory block representing the content of the vertex buffer, while the `unlock()` method notifies the graphics engine that the client-side copy of the vertex buffer's memory can be returned to the control of the graphics driver.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/vertex-buffer.md"}}],["443",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/vertex-format.js)\n\n# Code Explanation: VertexFormat.js\n\nThe `VertexFormat` class is a descriptor that defines the layout of vertex data inside a `VertexBuffer`. It is used to link the vertex data to a shader input. The class has the following properties:\n\n- `elements`: An array of vertex attribute elements.\n- `elements[].name`: The meaning of the vertex element. This is used to link the vertex data to a shader input. Can be:\n  - `SEMANTIC_POSITION`\n  - `SEMANTIC_NORMAL`\n  - `SEMANTIC_TANGENT`\n  - `SEMANTIC_BLENDWEIGHT`\n  - `SEMANTIC_BLENDINDICES`\n  - `SEMANTIC_COLOR`\n  - `SEMANTIC_TEXCOORD0`\n  - `SEMANTIC_TEXCOORD1`\n  - `SEMANTIC_TEXCOORD2`\n  - `SEMANTIC_TEXCOORD3`\n  - `SEMANTIC_TEXCOORD4`\n  - `SEMANTIC_TEXCOORD5`\n  - `SEMANTIC_TEXCOORD6`\n  - `SEMANTIC_TEXCOORD7`\n  - If vertex data has a meaning other than one of those listed above, use the user-defined semantics: `SEMANTIC_ATTR0` to `SEMANTIC_ATTR15`.\n- `elements[].numComponents`: The number of components of the vertex attribute. Can be 1, 2, 3, or 4.\n- `elements[].dataType`: The data type of the attribute. Can be:\n  - `TYPE_INT8`\n  - `TYPE_UINT8`\n  - `TYPE_INT16`\n  - `TYPE_UINT16`\n  - `TYPE_INT32`\n  - `TYPE_UINT32`\n  - `TYPE_FLOAT32`\n- `elements[].normalize`: If true, vertex attribute data will be mapped from a 0 to 255 range down to 0 to 1 when fed to a shader. If false, vertex attribute data is left unchanged. If this property is unspecified, false is assumed.\n- `elements[].offset`: The number of initial bytes at the start of a vertex that are not relevant to this attribute.\n- `elements[].stride`: The number of total bytes that are between the start of one vertex and the start of the next.\n- `elements[].size`: The size of the attribute in bytes.\n\nThe `VertexFormat` class has the following methods:\n\n- `constructor(graphicsDevice, description, vertexCount)`: Creates a new `VertexFormat` instance.\n  - `graphicsDevice`: The graphics device used to manage this vertex format.\n  - `description`: An array of vertex attribute descriptions.\n  - `description[].semantic`: The meaning of the vertex element. This is used to link the vertex data to a shader input. Can be:\n    - `SEMANTIC_POSITION`\n    - `SEMANTIC_NORMAL`\n    - `SEMANTIC_TANGENT`\n    - `SEMANTIC_BLENDWEIGHT`\n    - `SEMANTIC_BLENDINDICES`\n    - `SEMANTIC_COLOR`\n    - `SEMANTIC_TEXCOORD0`\n    - `SEMANTIC_TEXCOORD1`\n    - `SEMANTIC_TEXCOORD2`\n    - `SEMANTIC_TEXCOORD3`\n    - `SEMANTIC_TEXCOORD4`\n    - `SEMANTIC_TEXCOORD5`\n    - `SEMANTIC_TEXCOORD6`\n    - `SEMANTIC_TEXCOORD7`\n    - If vertex data has a meaning other than one of those listed above, use the user-defined semantics: `SEMANTIC_ATTR0` to `SEMANTIC_ATTR15`.\n  - `description[].components`: The number of components of the vertex attribute. Can be 1, 2, 3, or 4.\n  - `description[].type`: The data type of the attribute. Can be:\n    - `TYPE_INT8`\n    - `TYPE_UINT8`\n    - `TYPE_INT16`\n    - `TYPE_UINT16`\n    - `TYPE_INT32`\n    - `TYPE_UINT32`\n    - `TYPE_FLOAT32`\n  - `description[].normalize`: If true, vertex attribute data will be mapped from a 0 to 255 range down to 0 to 1 when fed to a shader. If false, vertex attribute data is left unchanged. If this property is unspecified, false is assumed.\n  - `vertexCount`: When specified, vertex format will be set up for non-interleaved format with a specified number of vertices. (example: PPPPNNNNCCCC), where arrays of individual attributes will be stored one right after the other (subject to alignment requirements). Note that in this case, the format depends on the number of vertices and needs to change when the number of vertices changes. When not specified, vertex format will be interleaved. (example: PNCPNCPNCPNC).\n- `update()`: Applies any changes made to the `VertexFormat`'s properties.\n- `_evaluateHash()`: Evaluates hash values for the format allowing fast compare of batching/rendering compatibility.\n- `static getDefaultInstancingFormat(graphicsDevice)`: The `VertexFormat` used to store matrices of type `Mat4` for hardware instancing.\n\nThe `VertexFormat` class is used in the PlayCanvas engine to define the layout of vertex data inside a `VertexBuffer`. It is used to link the vertex data to a shader input. The `VertexFormat` class is used in conjunction with the `VertexBuffer` class to create and manage vertex buffers. The `VertexFormat` class is also used in the `Mesh` class to define the layout of vertex data for a mesh.\n## Questions: \n 1. What is the purpose of the `VertexFormat` class?\n    \n    The `VertexFormat` class is a descriptor that defines the layout of vertex data inside a `VertexBuffer`.\n\n2. What are the possible values for the `semantic` property of a vertex attribute element?\n    \n    The `semantic` property of a vertex attribute element can be one of the following: `SEMANTIC_POSITION`, `SEMANTIC_NORMAL`, `SEMANTIC_TANGENT`, `SEMANTIC_BLENDWEIGHT`, `SEMANTIC_BLENDINDICES`, `SEMANTIC_COLOR`, `SEMANTIC_TEXCOORD0`, `SEMANTIC_TEXCOORD1`, `SEMANTIC_TEXCOORD2`, `SEMANTIC_TEXCOORD3`, `SEMANTIC_TEXCOORD4`, `SEMANTIC_TEXCOORD5`, `SEMANTIC_TEXCOORD6`, `SEMANTIC_TEXCOORD7`, or a user-defined semantic from `SEMANTIC_ATTR0` to `SEMANTIC_ATTR15`.\n\n3. What is the purpose of the `getDefaultInstancingFormat` method?\n    \n    The `getDefaultInstancingFormat` method returns the default `VertexFormat` used to store matrices of type `Mat4` for hardware instancing.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/vertex-format.md"}}],["444",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgl/webgl-buffer.js)\n\nThe code defines a class called WebglBuffer which is an implementation of the Buffer for WebGL. The purpose of this class is to create and manage a WebGL buffer object. \n\nThe class has a few methods and properties. The `destroy` method is used to delete the buffer object when it is no longer needed. The `initialized` property is a boolean that indicates whether the buffer has been initialized or not. The `loseContext` method is used to release the buffer object when the WebGL context is lost. The `unlock` method is used to create or update the buffer object with new data.\n\nThe `unlock` method takes four arguments: `device`, `usage`, `target`, and `storage`. The `device` argument is an object that contains the WebGL context. The `usage` argument is an integer that specifies how the buffer will be used. The `target` argument is an integer that specifies the target buffer object. The `storage` argument is an ArrayBuffer or an ArrayBufferView that contains the data to be stored in the buffer.\n\nThe `usage` argument can be one of four constants: `BUFFER_STATIC`, `BUFFER_DYNAMIC`, `BUFFER_STREAM`, or `BUFFER_GPUDYNAMIC`. These constants are imported from another file called `constants.js`. The `target` argument can be one of several WebGL buffer targets, such as `gl.ARRAY_BUFFER` or `gl.ELEMENT_ARRAY_BUFFER`.\n\nThe `unlock` method first checks if the buffer object has been created. If not, it creates a new buffer object using the `gl.createBuffer()` method. It then sets the WebGL buffer usage based on the `usage` argument and binds the buffer object to the target using `gl.bindBuffer()`. Finally, it stores the data in the buffer object using `gl.bufferData()`.\n\nOverall, this class is an important part of the PlayCanvas engine as it provides a way to create and manage WebGL buffer objects. It can be used in various parts of the engine, such as rendering and physics, to store and manipulate data efficiently. Here is an example of how this class might be used:\n\n```\nconst buffer = new WebglBuffer();\nconst data = new Float32Array([1.0, 2.0, 3.0, 4.0]);\nbuffer.unlock(device, BUFFER_STATIC, gl.ARRAY_BUFFER, data);\n```\n## Questions: \n 1. What is the purpose of this code and what does it do?\n    \n    This code defines a class called `WebglBuffer` which is a WebGL implementation of a buffer. It has methods for destroying the buffer, checking if it has been initialized, and unlocking the buffer for use.\n\n2. What are the possible values for the `usage` parameter in the `unlock` method and what do they do?\n    \n    The possible values for the `usage` parameter are `BUFFER_STATIC`, `BUFFER_DYNAMIC`, `BUFFER_STREAM`, and `BUFFER_GPUDYNAMIC`. They determine how the buffer will be used and how often it will be updated. `BUFFER_STATIC` is for data that will not change often, `BUFFER_DYNAMIC` is for data that will change frequently, `BUFFER_STREAM` is for data that will be updated every frame, and `BUFFER_GPUDYNAMIC` is for data that will be updated frequently on the GPU.\n\n3. What is the purpose of the `loseContext` method?\n    \n    The `loseContext` method sets the `bufferId` property to `null`, which effectively destroys the buffer. This is used when the WebGL context is lost and the buffer needs to be recreated when the context is restored.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgl/webgl-buffer.md"}}],["445",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgl/webgl-index-buffer.js)\n\nThe code defines a class called WebglIndexBuffer which is a WebGL implementation of the IndexBuffer. The purpose of this class is to provide a way to store and manipulate index data for rendering in WebGL. \n\nThe class extends the WebglBuffer class and has a constructor that takes an indexBuffer object as a parameter. The constructor initializes the glFormat property of the class based on the format of the indexBuffer. The format can be one of three types: INDEXFORMAT_UINT8, INDEXFORMAT_UINT16, or INDEXFORMAT_UINT32. Depending on the format, the glFormat property is set to the corresponding WebGL constant: gl.UNSIGNED_BYTE, gl.UNSIGNED_SHORT, or gl.UNSIGNED_INT.\n\nThe class also has an unlock method that takes an indexBuffer object as a parameter. The method calls the unlock method of the parent class, passing in the device, usage, buffer type, and storage of the indexBuffer.\n\nThis class is used in the larger PlayCanvas engine project to provide a way to store and manipulate index data for rendering in WebGL. It is likely used in conjunction with other classes and functions to create and render 3D models and scenes. \n\nHere is an example of how this class might be used in the PlayCanvas engine:\n\n```\nconst indexData = [0, 1, 2, 3, 4, 5];\nconst indexBuffer = new IndexBuffer(device, INDEXFORMAT_UINT16, 6, BUFFER_STATIC, indexData);\nconst webglIndexBuffer = new WebglIndexBuffer(indexBuffer);\nwebglIndexBuffer.unlock(indexBuffer);\n``` \n\nIn this example, an array of index data is created and used to create an IndexBuffer object. The IndexBuffer object is then used to create a WebglIndexBuffer object, which is then unlocked using the unlock method. This allows the index data to be stored and manipulated in WebGL for rendering.\n## Questions: \n 1. What is the purpose of this code?\n- This code provides a WebGL implementation of the IndexBuffer for the PlayCanvas engine.\n\n2. What is the superclass of WebglIndexBuffer?\n- The superclass of WebglIndexBuffer is WebglBuffer.\n\n3. What formats are supported by this implementation of IndexBuffer?\n- This implementation of IndexBuffer supports formats INDEXFORMAT_UINT8, INDEXFORMAT_UINT16, and INDEXFORMAT_UINT32.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgl/webgl-index-buffer.md"}}],["446",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgl/webgl-render-target.js)\n\nThe `WebglRenderTarget` class is a WebGL implementation of the `RenderTarget` class. It is used to create and manage framebuffers for rendering to textures. This class is not intended to be used directly by developers, but rather as a helper class for other classes in the PlayCanvas engine.\n\nThe `WebglRenderTarget` class has several properties and methods that are used to create and manage framebuffers. The `_glFrameBuffer` property is used to store the WebGL framebuffer object. The `_glDepthBuffer` property is used to store the WebGL depth buffer object. The `_glResolveFrameBuffer` property is used to store the WebGL resolve framebuffer object. The `_glMsaaColorBuffer` property is used to store the WebGL multisample anti-aliasing color buffer object. The `_glMsaaDepthBuffer` property is used to store the WebGL multisample anti-aliasing depth buffer object.\n\nThe `destroy` method is used to destroy the WebGL framebuffer object and its associated buffers. The `get initialized` method is used to check if the WebGL framebuffer object has been initialized. The `init` method is used to initialize the WebGL framebuffer object and its associated buffers. The `_checkFbo` method is used to check the completeness status of the currently bound WebGLFramebuffer object. The `loseContext` method is used to release the WebGL context and its associated buffers. The `resolve` method is used to resolve the multisample anti-aliasing buffer to the main framebuffer.\n\nOverall, the `WebglRenderTarget` class is an important helper class for other classes in the PlayCanvas engine that require rendering to textures. It provides a simple and efficient way to create and manage framebuffers in WebGL.\n## Questions: \n 1. What is the purpose of this code?\n- This code is a WebGL implementation of the RenderTarget.\n\n2. What is the significance of the `WebglRenderTarget` class?\n- The `WebglRenderTarget` class is significant because it contains methods for initializing, destroying, and resolving a WebGL render target.\n\n3. What is the purpose of the `Debug` class and how is it used in this code?\n- The `Debug` class is used to call a function that checks the completeness status of the currently bound WebGLFramebuffer object. It is used to ensure that the framebuffer creation did not fail and to log any errors that may have occurred.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgl/webgl-render-target.md"}}],["447",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgl/webgl-shader-input.js)\n\nThe code defines a class called `WebglShaderInput` which represents a shader uniform. A shader uniform is a variable in a shader program that can be set from outside the program. The purpose of this class is to manage the state of a shader uniform, including its name, type, location, and value.\n\nThe class constructor takes four parameters: a `graphicsDevice` object, which is used to manage the shader input; a `name` string, which is the name of the shader input; a `type` number, which is the type of the shader input; and a `locationId` number or `WebGLUniformLocation` object, which is the location id of the shader input.\n\nThe class has several properties, including `locationId`, which is the location id of the shader input; `scopeId`, which is the scope id for the attribute name; `version`, which is a `Version` object used to track changes to the shader input; `dataType`, which is the data type of the shader input; `value`, which is an array of values for the shader input; and `array`, which is an array used to hold texture unit ids.\n\nThe class also has a conditional statement that checks if the name of the shader input ends with \"[0]\". If it does, the data type is changed to an array data type based on the original data type. For example, if the original data type was `UNIFORMTYPE_FLOAT`, it is changed to `UNIFORMTYPE_FLOATARRAY`.\n\nThis class is likely used in the larger PlayCanvas engine project to manage the state of shader uniforms in WebGL programs. It provides a convenient way to set and update the values of shader uniforms, which are critical for rendering graphics in a WebGL context. Here is an example of how this class might be used:\n\n```javascript\nconst graphicsDevice = new GraphicsDevice();\nconst shaderInput = new WebglShaderInput(graphicsDevice, 'u_color', UNIFORMTYPE_VEC4, 0);\nshaderInput.value = [1, 0, 0, 1]; // set the value of the shader uniform to red\n```\n## Questions: \n 1. What is the purpose of the `WebglShaderInput` class?\n- The `WebglShaderInput` class represents a shader uniform and is used to manage shader inputs.\n\n2. What are the parameters of the `WebglShaderInput` constructor?\n- The `WebglShaderInput` constructor takes in a `graphicsDevice` object, a `name` string, a `type` number, and a `locationId` number or `WebGLUniformLocation` object.\n\n3. What is the purpose of the `value` and `array` properties of the `WebglShaderInput` class?\n- The `value` property is an array used to store the value of the shader input, while the `array` property is an array used to hold texture unit ids.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgl/webgl-shader-input.md"}}],["448",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgl/webgl-vertex-buffer.js)\n\nThe `WebglVertexBuffer` class is a WebGL implementation of the `VertexBuffer` class. It extends the `WebglBuffer` class and provides additional functionality specific to vertex buffers. \n\nThe `WebglVertexBuffer` class has a `vao` property which represents the vertex array object. The `destroy` method is responsible for destroying the vertex buffer and clearing up any bound vertex buffers. It calls the `destroy` method of the parent class `WebglBuffer` and sets the `boundVao` property of the device to `null`. It also calls the `bindVertexArray` method of the WebGL context with `null` as the argument to unbind the vertex array object.\n\nThe `loseContext` method is responsible for releasing any resources associated with the vertex buffer when the WebGL context is lost. It calls the `loseContext` method of the parent class `WebglBuffer` and sets the `vao` property to `null`.\n\nThe `unlock` method is responsible for unlocking the vertex buffer and making it available for use. It calls the `unlock` method of the parent class `WebglBuffer` with the appropriate arguments.\n\nOverall, the `WebglVertexBuffer` class provides a way to create and manage vertex buffers in a WebGL context. It can be used in conjunction with other classes in the PlayCanvas engine to create and render 3D graphics. For example, the `WebglVertexBuffer` class may be used in the `Mesh` class to store and manipulate vertex data for a 3D model. \n\nExample usage:\n\n```\nimport { WebglVertexBuffer } from \"playcanvas-engine\";\n\nconst device = // get WebGL device\nconst vertexBuffer = // create vertex buffer\n\nconst webglVertexBuffer = new WebglVertexBuffer(device);\nwebglVertexBuffer.unlock(vertexBuffer);\n```\n## Questions: \n 1. What is the purpose of this code and how does it fit into the PlayCanvas engine?\n- This code provides a WebGL implementation of the VertexBuffer and is part of the PlayCanvas engine's rendering system.\n\n2. What is the significance of the \"vao\" variable and how is it used?\n- The \"vao\" variable represents a vertex array object and is used to store the state of vertex attribute arrays. It is cleared and set to null in the \"destroy\" and \"loseContext\" methods.\n\n3. How does the \"unlock\" method work and what does it do?\n- The \"unlock\" method unlocks the vertex buffer and sets the appropriate WebGL state for the device. It calls the \"super.unlock\" method with the device, usage, buffer type, and storage of the vertex buffer.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgl/webgl-vertex-buffer.md"}}],["449",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-bind-group-format.js)\n\nThe `WebgpuBindGroupFormat` class is a wrapper over the `GPUBindGroupLayout` object in the WebGPU API. It is used to create a descriptor for a bind group, which is a collection of resources that can be bound to a shader. The `WebgpuBindGroupFormat` class is used to create a unique key for the bind group and to create the descriptor that is used to create the `GPUBindGroupLayout` object.\n\nThe `WebgpuBindGroupFormat` class has a constructor that takes a `BindGroupFormat` object as a parameter. The `BindGroupFormat` object contains information about the resources that are to be bound to the shader. The `WebgpuBindGroupFormat` class creates a descriptor for the bind group by iterating over the buffer and texture formats in the `BindGroupFormat` object. The descriptor is then used to create the `GPUBindGroupLayout` object.\n\nThe `WebgpuBindGroupFormat` class has a `getTextureSlot` method that takes a `BindGroupFormat` object and an index as parameters. The method returns the slot index for the texture at the specified index. The slot index is calculated by adding the number of buffer formats to the index multiplied by two.\n\nThe `WebgpuBindGroupFormat` class has a `createDescriptor` method that takes a `BindGroupFormat` object as a parameter. The method creates a descriptor for the bind group by iterating over the buffer and texture formats in the `BindGroupFormat` object. The descriptor contains an array of entries, where each entry corresponds to a buffer or texture format. The entry contains information about the binding, visibility, and resource type. The method also generates a unique key for the bind group by concatenating the information about each entry.\n\nThe `WebgpuBindGroupFormat` class is used in the PlayCanvas engine to create a descriptor for a bind group. The descriptor is then used to create the `GPUBindGroupLayout` object, which is used to bind resources to a shader. The `WebgpuBindGroupFormat` class is part of the WebGPU implementation of the PlayCanvas engine and is not intended to be used outside of the engine.\n## Questions: \n 1. What is the purpose of the `WebgpuBindGroupFormat` class?\n- The `WebgpuBindGroupFormat` class is a WebGPU implementation of the `BindGroupFormat`, which is a wrapper over `GPUBindGroupLayout`.\n\n2. What are the different types of `sampleTypes` and `samplerTypes`?\n- `sampleTypes` has three types: `float`, `unfilterable-float`, and `depth`.\n- `samplerTypes` has three types: `filtering`, `non-filtering`, and `comparison`.\n\n3. What is the purpose of the `getTextureSlot` method?\n- The `getTextureSlot` method returns the slot index of a texture binding given the index of the texture and the `BindGroupFormat`.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-bind-group-format.md"}}],["450",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-bind-group.js)\n\nThe `WebgpuBindGroup` class is a wrapper over the `GPUBindGroup` object in the WebGPU API. It provides a way to create and update a bind group descriptor in WebGPU format. A bind group is a collection of resources that can be bound together to be used in a shader. \n\nThe `WebgpuBindGroup` class has two methods: `update` and `destroy`. The `update` method takes a `bindGroup` object and creates a new `GPUBindGroup` object using the `createDescriptor` method. The `destroy` method destroys the current `GPUBindGroup` object. \n\nThe `createDescriptor` method creates a bind group descriptor in WebGPU format. It takes a `device` object and a `bindGroup` object as input. The `entries` array is populated with the resources in the `bindGroup` object. The resources can be uniform buffers or textures. For each uniform buffer, a `buffer` resource is added to the `entries` array. For each texture, a `view` and a `sampler` resource are added to the `entries` array. \n\nThe `createDescriptor` method returns a `GPUBindGroupDescriptor` object that can be used to create a `GPUBindGroup` object. \n\nThis class is used internally by the PlayCanvas engine to create and manage bind groups for shaders. It is not intended to be used directly by developers. \n\nExample usage:\n\n```\nconst device = new WebgpuGraphicsDevice();\nconst bindGroup = new BindGroup();\n\n// add resources to the bind group\n\nconst webgpuBindGroup = new WebgpuBindGroup();\nwebgpuBindGroup.update(bindGroup);\n```\n## Questions: \n 1. What is the purpose of this code and how does it fit into the PlayCanvas engine? \n\nThis code provides a WebGPU implementation of the BindGroup, which is a wrapper over GPUBindGroup. It is part of the PlayCanvas engine's graphics device functionality.\n\n2. What is the role of the `WebgpuDebug` class in this code? \n\nThe `WebgpuDebug` class is used to validate the device and provide debugging information, such as the debug format, descriptor, format, and bind group.\n\n3. What is the format of the descriptor returned by the `createDescriptor` method? \n\nThe `createDescriptor` method returns a descriptor of type `GPUBindGroupDescriptor`, which can be used to create a `GPUBindGroup`. The descriptor contains a layout and an array of entries, which specify the bindings and resources for the bind group.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-bind-group.md"}}],["451",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-buffer.js)\n\nThe `WebgpuBuffer` class is a part of the PlayCanvas engine project and provides a WebGPU implementation of the Buffer. The purpose of this class is to create and manage a GPU buffer that can be used for various purposes, such as storing vertex data, index data, or uniform data.\n\nThe `WebgpuBuffer` class has several methods and properties that allow for the creation, destruction, and manipulation of the GPU buffer. The `buffer` property is a private property that holds the actual GPU buffer object. The `initialized` getter returns a boolean value indicating whether the buffer has been initialized or not. The `loseContext` method is a no-op and does nothing.\n\nThe `unlock` method is the main method of the `WebgpuBuffer` class. It takes four parameters: `device`, `usage`, `target`, and `storage`. The `device` parameter is a reference to the `WebgpuGraphicsDevice` object that represents the graphics device. The `usage` parameter is an integer value that specifies how the buffer will be used. The `target` parameter is an integer value that specifies the target of the buffer, such as `GPUBufferUsage.VERTEX` or `GPUBufferUsage.INDEX`. The `storage` parameter is an ArrayBuffer or TypedArray that contains the data to be stored in the buffer.\n\nThe `unlock` method first checks if the buffer has been initialized. If not, it creates a new buffer object with the specified size and usage. The size of the buffer is calculated by rounding up the size of the data to the nearest multiple of 4. The data is then copied to the buffer using the `writeBuffer` method of the WebGPU queue object.\n\nIf the buffer has already been initialized, the data is simply copied to the buffer using the same method. The `unlock` method does not handle different usage types, such as `BUFFER_STATIC`, `BUFFER_DYNAMIC`, `BUFFER_STREAM`, or `BUFFER_GPUDYNAMIC`.\n\nOverall, the `WebgpuBuffer` class provides a simple and efficient way to create and manage GPU buffers in the PlayCanvas engine project. It can be used to store various types of data and can be easily integrated into other parts of the engine. Here is an example of how to use the `WebgpuBuffer` class to create a vertex buffer:\n\n```\nconst vertexData = new Float32Array([...]);\nconst buffer = new WebgpuBuffer();\nbuffer.unlock(device, GPUBufferUsage.VERTEX, vertexData.byteLength, vertexData);\n```\n## Questions: \n 1. What is the purpose of this code and how does it fit into the PlayCanvas engine?\n- This code is a WebGPU implementation of the Buffer and is part of the PlayCanvas engine.\n2. What is the significance of the `unlock` method and what parameters does it take?\n- The `unlock` method is used to write data to the GPU buffer and takes a `device` parameter of type `WebgpuGraphicsDevice`, as well as `usage`, `target`, and `storage` parameters of type `*`.\n3. What is the purpose of the `initialized` getter and what does it return?\n- The `initialized` getter returns a boolean indicating whether the `buffer` property has been initialized or not.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-buffer.md"}}],["452",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-clear-renderer.js)\n\n## WebgpuClearRenderer\n\nThe `WebgpuClearRenderer` class is a helper class that implements a viewport clear operation in the PlayCanvas engine. When rendering to a texture, the whole surface can be cleared using `loadOp`, but if only a viewport needs to be cleared, or if it needs to be cleared later during the rendering, this needs to be achieved by rendering a quad. This class renders a full-screen quad and expects the viewport/scissor to be set up to clip it to only the required area.\n\nThe `WebgpuClearRenderer` class has a constructor that takes a `device` object as an argument. The constructor initializes a shader that can write out color and depth values. The shader is defined in the `code` constant, which is a string containing the WGSL code for the shader. The shader is compiled using the `Shader` class, which takes the `device` object and an object containing the shader details as arguments. The shader details include the name of the shader, the shader language, and the vertex and fragment shaders.\n\nThe `WebgpuClearRenderer` class also initializes a uniform buffer that contains the color and depth values. The uniform buffer is defined using the `UniformBuffer` class, which takes the `device` object and a `UniformBufferFormat` object as arguments. The `UniformBufferFormat` object defines the format of the uniform buffer, which includes the name and type of each uniform.\n\nThe `WebgpuClearRenderer` class also initializes a bind group that contains the uniform buffer. The bind group is defined using the `BindGroup` class, which takes the `device` object, a `BindGroupFormat` object, and the uniform buffer as arguments. The `BindGroupFormat` object defines the format of the bind group, which includes the name and stage of each buffer.\n\nThe `WebgpuClearRenderer` class has a `clear` method that takes the `device`, `renderTarget`, `options`, and `defaultOptions` objects as arguments. The `renderTarget` object represents the render target to clear. The `options` object contains the clear options, such as the clear color and depth. The `defaultOptions` object contains the default clear options.\n\nThe `clear` method first checks the clear flags to determine which buffers to clear. If the `CLEARFLAG_COLOR` flag is set, the method sets the clear color to the specified color or the default color. If the `CLEARFLAG_DEPTH` flag is set, the method sets the clear depth to the specified depth or the default depth. If the `CLEARFLAG_STENCIL` flag is set, the method logs a warning that stencil clear is not supported.\n\nThe `clear` method then sets the blend state, depth state, and cull mode to the appropriate values. It then sets the shader, bind group, and vertex buffer, and draws the quad using the `device.draw` method.\n\nOverall, the `WebgpuClearRenderer` class provides a way to clear the viewport of a render target using a full-screen quad. It is used internally by the PlayCanvas engine to implement the viewport clear operation.\n## Questions: \n 1. What is the purpose of the `WebgpuClearRenderer` class?\n- The `WebgpuClearRenderer` class is a helper class that implements a viewport clear operation by rendering a full-screen quad, and expects the viewport/scissor to be set up to clip it to only the required area.\n\n2. What is the format of the bind group used in the `WebgpuClearRenderer` class?\n- The format of the bind group used in the `WebgpuClearRenderer` class is a `BindGroupFormat` that contains a single `BindBufferFormat` with the default uniform buffer slot name and shader stages for both vertex and fragment.\n\n3. Does the `WebgpuClearRenderer` class support stencil clear?\n- No, the `WebgpuClearRenderer` class does not support stencil clear at the moment, as indicated by the warning message printed to the console when the `CLEARFLAG_STENCIL` flag is set.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-clear-renderer.md"}}],["453",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-debug.js)\n\nThe code defines a class called `WebgpuDebug` that provides an internal debug system for the PlayCanvas engine's WebGPU graphics device. The class contains several static methods that are only executed in the debug build and are stripped out in other builds. \n\nThe `WebgpuDebug` class has three methods: `validate`, `memory`, and `internal`. Each of these methods starts a specific error scope for the graphics device. The `validate` method starts a validation error scope, the `memory` method starts an out-of-memory error scope, and the `internal` method starts an internal error scope. \n\nThe class also has an `end` method that ends the previous error scope and prints errors if any. The method takes additional parameters that form the error message. If an error occurs, the method logs the error message to the console. The method also keeps track of the number of times a duplicate error message is logged and logs the message up to a maximum of five times. \n\nThe `WebgpuDebug` class is used internally by the PlayCanvas engine to provide a debug system for the WebGPU graphics device. Developers can use the class to debug their applications and identify errors in the graphics device. \n\nHere is an example of how the `WebgpuDebug` class can be used:\n\n```\nimport { WebgpuDebug } from 'playcanvas';\n\n// create a new graphics device\nconst device = new WebgpuGraphicsDevice();\n\n// start a validation error scope\nWebgpuDebug.validate(device);\n\n// perform some operations that may cause errors\n\n// end the error scope and print errors if any\nWebgpuDebug.end(device, 'Error occurred while performing operation');\n```\n## Questions: \n 1. What is the purpose of the `WebgpuDebug` class?\n    \n    The `WebgpuDebug` class is an internal debug system for the PlayCanvas engine's WebGPU graphics device. It provides functions for starting and ending different error scopes and logging error messages.\n\n2. What is the significance of the `MAX_DUPLICATES` constant?\n    \n    The `MAX_DUPLICATES` constant sets the maximum number of times a duplicate error message can be logged. If an error message is logged more than `MAX_DUPLICATES` times, it will be ignored.\n\n3. What is the purpose of the `WebgpuDebug._loggedMessages` map?\n    \n    The `WebgpuDebug._loggedMessages` map keeps track of how many times each error message has been logged. If an error message is logged more than `MAX_DUPLICATES` times, it will be ignored.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-debug.md"}}],["454",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-index-buffer.js)\n\nThe code above is a module that provides a WebGPU implementation of the IndexBuffer class. It is a part of the PlayCanvas engine project and is used to create and manage index buffers for 3D graphics rendering.\n\nThe module imports the Debug class, which is used for debugging purposes, and the INDEXFORMAT_UINT8 and INDEXFORMAT_UINT16 constants from another module called constants.js. It also imports the WebgpuBuffer class, which is used as a base class for the WebgpuIndexBuffer class.\n\nThe WebgpuIndexBuffer class extends the WebgpuBuffer class and overrides its constructor and unlock methods. The constructor takes an indexBuffer object as a parameter and sets the format property of the class based on the format of the indexBuffer. If the format is INDEXFORMAT_UINT8, an assertion error is thrown because WebGPU does not support 8-bit index buffer format. If the format is INDEXFORMAT_UINT16, the format property is set to \"uint16\", otherwise it is set to \"uint32\".\n\nThe unlock method takes an indexBuffer object as a parameter and calls the unlock method of the WebgpuBuffer class with the appropriate parameters. It passes the device property of the indexBuffer object, the usage property of the indexBuffer object, GPUBufferUsage.INDEX, and the storage property of the indexBuffer object.\n\nFinally, the module exports the WebgpuIndexBuffer class for use in other parts of the PlayCanvas engine project.\n\nIn summary, this module provides a WebGPU implementation of the IndexBuffer class, which is used to manage index buffers for 3D graphics rendering. It ensures that the index buffer format is supported by WebGPU and provides a method for unlocking the index buffer.\n## Questions: \n 1. What is the purpose of this code and what does it do?\n- This code is a class implementation of a WebGPU IndexBuffer for the PlayCanvas engine. It converts the index buffer format to either uint16 or uint32 and unlocks the buffer for use.\n\n2. What is the significance of the Debug.assert statement in the constructor?\n- The Debug.assert statement checks if the index buffer format is not INDEXFORMAT_UINT8, and if it is, it throws an error message stating that WebGPU does not support 8-bit index buffer format.\n\n3. What is the relationship between WebgpuIndexBuffer and WebgpuBuffer?\n- WebgpuIndexBuffer extends WebgpuBuffer, which means that it inherits all the properties and methods of WebgpuBuffer and adds its own unique properties and methods specific to index buffers.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-index-buffer.md"}}],["455",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-render-pipeline.js)\n\nThe `WebgpuRenderPipeline` class is responsible for creating and caching render pipelines for the PlayCanvas engine. A render pipeline is a set of instructions that the GPU uses to render a scene. The class takes in various parameters such as the primitive type, vertex format, shader, render target, blend state, depth state, and cull mode to generate a unique key for each pipeline. If a pipeline with the same key already exists in the cache, it is returned. Otherwise, a new pipeline is created and added to the cache.\n\nThe `WebgpuRenderPipeline` class has a `get` method that takes in the parameters mentioned above and returns a render pipeline. The `getKey` method generates a unique key for each pipeline based on the parameters passed in. The `getPipelineLayout` method creates a pipeline layout based on the bind group formats passed in. The `getBlend` method returns a blend object based on the blend state passed in. The `getDepthStencil` method returns a depth stencil object based on the depth state and render target passed in. Finally, the `create` method creates a render pipeline based on the parameters passed in.\n\nThe class also has various arrays that map constants to their corresponding values. For example, `_primitiveTopology` maps primitive types to their corresponding string values. These arrays are used in the `create` method to set the values of various properties of the `GPURenderPipelineDescriptor` object.\n\nOverall, the `WebgpuRenderPipeline` class is an essential part of the PlayCanvas engine as it is responsible for creating and caching render pipelines, which are crucial for rendering scenes efficiently.\n## Questions: \n 1. What is the purpose of the `WebgpuRenderPipeline` class?\n- The `WebgpuRenderPipeline` class is responsible for generating and caching render pipelines for the PlayCanvas engine.\n\n2. What is the purpose of the `getKey` method?\n- The `getKey` method generates a unique key for a render pipeline based on its parameters, which is used to cache and retrieve pipelines from the `cache` map.\n\n3. What is the purpose of the `getDepthStencil` method?\n- The `getDepthStencil` method generates a `GPUDepthStencilState` object based on the `depthState` and `renderTarget` parameters, which is used to configure the depth and stencil tests for the render pipeline.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-render-pipeline.md"}}],["456",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-render-target.js)\n\nThe `WebgpuRenderTarget` class is a WebGPU implementation of the `RenderTarget` class. It is used to create and manage render targets for rendering one time. The class is not intended to be used directly, but rather as a part of the larger PlayCanvas engine project.\n\nThe class has several properties, including `initialized`, `colorFormat`, `key`, `depthFormat`, `hasStencil`, `multisampledColorBuffer`, `depthTexture`, `depthTextureInternal`, `assignedColorTexture`, and `renderPassDescriptor`. The `initialized` property is a boolean that indicates whether the render target has been initialized. The `colorFormat` property is a string that represents the format of the color buffer. The `key` property is a string that is used by render pipeline creation. The `depthFormat` property is a string that represents the format of the depth buffer. The `hasStencil` property is a boolean that indicates whether the depth buffer has a stencil component. The `multisampledColorBuffer` property is a GPUTexture that represents the multi-sampled color buffer. The `depthTexture` property is a GPUTexture that represents the depth buffer. The `depthTextureInternal` property is a boolean that indicates whether the depth texture is internally allocated/owned. The `assignedColorTexture` property is a GPUTexture that is assigned each frame and is not owned by the render target. The `renderPassDescriptor` property is a GPURenderPassDescriptor that is used when starting a render pass for this render target.\n\nThe `WebgpuRenderTarget` class has several methods, including `constructor`, `destroy`, `updateKey`, `setDepthFormat`, `assignColorTexture`, `init`, `setupForRenderPass`, `loseContext`, and `resolve`. The `constructor` method initializes the render target and sets the `colorFormat` and `key` properties. The `destroy` method releases associated resources and sets the `initialized` property to false. The `updateKey` method updates the `key` property. The `setDepthFormat` method sets the `depthFormat` property and the `hasStencil` property. The `assignColorTexture` method assigns a color buffer to the render target. The `init` method initializes the render target for rendering one time. The `setupForRenderPass` method updates the WebGPU render pass descriptor by render pass settings. The `loseContext` method sets the `initialized` property to false. The `resolve` method resolves the render target.\n\nOverall, the `WebgpuRenderTarget` class is an important part of the PlayCanvas engine project as it provides a WebGPU implementation of the `RenderTarget` class. It is used to create and manage render targets for rendering one time. The class provides several properties and methods that are used to initialize, update, and destroy the render target.\n## Questions: \n 1. What is the purpose of this code and how does it fit into the PlayCanvas engine?\n- This code is a WebGPU implementation of the RenderTarget and is used for rendering one time. It is part of the PlayCanvas engine's web graphics processing unit (WebGPU) module.\n\n2. What is the significance of the `key` property and how is it used?\n- The `key` property is a unique key used by render pipeline creation. It is based on the color format, depth format, and number of samples of the render target. It is used to identify the render target when creating a render pipeline.\n\n3. What is the purpose of the `assignColorTexture` method and how is it used?\n- The `assignColorTexture` method assigns a color buffer to the render target. This allows the color buffer of the main framebuffer to be swapped each frame to a buffer provided by the context. The assigned color texture is used as a render buffer or resolve target.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-render-target.md"}}],["457",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-shader.js)\n\nThe `WebgpuShader` class is a part of the PlayCanvas engine project and is responsible for implementing the WebGPU version of the Shader. The purpose of this class is to create shader modules for the vertex and fragment shaders, transpile the shader code, and process the shader source to allow for uniforms.\n\nThe class imports `Debug` and `DebugHelper` from the `core/debug.js` file, and `SHADERLANGUAGE_WGSL` from the `constants.js` file. It also imports `ShaderProcessor` from the `shader-processor.js` file, and `WebgpuDebug` from the `webgpu-debug.js` file.\n\nThe `WebgpuShader` class has several properties and methods. The `_vertexCode` and `_fragmentCode` properties store the transpiled vertex and fragment shader code, respectively. The `vertexEntryPoint` and `fragmentEntryPoint` properties store the names of the vertex and fragment entry point functions, respectively.\n\nThe constructor of the `WebgpuShader` class takes a `Shader` object as a parameter. If the `Shader` object's `definition` property has a `shaderLanguage` of `SHADERLANGUAGE_WGSL`, the `_vertexCode` and `_fragmentCode` properties are set to the `vshader` and `fshader` properties of the `definition` object, respectively. The `vertexEntryPoint` and `fragmentEntryPoint` properties are set to `'vertexMain'` and `'fragmentMain'`, respectively. The `ready` property of the `Shader` object is set to `true`.\n\nIf the `Shader` object's `definition` property does not have a `shaderLanguage` of `SHADERLANGUAGE_WGSL`, the `process()` method is called. The `process()` method processes the shader source to allow for uniforms, keeps a reference to processed shaders in debug mode, transpiles the vertex and fragment shader code, and sets the `failed` or `ready` property of the `Shader` object based on whether the transpilation was successful.\n\nThe `createShaderModule()` method creates a shader module for the vertex or fragment shader using the `createShaderModule()` method of the `wgpu` object of the `device` property of the `Shader` object. The `getVertexShaderModule()` and `getFragmentShaderModule()` methods call the `createShaderModule()` method with the `_vertexCode` and `_fragmentCode` properties, respectively.\n\nThe `transpile()` method transpiles the shader code from GLSL to WGSL using the `compileGLSL()` and `convertSpirV2WGSL()` methods of the `glslang` and `twgsl` objects of the `device` property of the `Shader` object, respectively.\n\nThe `vertexCode` and `fragmentCode` getters return the `_vertexCode` and `_fragmentCode` properties, respectively.\n\nThe `destroy()` method sets the `_vertexCode` and `_fragmentCode` properties to `null`.\n\nThe `loseContext()` and `restoreContext()` methods are empty and do not perform any actions.\n\nIn summary, the `WebgpuShader` class is responsible for creating shader modules for the vertex and fragment shaders, transpiling the shader code, and processing the shader source to allow for uniforms. It is a part of the PlayCanvas engine project and is used to implement the WebGPU version of the Shader.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `WebgpuShader` which is a WebGPU implementation of a shader. It contains methods for creating shader modules, processing shaders, and transpiling shader code.\n\n2. What is the `_vertexCode` property used for?\n- The `_vertexCode` property is used to store the transpiled vertex shader code.\n\n3. What is the purpose of the `process` method?\n- The `process` method processes the shader source to allow for uniforms, keeps a reference to processed shaders in debug mode, transpiles the vertex and fragment shaders, and sets the `meshUniformBufferFormat` and `meshBindGroupFormat` properties of the shader.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-shader.md"}}],["458",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-texture.js)\n\nThe code defines a class called `WebgpuTexture` which is a WebGPU implementation of a texture. The class contains methods for creating and destroying a texture, creating a texture view, getting a sampler, uploading texture data, and losing context. \n\nThe `WebgpuTexture` class is used to create a texture object that can be used in the PlayCanvas engine. The texture object is created by passing a `Texture` object to the constructor of the `WebgpuTexture` class. The `Texture` object contains information about the texture, such as its format, width, height, and whether it is a cubemap or volume texture. \n\nThe `create` method of the `WebgpuTexture` class creates a GPU texture object using the `GPUTextureDescriptor` object. The `GPUTextureDescriptor` object contains information about the texture, such as its size, format, and usage. The `createView` method creates a GPU texture view object using the `GPUTextureViewDescriptor` object. The `GPUTextureViewDescriptor` object contains information about the texture view, such as its format, dimension, and aspect. \n\nThe `getSampler` method of the `WebgpuTexture` class creates a GPU sampler object using the `GPUSamplerDescriptor` object. The `GPUSamplerDescriptor` object contains information about the sampler, such as its address mode and filter mode. The `getSampler` method returns a sampler object that can be used to sample the texture. \n\nThe `uploadImmediate` method of the `WebgpuTexture` class uploads texture data to the GPU. The `uploadData` method uploads the texture data to the GPU using the `writeTexture` method of the `GPUQueue` object. \n\nOverall, the `WebgpuTexture` class is an important part of the PlayCanvas engine as it provides a way to create and manage textures in a WebGPU environment. The class is used to create GPU texture and sampler objects, and to upload texture data to the GPU.\n## Questions: \n 1. What is the purpose of the `WebgpuTexture` class?\n- The `WebgpuTexture` class is a WebGPU implementation of the Texture.\n\n2. What are the `gpuTextureFormats` and `gpuAddressModes` maps used for?\n- The `gpuTextureFormats` map is used to map `PIXELFORMAT_***` to `GPUTextureFormat`. The `gpuAddressModes` map is used to map `ADDRESS_***` to `GPUAddressMode`.\n \n3. What is the purpose of the `getSampler` method?\n- The `getSampler` method returns a sampler for the texture, allowing it to be sampled using different samplers. Most textures are sampled as interpolated floats, but some can additionally be sampled using non-interpolated floats (raw data) or compare sampling (shadow maps).","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-texture.md"}}],["459",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-uniform-buffer.js)\n\nThe code above is a class called WebgpuUniformBuffer that extends the WebgpuBuffer class. It is a part of the PlayCanvas engine project and is used to implement a UniformBuffer in WebGPU. \n\nA UniformBuffer is a buffer that contains data that is used by shaders in a graphics pipeline. It is used to store data that is shared across multiple shaders, such as matrices, lighting information, and other data that is used to render a scene. \n\nThe WebgpuUniformBuffer class is used to create and manage a UniformBuffer in WebGPU. It takes a uniformBuffer object as a parameter in its constructor, which is used to create a new UniformBuffer. The destroy() method is used to destroy the UniformBuffer and clear up any bound uniform buffers. The unlock() method is used to unlock the UniformBuffer and make it available for use by shaders. \n\nThis class is useful in the larger PlayCanvas engine project because it allows developers to create and manage UniformBuffers in WebGPU. This is important because WebGPU is a new graphics API that is designed to be more efficient and performant than previous APIs like WebGL. By using WebGPU, developers can create more complex and detailed scenes with better performance. \n\nHere is an example of how the WebgpuUniformBuffer class might be used in the PlayCanvas engine project:\n\n```\n// create a new UniformBuffer\nconst uniformBuffer = new WebgpuUniformBuffer();\n\n// set the data for the UniformBuffer\nuniformBuffer.setData(data);\n\n// lock the UniformBuffer\nuniformBuffer.lock();\n\n// use the UniformBuffer in a shader\nshader.setUniformBuffer(uniformBuffer);\n\n// unlock the UniformBuffer\nuniformBuffer.unlock();\n```\n\nOverall, the WebgpuUniformBuffer class is an important part of the PlayCanvas engine project because it allows developers to create and manage UniformBuffers in WebGPU, which is a more efficient and performant graphics API.\n## Questions: \n 1. What is the purpose of the `WebgpuUniformBuffer` class?\n- The `WebgpuUniformBuffer` class is a WebGPU implementation of the UniformBuffer.\n\n2. What is the parent class of `WebgpuUniformBuffer`?\n- The parent class of `WebgpuUniformBuffer` is `WebgpuBuffer`.\n\n3. What is the purpose of the `destroy` method in `WebgpuUniformBuffer`?\n- The `destroy` method in `WebgpuUniformBuffer` is used to destroy the buffer and clear up any bound uniform buffers.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-uniform-buffer.md"}}],["460",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-utils.js)\n\nThe code above is a module called `WebgpuUtils` that is used to convert a combination of shader stages into GPU shader stages. It imports three constants from a file called `constants.js`: `SHADERSTAGE_VERTEX`, `SHADERSTAGE_FRAGMENT`, and `SHADERSTAGE_COMPUTE`. These constants are used to represent the different stages of a shader program.\n\nThe `shaderStage` method is a static method of the `WebgpuUtils` class. It takes a parameter called `stage`, which is a combination of the three shader stages. The method then uses bitwise operators to check which stages are included in the combination and sets the corresponding bits in the `ret` variable. Finally, the method returns the `ret` variable, which is a combination of the GPU shader stages.\n\nThis module is likely used in the larger PlayCanvas engine project to facilitate the creation and management of shader programs. By converting a combination of shader stages into GPU shader stages, this module can simplify the process of creating and managing shader programs. For example, if a developer wants to create a shader program that only uses the vertex and fragment stages, they can pass `SHADERSTAGE_VERTEX | SHADERSTAGE_FRAGMENT` to the `shaderStage` method and receive the corresponding GPU shader stages. This can then be used to create a shader program that only includes those stages.\n\nOverall, this module provides a useful utility for managing shader programs in the PlayCanvas engine project.\n## Questions: \n 1. What is the purpose of the `WebgpuUtils` class?\n    - The `WebgpuUtils` class provides a static method `shaderStage` that converts a combination of `SHADER_STAGE_*` constants into `GPUShaderStage.*` constants.\n\n2. What are the `SHADERSTAGE_VERTEX`, `SHADERSTAGE_FRAGMENT`, and `SHADERSTAGE_COMPUTE` constants?\n    - These constants are likely defined in the `../constants.js` file and are used to represent different shader stages (vertex, fragment, and compute) in the PlayCanvas engine.\n\n3. What is the output of the `shaderStage` method?\n    - The `shaderStage` method returns a number that represents a combination of `GPUShaderStage.*` constants based on the input `stage` parameter, which is a combination of `SHADER_STAGE_*` constants.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-utils.md"}}],["461",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-vertex-buffer-layout.js)\n\nThe code defines a class called `WebgpuVertexBufferLayout` that is used to create and cache vertex buffer layouts for use in the PlayCanvas engine. A vertex buffer layout is a description of the data that is stored in a vertex buffer, which is used to render 3D objects. The layout describes the format of the data, such as the data type and number of components, as well as how the data is arranged in memory.\n\nThe `WebgpuVertexBufferLayout` class has a `cache` property that is a `Map` object used to store previously created vertex buffer layouts. The `get` method of the class takes one or two `VertexFormat` objects as arguments and returns the corresponding vertex buffer layout. If the layout has not been created before, it is created using the `create` method and added to the cache.\n\nThe `create` method takes one or two `VertexFormat` objects as arguments and returns an array of `GPUVertexBufferLayout` objects. Each `GPUVertexBufferLayout` object describes the layout of a single vertex buffer. The method iterates over the elements of each `VertexFormat` object and creates an array of `attributes` that describe the format of each element. The `shaderLocation` property of each attribute is set to the location of the semantic associated with the element, which is defined in the `constants.js` file. The `offset` property is set to the offset of the element within the vertex buffer, and the `format` property is set to a string that describes the data type and number of components of the element.\n\nIf the `VertexFormat` objects are interleaved, meaning that the elements of each format are interleaved in memory, the `offset` property of each attribute is set to the offset of the element within the interleaved block. If the `VertexFormat` objects are not interleaved, each attribute is assumed to be in a separate block of memory, and the `offset` property is set to 0.\n\nThe `create` method creates a `GPUVertexBufferLayout` object for each block of attributes and adds it to the `layout` array. The `arrayStride` property of each `GPUVertexBufferLayout` object is set to the stride of the block of attributes, which is the sum of the strides of each element in the block. The `stepMode` property is set to `'vertex'` if the block is not instanced, meaning that each vertex is rendered once, or `'instance'` if the block is instanced, meaning that each instance of the object is rendered once.\n\nThe `getKey` method of the class is used to generate a unique key for each combination of `VertexFormat` objects. The key is a string that includes the rendering hash strings of the `VertexFormat` objects, which are unique identifiers that are used to compare `VertexFormat` objects for equality.\n\nOverall, the `WebgpuVertexBufferLayout` class is used to create and cache vertex buffer layouts for use in the PlayCanvas engine. The class is used to optimize the rendering of 3D objects by reducing the number of times that vertex buffer layouts need to be created. An example of how the class might be used in the larger project is shown below:\n\n```javascript\nimport { WebgpuVertexBufferLayout } from 'playcanvas-engine';\n\nconst vertexFormat0 = new VertexFormat([\n    { semantic: SEMANTIC_POSITION, dataType: TYPE_FLOAT32, numComponents: 3 },\n    { semantic: SEMANTIC_NORMAL, dataType: TYPE_FLOAT32, numComponents: 3 },\n    { semantic: SEMANTIC_TEXCOORD0, dataType: TYPE_FLOAT32, numComponents: 2 }\n]);\n\nconst vertexFormat1 = new VertexFormat([\n    { semantic: SEMANTIC_COLOR, dataType: TYPE_UINT8, numComponents: 4 }\n]);\n\nconst layout = new WebgpuVertexBufferLayout().get(vertexFormat0, vertexFormat1);\n```\n\nIn this example, two `VertexFormat` objects are created, one for position, normal, and texture coordinates, and one for color. The `WebgpuVertexBufferLayout` class is used to create a vertex buffer layout that interleaves the two formats, and the resulting layout is stored in the `layout` variable. The layout can then be used to create a vertex buffer that is used to render a 3D object.\n## Questions: \n 1. What is the purpose of the `WebgpuVertexBufferLayout` class?\n- The `WebgpuVertexBufferLayout` class is used to obtain a vertex layout of one or two vertex formats.\n\n2. What is the `cache` property used for?\n- The `cache` property is a map that stores previously created vertex layouts to avoid creating duplicate layouts.\n\n3. What is the purpose of the `getKey` method?\n- The `getKey` method is used to generate a unique key for a given set of vertex formats, which is used to retrieve the corresponding vertex layout from the cache.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-vertex-buffer-layout.md"}}],["462",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/graphics/webgpu/webgpu-vertex-buffer.js)\n\nThe code above is a class called `WebgpuVertexBuffer` that extends the `WebgpuBuffer` class. It is a part of the PlayCanvas engine project and is used to implement a VertexBuffer using WebGPU. \n\nA VertexBuffer is a type of buffer that stores vertex data, such as position, color, and texture coordinates, for rendering 3D objects. The `WebgpuVertexBuffer` class is responsible for creating and managing this buffer using WebGPU, which is a new API for rendering graphics on the web.\n\nThe `WebgpuVertexBuffer` class has two methods: `constructor` and `unlock`. The `constructor` method takes in two parameters: `vertexBuffer` and `format`. The `vertexBuffer` parameter is an object that contains information about the vertex buffer, such as its size and usage. The `format` parameter is an object that describes the format of the vertex data, such as the number of components and their types.\n\nThe `unlock` method is used to unlock the vertex buffer after it has been updated with new data. It takes in a `vertexBuffer` parameter, which is the same object that was passed to the `constructor` method. The method then calls the `unlock` method of the `WebgpuBuffer` class, passing in the device, usage, buffer type, and storage of the vertex buffer.\n\nThe `destroy` method is also present in the `WebgpuVertexBuffer` class, which is used to destroy the vertex buffer when it is no longer needed. It calls the `destroy` method of the `WebgpuBuffer` class and clears up any bound vertex buffers.\n\nOverall, the `WebgpuVertexBuffer` class is an important part of the PlayCanvas engine project, as it provides a way to create and manage VertexBuffers using WebGPU. It can be used by developers to render 3D objects on the web with improved performance and efficiency. \n\nExample usage:\n\n```\nconst vertexBuffer = new WebgpuVertexBuffer(myVertexBuffer, myFormat);\nvertexBuffer.unlock(myVertexBuffer);\nvertexBuffer.destroy(myDevice);\n```\n## Questions: \n 1. What is the purpose of the `WebgpuVertexBuffer` class?\n- The `WebgpuVertexBuffer` class is a WebGPU implementation of the VertexBuffer.\n\n2. What parameters does the `WebgpuVertexBuffer` constructor take?\n- The `WebgpuVertexBuffer` constructor takes a `vertexBuffer` and a `format` parameter.\n\n3. What is the purpose of the `destroy` and `unlock` methods in the `WebgpuVertexBuffer` class?\n- The `destroy` method clears up bound vertex buffers and the `unlock` method unlocks the vertex buffer and sets its usage, GPU buffer usage, and storage.","metadata":{"source":".autodoc/docs/markdown/src/platform/graphics/webgpu/webgpu-vertex-buffer.md"}}],["463",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/input/controller.js)\n\nThe `Controller` class is a general input handler that handles both mouse and keyboard input assigned to named actions. This allows you to define input handlers separately from defining keyboard/mouse configurations. The class is part of the PlayCanvas engine project and is located in a file that imports `type` and several constants from other files. It also imports `Keyboard` and `Mouse` classes from other files.\n\nThe `Controller` class has several methods that allow you to attach and detach the controller to an element, enable and disable the context menu, update the keyboard and mouse handlers, register actions against a controller axis, and check if an action is enabled or was enabled since the last update. \n\nThe `Controller` class constructor takes an optional `element` parameter and an optional `options` object that can contain a `keyboard`, `mouse`, or `gamepads` object. If an `element` is provided, the `attach` method is called to attach the keyboard and mouse event handlers to the element. \n\nThe `registerKeys`, `registerMouse`, and `registerPadButton` methods allow you to create or update an action that is enabled when the supplied keys, mouse button, or gamepad button is pressed. The `registerAxis` method allows you to register an action against a controller axis. \n\nThe `isPressed` and `wasPressed` methods allow you to check if an action is currently enabled or was enabled since the last update. The `getAxis` method returns the value of the specified controller axis.\n\nOverall, the `Controller` class provides a way to handle input from multiple sources and assign it to named actions, making it easier to manage input in a larger project. \n\nExample usage:\n\n```\nimport { Controller } from 'playcanvas';\n\nconst controller = new Controller(document);\n\ncontroller.registerKeys('move_forward', [pc.KEY_W]);\ncontroller.registerKeys('move_backward', [pc.KEY_S]);\ncontroller.registerKeys('strafe_left', [pc.KEY_A]);\ncontroller.registerKeys('strafe_right', [pc.KEY_D]);\n\ncontroller.registerMouse('shoot', pc.MOUSEBUTTON_LEFT);\n\ncontroller.registerPadButton('jump', PAD_1, pc.PAD_FACE_1);\n\ncontroller.registerAxis({\n    name: 'move',\n    positive: 'key',\n    positiveKey: pc.KEY_W,\n    negative: 'key',\n    negativeKey: pc.KEY_S\n});\n\ncontroller.update(dt);\n\nif (controller.isPressed('move_forward')) {\n    // move forward\n}\n\nif (controller.wasPressed('shoot')) {\n    // shoot\n}\n\nconst moveValue = controller.getAxis('move');\nif (moveValue !== 0) {\n    // move left or right\n}\n```\n## Questions: \n 1. What is the purpose of the `Controller` class?\n- The `Controller` class is a general input handler that handles both mouse and keyboard input assigned to named actions, allowing developers to define input handlers separately from defining keyboard/mouse configurations.\n\n2. What are the different types of input actions that can be registered with the `Controller` class?\n- The different types of input actions that can be registered with the `Controller` class are keyboard, mouse, and gamepad actions.\n\n3. How can a developer check if a specific action is enabled or was enabled since the last update?\n- A developer can check if a specific action is enabled or was enabled since the last update by calling the `isPressed` or `wasPressed` method of the `Controller` class and passing in the name of the action as a parameter.","metadata":{"source":".autodoc/docs/markdown/src/platform/input/controller.md"}}],["464",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/input/keyboard-event.js)\n\nThe code defines a class called `KeyboardEvent` which represents a key press or release event on a keyboard. This class is used in the PlayCanvas engine to handle keyboard input events. \n\nThe `KeyboardEvent` class has a constructor that takes two parameters: a `keyboard` object and an `event` object. The `keyboard` object is the instance of the `Keyboard` class that is firing the event. The `event` object is the original browser event that was fired. \n\nThe `KeyboardEvent` class has three properties: `key`, `element`, and `event`. The `key` property represents the keyCode of the key that has changed. The `element` property represents the element that fired the keyboard event. The `event` property represents the original browser event which was fired. \n\nThe `KeyboardEvent` class is used in the PlayCanvas engine to handle keyboard input events. For example, the following code shows how to use the `KeyboardEvent` class to handle a key down event for the space key:\n\n```\nvar onKeyDown = function (e) {\n    if (e.key === pc.KEY_SPACE) {\n        // space key pressed\n    }\n    e.event.preventDefault(); // Use original browser event to prevent browser action.\n};\napp.keyboard.on(\"keydown\", onKeyDown, this);\n```\n\nIn this code, the `onKeyDown` function is called when a key down event is fired on the keyboard. The `e` parameter is an instance of the `KeyboardEvent` class. The `if` statement checks if the `key` property of the `KeyboardEvent` instance is equal to the `pc.KEY_SPACE` constant, which represents the space key. If the space key is pressed, the code inside the `if` statement is executed. The `e.event.preventDefault()` line prevents the default browser action for the key down event.\n## Questions: \n 1. What is the purpose of the KeyboardEvent class?\n    \n    The KeyboardEvent class is used to represent a key press or release event and is passed into all event callbacks from the Keyboard.\n\n2. What parameters are required to create a new KeyboardEvent instance?\n    \n    A new KeyboardEvent instance requires a keyboard object and the original browser event that was fired.\n\n3. What properties are available on a KeyboardEvent instance?\n    \n    A KeyboardEvent instance has three properties: key (the keyCode of the key that has changed), element (the element that fired the keyboard event), and event (the original browser event which was fired).","metadata":{"source":".autodoc/docs/markdown/src/platform/input/keyboard-event.md"}}],["465",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/input/keyboard.js)\n\nThe code defines a `Keyboard` class that represents a keyboard device bound to an HTML element. It allows the detection of the state of key presses and fires events when a key is pressed, released, or held down. \n\nThe class extends the `EventHandler` class, which provides functionality for registering and unregistering event handlers. The `Keyboard` class constructor takes an optional `element` parameter that specifies the HTML element to attach the keyboard event handlers to. If no element is specified, the keyboard event handlers are not attached to any element.\n\nThe `Keyboard` class defines three events: `keydown`, `keyup`, and `keypress`. These events are fired when a key is pressed, released, or held down, respectively. The events are implemented using the `fire` method of the `EventHandler` class.\n\nThe `Keyboard` class provides methods for testing the state of a key. The `isPressed` method returns `true` if the specified key is currently pressed, and `false` otherwise. The `wasPressed` method returns `true` if the specified key was pressed since the last update, and `false` otherwise. The `wasReleased` method returns `true` if the specified key was released since the last update, and `false` otherwise.\n\nThe `Keyboard` class also defines several private methods that handle the browser keyboard events. These methods are `_handleKeyDown`, `_handleKeyUp`, and `_handleKeyPress`. They are called when a key is pressed, released, or held down, respectively. These methods update the internal state of the `Keyboard` object and fire the appropriate events.\n\nThe `Keyboard` class also defines several private properties that are used to store the state of the keyboard. These properties are `_keymap`, `_lastmap`, and `_element`. The `_keymap` property is an object that maps key identifiers to `true` if the key is currently pressed, and `false` otherwise. The `_lastmap` property is an object that maps key identifiers to `true` if the key was pressed since the last update, and `false` otherwise. The `_element` property is the HTML element that the keyboard event handlers are attached to.\n\nThe `Keyboard` class also defines several private helper functions. The `makeKeyboardEvent` function converts a browser keyboard event to a PlayCanvas keyboard event. The `toKeyCode` function converts a string or keycode to a keycode. The `toKeyIdentifier` function converts a keycode to a key identifier.\n\nOverall, the `Keyboard` class provides a convenient way to handle keyboard events in a PlayCanvas application. It allows the detection of the state of key presses and provides events that can be used to trigger actions in the application.\n## Questions: \n 1. What is the purpose of the `makeKeyboardEvent` function?\n- The `makeKeyboardEvent` function converts a browser keyboard event to a PlayCanvas keyboard event.\n\n2. What is the difference between `isPressed` and `wasPressed` methods in the `Keyboard` class?\n- The `isPressed` method returns true if the key is currently down, while the `wasPressed` method returns true if the key was pressed since the last update.\n\n3. What is the purpose of the `_handleVisibilityChange` method in the `Keyboard` class?\n- The `_handleVisibilityChange` method handles the browser visibilitychange event and calls `_handleWindowBlur` method if the document is hidden.","metadata":{"source":".autodoc/docs/markdown/src/platform/input/keyboard.md"}}],["466",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/input/mouse-event.js)\n\nThe code defines two entities: a function and a class. The function `isMousePointerLocked()` checks if the pointer lock is enabled in the browser. The class `MouseEvent` represents a mouse event object that is passed to events such as 'mousemove', 'mouseup', 'mousedown', and 'mousewheel'. \n\nThe `isMousePointerLocked()` function returns a boolean value indicating whether the pointer lock is enabled or not. The pointer lock is a feature that allows the mouse cursor to be locked to a specific element on the page, so that the cursor is hidden and the mouse movements are restricted to that element. This function checks if the pointer lock is enabled by checking if the `document.pointerLockElement`, `document.mozPointerLockElement`, or `document.webkitPointerLockElement` properties are defined. If any of these properties are defined, it means that the pointer lock is enabled.\n\nThe `MouseEvent` class represents a mouse event object that is passed to events such as 'mousemove', 'mouseup', 'mousedown', and 'mousewheel'. The constructor of this class takes two arguments: a `Mouse` device and the original browser event that fired. The `Mouse` device is an object that represents the mouse and provides methods for handling mouse events. The `MouseEvent` object has several properties that represent the state of the mouse during the event. \n\nThe `x` and `y` properties represent the x and y coordinates of the mouse pointer relative to the element that the `Mouse` device is attached to. The `wheelDelta` property represents the amount the mouse wheel has moved, and is only valid for `mousewheel` events. The `dx` and `dy` properties represent the change in x and y coordinates since the last mouse event. The `button` property represents the mouse button associated with the event, and can be `MOUSEBUTTON_LEFT`, `MOUSEBUTTON_MIDDLE`, or `MOUSEBUTTON_RIGHT`. The `buttons` property is an array that represents the state of all mouse buttons during the event. The `element` property represents the element that the mouse was fired from. The `ctrlKey`, `altKey`, `shiftKey`, and `metaKey` properties represent the state of the modifier keys during the event. Finally, the `event` property represents the original browser event that fired.\n\nThis class is used to create `MouseEvent` objects that can be passed to event handlers for mouse events. These objects provide a convenient way to access the state of the mouse during the event, and can be used to perform various actions based on the state of the mouse. For example, the `x` and `y` properties can be used to determine the position of the mouse cursor on the screen, and the `button` property can be used to determine which mouse button was pressed.\n## Questions: \n 1. What is the purpose of the `isMousePointerLocked` function?\n- The `isMousePointerLocked` function checks if pointer lock is currently enabled and returns a boolean value indicating whether it is enabled or not.\n\n2. What is the `MouseEvent` class used for?\n- The `MouseEvent` class is used to create a new instance of a mouse event object that is passed to events such as 'mousemove', 'mouseup', 'mousedown' and 'mousewheel'.\n\n3. What is the purpose of the `wheelDelta` property in the `MouseEvent` class?\n- The `wheelDelta` property represents the amount the mouse wheel has moved and is only valid for `mousewheel` events. It is used to determine the direction and amount of the mouse wheel movement.","metadata":{"source":".autodoc/docs/markdown/src/platform/input/mouse-event.md"}}],["467",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/input/mouse.js)\n\nThe code defines a `Mouse` class that represents a mouse device bound to a DOM element. The class extends the `EventHandler` class and provides methods to attach and detach mouse events to/from an element, enable/disable the context menu, and request/release the mouse pointer lock. The class also provides methods to check if the mouse pointer is locked, update the mouse state, and check if a mouse button is pressed, was pressed, or was released.\n\nThe `Mouse` class constructor takes an optional `element` parameter that specifies the element to attach the mouse events to. The constructor initializes the mouse state by setting the last mouse position and button states to zero. It also sets up event handlers for mouse up, down, move, and wheel events, and a context menu handler that prevents the default context menu from appearing. The `attach` method attaches the mouse events to the specified element, and the `detach` method removes the mouse events from the attached element. The `disableContextMenu` and `enableContextMenu` methods respectively disable and enable the context menu for the attached element. The `enablePointerLock` and `disablePointerLock` methods respectively request and release the mouse pointer lock, and take optional success and error callbacks. The `isPointerLocked` method checks if the mouse pointer is locked. The `update` method updates the mouse state by copying the current button state to the last button state. The `isPressed`, `wasPressed`, and `wasReleased` methods respectively check if a mouse button is currently pressed, was pressed since the last update, or was released since the last update. The `_handleUp`, `_handleDown`, `_handleMove`, and `_handleWheel` methods respectively handle mouse up, down, move, and wheel events by updating the mouse state and firing corresponding mouse events.\n\nThe `Mouse` class can be used in the larger project to handle mouse input for interactive applications and games. For example, it can be used to move a player character in response to mouse movement, or to fire a weapon in response to mouse clicks. The `Mouse` class can also be used to implement custom mouse cursors, or to enable/disable the context menu for specific elements. The `Mouse` class can be instantiated with an element parameter to attach the mouse events to a specific element, or without an element parameter to attach the mouse events to the window. The `Mouse` class can be extended or modified to add custom mouse event handlers or to support additional mouse input features.\n## Questions: \n 1. What is the purpose of the `Mouse` class?\n- The `Mouse` class is a device that is bound to a DOM element and handles mouse events such as mouse movement, button presses, and wheel movement.\n\n2. What is the difference between `isPressed`, `wasPressed`, and `wasReleased` methods?\n- The `isPressed` method returns true if the specified mouse button is currently pressed. The `wasPressed` method returns true if the specified mouse button was pressed since the last call to `update`. The `wasReleased` method returns true if the specified mouse button was released since the last call to `update`.\n\n3. What is the purpose of the `enablePointerLock` method?\n- The `enablePointerLock` method requests that the browser hides the mouse cursor and locks the mouse to the element, allowing raw access to mouse movement input without risking the mouse exiting the element.","metadata":{"source":".autodoc/docs/markdown/src/platform/input/mouse.md"}}],["468",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/input/touch-device.js)\n\nThe code defines a class called `TouchDevice` that extends the `EventHandler` class. The purpose of this class is to attach a `TouchDevice` to an HTML element and listen for touch events on that element. When a touch event occurs, the `TouchDevice` will fire an event that can be listened to by other parts of the code.\n\nThe `TouchDevice` constructor takes an HTML element as an argument and sets up event listeners for touchstart, touchend, touchmove, and touchcancel events on that element. When one of these events occurs, the corresponding method (`_handleTouchStart`, `_handleTouchEnd`, `_handleTouchMove`, or `_handleTouchCancel`) is called, which creates a new `TouchEvent` object and fires an event with that object.\n\nThe `TouchEvent` class is not defined in this file, but it is imported from another file called `touch-event.js`. This class is responsible for creating a new touch event object that contains information about the touch event, such as the touch position, touch type, and touch target.\n\nThe `TouchDevice` class provides two methods: `attach` and `detach`. The `attach` method takes an HTML element as an argument and attaches the `TouchDevice` to that element. If the `TouchDevice` is already attached to an element, the `detach` method is called first to detach it from the current element. The `detach` method removes the event listeners from the current element and sets the `_element` property to `null`.\n\nOverall, the `TouchDevice` class provides a way to listen for touch events on an HTML element and fire events when those touch events occur. This can be useful for implementing touch-based interactions in a web application or game. For example, a game might use a `TouchDevice` to listen for touch events on a canvas element and use those events to control the game's characters or objects.\n## Questions: \n 1. What is the purpose of the `TouchEvent` import?\n- The `TouchEvent` import is used to create new `TouchEvent` objects that are passed to event listeners when touch events occur.\n\n2. What is the purpose of the `attach` method?\n- The `attach` method is used to attach the `TouchDevice` to a new element in the DOM, and it detaches the device from any previously attached element.\n\n3. Why is `preventDefault` called in the `_handleTouchMove` method?\n- `preventDefault` is called in the `_handleTouchMove` method to avoid issues in Chrome Android when touch events are fired.","metadata":{"source":".autodoc/docs/markdown/src/platform/input/touch-device.md"}}],["469",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/input/touch-event.js)\n\nThe code defines three classes and a function that are used to handle touch events in the PlayCanvas engine. The `getTouchTargetCoords` function takes a browser Touch object and returns the coordinates of the touch relative to the target element. This function is used to calculate the x and y coordinates of a touch event relative to the element that the TouchDevice is attached to. The `Touch` class represents a single point touch on a TouchDevice. It has properties for the touch identifier, x and y coordinates, target element, and the original browser Touch object. The `TouchEvent` class is an event that corresponds to touchstart, touchend, touchmove, or touchcancel. It wraps the standard browser event and provides lists of Touch objects. It has properties for the target element, the original browser TouchEvent, a list of all touches currently in contact with the device, and a list of touches that have changed since the last event. The `getTouchById` method is used to get an event from one of the touch lists by the id. It is useful to access touches by their id so that you can be sure you are referencing the same touch.\n\nThese classes and function are used to handle touch events in the PlayCanvas engine. They are used to create and manage Touch and TouchEvent objects that are used by other parts of the engine to handle touch input. For example, the engine may use these objects to detect when a user has touched the screen and to determine the location of the touch. This information can then be used to trigger events or to update the state of the game. Here is an example of how the `TouchEvent` class might be used in the engine:\n\n```\nconst touchDevice = new TouchDevice(canvas);\n\ncanvas.addEventListener('touchstart', function(event) {\n    const touchEvent = new TouchEvent(touchDevice, event);\n    const touch = touchEvent.touches[0];\n    console.log(`Touch started at (${touch.x}, ${touch.y})`);\n});\n```\n\nIn this example, a new `TouchDevice` object is created and attached to the canvas element. An event listener is added to the canvas element for the touchstart event. When the event is triggered, a new `TouchEvent` object is created from the event and the `TouchDevice` object. The first touch in the `touches` list is then retrieved and its x and y coordinates are logged to the console. This is just one example of how these classes and function might be used in the PlayCanvas engine.\n## Questions: \n 1. What does the `getTouchTargetCoords` function do?\n- The `getTouchTargetCoords` function takes a browser Touch object and returns the coordinates of the touch relative to the target element.\n\n2. What is the purpose of the `Touch` class?\n- The `Touch` class is an instance of a single point touch on a TouchDevice, which contains information about the touch such as its identifier, x and y coordinates, target element, and the original browser Touch object.\n\n3. What is the `TouchEvent` class and what does it contain?\n- The `TouchEvent` class is an event corresponding to touchstart, touchend, touchmove or touchcancel, which wraps the standard browser event and provides lists of `Touch` objects. It contains information about the target element, the original browser TouchEvent, a list of all touches currently in contact with the device, and a list of touches that have changed since the last event.","metadata":{"source":".autodoc/docs/markdown/src/platform/input/touch-event.md"}}],["470",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/sound/instance3d.js)\n\nThe code defines a class called `SoundInstance3d` that extends the `SoundInstance` class. This class is used to play a `Sound` in 3D space. The class has several properties, including `_position`, `_velocity`, `maxDistance`, `refDistance`, `rollOffFactor`, and `distanceModel`. The `_position` and `_velocity` properties are of type `Vec3` and represent the position and velocity of the sound in 3D space. The `maxDistance` property is the maximum distance from the listener at which audio falloff stops. The `refDistance` property is the reference distance for reducing volume as the sound source moves further from the listener. The `rollOffFactor` property is the factor used in the falloff equation. The `distanceModel` property determines which algorithm to use to reduce the volume of the audio as it moves away from the listener.\n\nThe `SoundInstance3d` class has a constructor that takes a `manager`, a `sound`, and an `options` object as arguments. The `manager` argument is the sound manager, the `sound` argument is the sound to play, and the `options` argument is an object that can contain several optional properties, including `volume`, `pitch`, `loop`, `startTime`, `duration`, `position`, `distanceModel`, `refDistance`, `maxDistance`, and `rollOffFactor`. If the `position` property is specified in the `options` object, the `_position` property is set to that value. Otherwise, the `_position` property is initialized to a new `Vec3` object. The `maxDistance`, `refDistance`, `rollOffFactor`, and `distanceModel` properties are set to the values specified in the `options` object, or to default values if they are not specified.\n\nThe `SoundInstance3d` class has several methods, including `_initializeNodes()`, `set position(value)`, `get position()`, `set velocity(velocity)`, `get velocity()`, `set maxDistance(value)`, `get maxDistance()`, `set refDistance(value)`, `get refDistance()`, `set rollOffFactor(value)`, and `get rollOffFactor()`. The `_initializeNodes()` method allocates Web Audio resources for this instance. The `set position(value)` method sets the `_position` property to the specified value and updates the position of the sound in 3D space. The `get position()` method returns the `_position` property. The `set velocity(velocity)` method sets the `_velocity` property to the specified value. The `get velocity()` method returns the `_velocity` property. The `set maxDistance(value)` method sets the `maxDistance` property to the specified value. The `get maxDistance()` method returns the `maxDistance` property. The `set refDistance(value)` method sets the `refDistance` property to the specified value. The `get refDistance()` method returns the `refDistance` property. The `set rollOffFactor(value)` method sets the `rollOffFactor` property to the specified value. The `get rollOffFactor()` method returns the `rollOffFactor` property.\n\nIf the `hasAudioContext()` function returns `false`, the code defines several properties on the `SoundInstance3d` prototype object, including `position`, `maxDistance`, `refDistance`, `rollOffFactor`, and `distanceModel`. These properties are used to simulate 3D audio in browsers that do not support the Web Audio API. The `position` property is updated to simulate the position of the sound in 3D space. The `maxDistance`, `refDistance`, `rollOffFactor`, and `distanceModel` properties are used to calculate the volume of the sound based on its distance from the listener.\n\nOverall, the `SoundInstance3d` class is an important part of the PlayCanvas engine that allows sounds to be played in 3D space. The class provides several properties and methods that can be used to control the position, velocity, and volume of the sound. The class also provides a fallback mechanism for browsers that do not support the Web Audio API.\n## Questions: \n 1. What is the purpose of the `SoundInstance3d` class and how does it differ from the `SoundInstance` class it extends?\n- The `SoundInstance3d` class plays a `Sound` in 3D space and has additional properties related to 3D audio such as position, velocity, and distance model. It differs from the `SoundInstance` class in that it is specifically designed for 3D audio playback.\n\n2. What is the purpose of the `hasAudioContext` function and how is it used in this code?\n- The `hasAudioContext` function checks if the browser supports the Web Audio API and returns a boolean value. It is used to conditionally define properties of the `SoundInstance3d` class for browsers that do not support the Web Audio API.\n\n3. What is the purpose of the `fallOff` function and how is it used in this code?\n- The `fallOff` function calculates the volume reduction of a sound based on its distance from the listener and the specified distance model. It is used to adjust the volume of a `SoundInstance3d` based on its position relative to the listener.","metadata":{"source":".autodoc/docs/markdown/src/platform/sound/instance3d.md"}}],["471",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/sound/listener.js)\n\nThe code defines a class called `Listener` that represents an audio listener. This class is used internally in the PlayCanvas engine project. The purpose of the `Listener` class is to provide methods for getting and setting the position, velocity, and orientation of the listener. \n\nThe `Listener` class has a constructor that takes a `SoundManager` object as a parameter. The `SoundManager` object is used to get the audio context and the listener object. The `Listener` class has three private properties: `position`, `velocity`, and `orientation`. These properties are instances of the `Vec3` and `Mat4` classes from the PlayCanvas engine's `core/math` module. \n\nThe `Listener` class has several methods for getting and setting the position, velocity, and orientation of the listener. The `getPosition()` method returns the position of the listener as a `Vec3` object. The `setPosition(position)` method sets the position of the listener to the specified `Vec3` object. This method also updates the position of the listener object in the audio context. \n\nThe `getVelocity()` and `setVelocity(velocity)` methods are deprecated and do not have any implementation. The `setOrientation(orientation)` method sets the orientation matrix of the listener to the specified `Mat4` object. This method also updates the orientation of the listener object in the audio context. The `getOrientation()` method returns the orientation matrix of the listener as a `Mat4` object. \n\nThe `listener` property is a getter that returns the listener object from the audio context. This property is used internally to update the position and orientation of the listener object in the audio context. \n\nOverall, the `Listener` class provides a simple interface for getting and setting the position, velocity, and orientation of the listener object in the audio context. This class is used internally in the PlayCanvas engine project to manage audio playback. \n\nExample usage:\n\n```javascript\nimport { Listener } from 'playcanvas-sound';\n\nconst soundManager = new SoundManager();\nconst listener = new Listener(soundManager);\n\n// Set the position of the listener\nconst position = new Vec3(0, 0, 0);\nlistener.setPosition(position);\n\n// Set the orientation of the listener\nconst orientation = new Mat4();\norientation.setLookAt(position, new Vec3(0, 0, 1), new Vec3(0, 1, 0));\nlistener.setOrientation(orientation);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `Listener` that represents an audio listener used internally in the `SoundManager` of the PlayCanvas engine.\n\n2. What methods does the `Listener` class have?\n- The `Listener` class has methods for getting and setting the position, velocity, and orientation of the listener, as well as a getter method for retrieving the listener object itself.\n\n3. What is the purpose of the `Debug` object and the `Debug.warn` method calls?\n- The `Debug` object is used for logging debug messages, and the `Debug.warn` method calls are used to warn developers that the `getVelocity` and `setVelocity` methods are deprecated and not implemented.","metadata":{"source":".autodoc/docs/markdown/src/platform/sound/listener.md"}}],["472",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/sound/manager.js)\n\nThe code defines a SoundManager class that is responsible for loading and playing audio in a PlayCanvas engine project. The SoundManager class extends the EventHandler class, which allows it to handle events. The class has a constructor that initializes some properties, including the AudioContext, which is used to play audio. The AudioContext is lazy-loaded, meaning it is only created when it is needed. If the AudioContext is not supported, a warning message is logged to the console.\n\nThe SoundManager class has several methods that allow it to control audio playback. The `playSound` method creates a new Channel object and begins playback of a sound. The `playSound3d` method creates a new Channel3d object and begins playback of a sound at a specified position in 3D space. Both methods take a Sound object as their first argument and an optional options object as their second argument. The options object can be used to set properties such as volume, loop, maxDistance, minDistance, rollOffFactor, and distanceModel.\n\nThe SoundManager class also has methods for suspending and resuming audio playback. When audio is suspended, the context is paused, and when it is resumed, the context is resumed. The class also has a `destroy` method that fires a destroy event and closes the AudioContext.\n\nThe SoundManager class has a `volume` property that can be used to set the global volume for the manager. All SoundInstances will scale their volume with this volume. The `suspended` property returns a boolean indicating whether audio playback is currently suspended.\n\nThe SoundManager class has a `context` property that returns the Web Audio API context. If the context has not been created yet, it is created when this property is accessed. If the context is not running, the class registers event listeners for user input events (click, touchstart, mousedown) that will unlock the context when triggered.\n\nOverall, the SoundManager class provides a way to load and play audio in a PlayCanvas engine project. It allows for control of audio playback, including volume, suspension, and resumption. It also handles the creation and unlocking of the Web Audio API context.\n## Questions: \n 1. What is the purpose of the `SoundManager` class?\n- The `SoundManager` class is used to load and play audio, as well as apply system-wide settings like global volume, suspend, and resume.\n\n2. What is the `context` property used for?\n- The `context` property is used to get the Web Audio API context, which is lazily created if it doesn't exist yet. It also registers unlock listeners if the context state is not running.\n\n3. What are the `playSound` and `playSound3d` methods used for?\n- The `playSound` method is used to create a new `Channel` and begin playback of the sound, while the `playSound3d` method is used to create a new `Channel3d` and begin playback of the sound at the specified position in 3D space. Both methods return the channel playing the sound.","metadata":{"source":".autodoc/docs/markdown/src/platform/sound/manager.md"}}],["473",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/platform/sound/sound.js)\n\nThe code defines a class called `Sound` which represents the resource of an audio asset. The class has two properties: `audio` and `buffer`. The `audio` property is of type `HTMLAudioElement` and contains the audio data if the Web Audio API is not supported. On the other hand, the `buffer` property is of type `AudioBuffer` and contains the audio data if the Web Audio API is supported. \n\nThe class has a constructor that takes a parameter called `resource`. If the `resource` parameter is an instance of `Audio`, then the `audio` property is set to the `resource` parameter. Otherwise, the `buffer` property is set to the `resource` parameter.\n\nThe class also has a getter method called `duration` which returns the duration of the sound. If the sound is not loaded, it returns 0. The duration is obtained from either the `buffer` property or the `audio` property depending on which one is set.\n\nThis class is likely used in the larger PlayCanvas engine project to handle audio assets. It provides a way to represent audio resources and get their duration. Other parts of the engine can use this class to load and play audio assets. For example, a game developer using the PlayCanvas engine can create instances of the `Sound` class to represent sound effects or background music in their game. They can then use the `duration` method to get the length of the audio asset and play it using other parts of the engine. \n\nExample usage:\n\n```javascript\n// create a new Sound instance with an AudioBuffer object\nconst audioContext = new AudioContext();\nconst buffer = await audioContext.decodeAudioData(audioData);\nconst sound = new Sound(buffer);\n\n// get the duration of the sound\nconst duration = sound.duration;\n\n// play the sound using the PlayCanvas engine\nplayCanvas.audio.playSound(sound);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `Sound` that represents the resource of an audio asset.\n\n2. What is the difference between `audio` and `buffer` properties?\n- `audio` property contains the audio data if the Web Audio API is not supported, while `buffer` property contains the audio data if the Web Audio API is supported.\n\n3. How can the duration of the sound be obtained?\n- The duration of the sound can be obtained by accessing the `duration` getter property of the `Sound` instance. If the sound is not loaded, it returns 0.","metadata":{"source":".autodoc/docs/markdown/src/platform/sound/sound.md"}}],["474",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/OESVertexArrayObject.js)\n\nThe code provided is a JavaScript implementation of the OES_vertex_array_object extension for WebGL. This extension provides a way to encapsulate vertex array state on the GPU, allowing for more efficient switching between different vertex array configurations. \n\nThe code defines two classes: `WebGLVertexArrayObjectOES` and `WebGLVertexArrayObjectOES.VertexAttrib`. The former represents a vertex array object, while the latter represents a single vertex attribute. The `OESVertexArrayObject` class is the main class that implements the extension. \n\nThe `OESVertexArrayObject` class wraps the WebGL context and intercepts calls to certain WebGL functions related to vertex arrays. It maintains a list of vertex array objects and keeps track of the currently bound vertex array object. When a new vertex array object is bound, the class updates the WebGL state to match the state of the new object. \n\nThe `setupVertexArrayObject` function is provided as a convenience function to set up the extension. It checks if the extension is already available and, if not, installs the `OESVertexArrayObject` class as the implementation of the extension. \n\nOverall, this code provides a way to efficiently manage vertex array state in WebGL applications. By encapsulating vertex array state on the GPU, it reduces the overhead of switching between different vertex array configurations.\n## Questions: \n 1. What is the purpose of the `setupVertexArrayObject` function?\n- The `setupVertexArrayObject` function is used to install the `OES_vertex_array_object` extension in WebGL contexts that do not already have it installed.\n\n2. What is the `WebGLVertexArrayObjectOES` class used for?\n- The `WebGLVertexArrayObjectOES` class is used to represent a vertex array object in WebGL.\n\n3. What is the purpose of the `glErrorShadow` object?\n- The `glErrorShadow` object is used to keep track of WebGL errors that occur during the execution of the code.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/OESVertexArrayObject.md"}}],["475",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/array-fill.js)\n\nThe code above is a polyfill for the `fill()` method of the `Array` object in JavaScript. A polyfill is a piece of code that provides functionality that is not natively supported by a browser or runtime environment. In this case, the `fill()` method is not supported in some older browsers, so this code provides a way to use it in those environments.\n\nThe `fill()` method is used to fill all the elements of an array with a static value. It takes up to three arguments: the value to fill the array with, the starting index to begin filling from, and the ending index to stop filling at. If the starting and ending indices are not provided, the entire array is filled.\n\nThe code begins by importing a function called `defineProtoFunc` from another file. This function is used to define a new method on the `Array` prototype, which is the object that all arrays inherit from. The `fill()` method is then defined using this function.\n\nThe `fill()` method implementation follows the steps outlined in the ECMAScript specification for the method. It first checks if the `this` value is null or undefined, and throws an error if it is. It then creates a new object `O` that is a reference to the `this` value, ensuring that the method can be called on any array object.\n\nThe method then extracts the length of the array and the starting and ending indices from the arguments passed to it. It uses bitwise operators to ensure that the indices are integers, and calculates the final index to fill up to.\n\nFinally, the method loops through the array and sets each element to the provided value until it reaches the final index. It then returns the modified array.\n\nOverall, this code provides a way to use the `fill()` method on arrays in environments where it is not natively supported. It can be used in the larger PlayCanvas engine project to ensure consistent behavior across different browsers and runtime environments. An example usage of the `fill()` method with this polyfill would be:\n\n```\nconst arr = new Array(5);\narr.fill(0); // fills the array with 0s\n```\n## Questions: \n 1. What is the purpose of the `defineProtoFunc` function being imported at the beginning of the code?\n- The `defineProtoFunc` function is used to define a new function on the prototype of a given object.\n\n2. What is the `fill` function doing?\n- The `fill` function is a polyfill for the `Array.fill` method, which fills all the elements of an array with a static value.\n\n3. What is the purpose of the bitwise operators (`>>>`, `>>`) used in the code?\n- The bitwise operators are used to convert the arguments passed to the `fill` function into integers, as they may be passed as strings or other types.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/array-fill.md"}}],["476",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/array-find-index.js)\n\nThe code defines a new function called `findIndex` on the `Array` prototype. This function is used to find the index of the first element in an array that satisfies a given condition. The function takes a single argument, `predicate`, which is a function that takes three arguments: the current element being processed, its index in the array, and the array itself. The function returns a boolean value indicating whether the current element satisfies the condition.\n\nThe `findIndex` function first checks that the `this` value is not null or undefined, and then creates a new object `o` that is a reference to the `this` value. It then gets the length of the array and checks that the `predicate` argument is a function. If `thisArg` is supplied as a second argument, it is used as the `this` value inside the `predicate` function. Otherwise, `undefined` is used.\n\nThe function then loops through each element in the array, calling the `predicate` function with the current element, its index, and the array itself as arguments. If the `predicate` function returns `true`, the index of the current element is returned. If no element satisfies the condition, `-1` is returned.\n\nThis function can be used in a variety of scenarios where it is necessary to find the index of the first element in an array that satisfies a given condition. For example, it could be used to find the index of the first negative number in an array of integers:\n\n```\nconst arr = [1, 2, -3, 4, -5];\nconst index = arr.findIndex(num => num < 0);\nconsole.log(index); // Output: 2\n```\n\nIn this example, the `findIndex` function is used to find the index of the first negative number in the `arr` array. The `predicate` function checks whether the current element is less than zero, and returns `true` if it is. The function returns the index of the first negative number, which is `2`.\n## Questions: \n 1. What is the purpose of the `defineProtoFunc` function being imported at the beginning of the code?\n    \n    `defineProtoFunc` is a function being imported from another file that is used to define a new function on the prototype of an object.\n\n2. What is the purpose of the `findIndex` function being defined on the `Array` object?\n    \n    The `findIndex` function is being defined on the `Array` object to allow developers to find the index of the first element in an array that satisfies a given condition.\n\n3. What happens if the `predicate` argument passed to the `findIndex` function is not a function?\n    \n    If the `predicate` argument passed to the `findIndex` function is not a function, a `TypeError` exception will be thrown.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/array-find-index.md"}}],["477",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/array-find.js)\n\nThe code above is a polyfill for the `Array.prototype.find()` method, which is used to find the first element in an array that satisfies a given condition. The purpose of this code is to provide a fallback implementation of the `find()` method for older browsers that do not support it natively.\n\nThe `defineProtoFunc()` function is imported from another module and is used to define a new method on the `Array` prototype. The first argument passed to `defineProtoFunc()` is the object on which the new method will be defined, which in this case is the `Array` constructor. The second argument is the name of the new method, which is `'find'`. The third argument is a function that will be called when the new method is invoked.\n\nThe `find()` function defined in this code follows the specification outlined in the ECMAScript 2019 standard. It takes a single argument, `predicate`, which is a function that will be called for each element in the array. The `predicate` function should return `true` if the current element satisfies the condition, and `false` otherwise.\n\nThe `find()` function first checks that `this` is not `null` or `undefined`, and then creates a new object `o` that is a reference to `this`. It then gets the length of the array and checks that the `predicate` argument is a function. If `thisArg` is provided as a second argument, it is used as the `this` value inside the `predicate` function. Otherwise, `undefined` is used.\n\nThe function then loops through each element in the array, calling the `predicate` function with the current element, its index, and the array itself as arguments. If the `predicate` function returns `true`, the current element is returned. If no element satisfies the condition, `undefined` is returned.\n\nOverall, this code provides a fallback implementation of the `Array.prototype.find()` method for older browsers that do not support it natively. It can be used in the larger PlayCanvas engine project to ensure that the `find()` method works consistently across all browsers. For example, if the engine needs to find a specific entity in an array of entities, it can use the `find()` method to do so.\n## Questions: \n 1. What is the purpose of the `defineProtoFunc` function being imported at the beginning of the code?\n    \n    Answer: The `defineProtoFunc` function is used to define a new function on the prototype of a given object.\n\n2. What is the purpose of the `find` function being defined on the `Array` object?\n    \n    Answer: The `find` function is used to search an array for the first element that satisfies a given condition, as defined by the `predicate` function.\n\n3. What happens if the `predicate` argument passed to the `find` function is not a function?\n    \n    Answer: If the `predicate` argument is not a function, a `TypeError` exception is thrown with the message \"predicate must be a function\".","metadata":{"source":".autodoc/docs/markdown/src/polyfill/array-find.md"}}],["478",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/defineProtoFunc.js)\n\nThe code defines a function called `defineProtoFunc` that is used to polyfill prototype methods that are not iterated in for-in loops. This function takes in three parameters: `cls`, `name`, and `func`. \n\nThe `cls` parameter is an object constructor, `name` is a string representing the name of the prototype method to be polyfilled, and `func` is the function that will be used to polyfill the method. \n\nThe purpose of this function is to add a new method to an object's prototype if it does not already exist. This is done by using the `Object.defineProperty` method to define a new property on the object's prototype with the given `name` and `func`. The `configurable` property is set to `true` to allow the property to be deleted later if needed, and `enumerable` is set to `false` to prevent the property from being included in for-in loops. \n\nThis function is likely used throughout the PlayCanvas engine project to add new methods to object prototypes as needed. For example, if a new method is added to the `pc.Entity` prototype, this function could be used to ensure that the method is available on all `pc.Entity` instances. \n\nHere is an example of how this function could be used to add a new method to an object's prototype:\n\n```\nfunction MyObject() {\n  // constructor code\n}\n\ndefineProtoFunc(MyObject, 'myMethod', function() {\n  // method code\n});\n\nvar obj = new MyObject();\nobj.myMethod(); // calls the polyfilled method\n```\n## Questions: \n 1. What is the purpose of this function?\n    \n    This function is a shorthand method to polyfill prototype methods that are not iterated in for-in loops.\n\n2. What are the parameters of this function and what do they represent?\n    \n    The parameters of this function are `cls`, `name`, and `func`. `cls` represents the object constructor, `name` represents the name of the prototype method, and `func` represents the function to be added to the prototype.\n\n3. What does the `@ignore` tag in the JSDoc comment mean?\n    \n    The `@ignore` tag in the JSDoc comment indicates that this function should be ignored by the documentation generator.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/defineProtoFunc.md"}}],["479",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/math-log2.js)\n\nThe code above is a polyfill for the Math.log2() method. A polyfill is a piece of code that provides functionality that is not natively supported by a browser or runtime environment. In this case, the Math.log2() method is not supported by all browsers, so this polyfill provides a way to use it regardless of the browser being used.\n\nThe Math.log2() method returns the base-2 logarithm of a number. It is useful in various mathematical calculations, such as determining the number of bits needed to represent a number in binary form. The polyfill code checks if the Math.log2() method is already defined, and if not, it defines it using the Math.log() method and the Math.LOG2E constant.\n\nThis polyfill can be used in any JavaScript project that requires the Math.log2() method. For example, if a project needs to calculate the number of bits needed to represent a number in binary form, it can use the Math.log2() method. If the project needs to support older browsers that do not have the Math.log2() method, it can include this polyfill to ensure that the method is available.\n\nHere is an example of how the Math.log2() method can be used:\n\n```\nconst num = 16;\nconst bitsNeeded = Math.ceil(Math.log2(num));\nconsole.log(bitsNeeded); // Output: 4\n```\n\nIn this example, the Math.log2() method is used to calculate the number of bits needed to represent the number 16 in binary form. The Math.ceil() method is used to round up the result to the nearest integer, since the number of bits needed must be a whole number. The result is 4, which means that 4 bits are needed to represent the number 16 in binary form.\n## Questions: \n 1. **What is the purpose of this code?** \nThis code is a polyfill for the Math.log2 function, which calculates the base-2 logarithm of a number. It ensures that the function is available even if it is not supported by the browser.\n\n2. **Why is a polyfill necessary for Math.log2?** \nSome older browsers may not support the Math.log2 function, so a polyfill is used to provide a fallback implementation that can be used in those cases.\n\n3. **How does the polyfill work?** \nThe polyfill simply checks if the Math.log2 function is already defined, and if not, it defines it as a function that calculates the base-2 logarithm using the Math.log function and the Math.LOG2E constant. This ensures that the function is available for use in all browsers, regardless of whether or not they support it natively.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/math-log2.md"}}],["480",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/math-sign.js)\n\nThis code is a polyfill for the Math.sign() method in JavaScript. The Math.sign() method returns the sign of a number, indicating whether the number is positive, negative, or zero. If the number is positive, the method returns 1. If the number is negative, the method returns -1. If the number is zero, the method returns 0. \n\nThe purpose of this polyfill is to provide a fallback implementation of the Math.sign() method for browsers that do not support it. The code checks if the Math.sign() method is already defined, and if not, it defines a new implementation of the method using a ternary operator. \n\nThe ternary operator checks if the input number is greater than 0, and if so, returns 1. If the input number is less than 0, the operator returns -1. If the input number is 0, the operator returns 0. \n\nThe polyfill also includes some comments that explain the behavior of the Math.sign() method for different input values. For example, if the input number is NaN, the method returns NaN. If the input number is -0, the method returns -0. If the input number is +0, the method returns +0. \n\nIn the larger PlayCanvas engine project, this polyfill may be used to ensure that the Math.sign() method works consistently across different browsers and platforms. By providing a fallback implementation of the method, the project can avoid compatibility issues and ensure that the sign of a number is always calculated correctly. \n\nExample usage of the Math.sign() method:\n\n```\nMath.sign(5); // returns 1\nMath.sign(-5); // returns -1\nMath.sign(0); // returns 0\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code is a polyfill for the Math.sign() method, which returns the sign of a number (positive, negative, or zero).\n\n2. Why is a polyfill needed for Math.sign()?\n    \n    Math.sign() is not supported in all browsers, so a polyfill is needed to ensure consistent behavior across different environments.\n\n3. How does the polyfill work?\n    \n    The polyfill uses a series of conditional statements to determine the sign of the input number, and returns either 1, -1, or 0 depending on the sign. If the input is NaN or not a number, it returns NaN.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/math-sign.md"}}],["481",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/number-isfinite.js)\n\nThe code above is a polyfill for the `Number.isFinite()` method. This method is used to determine whether a given value is a finite number or not. If the value is a finite number, the method returns `true`, otherwise it returns `false`. \n\nThe polyfill is necessary because the `Number.isFinite()` method is not supported in all browsers. The code checks if the method is already defined, and if not, it defines it using a function that checks if the value is a number and if it is finite. \n\nThis polyfill can be used in the larger PlayCanvas engine project to ensure that the `Number.isFinite()` method is available in all browsers, regardless of whether or not it is natively supported. This is important because the PlayCanvas engine relies on this method to perform calculations and other operations that require the use of finite numbers. \n\nHere is an example of how the `Number.isFinite()` method can be used in the PlayCanvas engine:\n\n```javascript\nvar num = 10;\nif (Number.isFinite(num)) {\n    console.log(\"The number is finite.\");\n} else {\n    console.log(\"The number is not finite.\");\n}\n```\n\nIn this example, the `Number.isFinite()` method is used to check if the variable `num` is a finite number. If it is, the code logs \"The number is finite.\" to the console. If it is not, the code logs \"The number is not finite.\" to the console. \n\nOverall, the polyfill for the `Number.isFinite()` method is a small but important piece of code that helps ensure the PlayCanvas engine can function properly in all browsers.\n## Questions: \n 1. What is the purpose of this code?\n   - This code is a polyfill for the `Number.isFinite` method, which checks if a given value is a finite number.\n\n2. Why is a polyfill needed for `Number.isFinite`?\n   - `Number.isFinite` is a relatively new method in JavaScript and may not be supported by all browsers. A polyfill is used to provide a fallback implementation for browsers that do not support the method.\n\n3. How does this polyfill work?\n   - The polyfill checks if the `Number.isFinite` method is undefined and if so, defines a new function that checks if the given value is a number and if it is finite using the `isFinite` method.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/number-isfinite.md"}}],["482",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/object-assign.js)\n\nThe code above is a polyfill for the `Object.assign()` method. The `Object.assign()` method is used to copy the values of all enumerable properties from one or more source objects to a target object. This method is used to merge objects or to create a shallow copy of an object. \n\nThe purpose of this polyfill is to provide a fallback implementation of the `Object.assign()` method for older browsers that do not support it. The code checks if the `Object.assign()` method is already defined and if not, it defines it as a new method on the `Object` constructor. \n\nThe `Object.defineProperty()` method is used to define the `Object.assign()` method. The `value` property of the method is set to a function that takes two arguments: `target` and `varArgs`. The `target` argument is the object that will receive the properties, and the `varArgs` argument is an array of objects that contain the properties to be copied. \n\nThe function first checks if the `target` argument is `null` or `undefined`. If it is, a `TypeError` is thrown. The function then creates a new object `to` using the `Object()` constructor and assigns it the properties of the `target` object. \n\nThe function then loops through the `varArgs` array and assigns the properties of each object to the `to` object. If the current object in the loop is `null` or `undefined`, it is skipped. If the current object has a property that is not already defined in the `to` object, that property is added to the `to` object. \n\nFinally, the `to` object is returned. \n\nThis polyfill is important for ensuring that the `Object.assign()` method works consistently across all browsers. It can be used in any project that relies on the `Object.assign()` method, including the PlayCanvas engine. \n\nExample usage:\n\n```\nconst obj1 = { a: 1 };\nconst obj2 = { b: 2 };\nconst obj3 = { c: 3 };\n\nconst mergedObj = Object.assign(obj1, obj2, obj3);\n\nconsole.log(mergedObj); // { a: 1, b: 2, c: 3 }\n```\n## Questions: \n 1. What is the purpose of this code?\n   - This code is a polyfill for the `Object.assign` method, which is used to copy the values of all enumerable properties from one or more source objects to a target object.\n\n2. What is the significance of the `writable`, `enumerable`, and `configurable` properties in the `Object.defineProperty` method?\n   - These properties determine the behavior of the `Object.assign` method when it is called on the target object. `writable` specifies whether the target object can be modified, `enumerable` specifies whether the properties can be enumerated, and `configurable` specifies whether the properties can be deleted or have their attributes changed.\n\n3. What is the purpose of the `if (typeof Object.assign != 'function')` statement?\n   - This statement checks whether the `Object.assign` method is already defined in the current environment. If it is not defined, the polyfill is added to the `Object` prototype so that it can be used.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/object-assign.md"}}],["483",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/object-values.js)\n\nThe code above is a polyfill for the `Object.values()` method, which is used to extract the values of an object and return them as an array. This method is not supported in all browsers, so this code provides a fallback for those that do not support it.\n\nThe `Object.values()` method takes an object as its parameter and returns an array containing the values of all the enumerable properties of the object. This is useful when you need to extract the values of an object and use them in an array format.\n\nThe code above checks if the `Object.values()` method is already defined. If it is not defined, it creates a new function that uses the `Object.keys()` method to extract the keys of the object and then uses the `map()` method to return an array of the values of each key.\n\nHere is an example of how this code can be used:\n\n```\nconst myObject = { a: 1, b: 2, c: 3 };\nconst myArray = Object.values(myObject);\nconsole.log(myArray); // [1, 2, 3]\n```\n\nIn this example, the `Object.values()` method is used to extract the values of the `myObject` object and store them in the `myArray` array.\n\nOverall, this code is a useful addition to the PlayCanvas engine project as it provides a fallback for the `Object.values()` method in browsers that do not support it. This ensures that the project can be used in a wider range of environments without encountering errors.\n## Questions: \n 1. **What does this code do?** \nThis code is defining a polyfill for the `Object.values()` method, which returns an array of the values of an object's enumerable properties.\n\n2. **Why is a polyfill necessary for `Object.values()`?** \nThe `Object.values()` method was introduced in ECMAScript 2017 and may not be supported by all browsers or environments. This polyfill ensures that the method is available even if it is not natively supported.\n\n3. **How does this polyfill work?** \nThe polyfill checks if `Object.values()` is already defined and, if not, defines it as a function that uses `Object.keys()` to get an array of the object's keys and then maps over that array to return an array of the corresponding values.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/object-values.md"}}],["484",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/pointer-lock.js)\n\nThe code above is responsible for applying PointerLock shims to the PlayCanvas engine. PointerLock is a browser API that allows for locking the mouse cursor to a specific element on the page, which is useful for games and other interactive applications. The code checks if the browser supports PointerLock and applies shims if necessary to ensure that the API works as expected.\n\nThe code starts by checking if the `navigator` and `document` objects are defined, which indicates that the code is running in a browser environment. If not, the code returns and does nothing. Otherwise, it sets the `navigator.pointer` object to the appropriate vendor-specific implementation of the PointerLock API (`navigator.webkitPointer` or `navigator.mozPointer`).\n\nNext, the code sets up event listeners for the `pointerlockchange` and `pointerlockerror` events, which are fired when the pointer lock state changes or an error occurs. The event listeners create custom events using the `document.createEvent()` method and dispatch them using the `document.dispatchEvent()` method.\n\nThe code then sets up `requestPointerLock` and `exitPointerLock` methods on the `Element.prototype` object. These methods are used to request and release pointer lock on an element. The code first checks if the browser supports the `mozRequestPointerLock` method, which is used by Firefox. If so, it sets the `requestPointerLock` method to call `mozRequestPointerLock()` on the element. Otherwise, it checks if the browser supports the standard `requestPointerLock` method or the vendor-specific `webkitRequestPointerLock` or `mozRequestPointerLock` methods. If none of these methods are supported and the `navigator.pointer` object is defined, the code sets the `requestPointerLock` method to manually lock the pointer using the `navigator.pointer.lock()` method.\n\nFinally, the code sets up the `exitPointerLock` method on the `document` object. This method is used to release pointer lock on the current element. The code first checks if the browser supports the standard `exitPointerLock` method or the vendor-specific `webkitExitPointerLock` or `mozExitPointerLock` methods. If none of these methods are supported and the `navigator.pointer` object is defined, the code manually unlocks the pointer using the `navigator.pointer.unlock()` method.\n\nOverall, this code ensures that the PointerLock API works as expected in the PlayCanvas engine, regardless of the browser being used. It sets up event listeners and shim methods to ensure that the API is consistent across different browsers and vendor-specific implementations. Here is an example of how the `requestPointerLock` method might be used in the PlayCanvas engine:\n\n```\nvar canvas = document.getElementById('my-canvas');\ncanvas.addEventListener('click', function () {\n    canvas.requestPointerLock();\n});\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code applies PointerLock shims to the browser to enable pointer locking for mouse input in the PlayCanvas engine.\n\n2. What browsers does this code support?\n    \n    This code supports browsers that implement the Pointer Lock API, including Firefox, Chrome, and Safari.\n\n3. What is the difference between `pointerlockchange` and `pointerlockerror` events?\n    \n    `pointerlockchange` event is triggered when the pointer lock state changes, while `pointerlockerror` event is triggered when an error occurs while trying to acquire or release the pointer lock.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/pointer-lock.md"}}],["485",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/string.js)\n\nThis code provides polyfills for several string methods that may not be supported in all browsers. A polyfill is a piece of code that provides functionality that is not natively available in a browser. \n\nThe `defineProtoFunc` function is imported from another file and is used to define the polyfill functions on the `String` prototype. The `endsWith`, `includes`, `startsWith`, and `trimEnd` methods are defined using this function.\n\nThe `endsWith` method checks if a string ends with a given substring. If the optional `this_len` parameter is provided, it limits the length of the string to search. If the string is shorter than `this_len`, the entire string is searched. If the string ends with the given substring, `true` is returned, otherwise `false`.\n\nThe `includes` method checks if a string contains a given substring. If the optional `start` parameter is provided, it specifies the position in the string to start the search. If the substring is found, `true` is returned, otherwise `false`.\n\nThe `startsWith` method checks if a string starts with a given substring. If the optional `rawPos` parameter is provided, it specifies the position in the string to start the search. If `rawPos` is negative or greater than the length of the string, it is treated as 0. If the string starts with the given substring, `true` is returned, otherwise `false`.\n\nThe `trimEnd` method removes whitespace from the end of a string. It uses a regular expression to match whitespace characters and removes them from the end of the string. The resulting string is returned.\n\nThese polyfills can be used in the PlayCanvas engine project to ensure that these string methods work consistently across all browsers. They can be used in any code that relies on these methods, such as code that manipulates strings or searches for substrings. For example, if a script needs to check if a URL ends with a certain string, it can use the `endsWith` method provided by this code:\n\n```\nconst url = 'https://example.com/page1';\nif (url.endsWith('/page1')) {\n    // do something\n}\n```\n## Questions: \n 1. What is the purpose of the `defineProtoFunc` function being imported at the beginning of the code?\n    \n    The `defineProtoFunc` function is used to define new methods on the `String` prototype object.\n\n2. What are the polyfills being used in this code and why are they needed?\n    \n    The polyfills being used are for the `endsWith`, `includes`, `startsWith`, and `trimEnd` methods of the `String` object. They are needed because these methods may not be supported in all browsers, so the polyfills provide a way to ensure consistent behavior across different environments.\n\n3. Are there any potential issues with using these polyfills in certain situations?\n    \n    It's possible that using these polyfills could cause performance issues in certain situations, especially if they are used frequently or on large strings. Additionally, there may be edge cases where the polyfills do not behave exactly like the native methods, so developers should be aware of these differences when using them.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/string.md"}}],["486",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/polyfill/typedarray-fill.js)\n\nThis code is importing a function called `defineProtoFunc` from a file called `defineProtoFunc.js`. It then creates an array called `typedArrays` which contains all of the different types of typed arrays available in JavaScript. These include `Int8Array`, `Uint8Array`, `Uint8ClampedArray`, `Int16Array`, `Uint16Array`, `Int32Array`, `Uint32Array`, and `Float32Array`.\n\nThe code then loops through each of the typed arrays in the `typedArrays` array and uses the `defineProtoFunc` function to add two new methods to each typed array: `fill` and `join`. The `fill` method is used to fill all the elements of an array with a static value, while the `join` method is used to join all elements of an array into a string.\n\nThe purpose of this code is to provide a polyfill for the `fill` and `join` methods for typed arrays in JavaScript. A polyfill is a piece of code that provides functionality that is not natively supported by a browser or JavaScript engine. By adding these methods to the typed arrays, developers can use them in their code without worrying about whether or not they are supported by the user's browser or JavaScript engine.\n\nFor example, if a developer wanted to fill a `Float32Array` with the value `0`, they could use the `fill` method like this:\n\n```\nconst myArray = new Float32Array(10);\nmyArray.fill(0);\n```\n\nThis would fill the entire `myArray` with the value `0`. Similarly, if a developer wanted to join all the elements of a `Uint16Array` into a string separated by commas, they could use the `join` method like this:\n\n```\nconst myArray = new Uint16Array([1, 2, 3, 4, 5]);\nconst myString = myArray.join(\",\");\n```\n\nThis would create a string that looks like this: `\"1,2,3,4,5\"`. Overall, this code is a useful addition to the PlayCanvas engine project as it provides a consistent way for developers to work with typed arrays across different browsers and JavaScript engines.\n## Questions: \n 1. What is the purpose of the `defineProtoFunc` function being imported from `defineProtoFunc.js`?\n   - The `defineProtoFunc` function is used to add new methods to the prototype of a given object.\n\n2. What is the significance of the `typedArrays` array?\n   - The `typedArrays` array contains a list of all the typed array constructors in JavaScript, which are then used to add new methods to their prototypes.\n\n3. Why are the `fill` and `join` methods being added to the prototypes of the typed arrays?\n   - The `fill` and `join` methods are being added to the prototypes of the typed arrays to provide a polyfill for browsers that do not support these methods natively.","metadata":{"source":".autodoc/docs/markdown/src/polyfill/typedarray-fill.md"}}],["487",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/animation/animation.js)\n\nThe code defines three classes: `Key`, `Node`, and `Animation`. These classes are used to create and manage animations in the PlayCanvas engine.\n\nThe `Key` class represents a single keyframe in an animation. It has four properties: `time`, `position`, `rotation`, and `scale`. `time` is a number representing the time in seconds at which the keyframe occurs. `position`, `rotation`, and `scale` are vectors representing the position, rotation, and scale of the animated object at the given time.\n\nThe `Node` class represents a single node in an animation hierarchy. It has two properties: `_name` and `_keys`. `_name` is a string representing the name of the node. `_keys` is an array of `Key` objects representing the keyframes for the node.\n\nThe `Animation` class represents an animation sequence. It has four properties: `name`, `duration`, `_nodes`, and `_nodeDict`. `name` is a string representing the name of the animation. `duration` is a number representing the duration of the animation in seconds. `_nodes` is an array of `Node` objects representing the nodes in the animation hierarchy. `_nodeDict` is an object that maps node names to `Node` objects.\n\nThe `Animation` class also has three methods: `getNode`, `addNode`, and `get nodes`. `getNode` takes a node name as an argument and returns the `Node` object with that name. `addNode` takes a `Node` object as an argument and adds it to the `_nodes` array and `_nodeDict` object. `get nodes` is a getter method that returns the `_nodes` array.\n\nOverall, these classes provide a way to create and manage animations in the PlayCanvas engine. For example, a developer could create an `Animation` object, add `Node` objects to it, and then use the `Key` class to define the keyframes for each node. The `Animation` object could then be used to animate objects in a scene. Here is an example of how these classes might be used:\n\n```\n// Create a new animation\nconst animation = new Animation();\nanimation.name = 'MyAnimation';\nanimation.duration = 5;\n\n// Create a new node\nconst node = new Node();\nnode._name = 'MyNode';\n\n// Add keyframes to the node\nnode._keys.push(new Key(0, new pc.Vec3(0, 0, 0), new pc.Quat(), new pc.Vec3(1, 1, 1)));\nnode._keys.push(new Key(5, new pc.Vec3(0, 0, 10), new pc.Quat(), new pc.Vec3(1, 1, 1)));\n\n// Add the node to the animation\nanimation.addNode(node);\n\n// Use the animation to animate an object\nconst entity = new pc.Entity();\nentity.addComponent('model', { type: 'box' });\nentity.addComponent('animation', { assets: [], speed: 1 });\nentity.animation.play('MyAnimation');\n```\n## Questions: \n 1. What is the purpose of the `Key` class?\n    \n    The `Key` class represents a keyframe in an animation and stores information about its time, position, rotation, and scale.\n\n2. What is the relationship between the `Node` and `Animation` classes?\n    \n    The `Node` class represents a node in a skeletal hierarchy and contains an array of keyframes. The `Animation` class is a sequence of keyframe arrays which map to the nodes of a skeletal hierarchy and controls how the nodes of the hierarchy are transformed over time.\n\n3. What is the purpose of the `_nodeDict` property in the `Animation` class?\n    \n    The `_nodeDict` property is a dictionary that maps node names to their corresponding `Node` instances. It is used by the `getNode` method to retrieve a `Node` instance by name.","metadata":{"source":".autodoc/docs/markdown/src/scene/animation/animation.md"}}],["488",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/animation/skeleton.js)\n\nThe code defines a class called `Skeleton` that represents a skeleton used to play animations. The class contains methods to add time to the animation, blend two skeletons together, link a skeleton to a node hierarchy, and synchronize the linked node hierarchy with the current state of the skeleton. \n\nThe `Skeleton` class has a constructor that takes a `graph` parameter, which is the root `GraphNode` of the skeleton. The constructor creates an array of `InterpolatedKey` objects, which represent the interpolated keyframes for each node in the skeleton. The `addInterpolatedKeys` method is called recursively to add an `InterpolatedKey` object for each node in the skeleton hierarchy. \n\nThe `animation` property is a setter that sets the current animation for the skeleton and resets the current time to zero. The `currentTime` property is a getter and setter that gets or sets the current time of the animation in seconds. The `addTime` method progresses the animation by the supplied time delta and updates the interpolated keyframes for each node in the skeleton. The `blend` method blends two skeletons together using a specified alpha value. The `setGraph` method links the skeleton to a node hierarchy, and the `updateGraph` method synchronizes the linked node hierarchy with the current state of the skeleton.\n\nOverall, the `Skeleton` class is an important part of the PlayCanvas engine, as it provides the functionality to animate 3D models and synchronize their animations with the node hierarchy. Here is an example of how to use the `Skeleton` class to animate a 3D model:\n\n```javascript\nconst model = app.root.findByName('MyModel');\nconst skeleton = model.model.skeleton;\nskeleton.animation = model.model.animations[0];\nskeleton.currentTime = 0;\nskeleton.looping = true;\n\napp.on('update', (deltaTime) => {\n    skeleton.addTime(deltaTime);\n    skeleton.updateGraph();\n});\n```\n## Questions: \n 1. What is the purpose of the `InterpolatedKey` class?\n- The `InterpolatedKey` class represents a keyframe that has been interpolated from two other keyframes. It stores the resulting position, rotation, and scale values, as well as the name of the node it corresponds to and an optional target node.\n\n2. How does the `addTime` method handle looping animations?\n- If the animation is set to loop, and the time delta takes the animation past its end point, the method will wrap around to the beginning of the animation. Otherwise, the animation's current time will remain at its duration.\n\n3. What is the purpose of the `updateGraph` method?\n- The `updateGraph` method synchronizes the currently linked node hierarchy with the current state of the skeleton. It converts the interpolated keyframe at each node in the skeleton into the local transformation matrix at each corresponding node in the linked node hierarchy.","metadata":{"source":".autodoc/docs/markdown/src/scene/animation/skeleton.md"}}],["489",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/area-light-luts.js)\n\nThe code defines a class called `AreaLightLuts` that manages Look-Up Tables (LUTs) for area lights. The LUTs are textures that are used to calculate the lighting of an area light. The class provides methods to create, set, and apply these textures to the device. \n\nThe `AreaLightLuts` class has a static method called `set` that takes in a device, and two matrices `ltcMat1` and `ltcMat2`. These matrices are used to create the LUT textures. The method first determines the format of the LUT texture based on the device's areaLightLutFormat. It then converts the matrices into the appropriate format and creates two textures using the `createTexture` method. Finally, it applies the textures to the device using the `applyTextures` method.\n\nThe `createTexture` method creates a new texture with the specified format, size, and filtering options. The `applyTextures` method removes any previous LUT textures from the device cache and adds the new textures to the cache. It then sets the textures as uniforms in the device's scope.\n\nThe `AreaLightCacheEntry` class is used to hold the LUT textures in the device cache. It has a constructor that takes in two textures and a `destroy` method that destroys the textures.\n\nThe `createPlaceholder` method creates a placeholder LUT texture that is used when no LUT texture is available. It creates a new texture with the `createTexture` method and fills it with zeros. It then applies the texture to the device using the `applyTextures` method.\n\nOverall, the `AreaLightLuts` class provides a convenient way to manage LUT textures for area lights in the PlayCanvas engine. It abstracts away the details of creating and applying these textures to the device, making it easier for developers to work with area lights.\n## Questions: \n 1. What is the purpose of the `AreaLightLuts` class?\n- The `AreaLightLuts` class manages LUT tables for area lights, creates and applies LUT textures to device cache, and sets them as uniforms.\n\n2. What is the `deviceCache` variable used for?\n- The `deviceCache` variable is a device cache storing LUT textures and taking care of their removal when the device is destroyed.\n\n3. What is the purpose of the `createPlaceholder` method?\n- The `createPlaceholder` method creates a placeholder LUT texture used by area lights, fills it with zeros, and applies it as a texture uniform.","metadata":{"source":".autodoc/docs/markdown/src/scene/area-light-luts.md"}}],["490",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/batching/batch-group.js)\n\n## Code Explanation: BatchGroup Class\n\nThe `BatchGroup` class is a part of the PlayCanvas engine project and is used to hold mesh batching settings and a unique id. It is created via the `BatchManager#addGroup` method. \n\nThe class has several properties, including `id`, `name`, `dynamic`, `maxAabbSize`, and `layers`. The `id` property is a unique identifier that can be assigned to model, render, and element components. The `name` property is the name of the group. The `dynamic` property determines whether objects within this batch group should support transforming at runtime. The `maxAabbSize` property is the maximum size of any dimension of a bounding box around batched objects. The `layers` property is an array of layer IDs. The default value is an array containing the `LAYERID_WORLD` constant. The whole batch group will belong to these layers, and layers of source models will be ignored.\n\nThe class also has several static properties, including `MODEL`, `ELEMENT`, `SPRITE`, and `RENDER`. These properties are used to specify the type of batch group. For example, `MODEL` is used for model batch groups, `ELEMENT` is used for element batch groups, `SPRITE` is used for sprite batch groups, and `RENDER` is used for render batch groups.\n\nThe class has a constructor that takes several parameters, including `id`, `name`, `dynamic`, `maxAabbSize`, and `layers`. The `id` parameter is a unique identifier that can be assigned to model, render, and element components. The `name` parameter is the name of the group. The `dynamic` parameter determines whether objects within this batch group should support transforming at runtime. The `maxAabbSize` parameter is the maximum size of any dimension of a bounding box around batched objects. The `layers` parameter is an array of layer IDs. The default value is an array containing the `LAYERID_WORLD` constant. The whole batch group will belong to these layers, and layers of source models will be ignored.\n\nOverall, the `BatchGroup` class is an important part of the PlayCanvas engine project and is used to hold mesh batching settings and a unique id. It is created via the `BatchManager#addGroup` method and is used to specify the type of batch group, as well as several other properties.\n## Questions: \n 1. What is the purpose of the `BatchGroup` class?\n    \n    The `BatchGroup` class holds mesh batching settings and a unique id, and can be used to assign ids to model, render, and element components.\n\n2. What is the significance of the `maxAabbSize` property?\n    \n    The `maxAabbSize` property specifies the maximum size of any dimension of a bounding box around batched objects, and is used by `BatchManager#prepare` to split objects into local groups based on this size.\n\n3. What is the default value of the `layers` property?\n    \n    The default value of the `layers` property is an array containing the `LAYERID_WORLD` constant. The whole batch group will belong to these layers, and layers of source models will be ignored.","metadata":{"source":".autodoc/docs/markdown/src/scene/batching/batch-group.md"}}],["491",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/batching/batch.js)\n\nThe code defines a class called `Batch` that holds information about batched mesh instances. This class is used in the PlayCanvas engine to combine multiple mesh instances into a single mesh instance, which can improve rendering performance. \n\nThe `Batch` class has several properties, including `origMeshInstances`, which is an array of the original mesh instances that were combined to create the batch. The `meshInstance` property is the resulting combined mesh instance. The `dynamic` property indicates whether the batch supports transforming mesh instances at runtime. The `batchGroupId` property links the batch to a specific batch group. \n\nThe `Batch` class has several methods. The `destroy` method removes the batch meshes from all layers and destroys the mesh instance. The `addToLayers` method adds the batch mesh instance to the specified layers in the scene. The `removeFromLayers` method removes the batch mesh instance from the specified layers in the scene. The `updateBoundingBox` method updates the bounding box for the batch based on the bounding boxes of the original mesh instances. \n\nOverall, the `Batch` class is an important part of the PlayCanvas engine's rendering pipeline. It allows multiple mesh instances to be combined into a single mesh instance, which can improve rendering performance. The `Batch` class can be used by developers to optimize their PlayCanvas projects by reducing the number of draw calls and improving rendering performance. \n\nExample usage:\n\n```javascript\n// create an array of mesh instances to be batched\nconst meshInstances = [meshInstance1, meshInstance2, meshInstance3];\n\n// create a new batch\nconst batch = new Batch(meshInstances, true, 0);\n\n// add the batch to the scene layers\nbatch.addToLayers(scene, ['default']);\n\n// update the bounding box for the batch\nbatch.updateBoundingBox();\n\n// remove the batch from the scene layers and destroy it\nbatch.destroy(scene, ['default']);\n```\n## Questions: \n 1. What is the purpose of the `Batch` class and how is it used in the PlayCanvas engine?\n- The `Batch` class holds information about batched mesh instances and is created in `BatchManager#create`. It is used to combine multiple mesh instances into a single mesh instance for performance optimization.\n\n2. What is the significance of the `dynamic` property in the `Batch` class?\n- The `dynamic` property indicates whether the batch supports transforming mesh instances at runtime. This is important for cases where the mesh instances need to be updated or animated during gameplay.\n\n3. How does the `updateBoundingBox` method work and what is its purpose?\n- The `updateBoundingBox` method updates the bounding box for a batch by computing the union of the bounding boxes of all the original mesh instances. This is necessary to ensure that the batch is correctly culled and rendered by the engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/batching/batch.md"}}],["492",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/batching/skin-batch-instance.js)\n\nThe code defines a class called `SkinBatchInstance` which is derived from another class called `SkinInstance`. The purpose of this class is to make changes to the `SkinInstance` class so that it is suitable for batching. \n\nBatching is a technique used in computer graphics to improve performance by reducing the number of draw calls made to the graphics card. In this case, the `SkinBatchInstance` class is used to batch together multiple instances of skinned meshes. \n\nThe constructor of the `SkinBatchInstance` class takes in a `device`, an array of `nodes`, and a `rootNode`. The `nodes` array contains the bones of the skinned mesh, and the `rootNode` is the root node of the mesh. The constructor initializes the `SkinBatchInstance` object by calling the `init` method of the `SkinInstance` class, passing in the number of bones in the `nodes` array. It then sets the `device` and `rootNode` properties of the object, and sets the `bones` property to the `nodes` array. \n\nThe `updateMatrices` method of the `SkinBatchInstance` class is empty, indicating that it does not need to be overridden for batching. \n\nThe `updateMatrixPalette` method of the `SkinBatchInstance` class is used to update the matrix palette of the skinned mesh. The matrix palette is an array of matrices that are used to transform the vertices of the mesh. The method loops through each bone in the `bones` array, gets its world transform matrix, and copies it into the matrix palette. The matrix is transposed from a 4x4 matrix to a 4x3 matrix, which is the format used by the vertex shader. Finally, the `uploadBones` method of the `SkinInstance` class is called to upload the matrix palette to the graphics card. \n\nThe `SkinBatchInstance` class is exported so that it can be used in other parts of the PlayCanvas engine project. An example of how it might be used is to create multiple instances of a skinned mesh and add them to a batch. This would reduce the number of draw calls made to the graphics card, improving performance.\n## Questions: \n 1. What is the purpose of this code and how does it relate to the PlayCanvas engine?\n- This code defines a class called `SkinBatchInstance` that is derived from `SkinInstance` and is used for batching. It is part of the PlayCanvas engine's skinning system.\n\n2. What parameters does the `SkinBatchInstance` constructor take and what do they represent?\n- The constructor takes a `device` object, an array of `nodes`, and a `rootNode`. The `device` is a graphics device used for rendering, `nodes` is an array of nodes that represent the bones of the skin, and `rootNode` is the root node of the skin.\n\n3. What is the purpose of the `updateMatrixPalette` method and how does it work?\n- The `updateMatrixPalette` method updates the matrix palette used for skinning by copying the world transforms of the skin's bones into the matrix palette. It transposes the matrices from 4x4 to 4x3 format and uploads them to the graphics device for use in rendering.","metadata":{"source":".autodoc/docs/markdown/src/scene/batching/skin-batch-instance.md"}}],["493",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/composition/light-composition-data.js)\n\nThe code defines a class called `LightCompositionData` which is used to store data needed by the `LayerComposition` to manage a light. The class has two properties: `shadowCastersSet` and `shadowCastersList`. `shadowCastersSet` is a set used for fast de-duplication of shadow casters, while `shadowCastersList` is an array used for fast iteration of shadow casters.\n\nThe class has three methods: `constructor`, `clearShadowCasters`, and `addShadowCasters`. The `constructor` method initializes the `shadowCastersSet` and `shadowCastersList` properties. The `clearShadowCasters` method clears the `shadowCastersSet` and `shadowCastersList` properties. The `addShadowCasters` method adds unique shadow casters to the `shadowCastersSet` and `shadowCastersList` properties.\n\nThe `addShadowCasters` method takes an array of shadow casters as an argument and iterates over it. For each item in the array, it checks if the item is already in the `shadowCastersSet`. If the item is not in the set, it adds the item to both the `shadowCastersSet` and `shadowCastersList`.\n\nThis class can be used in the larger PlayCanvas engine project to manage lights and their shadow casters. For example, when a new light is added to the scene, an instance of `LightCompositionData` can be created to store the light's shadow casters. The `addShadowCasters` method can be called to add the light's shadow casters to the `shadowCastersSet` and `shadowCastersList`. The `clearShadowCasters` method can be called to clear the shadow casters when the light is removed from the scene.\n\nExample usage:\n\n```\nconst lightData = new LightCompositionData();\nconst shadowCasters = [mesh1, mesh2, mesh3];\nlightData.addShadowCasters(shadowCasters);\n```\n\nIn this example, an instance of `LightCompositionData` is created and an array of shadow casters is passed to the `addShadowCasters` method to add them to the `shadowCastersSet` and `shadowCastersList`.\n## Questions: \n 1. What is the purpose of the LightCompositionData class?\n    - The LightCompositionData class is used to store data needed to manage a light in the LayerComposition.\n\n2. What is the difference between the shadowCastersSet and shadowCastersList?\n    - The shadowCastersSet is used for fast de-duplication of shadow casters, while the shadowCastersList is used for fast iteration.\n\n3. What does the addShadowCasters method do?\n    - The addShadowCasters method adds unique shadow casters to both the shadowCastersSet and shadowCastersList.","metadata":{"source":".autodoc/docs/markdown/src/scene/composition/light-composition-data.md"}}],["494",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/composition/render-action.js)\n\nThe `RenderAction` class represents an entry in the final order of rendering of cameras and layers in the PlayCanvas engine. It is populated at runtime based on `LayerComposition`. \n\nThe class has several properties that are used to store information about the render action. The `layerIndex` property is an index into a layer stored in `LayerComposition.layerList`. The `cameraIndex` property is an index into a camera array of the layer, stored in `Layer.cameras`. The `camera` property is of type `CameraComponent` and represents the camera that is used for rendering. The `renderTarget` property is a render target that this render action renders to (taken from either camera or layer). The `lightClusters` property is of type `WorldClusters` and represents the light clusters. The `clearColor`, `clearDepth`, and `clearStencil` properties are used to store clear flags. The `triggerPostprocess` property is a boolean that is true if this render action should trigger postprocessing callback for the camera. The `firstCameraUse` and `lastCameraUse` properties are booleans that are true if this is the first or last render action using this camera. The `directionalLightsSet` property is a set that stores directional lights that need to update their shadows for this render action. The `directionalLights` property is an array that stores the same directional lights as the `directionalLightsSet` property. The `directionalLightsIndices` property is an array that stores the same directional lights as indices into `LayerComposition._lights`. The `viewBindGroups` property is an array of view bind groups (the number of these corresponds to the number of views when XR is used).\n\nThe `RenderAction` class has several methods. The `destroy` method releases GPU resources. The `reset` method prepares the render action for re-use. The `isLayerEnabled` method takes a `LayerComposition` object as a parameter and returns true if the layer/sublayer referenced by the render action is enabled. The `collectDirectionalLights` method takes `cameraLayers`, `dirLights`, and `allLights` as parameters and stores directional lights that are needed for this camera based on layers it renders.\n\nOverall, the `RenderAction` class is an important part of the PlayCanvas engine as it represents an entry in the final order of rendering of cameras and layers. It stores important information about the render action and has methods that are used to prepare the render action for re-use and to collect directional lights.\n## Questions: \n 1. What is the purpose of the `RenderAction` class?\n- The `RenderAction` class represents an entry in the final order of rendering of cameras and layers in the PlayCanvas engine, and is populated at runtime based on `LayerComposition`.\n\n2. What is the significance of the `hasDirectionalShadowLights` getter?\n- The `hasDirectionalShadowLights` getter returns `true` if there are any directional lights that need to update their shadows for this render action.\n\n3. What is the `collectDirectionalLights` method used for?\n- The `collectDirectionalLights` method is used to store directional lights that are needed for this camera based on the layers it renders, and only shadow casting lights are considered.","metadata":{"source":".autodoc/docs/markdown/src/scene/composition/render-action.md"}}],["495",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/compress/compress-utils.js)\n\nThe code defines an object called `CompressUtils` that contains three private functions. These functions are used to compress and decompress entity data in a PlayCanvas scene. \n\nThe first function, `setCompressedPRS`, sets the position, rotation, and scale of an entity using compressed scene format. It takes in an entity, a JSON entity data object, and a compression metadata object. The function first checks if the entity data object has a property called `___1`. If it does, it retrieves the position data from the `singleVecs` property of the compression metadata object and sets the entity's local position using the `setLocalPosition` method. If the entity data object does not have a `___1` property, the function retrieves the position data from the `tripleVecs` property of the compression metadata object using the index stored in the `___2` property of the entity data object. It then sets the entity's local position using the `setLocalPosition` method. The function then repeats this process for rotation and scale data.\n\nThe second function, `oneCharToKey`, retrieves the original field name (key) for a single character key from a compressed entity. It takes in a compressed key string and a compression metadata object. The function calculates the index of the field name in the `fieldArray` property of the compression metadata object using the ASCII code of the compressed key string's character minus the `fieldFirstCode` property of the compression metadata object.\n\nThe third function, `multCharToKey`, retrieves the original field name (key) for a multi-character key from a compressed entity. It takes in a compressed key string and a compression metadata object. The function calculates the index of the field name in the `fieldArray` property of the compression metadata object using the ASCII codes of each character in the compressed key string and the `fieldCodeBase` and `fieldFirstCode` properties of the compression metadata object.\n\nThese functions are used in the larger PlayCanvas engine project to compress and decompress entity data in a scene. The `setCompressedPRS` function is used to set the position, rotation, and scale of entities in a scene using compressed data, which can improve performance and reduce memory usage. The `oneCharToKey` and `multCharToKey` functions are used to retrieve the original field names of compressed entity data, which is necessary for correctly interpreting the data.\n## Questions: \n 1. What is the purpose of the `CompressUtils` object?\n- The `CompressUtils` object provides methods for working with compressed entity data in the PlayCanvas engine.\n\n2. What is the difference between the `setCompressedPRS` and `oneCharToKey` methods?\n- The `setCompressedPRS` method sets the position, rotation, and scale of an entity using compressed scene format, while the `oneCharToKey` method retrieves the original field name (key) for a single character key from a compressed entity.\n\n3. What is the `multCharToKey` method used for?\n- The `multCharToKey` method retrieves the original field name (key) for a multi-character key from a compressed entity.","metadata":{"source":".autodoc/docs/markdown/src/scene/compress/compress-utils.md"}}],["496",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/compress/decompress.js)\n\nThe `Decompress` class is used to reconstruct the original object field names in a compressed scene. This class is part of the PlayCanvas engine project. The purpose of this class is to decompress a compressed scene by reconstructing the original object field names. This is done by iterating over the compressed scene and replacing the compressed keys with their original names.\n\nThe `Decompress` class has a constructor that takes two parameters: `node` and `data`. The `node` parameter is the current node of the object being decompressed, initially the 'entities' field of a scene. The `data` parameter is the compression metadata. The `run` method is used to start the decompression process. It checks the type of the current node and calls the appropriate method to handle it. If the node is an object, it calls the `_handleMap` method. If the node is an array, it calls the `_handleArray` method. If the node is neither an object nor an array, it sets the result to the node.\n\nThe `_handleMap` method is called when the current node is an object. It creates a new object and iterates over the keys of the current node. For each key, it calls the `_handleKey` method to replace the compressed key with the original key and recursively decompresses the value of the key.\n\nThe `_handleKey` method is called for each key in the `_handleMap` method. It takes the original key as a parameter and replaces it with the original key if the length of the key is greater than 2. If the length of the key is 1, it calls the `oneCharToKey` method of the `CompressUtils` class to replace the compressed key with the original key. If the length of the key is 2, it calls the `multCharToKey` method of the `CompressUtils` class to replace the compressed key with the original key. It then recursively decompresses the value of the key.\n\nThe `_handleArray` method is called when the current node is an array. It creates a new array and iterates over the elements of the current node. For each element, it calls the `_handleArElt` method to recursively decompress the element.\n\nThe `_handleArElt` method is called for each element in the `_handleArray` method. It takes the element as a parameter and recursively decompresses it.\n\nThis class is used in the larger PlayCanvas engine project to decompress compressed scenes. It is used to reconstruct the original object field names in a compressed scene. This class can be used as follows:\n\n```\nconst decompressedScene = new Decompress(compressedScene, compressionMetadata).run();\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines a class called `Decompress` that is used to reconstruct original object field names in a compressed scene.\n\n2. What parameters does the `Decompress` constructor take?\n- The `Decompress` constructor takes two parameters: `node`, which is the current node of the object being decompressed, and `data`, which is compression metadata.\n\n3. What is the output of the `run` method?\n- The output of the `run` method is the result of decompressing the input object. The output is determined by the type of the input object: if it is an object, the output is a new object with decompressed keys and values; if it is an array, the output is a new array with decompressed elements; otherwise, the output is the input object itself.","metadata":{"source":".autodoc/docs/markdown/src/scene/compress/decompress.md"}}],["497",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/frame-graph.js)\n\nThe `FrameGraph` class represents a single rendering frame as a sequence of render passes. It is used to manage the rendering pipeline of the PlayCanvas engine. \n\nThe `renderPasses` property is an array that holds all the render passes that make up the frame. The `addRenderPass` method is used to add a render pass to the frame. The `reset` method clears the `renderPasses` array.\n\nThe `compile` method is used to prepare the frame for rendering. It walks over the `renderPasses` array and creates a map of render targets to their previous occurrence. If a render pass uses a render target that has been used before, the method checks if the previous pass stores data and updates the `prevPass` accordingly. If a render pass requires a cubemap texture, the method checks if the previous pass used the same texture and disables mipmap generation if necessary. Finally, the method handles the last passes rendering to each render target.\n\nThe `render` method is used to render the frame. It calls the `compile` method and then renders each render pass in the `renderPasses` array.\n\nOverall, the `FrameGraph` class is an important part of the PlayCanvas engine's rendering pipeline. It manages the sequence of render passes that make up a frame and ensures that each pass is executed correctly. Developers can use this class to create custom rendering pipelines or modify the existing one to suit their needs. For example, they can add custom render passes to the frame or modify the existing ones to achieve specific effects.\n## Questions: \n 1. What is the purpose of the `FrameGraph` class?\n- The `FrameGraph` class represents a single rendering frame as a sequence of render passes.\n\n2. What is the `renderTargetMap` used for?\n- The `renderTargetMap` is used during frame graph compilation to map a render target to its previous occurrence.\n\n3. What happens during the `compile` method?\n- During the `compile` method, the code walks over render passes to find passes rendering to the same cubemap texture and skips the mipmap generation till the last rendering to the cubemap to avoid mipmaps being generated after each face. It also handles what's left in the map - last passes rendering to each render target.","metadata":{"source":".autodoc/docs/markdown/src/scene/frame-graph.md"}}],["498",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/graphics/env-lighting.js)\n\nThe code defines a class called `EnvLighting` that contains helper functions for prefiltering lighting data. The purpose of this class is to generate a skybox cubemap, a lighting source cubemap, and an environment lighting atlas containing prefiltered reflections and ambient lighting. These textures are used to create realistic lighting in a 3D scene.\n\nThe `generateSkyboxCubemap` function takes a source texture and generates a cubemap in the correct pixel format. The resulting cubemap is used as a skybox in the scene. The `generateLightingSource` function creates a cubemap texture in the format needed to precalculate lighting data. This texture is used as a lighting source in the scene. The `generateAtlas` function generates an environment lighting atlas containing prefiltered reflections and ambient lighting. This atlas is used to create realistic lighting in the scene.\n\nThe `generatePrefilteredAtlas` function generates an environment lighting atlas from prefiltered cubemap data. This function takes an array of 6 prefiltered textures and generates an atlas texture. The resulting atlas texture is used to create realistic lighting in the scene.\n\nThe code also defines several helper functions that are used by the main functions. The `calcLevels` function calculates the number of mipmap levels given texture dimensions. The `supportsFloat16` and `supportsFloat32` functions check if the device supports float16 and float32 textures, respectively. The `lightingSourcePixelFormat` and `lightingPixelFormat` functions return the pixel format for the lighting source and runtime lighting, respectively. The `createCubemap` function creates a cubemap texture.\n\nOverall, this code is an important part of the PlayCanvas engine project as it provides the necessary functionality to generate realistic lighting in a 3D scene. The functions defined in this class are used extensively throughout the engine to create and manipulate lighting textures.\n## Questions: \n 1. What is the purpose of the `generateAtlas` method in the `EnvLighting` class?\n- The `generateAtlas` method is used to generate an environment lighting atlas containing prefiltered reflections and ambient lighting from a source texture.\n\n2. What is the difference between `lightingSourcePixelFormat` and `lightingPixelFormat` methods?\n- `lightingSourcePixelFormat` method is used to determine the pixel format for the lighting source texture, which should be stored in HDR format. `lightingPixelFormat` method is used to determine the pixel format for the runtime lighting texture, which can be in RGBM format.\n\n3. What is the significance of the `fixCubemapSeams` variable?\n- The `fixCubemapSeams` variable is used to specify whether or not to fix seams in the cubemap texture. If set to true, it will fix seams at the cost of some performance.","metadata":{"source":".autodoc/docs/markdown/src/scene/graphics/env-lighting.md"}}],["499",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/graphics/lightmap-cache.js)\n\nThe code above defines a static class called `LightmapCache` that implements a cache of lightmaps generated at runtime using Lightmapper. The purpose of this class is to automatically release real-time baked lightmaps when mesh instances using them are destroyed. \n\nThe `LightmapCache` class uses a `RefCountedCache` class from the `../../core/ref-counted-cache.js` file to implement the cache. The `RefCountedCache` class is a utility class that keeps track of the number of references to an object and automatically destroys it when the reference count reaches zero. \n\nThe `LightmapCache` class has three static methods: `incRef`, `decRef`, and `destroy`. The `incRef` method adds a texture reference to the lightmap cache by calling the `incRef` method of the `RefCountedCache` class. The `decRef` method removes a texture reference from the lightmap cache by calling the `decRef` method of the `RefCountedCache` class. The `destroy` method destroys the cache by calling the `destroy` method of the `RefCountedCache` class. \n\nThis class can be used in the larger PlayCanvas engine project to manage the caching of lightmaps generated at runtime. Developers can use the `LightmapCache` class to automatically release real-time baked lightmaps when mesh instances using them are destroyed. This can help to optimize the performance of the engine by reducing memory usage and preventing memory leaks. \n\nHere is an example of how the `LightmapCache` class can be used in the PlayCanvas engine project:\n\n```\n// create a new lightmap texture\nconst lightmapTexture = new Texture();\n\n// add the texture to the lightmap cache\nLightmapCache.incRef(lightmapTexture);\n\n// use the texture in a mesh instance\nconst meshInstance = new MeshInstance(mesh, material);\nmeshInstance.lightmapTexture = lightmapTexture;\n\n// destroy the mesh instance\nmeshInstance.destroy();\n\n// remove the texture from the lightmap cache\nLightmapCache.decRef(lightmapTexture);\n```\n## Questions: \n 1. What is the purpose of the `RefCountedCache` import?\n- The `RefCountedCache` import is used to implement the cache of lightmaps generated at runtime using Lightmapper.\n\n2. How does the `LightmapCache` class work?\n- The `LightmapCache` class is a pure static class that allows automatic release of realtime baked lightmaps when mesh instances using them are destroyed. It has methods to add and remove texture references from the lightmap cache.\n\n3. What is the significance of the `export { LightmapCache };` statement at the end of the code?\n- The `export { LightmapCache };` statement at the end of the code exports the `LightmapCache` class, making it available for use in other files/modules.","metadata":{"source":".autodoc/docs/markdown/src/scene/graphics/lightmap-cache.md"}}],["500",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/graphics/post-effect.js)\n\nThe code defines a base class for all post effects in the PlayCanvas engine. Post effects are used to apply effects to a render target and then render the result to an output render target or the screen. The class contains a constructor that takes the graphics device of the application as a parameter. It also has a property that can be set to true if a depth map is necessary. \n\nThe class has a static quadVertexShader property that defines a simple vertex shader used to render a quad. The shader requires 'vec2 aPosition' in the vertex buffer and generates uv coordinates vUv0 for use in the fragment shader. \n\nThe class has a render method that takes an inputTarget, an outputTarget, and a rect as parameters. The inputTarget is the input render target, the outputTarget is the output render target, and the rect is the rect of the current camera. If the rect is not specified, it defaults to [0, 0, 1, 1]. \n\nThe class also has a drawQuad method that takes a target, a shader, and a rect as parameters. The target is the output render target, the shader is the shader to be used for drawing the rectangle, and the rect is the normalized screen-space position and size of the rectangle. If the rect is not specified, it defaults to [0, 0, 1, 1]. \n\nOverall, this class provides a base for creating post effects in the PlayCanvas engine. Developers can extend this class to create custom post effects that can be applied to render targets and rendered to the screen or an output render target. The class provides methods for rendering and drawing quads, which are commonly used in post-processing effects.\n## Questions: \n 1. What is the purpose of the `PostEffect` class?\n- The `PostEffect` class is a base class for all post effects. It takes a render target as input, applies effects to it, and then renders the result to an output render target or the screen if no output is specified.\n\n2. What is the purpose of the `drawQuad` method?\n- The `drawQuad` method draws a screen-space rectangle in a render target using a specified shader.\n\n3. What is the purpose of the `needsDepthBuffer` property?\n- The `needsDepthBuffer` property is a boolean that should be set to `true` by the custom post effect if a depth map is necessary. Its default value is `false`.","metadata":{"source":".autodoc/docs/markdown/src/scene/graphics/post-effect.md"}}],["501",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/graphics/prefilter-cubemap.js)\n\nThe code defines a function `shFromCubemap` that generates spherical harmonics coefficients from a cubemap texture. Spherical harmonics are a set of functions that can be used to represent lighting in 3D graphics. The function takes two arguments: a device object and a source cubemap texture. The function first checks that the source texture is in the correct format and has been synced to the CPU. If the texture is not in the correct format or has not been synced, the function returns null. \n\nThe function then creates an array of vectors that represent the directions of each texel in the cubemap. It then loops over each face of the cubemap and each texel in the face, computing the solid angle of the texel and using it to weight the contribution of the texel to the spherical harmonics coefficients. The function uses the texel's color and alpha values to compute the contribution of the texel to the coefficients. If the source texture is in RGBM format, the function applies an additional transformation to the color value. \n\nThe function returns an array of 27 floating-point values that represent the spherical harmonics coefficients. These coefficients can be used to represent the lighting in a scene and can be used to compute the lighting at any point in the scene. \n\nThis function is likely used in the larger PlayCanvas engine project to generate lighting information for 3D scenes. The spherical harmonics coefficients generated by this function can be used to efficiently compute the lighting at any point in the scene, which is useful for real-time rendering applications. The function is also likely used in conjunction with other rendering techniques, such as physically-based rendering, to create realistic lighting in 3D scenes. \n\nExample usage:\n\n```javascript\nimport { shFromCubemap } from 'playcanvas-engine';\n\n// create a device object\nconst device = new Device();\n\n// load a cubemap texture\nconst cubemap = new Texture(device, {\n    name: 'cubemap',\n    cubemap: true,\n    type: TEXTURETYPE_DEFAULT,\n    format: PIXELFORMAT_RGBA8,\n    width: 512,\n    height: 512,\n    mipmaps: false\n});\ncubemap.loadFromUrl('path/to/cubemap.png');\n\n// generate spherical harmonics coefficients from the cubemap\nconst sh = shFromCubemap(device, cubemap);\n\n// use the spherical harmonics coefficients to compute lighting in a scene\n// ...\n```\n## Questions: \n 1. What is the purpose of the `shFromCubemap` function?\n- The `shFromCubemap` function takes in a cubemap texture and returns a spherical harmonics (SH) representation of the texture.\n\n2. What is the significance of the `solidAngle` variable in the `texelCoordSolidAngle` function?\n- The `solidAngle` variable represents the projected area of a texel on a cube face, which is used to calculate the SH coefficients for the cubemap.\n\n3. What is the purpose of the `BlendState` and `RenderTarget` classes imported in the code?\n- The `BlendState` class is used to set the blending mode for rendering, while the `RenderTarget` class is used to specify the render target for rendering operations.","metadata":{"source":".autodoc/docs/markdown/src/scene/graphics/prefilter-cubemap.md"}}],["502",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/graphics/quad-render-utils.js)\n\nThe code defines two functions, `drawQuadWithShader` and `drawTexture`, that are used to draw screen-space quads and textures, respectively. These functions are part of the PlayCanvas engine project and are located in a file that imports various modules from the engine.\n\nThe `drawQuadWithShader` function takes in a graphics device, a render target, a shader, a viewport rectangle, and a scissor rectangle. It sets the cull mode to none and the depth state to no depth, prepares a quad for rendering with the given shader, and renders the quad to the render target using a render pass. If no viewport or scissor rectangle is provided, it defaults to the full size of the render target or the device. This function is useful for drawing post-processing effects or other screen-space effects.\n\nThe `drawTexture` function takes in a graphics device, a texture, a render target, a shader, a viewport rectangle, and a scissor rectangle. It sets the texture as a constant texture source, and then calls `drawQuadWithShader` to render the texture to the render target. If no shader is provided, it defaults to the copy shader of the graphics device. This function is useful for drawing textures to the screen or to a render target.\n\nBoth functions have an optional `useBlend` parameter that is no longer used and will log a warning if provided.\n\nOverall, these functions provide a convenient way to draw screen-space quads and textures with a given shader and are likely used extensively throughout the PlayCanvas engine project.\n## Questions: \n 1. What is the purpose of the `drawQuadWithShader` function?\n- The `drawQuadWithShader` function is used to draw a screen-space quad using a specific shader, with options to specify the render target, viewport and scissor rectangle.\n\n2. What is the purpose of the `drawTexture` function?\n- The `drawTexture` function is used to draw a texture in screen-space, with options to specify the render target, viewport and scissor rectangle.\n\n3. What is the purpose of the `Debug` and `DebugHelper` imports?\n- The `Debug` and `DebugHelper` imports are used for debugging purposes, such as logging warnings and pushing/popping GPU markers.","metadata":{"source":".autodoc/docs/markdown/src/scene/graphics/quad-render-utils.md"}}],["503",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/graphics/quad-render.js)\n\nThe code defines a class called `QuadRender` that is used to render a quad using a specified shader. The class is part of the PlayCanvas engine project and is located in a file within the project. \n\nThe `QuadRender` class has a constructor that takes a `Shader` object as an argument. The constructor creates a new instance of the `QuadRender` class and initializes its properties. If the device supports uniform buffers, the constructor adds uniform buffer support to the shader and creates a uniform buffer and a bind group. The uniform buffer is used to store the shader's uniform data, and the bind group is used to bind the uniform buffer to the shader. \n\nThe `QuadRender` class has a `render` method that is used to render the quad. The method takes two optional arguments: `viewport` and `scissor`. If the `viewport` argument is provided, the method modifies the viewport and scissor settings of the device to render the quad within the specified viewport and scissor rectangle. If the `viewport` argument is not provided, the method uses the default viewport and scissor settings of the device. \n\nThe `QuadRender` class has a `destroy` method that is used to destroy the resources associated with the instance of the class. The method destroys the uniform buffer and the bind group. \n\nThe `QuadRender` class is used to render a quad using a specified shader. The class can be used to render a quad in a variety of scenarios, such as rendering a background image or a user interface element. \n\nExample usage of the `QuadRender` class:\n\n```javascript\nconst shader = pc.createShaderFromCode(app.graphicsDevice, vertexShader, fragmentShader, `MyShader`);\nconst quad = new QuadRender(shader);\nquad.render();\nquad.destroy();\n```\n## Questions: \n 1. What is the purpose of the `QuadRender` class?\n    \n    The `QuadRender` class is an object that renders a quad using a specified shader.\n\n2. What graphics features does the `QuadRender` class support?\n    \n    The `QuadRender` class supports uniform buffers and bind groups, and it uses a tristrip primitive type.\n\n3. How can a developer use the `QuadRender` class?\n    \n    A developer can create a new `QuadRender` instance with a specified shader, call its `render` method to render the quad, and then destroy the instance with its `destroy` method.","metadata":{"source":".autodoc/docs/markdown/src/scene/graphics/quad-render.md"}}],["504",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/immediate/immediate-batch.js)\n\nThe code defines a helper class called `ImmediateBatch` that is used for rendering lines in real-time. The class is used to store data for a single batch of line rendering using a single material. The class is exported for use in other parts of the PlayCanvas engine project.\n\nThe `ImmediateBatch` class has a constructor that takes three parameters: `device`, `material`, and `layer`. The `device` parameter is an instance of the `GraphicsDevice` class, which is used to create a new `Mesh` instance. The `material` parameter is an instance of the `Material` class, which is used to render the lines. The `layer` parameter is an integer that specifies the layer on which the lines should be rendered.\n\nThe `ImmediateBatch` class has two methods: `addLines` and `addLinesArrays`. The `addLines` method is used to add line positions and colors to the batch. The method expects position in `Vec3` and colors in `Color` format. The `addLinesArrays` method is used to add line positions and colors to the batch. The method expects positions as arrays of numbers and color as an instance of `Color` or an array of numbers specifying the same number of vertices as positions.\n\nThe `ImmediateBatch` class has an `onPreRender` method that is called before the lines are rendered. The method takes two parameters: `visibleList` and `transparent`. The `visibleList` parameter is an instance of the `VisibleList` class, which is used to store the list of visible mesh instances. The `transparent` parameter is a boolean that specifies whether the lines are transparent or not. The method prepares the mesh if its transparency matches and updates the mesh vertices. It then injects the mesh instance into the visible list to be rendered.\n\nThe code also defines a constant called `identityGraphNode`, which is an instance of the `GraphNode` class. The `identityGraphNode` is used to store the identity matrix and is used to create a new `MeshInstance` instance.\n\nIn summary, the `ImmediateBatch` class is a helper class used to store data for a single batch of line rendering using a single material. The class has methods for adding line positions and colors to the batch and a method for rendering the lines. The class is exported for use in other parts of the PlayCanvas engine project.\n## Questions: \n 1. What is the purpose of the `ImmediateBatch` class?\n- The `ImmediateBatch` class is a helper class that stores data for a single batch of line rendering using a single material.\n\n2. What is the `onPreRender` method used for?\n- The `onPreRender` method is used to prepare the mesh for rendering if its transparency matches, update mesh vertices, clear lines after they were rendered, and inject the mesh instance into the visible list to be rendered.\n\n3. What is the purpose of the `identityGraphNode` variable?\n- The `identityGraphNode` variable is a `GraphNode` instance that represents the identity transform and is used as a reference for the `MeshInstance` constructor.","metadata":{"source":".autodoc/docs/markdown/src/scene/immediate/immediate-batch.md"}}],["505",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/immediate/immediate-batches.js)\n\nThe code defines a helper class called `ImmediateBatches` that stores line batches for a single layer in the PlayCanvas engine project. The purpose of this class is to manage and optimize the rendering of immediate mode geometry, which is geometry that is rendered directly without being stored in a buffer. \n\nThe `ImmediateBatches` class has a constructor that takes a `device` parameter, which is an instance of the `GraphicsDevice` class that represents the graphics hardware and provides access to low-level rendering functions. The class also has a `map` property that is a `Map` object that stores a mapping between `Material` objects and `ImmediateBatch` objects. \n\nThe `getBatch` method takes a `material` parameter and a `layer` parameter and returns an `ImmediateBatch` object for the specified material and layer. If an `ImmediateBatch` object does not already exist for the specified material, a new one is created and added to the `map` property. This method is used to retrieve an `ImmediateBatch` object for a specific material and layer when rendering immediate mode geometry.\n\nThe `onPreRender` method takes a `visibleList` parameter and a `transparent` parameter and calls the `onPreRender` method of each `ImmediateBatch` object in the `map` property. This method is called before rendering each frame and is used to update the state of each `ImmediateBatch` object based on the current visibility and transparency settings.\n\nThe `ImmediateBatches` class is exported for use in other parts of the PlayCanvas engine project. It is used by other classes and functions that need to render immediate mode geometry efficiently. For example, the `ImmediateRenderer` class uses `ImmediateBatches` objects to render lines and other immediate mode geometry. \n\nOverall, the `ImmediateBatches` class is an important part of the PlayCanvas engine project that helps to optimize the rendering of immediate mode geometry. By managing and reusing `ImmediateBatch` objects for different materials and layers, it reduces the overhead of rendering immediate mode geometry and improves performance.\n## Questions: \n 1. What is the purpose of the `ImmediateBatch` class that is imported at the beginning of the file?\n- The `ImmediateBatch` class is a helper class used to store batches of lines for rendering.\n\n2. What is the `onPreRender` method used for?\n- The `onPreRender` method is called before rendering and is used to update the batches for each material.\n\n3. What is the purpose of the `getBatch` method?\n- The `getBatch` method is used to retrieve the batch for a specific material and layer. If the batch does not exist, it creates a new one and adds it to the map.","metadata":{"source":".autodoc/docs/markdown/src/scene/immediate/immediate-batches.md"}}],["506",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/immediate/immediate.js)\n\nThe `Immediate` class in the PlayCanvas engine project is responsible for rendering debug lines and shapes in the game engine. It is used to draw simple shapes and lines that are useful for debugging and visualizing game objects. The class is initialized with a reference to the device object, which is used to create and manage graphics resources.\n\nThe `Immediate` class contains several methods for drawing different types of shapes, including wireframe boxes and spheres. These shapes are drawn using a set of vertices and colors that are passed to the graphics device. The class also contains methods for creating and managing materials used for rendering the shapes.\n\nThe `Immediate` class uses a set of `ImmediateBatches` objects to manage the rendering of the debug lines and shapes. Each `ImmediateBatches` object is associated with a layer in the game engine, and contains a set of line batches for that layer. The `Immediate` class maintains a map of `ImmediateBatches` objects for each layer in the game engine.\n\nWhen a shape is drawn using the `Immediate` class, it is added to the appropriate `ImmediateBatches` object for the current layer. The `Immediate` class also maintains a set of all `ImmediateBatches` objects that were used in the current frame, as well as a set of all layers that were updated during the frame.\n\nThe `Immediate` class also contains methods for creating and managing shaders used for rendering the debug shapes. These shaders are simple vertex and fragment shaders that are used to render the shapes with the appropriate materials.\n\nOverall, the `Immediate` class is an important part of the PlayCanvas engine project, as it provides a simple and efficient way to render debug lines and shapes in the game engine. It is used extensively throughout the engine for debugging and visualization purposes.\n## Questions: \n 1. What is the purpose of the `Immediate` class?\n- The `Immediate` class is used for rendering debug lines and shapes in real-time.\n\n2. What is the `getBatch` method used for?\n- The `getBatch` method is used to retrieve a batch for rendering lines to a specific layer with the required depth testing state.\n\n3. What is the purpose of the `onPreRenderLayer` method?\n- The `onPreRenderLayer` method is called just before the layer is rendered to allow lines for the layer to be added from inside the frame getting rendered. It updates line batches for the specified sub-layer and adds mesh instances for the specified layer to the visible list.","metadata":{"source":".autodoc/docs/markdown/src/scene/immediate/immediate.md"}}],["507",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/lighting/lighting-params.js)\n\nThe code defines a class called `LightingParams` that allows configuration of global lighting parameters for the PlayCanvas engine. The class has several private properties that are used to store the current lighting settings, such as whether area lights, shadows, or cookies are enabled, the resolution of the shadow and cookie texture atlases, and the maximum number of lights per cell. \n\nThe class also has several public properties that can be used to modify the lighting settings, such as the number of cells along each world-space axis that the space containing lights is subdivided into, the maximum number of lights a cell can store, and the resolution of the atlas texture storing all non-directional cookie and shadow textures. \n\nThe `applySettings` method is used to apply the current lighting settings to a given render object. This method is called internally by the engine and is not intended to be called directly by users. \n\nThe purpose of this class is to provide a way for developers to configure the global lighting parameters of the PlayCanvas engine. This allows for greater control over the lighting in a scene and can be used to optimize performance by adjusting the number of lights and the resolution of the texture atlases. \n\nHere is an example of how to create a new `LightingParams` object and modify its properties:\n\n```javascript\nimport { LightingParams } from 'playcanvas';\n\nconst lightingParams = new LightingParams(true, 2048, () => {\n    console.log('Dirty lights function called');\n});\n\nlightingParams.cells = new Vec3(20, 5, 20);\nlightingParams.maxLightsPerCell = 128;\nlightingParams.shadowAtlasResolution = 4096;\nlightingParams.shadowType = SHADOW_PCF5;\nlightingParams.cookiesEnabled = true;\nlightingParams.areaLightsEnabled = true;\nlightingParams.shadowsEnabled = false;\n```\n## Questions: \n 1. What is the purpose of the LightingParams class?\n- The LightingParams class allows configuration of global lighting parameters for clustered lighting in the PlayCanvas engine.\n\n2. What is the significance of the debugLayer property?\n- The debugLayer property specifies the layer ID of a layer to contain the debug rendering of clustered lighting. It is undefined by default, which disables the debug rendering.\n\n3. What is the purpose of the applySettings method?\n- The applySettings method applies the lighting settings from a render object to the LightingParams object, allowing for dynamic changes to the lighting parameters during runtime.","metadata":{"source":".autodoc/docs/markdown/src/scene/lighting/lighting-params.md"}}],["508",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/lighting/world-clusters-debug.js)\n\nThe code defines a class called `WorldClustersDebug` that provides a static method `render` to visualize the occupancy of cells in a 3D grid. The class imports several modules from the PlayCanvas engine, including `Color`, `Mat4`, and `Vec3` from the `core/math` package, `PRIMITIVE_TRIANGLES` from `platform/graphics/constants`, and `BLEND_ADDITIVEALPHA` from `constants`. It also imports `GraphNode`, `Mesh`, `MeshInstance`, and `StandardMaterial` from the engine's `scene` module.\n\nThe `render` method takes two arguments: `worldClusters` and `scene`. `worldClusters` is an object that contains information about the cells in the 3D grid, including their positions and occupancy. `scene` is the scene object that the visualization will be rendered in.\n\nThe `render` method first extracts some information from `worldClusters`, including the number of cells in each dimension, the minimum and maximum bounds of the grid, and the number of lights in each cell. It then generates a set of grid lines to visualize the boundaries of each cell in the grid. The grid lines are stored in two arrays, `gridPositions` and `gridColors`, which are static properties of the `WorldClustersDebug` class.\n\nNext, the method generates a set of cubes to visualize the occupancy of each cell in the grid. The cubes are added to a dynamic mesh that is created if it does not already exist. The color of each cube is determined by the number of lights in the corresponding cell, with low occupancy cells being colored white and high occupancy cells being colored red. The cubes are rendered using an additive alpha blend mode, which allows them to be overlaid on top of other objects in the scene.\n\nFinally, the method renders the grid lines and the cubes using the `drawLineArrays` and `drawMesh` methods of the `scene` object. The grid lines are rendered as a set of colored lines, while the cubes are rendered as a set of triangles.\n\nThis code is likely used in the PlayCanvas engine to help developers visualize the occupancy of cells in a 3D grid, which can be useful for debugging and optimizing lighting and other effects in a scene. The `WorldClustersDebug` class could be used in conjunction with other tools and techniques to help developers fine-tune the performance and visual quality of their PlayCanvas projects.\n## Questions: \n 1. What is the purpose of the `WorldClustersDebug` class?\n- The `WorldClustersDebug` class is used to render debug information for the `worldClusters` object in a scene.\n\n2. What is the format of the data used to generate the grid lines?\n- The grid lines are generated using three nested for-loops that iterate over the `cells` of the `worldClusters` object and calculate the positions of the lines based on the `boundsMin`, `boundsMax`, and `cellDelta` values.\n\n3. What type of material is used to render the cell occupancy cubes?\n- The material used to render the cell occupancy cubes is a `StandardMaterial` with various properties set, including `useLighting = false`, `emissiveVertexColor = true`, and `blendType = BLEND_ADDITIVEALPHA`.","metadata":{"source":".autodoc/docs/markdown/src/scene/lighting/world-clusters-debug.md"}}],["509",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/materials/basic-material.js)\n\nThe code defines a class called `BasicMaterial` that extends the `Material` class. The `BasicMaterial` class is used for rendering unlit geometry, either using a constant color or a color map modulated with a color. \n\nThe `BasicMaterial` class has several properties, including `color`, which is the flat color of the material, and `colorMap`, which is the color map of the material. If `colorMap` is specified, it is modulated by the `color` property. \n\nThe `BasicMaterial` class also has a method called `updateUniforms`, which updates the material's uniforms. It sets the `uColor` uniform to the `color` property and sets the `texture_diffuseMap` uniform to the `colorMap` property if it is not null. \n\nThe `BasicMaterial` class also has a method called `getShaderVariant`, which returns a shader variant for the material. It takes several parameters, including `device`, `scene`, `objDefs`, `staticLightList`, `pass`, `sortedLights`, `viewUniformFormat`, `viewBindGroupFormat`, and `vertexFormat`. It uses these parameters to determine the options for the shader variant and then returns the appropriate shader program from the program library. \n\nOverall, the `BasicMaterial` class is an important part of the PlayCanvas engine as it provides a way to render unlit geometry with a constant color or a color map. It is used in many different parts of the engine, including the rendering pipeline and the editor. Developers can use the `BasicMaterial` class to create custom materials for their projects. \n\nExample usage:\n\n```javascript\n// Create a new Basic material\nvar material = new pc.BasicMaterial();\n\n// Set the material to have a texture map that is multiplied by a red color\nmaterial.color.set(1, 0, 0);\nmaterial.colorMap = diffuseMap;\n\n// Notify the material that it has been modified\nmaterial.update();\n```\n## Questions: \n 1. What is the purpose of the `BasicMaterial` class?\n- The `BasicMaterial` class is used for rendering unlit geometry with a constant color or a color map modulated with a color.\n\n2. What are the properties of the `BasicMaterial` class?\n- The `BasicMaterial` class has properties such as `color` (flat color of the material), `colorMap` (color map of the material), and `vertexColors` (whether to use vertex colors).\n\n3. What is the `getShaderVariant` method used for?\n- The `getShaderVariant` method is used to get the appropriate shader program for rendering the material based on the object definitions, lighting, and other options.","metadata":{"source":".autodoc/docs/markdown/src/scene/materials/basic-material.md"}}],["510",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/materials/default-material.js)\n\nThe code above is a module that exports two functions: `getDefaultMaterial` and `setDefaultMaterial`. These functions are used to get and set the default material for a graphics device in the PlayCanvas engine.\n\nThe `getDefaultMaterial` function takes a `GraphicsDevice` object as a parameter and returns the default instance of the `StandardMaterial` class. This material is used as a fallback when no other material is specified. The function first checks if the default material has already been cached for the given device using the `defaultMaterialDeviceCache` object. If it has, the cached material is returned. If not, a new instance of `StandardMaterial` is created and cached for future use. The `Debug.assert` method is used to ensure that the material exists before returning it.\n\nThe `setDefaultMaterial` function takes a `GraphicsDevice` object and an instance of `StandardMaterial` as parameters. It sets the default material for the given device by caching the material in the `defaultMaterialDeviceCache` object. If the material is already cached, it is replaced with the new material.\n\nThese functions are used throughout the PlayCanvas engine to ensure that a default material is always available for rendering. For example, if a mesh is created without a material, the default material will be used instead. This helps to prevent errors and ensure that all objects are rendered correctly.\n\nHere is an example of how these functions might be used in the PlayCanvas engine:\n\n```\nimport { GraphicsDevice } from '../../platform/graphics/graphics-device.js';\nimport { setDefaultMaterial, getDefaultMaterial } from './default-material.js';\n\nconst device = new GraphicsDevice();\n\n// Set the default material for the device\nconst material = new StandardMaterial();\nsetDefaultMaterial(device, material);\n\n// Get the default material for the device\nconst defaultMaterial = getDefaultMaterial(device);\n```\n## Questions: \n 1. What is the purpose of the `Debug` import and how is it used in this code?\n   \n   The `Debug` import is used to assert that the `material` variable is not null in both `getDefaultMaterial` and `setDefaultMaterial` functions. This helps catch potential errors early on during development.\n\n2. What is the `defaultMaterialDeviceCache` and how is it used in this code?\n   \n   `defaultMaterialDeviceCache` is a cache that stores the default material. It is used in both `getDefaultMaterial` and `setDefaultMaterial` functions to retrieve and store the default material for a given graphics device.\n\n3. What is the `StandardMaterial` and how is it related to this code?\n   \n   `StandardMaterial` is a class that represents a standard material used in 3D graphics rendering. It is used in both `getDefaultMaterial` and `setDefaultMaterial` functions to get and set the default material for a given graphics device.","metadata":{"source":".autodoc/docs/markdown/src/scene/materials/default-material.md"}}],["511",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/materials/lit-options.js)\n\nThe code defines a class called `LitOptions` which is used to specify a set of parameters that determine how the lit-shader gets generated. The class contains a large number of properties that can be set to enable or disable various features of the shader. These properties include options for enabling/disabling alpha testing, hardware instancing, morphing, and various types of maps (e.g. light maps, normal maps, etc.). There are also options for specifying custom shader chunks and for overriding the default fragment shader with a custom one. \n\nThe purpose of this class is to provide a way for developers to customize the lit-shader to suit their needs. By setting the various properties of the `LitOptions` object, developers can enable or disable specific features of the shader, or even replace entire chunks of the shader code with custom code. This allows for a high degree of flexibility in how the shader is generated, which can be useful in a variety of contexts.\n\nFor example, a developer might use `LitOptions` to create a custom shader that includes support for a specific type of map that is not supported by the default shader. They could do this by setting the appropriate property of the `LitOptions` object to `true`, and then overriding the default shader code with custom code that handles the new map type. \n\nOverall, `LitOptions` is an important part of the PlayCanvas engine, as it provides developers with a powerful tool for customizing the lit-shader to suit their needs.\n## Questions: \n 1. What is the purpose of the `LitOptions` class?\n- The `LitOptions` class determines how the lit-shader gets generated by specifying a set of parameters which triggers different fragment and vertex shader generation in the backend.\n\n2. What are some of the properties that can be set on an instance of `LitOptions`?\n- Some of the properties that can be set on an instance of `LitOptions` include `alphaTest`, `useInstancing`, `useMorphPosition`, `useMorphNormal`, `useDiffuseMap`, `useSpecular`, `useMetalness`, `fog`, `gamma`, `toneMap`, and many others.\n\n3. What is the purpose of the `pass` getter and setter in `LitOptions`?\n- The `pass` getter and setter in `LitOptions` are used to get and set the `_pass` property, which is used by the parent `StandardMaterialOptions` class. However, attempting to set `pass` directly on an instance of `LitOptions` will result in a warning message being logged to the console.","metadata":{"source":".autodoc/docs/markdown/src/scene/materials/lit-options.md"}}],["512",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/materials/standard-material-options.js)\n\nThe code defines a class called `StandardMaterialOptions` that contains a set of options used to control the shader frontend shader generation for the PlayCanvas engine. The purpose of this class is to provide a way to customize the appearance of materials in a 3D scene. \n\nThe class contains a number of boolean properties that control whether certain constants should affect the material's appearance. For example, the `diffuseTint` property controls whether the `diffuse` constant should affect the diffuse color of the material. Similarly, the `metalnessTint` property controls whether the `metalness` constant should affect the metalness value of the material. \n\nThe class also contains properties that control the encoding of certain textures, such as the `emissiveEncoding` and `lightMapEncoding` properties. Additionally, there are properties that control whether certain channels in normal maps should be inverted, such as the `glossInvert` property.\n\nOne notable property is `litOptions`, which is an instance of the `LitOptions` class. This class contains options related to lighting, such as whether to use normal maps or not. The `pass` property of `StandardMaterialOptions` is used to set the value of `LitOptions`'s `pass` property, which determines the rendering pass for the material.\n\nOverall, `StandardMaterialOptions` provides a way to customize the appearance of materials in a 3D scene by controlling various shader options. It is likely used in conjunction with other classes in the PlayCanvas engine to create and render 3D scenes. \n\nExample usage:\n\n```javascript\nimport { StandardMaterialOptions } from \"playcanvas-engine\";\n\nconst options = new StandardMaterialOptions();\noptions.diffuseTint = true;\noptions.metalnessTint = true;\noptions.litOptions.useNormalMap = true;\noptions.pass = 1;\n```\n## Questions: \n 1. What is the purpose of the `LitOptions` import and how is it used in this code?\n- The `LitOptions` import is used to create a new instance of the `LitOptions` class and assign it to the `litOptions` property of the `StandardMaterialOptions` class.\n2. What is the significance of the `_pass` property and how is it related to `pass`?\n- The `_pass` property is a private property that is used to store the value of `pass`. The `pass` getter and setter methods are used to get and set the value of `_pass` respectively, and also update the `_pass` property of the `litOptions` object.\n3. What is the purpose of the `chunks` property and how is it used?\n- The `chunks` property is an array that is used to store the names of shader chunks that should be included in the shader program. It is not used or modified in this code, but can be used by other parts of the engine to customize the shader program.","metadata":{"source":".autodoc/docs/markdown/src/scene/materials/standard-material-options.md"}}],["513",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/materials/standard-material-parameters.js)\n\nThe code defines a function and several constants related to the parameters of a standard material used in the PlayCanvas engine. The `_textureParameter` function takes a name, and two optional boolean parameters, `channel` and `vertexColor`, and returns an object with properties related to the texture map of that name. The `standardMaterialParameterTypes` constant is an object that defines the types of parameters that can be used in a standard material, including strings, numbers, booleans, and textures. It also includes several properties that are objects returned by the `_textureParameter` function. \n\nThe `standardMaterialTextureParameters` and `standardMaterialCubemapParameters` constants are arrays that contain the names of the texture and cubemap parameters, respectively, that can be used in a standard material. The `standardMaterialRemovedParameters` constant is an object that contains the names of several parameters that have been removed from the standard material.\n\nThese constants are used throughout the PlayCanvas engine to define and manipulate standard materials. For example, when creating a new material, the developer can use the `standardMaterialParameterTypes` object to specify the types of parameters that the material will have. They can also use the `standardMaterialTextureParameters` and `standardMaterialCubemapParameters` arrays to specify which texture and cubemap parameters the material will use. \n\nOverall, this code provides a standardized way of defining and working with materials in the PlayCanvas engine, making it easier for developers to create and manipulate materials in their projects.\n## Questions: \n 1. What is the purpose of the `_textureParameter` function?\n- The `_textureParameter` function is used to generate an object containing texture-related parameters for a material.\n\n2. What are the different types of parameters that can be used in a standard material?\n- The different types of parameters that can be used in a standard material include strings, booleans, numbers, enums, textures, cubemaps, rgb values, and bounding boxes.\n\n3. What is the purpose of the `standardMaterialRemovedParameters` object?\n- The `standardMaterialRemovedParameters` object contains a list of parameters that have been removed from the standard material.","metadata":{"source":".autodoc/docs/markdown/src/scene/materials/standard-material-parameters.md"}}],["514",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/materials/standard-material-validator.js)\n\nThe code defines a class called `StandardMaterialValidator` that is responsible for validating input data for a standard material. The class has a `validate` method that takes in the input data and validates it against the defined standard material properties and types. If the `removeInvalid` flag is set to true, then invalid properties are removed from the data.\n\nThe class has an `_createEnumValidator` method that creates a validator function for validating enum values. The validator function takes in a value and checks if it is one of the allowed values for the enum.\n\nThe class has an `enumValidators` object that contains validators for the different enums used in the standard material. The enums include `occludeSpecular`, `cull`, `blendType`, `depthFunc`, and `shadingModel`.\n\nThe `validate` method loops through the input data and checks the type of each property against the defined standard material types. If the type is not recognized, the property is ignored. If the type is an enum, the validator function for that enum is used to validate the value. If the value is invalid, the property is marked as invalid and removed if the `removeInvalid` flag is set to true. If the type is a number, boolean, string, vec2, rgb, texture, boundingbox, or cubemap, the value is validated accordingly.\n\nThe `setInvalid` method is called when a property is found to be invalid. It sets the `valid` flag to false, logs a warning message, and removes the property if the `removeInvalid` flag is set to true.\n\nThe purpose of this class is to ensure that input data for a standard material is valid and conforms to the defined standard material properties and types. This is important for ensuring that the standard material behaves as expected and that unexpected errors do not occur. The class can be used in the larger project to validate input data for standard materials before they are used to create materials. \n\nExample usage:\n\n```\nconst validator = new StandardMaterialValidator();\nconst data = {\n    diffuse: [1, 1, 1],\n    specular: [1, 1, 1],\n    occludeSpecular: 'invalid',\n    cull: 'invalid',\n    blendType: 'invalid',\n    depthFunc: 'invalid',\n    shadingModel: 'invalid'\n};\nconst isValid = validator.validate(data);\nconsole.log(isValid); // false\nconsole.log(data); // { diffuse: [1, 1, 1], specular: [1, 1, 1] }\n```\n## Questions: \n 1. What is the purpose of the `StandardMaterialValidator` class?\n- The `StandardMaterialValidator` class is used to validate input data against defined standard-material properties and types, and remove invalid properties from the data if the `removeInvalid` flag is set to true.\n\n2. What are the different types of blend modes supported by this code?\n- The different types of blend modes supported by this code include `BLEND_SUBTRACTIVE`, `BLEND_ADDITIVE`, `BLEND_NORMAL`, `BLEND_NONE`, `BLEND_PREMULTIPLIED`, `BLEND_MULTIPLICATIVE`, `BLEND_ADDITIVEALPHA`, `BLEND_MULTIPLICATIVE2X`, `BLEND_SCREEN`, `BLEND_MIN`, and `BLEND_MAX`.\n\n3. What is the purpose of the `_createEnumValidator` method?\n- The `_createEnumValidator` method is used to create a function that validates whether a given value is included in a specified array of values. This method is used to validate input data against enumerated types such as `occludeSpecular`, `cull`, `blendType`, `depthFunc`, and `shadingModel`.","metadata":{"source":".autodoc/docs/markdown/src/scene/materials/standard-material-validator.md"}}],["515",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/model.js)\n\nThe code defines a class called `Model` that represents a graphical object that can be added to or removed from a scene. It contains a hierarchy and any number of mesh instances. The class has several properties including `graph`, an instance of `GraphNode` class that represents the root node of the model's graph node hierarchy, `meshInstances`, an array of `MeshInstance` objects contained in this model, `skinInstances`, an array of `SkinInstance` objects contained in this model, and `morphInstances`, an array of `MorphInstance` objects contained in this model.\n\nThe class has a constructor that initializes some properties including `cameras` and `lights`. It also has a method called `getMaterials()` that returns an array of materials used by the mesh instances in the model. The method loops through the `meshInstances` array and adds the material of each mesh instance to the `materials` array if it is not already in the array.\n\nThe class has a method called `clone()` that clones the model. The method creates a new hierarchy and mesh instances, but meshes are shared between the clone and the specified model. The method duplicates the node hierarchy, skin instances, morph instances, and mesh instances. It then creates a new instance of the `Model` class and sets its properties to the cloned values. Finally, it synchronizes the hierarchy of the cloned model.\n\nThe class also has a method called `destroy()` that destroys skinning texture and possibly deletes vertex/index buffers of a model. It loops through the `meshInstances` array and calls the `destroy()` method of each mesh instance.\n\nThe class has a method called `generateWireframe()` that generates the necessary internal data for a model to be renderable as wireframe. Once this function has been called, any mesh instance in the model can have its `renderStyle` property set to `RENDERSTYLE_WIREFRAME`.\n\nOverall, the `Model` class provides a way to represent a graphical object in a scene and manipulate its properties. It can be used to create, clone, destroy, and render models in a PlayCanvas engine project.\n## Questions: \n 1. What is the purpose of the `Model` class?\n    \n    The `Model` class is a graphical object that can be added to or removed from a scene. It contains a hierarchy and any number of mesh instances.\n\n2. How can a developer clone a `Model` instance?\n    \n    A developer can clone a `Model` instance by calling the `clone()` method on the instance. The returned model has a newly created hierarchy and mesh instances, but meshes are shared between the clone and the specified model.\n\n3. What is the purpose of the `generateWireframe()` method?\n    \n    The `generateWireframe()` method generates the necessary internal data for a model to be renderable as wireframe. Once this function has been called, any mesh instance in the model can have its renderStyle property set to `RENDERSTYLE_WIREFRAME`.","metadata":{"source":".autodoc/docs/markdown/src/scene/model.md"}}],["516",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/morph-target.js)\n\nThe code defines a class called `MorphTarget` which represents a deformation data to apply to an existing mesh. The class is used to create multiple morph targets that can be blended together on a mesh. This is useful for effects that are hard to achieve with conventional animation and skinning. \n\nThe class has several properties and methods that allow for the creation and manipulation of morph targets. The `constructor` method takes an object with several optional arguments. The `deltaPositions` argument is an array of 3-dimensional vertex position offsets. The `deltaPositionsType` argument is a format to store position offsets inside a `VertexBuffer`. The `deltaNormals` argument is an array of 3-dimensional vertex normal offsets. The `deltaNormalsType` argument is a format to store normal offsets inside a `VertexBuffer`. The `name` argument is a string that represents the name of the morph target. The `aabb` argument is a `BoundingBox` object that represents the bounding box of the morph target. The `defaultWeight` argument is a number that represents the default blend weight to use for this morph target. The `preserveData` argument is a boolean that, when true, allows the morph target to keep its data passed using the options, allowing the clone operation.\n\nThe `MorphTarget` class has several methods that allow for the manipulation of morph targets. The `destroy` method destroys the morph target and its associated resources. The `clone` method returns an identical copy of the specified morph target. This can only be used if the morph target was created with `preserveData` set to true. The `get` methods return the name, default weight, and bounding box of the morph target. The `morphPositions` and `morphNormals` methods return a boolean that indicates whether the morph target has vertex positions and normals, respectively.\n\nThe `MorphTarget` class also has several private methods that are used internally. The `_postInit` method releases the original data and marks the morph target as used. The `_initVertexBuffers` method initializes the vertex buffers for the morph target. The `_createVertexBuffer` method creates a vertex buffer with the specified type and semantic. The `_setTexture` method sets the texture for the morph target.\n\nIn summary, the `MorphTarget` class is used to create and manipulate morph targets that can be blended together on a mesh. The class has several properties and methods that allow for the creation and manipulation of morph targets. The class is part of the PlayCanvas engine project and can be used to create complex effects that are hard to achieve with conventional animation and skinning.\n## Questions: \n 1. What is the purpose of the `MorphTarget` class?\n- The `MorphTarget` class contains deformation data to apply to an existing mesh, and can be blended with other morph targets for special effects that are difficult to achieve with conventional animation and skinning.\n\n2. What are the optional arguments that can be passed to the `MorphTarget` constructor?\n- The `MorphTarget` constructor can be passed an object with optional arguments including `deltaPositions` (an array of 3-dimensional vertex position offsets), `deltaPositionsType` (a format to store position offsets), `deltaNormals` (an array of 3-dimensional vertex normal offsets), `deltaNormalsType` (a format to store normal offsets), `name` (a string name), `aabb` (a bounding box), `defaultWeight` (a default blend weight), and `preserveData` (a boolean to keep the morph target data for cloning).\n\n3. What methods and properties are available on a `MorphTarget` instance?\n- A `MorphTarget` instance has methods to destroy the instance and create a copy of the instance (`destroy()` and `clone()`), as well as properties to get the name, default weight, and bounding box of the morph target (`name`, `defaultWeight`, and `aabb`). It also has properties to check if there are morph positions or normals (`morphPositions` and `morphNormals`).","metadata":{"source":".autodoc/docs/markdown/src/scene/morph-target.md"}}],["517",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/morph.js)\n\nThe code defines a class called `Morph` which represents a collection of morph targets and associated data. Morph targets are used to animate 3D models by changing the positions and normals of their vertices. The `Morph` class provides functionality for managing and rendering these morph targets.\n\nThe class imports several modules from the PlayCanvas engine, including `Debug`, `RefCountedObject`, `Vec3`, `FloatPacking`, `BoundingBox`, `Texture`, `VertexBuffer`, and `VertexFormat`. It also imports several constants from the `constants.js` module.\n\nThe `Morph` class has a constructor that takes an array of morph targets, a graphics device, and an options object as arguments. The morph targets are represented by instances of the `MorphTarget` class, which is not defined in this file. The graphics device is used to manage the morph targets, and the options object is used to pass optional arguments to the constructor. The `preferHighPrecision` option specifies whether high-precision storage should be used for the morph targets. If this option is set to `true`, the morph targets will be faster to create and will allow for higher precision, but will take up more memory and may be slower to render.\n\nThe `Morph` class has several properties, including `_aabb`, `preferHighPrecision`, `_targets`, `_renderTextureFormat`, `_textureFormat`, `_useTextureMorph`, `_morphPositions`, `_morphNormals`, `morphTextureWidth`, `morphTextureHeight`, and `vertexBufferIds`. The `_aabb` property is a bounding box that contains the morph targets. The `preferHighPrecision` property specifies whether high-precision storage is preferred. The `_targets` property is an array of morph targets. The `_renderTextureFormat` and `_textureFormat` properties specify the formats of the textures used to store the morph targets. The `_useTextureMorph` property specifies whether texture-based morphing is used. The `_morphPositions` and `_morphNormals` properties specify whether positions and normals are morphed. The `morphTextureWidth` and `morphTextureHeight` properties specify the dimensions of the textures used to store the morph targets. The `vertexBufferIds` property is a vertex buffer that maps vertices to texture coordinates.\n\nThe `Morph` class has several methods, including `_init`, `_findSparseSet`, `_initTextureBased`, `destroy`, `targets`, and `_updateMorphFlags`. The `_init` method initializes the morph targets. The `_findSparseSet` method finds a sparse set of vertices for the morph targets. The `_initTextureBased` method initializes texture-based morphing. The `destroy` method frees video memory allocated by the `Morph` object. The `targets` method returns an array of morph targets. The `_updateMorphFlags` method updates the `_morphPositions` and `_morphNormals` properties based on the morph targets.\n\nOverall, the `Morph` class provides functionality for managing and rendering morph targets. It allows for both attribute-based and texture-based morphing, and provides options for controlling the precision and performance of the morph targets.\n## Questions: \n 1. What is the purpose of the `Morph` class?\n- The `Morph` class contains a list of morph targets, a combined delta AABB, and associated data for morphing positions and normals of a mesh.\n2. What is the difference between `preferHighPrecision` and `useTextureMorph`?\n- `preferHighPrecision` is a boolean option that determines whether high precision storage should be preferred for morph target data, while `useTextureMorph` is a boolean flag that indicates whether texture-based morphing is enabled.\n3. What is the purpose of the `_findSparseSet` method?\n- The `_findSparseSet` method finds a sparse set of vertices for all target deltas and builds a vertex id buffer, which is used to map each vertex to a pixel in a texture for texture-based morphing.","metadata":{"source":".autodoc/docs/markdown/src/scene/morph.md"}}],["518",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/particle-system/gpu-updater.js)\n\nThe code defines a class called `ParticleGPUUpdater` that wraps GPU particle render state and setup from `ParticleEmitter`. The purpose of this class is to update the GPU particle system with the latest emitter state and render the particles on the screen. \n\nThe class imports various math and graphics-related modules from the PlayCanvas engine. It also imports a function called `drawQuadWithShader` from `quad-render-utils.js` module. \n\nThe class constructor takes two arguments: `emitter` and `gd`. `emitter` is an instance of `ParticleEmitter` class, and `gd` is an instance of the PlayCanvas graphics device. \n\nThe class has several properties that are used to set the values of shader uniforms. These properties are Float32Array objects that hold the values of various emitter properties such as emitter position, emitter scale, spawn bounds, initial velocity, lifetime, etc. \n\nThe class has a method called `_setInputBounds()` that sets the values of `inBoundsSizeUniform` and `inBoundsCenterUniform` properties. These properties are used to calculate the size and center of the emitter's bounding box. \n\nThe class has a method called `randomize()` that sets the values of `frameRandomUniform` property to random values between 0 and 1. \n\nThe class has a method called `update()` that updates the GPU particle system with the latest emitter state. The method takes four arguments: `device`, `spawnMatrix`, `extentsInnerRatioUniform`, `delta`, and `isOnStop`. `device` is the PlayCanvas graphics device, `spawnMatrix` is the spawn matrix of the emitter, `extentsInnerRatioUniform` is the extents inner ratio of the emitter, `delta` is the time elapsed since the last update, and `isOnStop` is a boolean flag that indicates whether the emitter is stopped. \n\nThe method sets the blend state, depth state, and cull mode of the graphics device to default, no depth, and none, respectively. It then calls the `randomize()` method to set the values of `frameRandomUniform` property. \n\nThe method sets the values of various shader uniforms using the properties defined in the class. It then calls the `drawQuadWithShader()` function to render the particles on the screen. \n\nFinally, the method sets the values of `particleTexIN` and `particleTexOUT` parameters of the emitter's material to `texOUT` and `texIN`, respectively. It also sets the `beenReset` property of the emitter to false and swaps the values of `particleTexIN` and `particleTexOUT` properties of the emitter. \n\nIn summary, the `ParticleGPUUpdater` class is responsible for updating the GPU particle system with the latest emitter state and rendering the particles on the screen. It sets the values of various shader uniforms using the emitter properties and calls the `drawQuadWithShader()` function to render the particles.\n## Questions: \n 1. What is the purpose of the `ParticleGPUUpdater` class?\n- The `ParticleGPUUpdater` class wraps GPU particles render state and setup from ParticleEmitter.\n\n2. What are the inputs to the `update` method of the `ParticleGPUUpdater` class?\n- The inputs to the `update` method of the `ParticleGPUUpdater` class are `device`, `spawnMatrix`, `extentsInnerRatioUniform`, `delta`, and `isOnStop`.\n\n3. What is the purpose of the `randomize` method of the `ParticleGPUUpdater` class?\n- The `randomize` method of the `ParticleGPUUpdater` class generates random values for the `frameRandomUniform` array.","metadata":{"source":".autodoc/docs/markdown/src/scene/particle-system/gpu-updater.md"}}],["519",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/render.js)\n\nThe code defines a class called `Render` which is a resource of a Render Asset. A `Render` instance contains an array of meshes that are referenced by a single hierarchy node in a GLB model. The class extends the `EventHandler` class, which means it can emit events and listen to events. \n\nThe `Render` class has a constructor that initializes an array of meshes to null. The meshes are reference counted, and this class owns the references and is responsible for releasing the meshes when they are no longer referenced. The `Render` class has two methods, `decRefMeshes()` and `incRefMeshes()`, which decrement and increment the reference count of the meshes respectively. The `destroy()` method sets the meshes to null.\n\nThe `Render` class has two properties, `meshes` and `_meshes`. The `meshes` property is a getter/setter that sets and gets the `_meshes` property. When the `meshes` property is set, it decrements the reference count of the existing meshes, assigns new meshes, increments the reference count of the new meshes, and fires a `set:meshes` event. When the `meshes` property is read, it returns the `_meshes` property.\n\nThe `Render` class also has a `set:meshes` event that is fired when the meshes are set. The event passes an array of meshes as an argument.\n\nThis code is used to manage the meshes of a `Render` instance. It ensures that the meshes are reference counted and that they are destroyed when they are no longer referenced. It also provides a way to set and get the meshes and to listen to the `set:meshes` event. This code is part of the PlayCanvas engine project and is used to render 3D models in a web browser. \n\nExample usage:\n\n```javascript\nimport { Render } from 'playcanvas-engine';\n\nconst render = new Render();\n\n// set the meshes\nrender.meshes = [mesh1, mesh2, mesh3];\n\n// listen to the set:meshes event\nrender.on('set:meshes', (meshes) => {\n    console.log(meshes);\n});\n\n// get the meshes\nconst meshes = render.meshes;\n```\n## Questions: \n 1. What is the purpose of the `Render` class and how is it used in the PlayCanvas engine?\n- The `Render` class is a resource of a Render Asset that contains an array of meshes referenced by a single hierarchy node in a GLB model. It is used to manage the reference counting and releasing of meshes.\n2. What events are fired when the `meshes` property is set and who listens to them?\n- The `set:meshes` event is fired when the `meshes` property is set, and it takes an array of meshes as its argument. It is ignored by the class itself, but other objects can listen to it.\n3. How are the reference counts of meshes managed in the `Render` class?\n- The `Render` class owns the references to the meshes and is responsible for releasing them when they are no longer referenced. The `decRefMeshes()` method decrements the reference count of each mesh and destroys the ones with zero references, while the `incRefMeshes()` method increments the reference count of all meshes.","metadata":{"source":".autodoc/docs/markdown/src/scene/render.md"}}],["520",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/renderer/cookie-renderer.js)\n\nThe code defines a helper class called `CookieRenderer` that is used by the PlayCanvas engine to render cookies into a texture atlas. Cookies are textures that are used to add additional detail to lighting in a scene. The `CookieRenderer` class is used by the clustered lighting system to render cookies into the texture atlas, similar to the shadow renderer.\n\nThe `CookieRenderer` class has a constructor that takes a `device` and a `lightTextureAtlas` as arguments. The `device` is an object that represents the graphics device used by the engine, and the `lightTextureAtlas` is a texture atlas that is used to store the cookies.\n\nThe `CookieRenderer` class has a `render` method that takes a `light` and a `renderTarget` as arguments. The `light` is an object that represents a light in the scene, and the `renderTarget` is the render target that the cookies will be rendered to. The `render` method checks if the light is enabled, has a cookie, and is visible in the current frame. If these conditions are met, the method renders the cookie to the render target.\n\nThe `CookieRenderer` class has a `destroy` method that is currently empty. This method can be used to clean up any resources that were created by the `CookieRenderer` class.\n\nThe `CookieRenderer` class has several private methods and properties that are used to render the cookies. These include `getShader`, `shader2d`, `shaderCube`, `createTexture`, `_invViewProjMatrices`, and `initInvViewProjMatrices`. These methods and properties are used to create and manage the shaders and textures that are used to render the cookies.\n\nThe `CookieRenderer` class uses several other classes and constants from the PlayCanvas engine, including `Vec4`, `Mat4`, `ADDRESS_CLAMP_TO_EDGE`, `FILTER_NEAREST`, `PIXELFORMAT_RGBA8`, `DebugGraphics`, `drawQuadWithShader`, `Texture`, `LIGHTTYPE_OMNI`, `createShaderFromCode`, `LightCamera`, and `BlendState`. These classes and constants are used to create and manage the graphics resources that are used to render the cookies.\n\nOverall, the `CookieRenderer` class is an important part of the PlayCanvas engine's lighting system. It allows cookies to be rendered into a texture atlas, which can then be used to add additional detail to lighting in a scene.\n## Questions: \n 1. What is the purpose of the `CookieRenderer` class?\n- The `CookieRenderer` class is a helper class used by the clustered lighting system to render cookies into the texture atlas, similarly to the shadow renderer.\n\n2. What is the difference between `textureBlitFragmentShader` and `textureCubeBlitFragmentShader`?\n- `textureBlitFragmentShader` is used for 2D textures, while `textureCubeBlitFragmentShader` is used for cubemaps. The latter uses the inverse view projection matrices for the 6 faces to copy the cubemap faces into the atlas.\n\n3. What is the significance of the `invViewProjId` uniform?\n- The `invViewProjId` uniform is used to pass the inverse view projection matrix to the shader, which is used to convert the UV coordinates of the cubemap faces to world space positions for sampling the texture.","metadata":{"source":".autodoc/docs/markdown/src/scene/renderer/cookie-renderer.md"}}],["521",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/renderer/light-camera.js)\n\nThe code defines a helper static class called `LightCamera` that provides shared functionality for shadow and cookie cameras used by lights in the PlayCanvas engine. The class contains two static methods: `create` and `evalSpotCookieMatrix`.\n\nThe `create` method creates a new camera object with specific settings based on the type of light. The method takes three parameters: `name` (string), `lightType` (number), and `face` (number). The `name` parameter is used to name the camera's graph node. The `lightType` parameter specifies the type of light (omni, spot, or directional) and determines the camera's projection type and rotation. The `face` parameter is used only for omni lights and specifies the cubemap face to render. The method returns the newly created camera object.\n\nThe `evalSpotCookieMatrix` method calculates the view-projection matrix for a spot light cookie. It takes one parameter: `light` (object). The method first checks if a temporary cookie camera object has already been created. If not, it creates one using the `create` method with the `LIGHTTYPE_SPOT` parameter. It then sets the camera's field of view based on the light's outer cone angle. The method positions and rotates the camera's graph node to match the light's position and rotation, and then calculates the view-projection matrix using the camera's projection matrix and the inverse of the camera's transform matrix. The method then applies the light's atlas viewport to the matrix and returns the resulting cookie matrix.\n\nOverall, the `LightCamera` class provides a convenient way to create and manipulate camera objects for use with lights in the PlayCanvas engine. The `create` method allows for easy creation of cameras with specific settings based on the type of light, while the `evalSpotCookieMatrix` method simplifies the calculation of view-projection matrices for spot light cookies.\n## Questions: \n 1. What is the purpose of the `LightCamera` class?\n- The `LightCamera` class is a helper static class that provides shared functionality for shadow and cookie cameras used by the lights.\n\n2. What are the different types of lights supported by the PlayCanvas engine?\n- The PlayCanvas engine supports three types of lights: `LIGHTTYPE_DIRECTIONAL`, `LIGHTTYPE_OMNI`, and `LIGHTTYPE_SPOT`.\n\n3. What is the purpose of the `evalSpotCookieMatrix` method in the `LightCamera` class?\n- The `evalSpotCookieMatrix` method is used to calculate the spot light cookie view-projection matrix when the shadow matrix is not available. It creates a temporary camera and sets its position and rotation based on the light's position and rotation. It then calculates the view-projection matrix and applies it to the light's cookie matrix.","metadata":{"source":".autodoc/docs/markdown/src/scene/renderer/light-camera.md"}}],["522",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/renderer/shadow-map-cache.js)\n\nThe `ShadowMapCache` class is responsible for managing a cache of shadow maps used by lights in the PlayCanvas engine. In the normal case, each light has a unique shadow map. However, in two specific cases, the `ShadowMapCache` is used to limit allocations and re-use shadow maps. \n\nThe first case is when lights are baked to lightmaps one at a time by the `Lightmapper`. In this case, shadow maps are re-used to limit allocations and are deleted when baking is done. The second case is when VSM (Variance Shadow Mapping) blur is done by the `ShadowRenderer`. In this case, a temporary buffer is grabbed from the cache.\n\nThe `ShadowMapCache` class has a `cache` property that is a `Map` object that maps a shadow map key to an array of shadow maps in the cache. The `getKey` method generates a string key for the shadow map required by the light. The key is based on the light's type, shadow type, and shadow resolution. If a matching shadow buffer is found in the cache, it is returned. Otherwise, a new shadow map is created and added to the cache.\n\nThe `ShadowMapCache` class has a `clear` method that removes all shadow maps from the cache. It also has a `destroy` method that clears the cache and sets it to null.\n\nThis class is used in the PlayCanvas engine to optimize the use of shadow maps and reduce memory allocations. It is used by the `Lightmapper` and `ShadowRenderer` to re-use shadow maps and limit allocations. \n\nExample usage:\n\n```javascript\nimport { ShadowMapCache } from './shadow-map-cache.js';\n\nconst shadowMapCache = new ShadowMapCache();\n\n// get shadow map for a light\nconst shadowMap = shadowMapCache.get(device, light);\n\n// use shadow map for rendering\n\n// return shadow map to cache\nshadowMapCache.add(light, shadowMap);\n\n// clear cache\nshadowMapCache.clear();\n\n// destroy cache\nshadowMapCache.destroy();\n```\n## Questions: \n 1. What is the purpose of the `ShadowMapCache` class?\n- The `ShadowMapCache` class is used to store and manage shadow maps for lights in the PlayCanvas engine, and can be used to re-use shadow maps to limit allocations.\n\n2. What is the `getKey` method used for?\n- The `getKey` method is used to generate a unique string key for a shadow map based on the light's properties, such as whether it is a cube map, its shadow type, and its resolution.\n\n3. What is the difference between using the `get` and `add` methods of the `ShadowMapCache` class?\n- The `get` method is used to retrieve a shadow map from the cache, or create a new one if none are available, while the `add` method is used to add a shadow map back to the cache for reuse.","metadata":{"source":".autodoc/docs/markdown/src/scene/renderer/shadow-map-cache.md"}}],["523",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/renderer/shadow-map.js)\n\nThe code defines a class called `ShadowMap` that is used to create and manage shadow maps for different types of lights in the PlayCanvas engine. A shadow map is a texture that stores depth information from a light's point of view, which is used to determine which parts of a scene are in shadow and which are not. \n\nThe `ShadowMap` class has a constructor that takes a texture and an array of render targets as arguments. The texture is the actual buffer that is shared by all the shadow map render targets, while the render targets are used to render the scene from the light's point of view. The class also has a `destroy` method that destroys the texture and all the render targets associated with the shadow map.\n\nThe class has several static methods that are used to create different types of shadow maps. The `create` method is used to create a shadow map for a light. It takes a device object and a light object as arguments and returns a shadow map object. The method checks the type of the light and calls either the `createCubemap` or `create2dMap` method to create the shadow map.\n\nThe `create2dMap` method creates a 2D shadow map for a directional or spot light. It takes a device object, a size, and a shadow type as arguments and returns a shadow map object. The method creates a texture with the specified size and format, and a render target with the texture as its color buffer and depth buffer. The method also sets the texture's filtering and addressing modes based on the shadow type.\n\nThe `createCubemap` method creates a cubemap shadow map for an omni light. It takes a device object and a size as arguments and returns a shadow map object. The method creates a cubemap texture with the specified size and format, and six render targets with the texture as their color buffer and depth buffer, one for each face of the cubemap.\n\nThe `getShadowFormat` and `getShadowFiltering` methods are used to determine the format and filtering mode of the shadow map texture based on the shadow type and the device's capabilities.\n\nOverall, the `ShadowMap` class provides a way to create and manage shadow maps for different types of lights in the PlayCanvas engine. It is an important component of the engine's rendering system and is used extensively in rendering scenes with dynamic lighting.\n## Questions: \n 1. What is the purpose of the ShadowMap class?\n- The ShadowMap class is used to create and manage shadow maps for different types of lights in the PlayCanvas engine.\n\n2. What are the different types of shadow filtering used in the PlayCanvas engine?\n- The different types of shadow filtering used in the PlayCanvas engine are FILTER_NEAREST and FILTER_LINEAR.\n\n3. How is the ShadowMap class used to create a shadow map for a light?\n- The ShadowMap class is used to create a shadow map for a light by calling the static create method and passing in the device and light object as parameters. The method determines the type of shadow map to create based on the type of light and returns a new ShadowMap object.","metadata":{"source":".autodoc/docs/markdown/src/scene/renderer/shadow-map.md"}}],["524",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/renderer/shadow-renderer-directional.js)\n\nThe code defines a class called `ShadowRendererDirectional` that is responsible for rendering directional shadows in the PlayCanvas engine. The class contains methods for culling shadow maps, generating frustum split distances, and building a frame graph for rendering directional shadows. \n\nThe `cull` method is responsible for culling shadow casters and calculating the depth range of the caster's AABB from the point of view of the shadow camera. It takes a light, an array of draw calls, and a camera as input. The method generates splits for the cascades, assigns a render target, and sets the viewport. It then calculates the center of the frustum slice, the radius of the world space bounding sphere for the frustum slice, and the axis of the light coordinate system. The method transforms the sphere's center into the center of the shadow map, pixel-aligned, and looks at the center from far away to include all casters during culling. Finally, it adjusts the shadow camera's near and far plane to the depth range of casters to maximize precision of values stored in the shadow map.\n\nThe `generateSplitDistances` method generates frustum split distances. It takes a light, a near distance, and a far distance as input. The method fills the shadow cascade distances with the far distance and lerps between linear and logarithmic distance, called practical split distance, to generate the split distances.\n\nThe `addLightRenderPasses` method prepares render targets/cameras for rendering and sets up a render pass using any of the cameras. It takes a frame graph, a light, and a camera as input. The method renders all faces inside the render pass and applies VSM blur if needed.\n\nThe `buildFrameGraph` method builds a frame graph for rendering of directional shadows for the render action. It takes a frame graph, a render action, and a camera as input. The method creates required render passes per light and adds them to the frame graph.\n\nOverall, the `ShadowRendererDirectional` class is an essential part of the PlayCanvas engine that enables the rendering of directional shadows. It provides methods for culling shadow maps, generating frustum split distances, and building a frame graph for rendering directional shadows.\n## Questions: \n 1. What is the purpose of the `getDepthRange` function?\n- The `getDepthRange` function evaluates the depth range that the axis-aligned bounding box (AABB) of visible shadow casters takes in the space of the camera, and returns the minimum and maximum depth values.\n\n2. What is the significance of the `shadowUpdateOverrides` property?\n- The `shadowUpdateOverrides` property is used to manually control which shadow cascades should be rendered in the current frame. If the value for a cascade is set to `SHADOWUPDATE_NONE`, that cascade will not be rendered.\n\n3. What is the role of the `ShadowRendererDirectional` class in the PlayCanvas engine?\n- The `ShadowRendererDirectional` class is responsible for rendering directional shadows in the PlayCanvas engine. It generates split distances for the cascades, culls shadow casters, calculates depth range of the caster's AABB, and builds a frame graph for rendering of directional shadows.","metadata":{"source":".autodoc/docs/markdown/src/scene/renderer/shadow-renderer-directional.md"}}],["525",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/renderer/shadow-renderer-local.js)\n\nThe `ShadowRendererLocal` class is responsible for rendering shadows for local lights in the PlayCanvas engine. Local lights are lights that have a position and a limited range, as opposed to directional lights which have no position and infinite range. The class contains methods for culling shadow casters, preparing lights for rendering shadows, and setting up render passes for rendering shadows.\n\nThe `cull` method is responsible for culling shadow casters for a given light. It first checks if the light has a shadow map allocated, and if not, it creates one. It then calculates the number of faces to render based on the light type (spot or omni), and for each face, it sets up a shadow camera and culls shadow casters using the `shadowRenderer.cullShadowCasters` method.\n\nThe `prepareLights` method is responsible for preparing lights for rendering shadows. It takes an array of `lights` and adds any lights that need to render shadows to the `shadowLights` array. It then prepares each face of the shadow camera for each light that needs to render shadows.\n\nThe `prepareClusteredRenderPass` method sets up a render pass for rendering shadows for local clustered lights. It first prepares the shadow cameras for rendering, and then sets up the render pass using any of the cameras. It then renders shadows for each light in the `shadowLights` array, and clears the array when done.\n\nThe `setupNonClusteredFaceRenderPass` method sets up a render pass for rendering a single face of a shadow map for a non-clustered light. It prepares the shadow camera for rendering, creates a new render pass, and sets up the render pass using the shadow camera. It then adds the render pass to the frame graph.\n\nThe `buildNonClusteredRenderPasses` method sets up render passes for rendering shadows for local non-clustered lights. It loops through each light, and if the light needs to render shadows, it sets up a render pass for each face of the shadow map.\n\nOverall, the `ShadowRendererLocal` class is an important part of the PlayCanvas engine's rendering pipeline, as it enables local lights to cast shadows. It provides methods for culling shadow casters, preparing lights for rendering shadows, and setting up render passes for rendering shadows. These methods are used in conjunction with other parts of the engine to render shadows for local lights.\n## Questions: \n 1. What is the purpose of this code file?\n- This code file contains the implementation of the `ShadowRendererLocal` class, which is responsible for rendering shadows for local lights in the PlayCanvas engine.\n\n2. What types of lights are supported for shadow rendering?\n- The code supports two types of lights for shadow rendering: `LIGHTTYPE_OMNI` for omni-directional lights and `LIGHTTYPE_SPOT` for spot lights.\n\n3. How are the shadow maps allocated and rendered?\n- The code allocates shadow maps for lights unless in clustered lighting mode, and then culls shadow casters and renders shadows for each face of the light's shadow map. For non-clustered lights, each shadow face is rendered in a separate render pass to a separate render target.","metadata":{"source":".autodoc/docs/markdown/src/scene/renderer/shadow-renderer-local.md"}}],["526",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunk-utils.js)\n\nThe code above defines two tables, `decodeTable` and `encodeTable`, which map texture encodings to their corresponding decoding and encoding functions, respectively. The `ChunkUtils` class provides two static methods, `decodeFunc` and `encodeFunc`, which take an encoding string as input and return the name of the corresponding decoding or encoding function from the appropriate table. If the input encoding is not found in the table, the default function `decodeGamma` or `encodeGamma` is returned.\n\nThis code is likely used in the larger PlayCanvas engine project to facilitate the decoding and encoding of textures with different encodings. By providing a mapping between encodings and their corresponding functions, this code allows other parts of the engine to easily access the appropriate decoding or encoding function for a given texture. For example, if a texture is encoded with the `rgbm` encoding, another part of the engine could use `ChunkUtils.decodeFunc('rgbm')` to retrieve the name of the corresponding decoding function, `decodeRGBM`, and then use that function to decode the texture.\n\nHere is an example of how this code might be used in the larger PlayCanvas engine project:\n\n```\nimport { ChunkUtils } from 'playcanvas-engine';\n\n// assume we have a texture with the encoding 'srgb'\nconst textureEncoding = 'srgb';\n\n// get the name of the decoding function for the texture encoding\nconst decodeFuncName = ChunkUtils.decodeFunc(textureEncoding);\n\n// retrieve the actual decoding function from the appropriate module\nconst decodeFunc = require(`playcanvas-engine/decoders/${decodeFuncName}`);\n\n// use the decoding function to decode the texture\nconst decodedTexture = decodeFunc(textureData);\n```\n## Questions: \n 1. What is the purpose of the `decodeTable` and `encodeTable` objects?\n    - The `decodeTable` and `encodeTable` objects map texture encodings to their corresponding decode and encode functions, respectively.\n\n2. What is the significance of the `ChunkUtils` class?\n    - The `ChunkUtils` class provides static methods for retrieving the appropriate decode and encode functions based on a given texture encoding.\n\n3. What happens if an unknown texture encoding is passed to the `decodeFunc` or `encodeFunc` methods?\n    - If an unknown texture encoding is passed to the `decodeFunc` or `encodeFunc` methods, the default `decodeGamma` or `encodeGamma` function will be returned, respectively.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunk-utils.md"}}],["527",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/chunk-validation.js)\n\nThis code defines a set of constants and functions related to shader chunks in the PlayCanvas engine. Shader chunks are small pieces of shader code that can be combined to create more complex shaders. The `chunkVersions` object defines the API version of each chunk, which is used to validate user-supplied chunks. The `removedChunks` object lists chunks that have been removed from the API and are no longer supported.\n\nThe `validateUserChunks` function takes a user-supplied chunk object as input and checks each chunk against the `shaderChunks` and `removedChunks` objects to ensure that it is supported and up-to-date. If a chunk is not found in either object, a warning is logged. If a chunk is found in `shaderChunks`, its API version is compared to the user-supplied version to check for compatibility.\n\nThis code is likely used in the larger PlayCanvas engine project to ensure that user-supplied shader chunks are valid and compatible with the engine's API. It provides a way to validate user input and prevent errors or crashes caused by incompatible or unsupported shader code. \n\nExample usage:\n\n```\nimport { validateUserChunks } from './shaderChunkValidator.js';\n\nconst myShaderChunks = {\n  APIVersion: '1.62',\n  myCustomChunk: 'void main() { ... }'\n};\n\nvalidateUserChunks(myShaderChunks);\n// logs a warning if myCustomChunk is not a supported or outdated chunk\n```\n## Questions: \n 1. What is the purpose of the `chunkVersions` object?\n   - The `chunkVersions` object maps the names of shader chunks to their corresponding API versions.\n\n2. What is the purpose of the `removedChunks` object?\n   - The `removedChunks` object lists shader chunks that have been removed from the API and their corresponding API versions.\n\n3. What is the purpose of the `validateUserChunks` function?\n   - The `validateUserChunks` function checks if user-supplied shader chunks are supported by the engine and warns the user if they are outdated or not supported.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/chunk-validation.md"}}],["528",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/chunks-lightmapper.js)\n\nThe code above is a module that exports an object called `shaderChunksLightmapper`. This object contains four properties, each of which is a reference to a fragment shader code file. These files are used in the PlayCanvas engine's lightmapping system.\n\nThe `bakeDirLmEndPS` and `bakeLmEndPS` shaders are used to generate lightmaps for a scene. The former is used to bake directional lightmaps, while the latter is used to bake ambient lightmaps. These shaders take in various parameters such as the lightmap texture, the scene texture, and the lightmap resolution.\n\nThe `dilatePS` shader is used to dilate the lightmap generated by the `bakeLmEndPS` shader. This is done to smooth out the edges of the lightmap and prevent any artifacts from appearing.\n\nThe `bilateralDeNoisePS` shader is used to remove any noise from the lightmap. This is done by applying a bilateral filter to the lightmap texture.\n\nOverall, this module provides the necessary fragment shaders for the PlayCanvas engine's lightmapping system. These shaders can be used by developers to generate high-quality lightmaps for their scenes, which can greatly improve the visual quality of their games or applications. \n\nExample usage:\n\n```javascript\nimport { shaderChunksLightmapper } from 'playcanvas-engine';\n\n// Use the bakeDirLmEndPS shader to generate a directional lightmap\nconst directionalLightmapShader = shaderChunksLightmapper.bakeDirLmEndPS;\n// Set the necessary parameters for the shader\ndirectionalLightmapShader.setParameter('lightmapTexture', lightmapTexture);\ndirectionalLightmapShader.setParameter('sceneTexture', sceneTexture);\ndirectionalLightmapShader.setParameter('lightmapResolution', lightmapResolution);\n// Render the scene with the directional lightmap shader\n\n// Use the bilateralDeNoisePS shader to remove noise from the lightmap\nconst denoiseShader = shaderChunksLightmapper.bilateralDeNoisePS;\n// Set the necessary parameters for the shader\ndenoiseShader.setParameter('lightmapTexture', lightmapTexture);\n// Render the denoised lightmap\n```\n## Questions: \n 1. What is the purpose of this code file?\n- This code file exports an object containing shader code for the PlayCanvas engine's lightmapper.\n\n2. What are the contents of the `shaderChunksLightmapper` object?\n- The `shaderChunksLightmapper` object contains four properties, each of which is a string representing shader code for a different aspect of the lightmapper: `bakeDirLmEndPS`, `bakeLmEndPS`, `dilatePS`, and `bilateralDeNoisePS`.\n\n3. Where are the shader code files being imported from?\n- The shader code files are being imported from the `./lightmapper/frag` directory within the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/chunks-lightmapper.md"}}],["529",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/decode.js)\n\nThe code provided is a set of functions that are used to decode different types of color formats. These functions are written in GLSL (OpenGL Shading Language) and are exported as a default module. The purpose of these functions is to convert color values from their compressed or encoded form into a more usable format. \n\nThe first set of functions, `decodeLinear` and `decodeGamma`, are used to convert linear and gamma-encoded colors, respectively. Gamma encoding is a technique used to represent color values in a way that is more perceptually uniform to the human eye. The `decodeLinear` function takes a `vec4` input and returns the `rgb` component of the input. The `decodeGamma` function takes a `float` or `vec3` input and applies a gamma correction of 2.2 to the input. The `decodeGamma` function that takes a `vec4` input applies the gamma correction to the `xyz` components of the input.\n\nThe next set of functions, `decodeRGBM` and `decodeRGBP`, are used to decode RGBM and RGBP color formats, respectively. These formats are used to compress high dynamic range (HDR) color values into a smaller range of values. The `decodeRGBM` function takes a `vec4` input and returns the decoded color value. The `decodeRGBP` function takes a `vec4` input and returns the decoded color value.\n\nThe final function, `decodeRGBE`, is used to decode RGBE color format. RGBE is another format used to represent HDR color values. The `decodeRGBE` function takes a `vec4` input and returns the decoded color value. If the `a` component of the input is 0, the function returns a black color. Otherwise, it applies a conversion based on the `w` component of the input.\n\nThe last function, `passThrough`, simply returns the input value without any decoding or conversion.\n\nThese functions are likely used in the larger PlayCanvas engine project to handle different types of color formats that may be used in 3D graphics rendering. They provide a way to convert these formats into a more usable form for rendering and display. For example, the `decodeGamma` function may be used to convert gamma-encoded textures into a linear format that can be used for lighting calculations. Overall, these functions provide a useful set of tools for handling different types of color formats in 3D graphics.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines several functions for decoding different types of color data.\n\n2. What is the input and output of the `decodeRGBE` function?\n- The input is a `vec4` representing a color in RGBE format, and the output is a `vec3` representing the decoded color in linear space.\n\n3. What is the difference between the `decodeRGBM` and `decodeRGBP` functions?\n- Both functions decode a color in RGBM format, but `decodeRGBM` uses a squaring operation while `decodeRGBP` uses a linear interpolation.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/decode.md"}}],["530",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/encode.js)\n\nThe code provided is a set of functions that encode color values in different formats. These functions are used to convert color values from one format to another, which is useful in computer graphics and game development. The functions are written in GLSL, which is a shading language used in graphics processing units (GPUs) to create visual effects.\n\nThe first function, `encodeLinear`, takes a 3-component vector representing a color value in linear space and returns a 4-component vector with the same values and an alpha value of 1. This function is useful when working with color values that are already in linear space, such as when working with HDR (high dynamic range) images.\n\nThe second function, `encodeGamma`, takes a 3-component vector representing a color value in linear space and returns a 4-component vector with the same values transformed to gamma space using a power function with an exponent of 1/2.2. This function is useful when working with color values that are in linear space but need to be displayed on a monitor, which typically uses a gamma curve to display colors.\n\nThe third function, `encodeRGBM`, takes a 3-component vector representing a color value in linear space and returns a 4-component vector with the same values encoded using a modified RGBM format. This format is used to store high dynamic range color values in a low dynamic range format, such as when storing color values in a texture. The function first applies a power function with an exponent of 0.5 to the color values, then scales them down by a factor of 8. The alpha value is calculated based on the maximum component of the color values and is rounded to the nearest 8-bit value. The color values are then divided by the alpha value to get the final encoded values.\n\nThe fourth function, `encodeRGBP`, takes a 3-component vector representing a color value in linear space and returns a 4-component vector with the same values encoded using an RGBP format. This format is similar to RGBM but uses a different encoding scheme. The function first applies a power function with an exponent of 0.5 to the color values, then calculates the maximum component value and scales it to a range of 1 to 8. The scaling factor is then used to calculate the alpha value, which is rounded to the nearest 8-bit value. The color values are then divided by the scaling factor to get the final encoded values.\n\nThe fifth function, `encodeRGBE`, takes a 3-component vector representing a color value in linear space and returns a 4-component vector with the same values encoded using an RGBE format. This format is used to store high dynamic range color values in a compact format. The function first calculates the maximum component value and uses it to calculate an exponent value. The color values are then divided by 2 raised to the exponent value, and the exponent value is added to 128 and divided by 255 to get the alpha value. The final encoded values are returned as a 4-component vector.\n\nOverall, these functions are useful for converting color values between different formats, which is important in computer graphics and game development. They can be used in various parts of the PlayCanvas engine, such as when working with textures, lighting, and post-processing effects. For example, the `encodeRGBM` function can be used to store high dynamic range lighting information in a low dynamic range texture, while the `encodeGamma` function can be used to display color values on a monitor with a gamma curve.\n## Questions: \n 1. What is the purpose of this code?\n- This code exports five functions that encode different color spaces into a vec4 format.\n\n2. What is the input and output of each function?\n- Each function takes a vec3 color source as input and returns a vec4 encoded color as output.\n\n3. What encoding methods are used in this code?\n- The code uses various encoding methods such as linear encoding, gamma encoding, modified RGBM encoding, RGBP encoding, and RGBE encoding.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/encode.md"}}],["531",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/envAtlas.js)\n\nThe code above is a GLSL shader code that defines three functions used for mapping textures in the PlayCanvas engine. The purpose of these functions is to map a normalized equirectangular UV to a given rectangle, taking into account a 1-pixel seam. \n\nThe first function, `mapUv`, takes a normalized equirectangular UV and a rectangle as input and returns a mapped UV. The rectangle is defined by a vec4 that contains the x and y coordinates of the top-left corner of the rectangle, as well as its width and height. The function uses the `mix` function to interpolate between the x and y coordinates of the rectangle and the corresponding coordinates of the UV, taking into account the 1-pixel seam.\n\nThe second function, `mapRoughnessUv`, takes a normalized equirectangular UV and a roughness level as input and returns a mapped UV. The roughness level is a float value that determines the level of roughness of the material. The function uses the `exp2` function to calculate the size of the atlas rectangle based on the roughness level, and then calls the `mapUv` function to map the UV to the correct atlas rectangle.\n\nThe third function, `mapShinyUv`, takes a normalized equirectangular UV and a shiny level as input and returns a mapped UV. The shiny level is a float value that determines the level of shininess of the material. The function uses the `exp2` function to calculate the size of the atlas rectangle based on the shiny level, and then calls the `mapUv` function to map the UV to the correct atlas rectangle.\n\nThese functions are used in the PlayCanvas engine to map textures to the correct atlas rectangle based on their roughness and shininess levels. This is important for creating realistic materials in 3D scenes. For example, a rough surface will have a different texture than a smooth surface, and the PlayCanvas engine uses these functions to ensure that the correct texture is applied to each surface. \n\nHere is an example of how these functions might be used in a PlayCanvas project:\n\n```\n// create a material with a roughness level of 0.5 and a shiny level of 0.8\nvar material = new pc.StandardMaterial();\nmaterial.roughness = 0.5;\nmaterial.shininess = 0.8;\n\n// map the texture to the correct atlas rectangle based on the roughness and shininess levels\nvar uv = new pc.Vec2(0.5, 0.5);\nvar roughnessUv = mapRoughnessUv(uv, material.roughness);\nvar shinyUv = mapShinyUv(roughnessUv, material.shininess);\n\n// set the texture coordinates of the material to the mapped UV\nmaterial.diffuseMapUv = shinyUv;\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines functions to map normalized equirect UV coordinates to the correct atlas rectangle based on roughness and shiny levels.\n\n2. What is the significance of the `atlasSize` and `seamSize` constants?\n    \n    `atlasSize` is fixed at 512 pixels and represents the size of the atlas. `seamSize` is calculated as 1 pixel divided by `atlasSize` and represents the size of the seam between atlas rectangles.\n\n3. What is the expected input and output of the `mapUv`, `mapRoughnessUv`, and `mapShinyUv` functions?\n    \n    The `mapUv` function takes a normalized equirect UV coordinate and a rectangle and returns a mapped UV coordinate. The `mapRoughnessUv` and `mapShinyUv` functions take a normalized equirect UV coordinate and a roughness or shiny level and return a mapped UV coordinate.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/envAtlas.md"}}],["532",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/envConst.js)\n\nThis code exports a GLSL function called `processEnvironment` that takes in a `vec3` color and returns the same color. The purpose of this function is likely to be used in the larger PlayCanvas engine project as a way to process the environment color in a shader program.\n\nIn a shader program, the environment color can be used to simulate lighting and reflections in a scene. By passing the environment color through the `processEnvironment` function, developers can apply custom processing to the color before it is used in the shader program. This could include things like color correction, contrast adjustments, or even adding special effects like noise or distortion.\n\nHere is an example of how this function might be used in a shader program:\n\n```glsl\nuniform samplerCube environmentMap;\n\nvoid main() {\n    vec3 environmentColor = textureCube(environmentMap, reflect(-normalize(vWorldPosition), vWorldNormal)).rgb;\n    vec3 processedColor = processEnvironment(environmentColor);\n    // use processedColor in lighting and reflection calculations\n}\n```\n\nIn this example, the `environmentMap` uniform is a cube map texture that represents the environment surrounding the scene. The `reflect` function is used to calculate the reflection vector for the current fragment, which is then used to sample the environment map and get the corresponding color. This color is then passed through the `processEnvironment` function to apply any custom processing before it is used in lighting and reflection calculations.\n\nOverall, this code provides a simple but powerful way for developers to customize how the environment color is processed in their shader programs, allowing for greater control over the final look and feel of their scenes.\n## Questions: \n 1. What is the purpose of the `processEnvironment` function?\n   - The `processEnvironment` function takes in a `vec3` color and returns the same color without any processing.\n2. What does the `/* glsl */` comment indicate?\n   - The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax.\n3. How is this code intended to be used within the PlayCanvas engine?\n   - It is unclear how this code is intended to be used within the PlayCanvas engine without additional context.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/envConst.md"}}],["533",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/envMultiply.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) that is used to process the environment color in the PlayCanvas engine. The purpose of this code is to adjust the intensity of the skybox in the scene by multiplying the color of the environment with a uniform float value called `skyboxIntensity`. \n\nThe `processEnvironment` function takes in a `vec3` color value representing the environment color and returns the processed color value. The processing is done by multiplying the input color with the `skyboxIntensity` value. This function is called by other shaders in the PlayCanvas engine to adjust the intensity of the skybox in the scene.\n\nThe `export default` statement at the beginning of the code exports this shader code as a default export, which can be imported and used in other parts of the PlayCanvas engine project. For example, this shader code can be imported and used in a material definition to adjust the skybox intensity of a mesh in the scene. \n\nHere is an example of how this shader code can be used in a material definition:\n\n```\nvar material = new pc.StandardMaterial();\nmaterial.shader = // load the shader that contains the processEnvironment function\nmaterial.setParameter('skyboxIntensity', 0.5); // set the skybox intensity to 0.5\n```\n\nIn summary, this shader code is used to adjust the intensity of the skybox in the scene by multiplying the environment color with a uniform float value called `skyboxIntensity`. It can be imported and used in other parts of the PlayCanvas engine project, such as material definitions, to adjust the skybox intensity of a mesh in the scene.\n## Questions: \n 1. What is the purpose of the `skyboxIntensity` uniform variable?\n- The `skyboxIntensity` uniform variable is used to adjust the intensity of the skybox color.\n\n2. What is the input parameter `color` used for in the `processEnvironment` function?\n- The `color` parameter is multiplied by the `skyboxIntensity` uniform variable and returned as the output of the function.\n\n3. What type of code is this and how is it intended to be used?\n- This is a GLSL shader code that is intended to be used for processing the environment color in a 3D graphics application, such as a game engine. It is exported as a default module.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/envMultiply.md"}}],["534",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/fixCubemapSeamsNone.js)\n\nThe code above defines several functions related to fixing seams in textures. Seams are visible lines or gaps that can appear when a texture is tiled or repeated. These functions are intended to be used within the PlayCanvas engine to address this issue.\n\nThe first function, `fixSeams`, takes a `vec3` (a 3-component vector) and a `mipmapIndex` as input, and returns a `vec3`. The purpose of this function is to fix seams in a texture at a specific mipmap level. Mipmaps are pre-generated versions of a texture at different resolutions, used to optimize rendering performance. The function does not actually modify the input vector, but rather returns a new vector that has been adjusted to fix any seams.\n\nThe second function, also called `fixSeams`, takes only a `vec3` as input and returns a `vec3`. This function is similar to the first one, but it does not take a mipmap index as input. Instead, it is intended to be used for textures that do not have mipmaps.\n\nThe third function, `fixSeamsStatic`, takes a `vec3` and an `invRecMipSize` as input, and returns a `vec3`. This function is similar to the first one, but it is intended to be used for textures that are not being rendered with mipmapping. The `invRecMipSize` parameter is the inverse of the texture size, and is used to calculate the amount of adjustment needed to fix seams.\n\nThe fourth function, `calcSeam`, takes a `vec3` as input and returns a `vec3`. This function is used to calculate the amount of adjustment needed to fix seams in a texture. It does not modify the input vector, but rather returns a new vector that represents the amount of adjustment needed.\n\nThe fifth function, `applySeam`, takes a `vec3`, a `seam`, and a `scale` as input, and returns a `vec3`. This function is used to apply the adjustment calculated by `calcSeam` to a texture. The `seam` parameter is the amount of adjustment needed, and the `scale` parameter is used to adjust the strength of the adjustment.\n\nOverall, these functions are used to fix seams in textures within the PlayCanvas engine. They are designed to work with textures that are being rendered with or without mipmapping, and can be used to adjust the amount of adjustment needed based on the texture size and other factors. Developers using the PlayCanvas engine can call these functions as needed to ensure that their textures are rendered seamlessly.\n## Questions: \n 1. **What is the purpose of the `fixSeams` function and why are there multiple versions of it?**\n    \n    The `fixSeams` function is used to fix texture seams in the engine. There are multiple versions of it because they take different parameters, such as `mipmapIndex` and `invRecMipSize`, which are used to calculate the correct texture coordinates for fixing the seams at different levels of detail.\n    \n2. **What is the `calcSeam` function used for?**\n    \n    The `calcSeam` function is used to calculate the seam vector for a given texture coordinate. It returns a zero vector in this implementation, so it may not be fully implemented or may be used in conjunction with other functions to calculate the seam vector.\n    \n3. **What is the purpose of the `applySeam` function and how is the `scale` parameter used?**\n    \n    The `applySeam` function is used to apply the calculated seam vector to a given texture coordinate. The `scale` parameter is used to control the strength of the seam effect, with higher values resulting in a more noticeable seam.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/fixCubemapSeamsNone.md"}}],["535",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/fixCubemapSeamsStretch.js)\n\nThe code provided is a set of GLSL functions that are used to fix texture seams in 3D models. Texture seams are visible lines or gaps that appear on the surface of a 3D model when two or more textures are joined together. These seams are caused by the way textures are mapped onto the surface of the model and can be particularly noticeable in games or other real-time applications.\n\nThe `fixSeams` function takes a 3D vector and a mipmap index as input and returns a modified 3D vector. The mipmap index is used to calculate a scaling factor that is applied to the vector to fix any seams that may be present. The function first calculates the absolute value of the input vector and then finds the maximum value of the x, y, and z components. If any of the components are not equal to the maximum value, the corresponding component of the vector is scaled by the calculated factor. The modified vector is then returned.\n\nThe `fixSeamsStatic` function is similar to `fixSeams`, but instead of taking a mipmap index as input, it takes an inverse reciprocal mip size. This function is used when the mip level is fixed and does not change.\n\nThe `calcSeam` function takes a 3D vector as input and returns a 3D vector that represents the location of any seams in the input vector. The function first calculates the absolute value of the input vector and then finds the maximum value of the x, y, and z components. The output vector has a value of 1.0 for any component that is not equal to the maximum value and 0.0 for any component that is equal to the maximum value.\n\nThe `applySeam` function takes a 3D vector, a 3D seam vector, and a scaling factor as input and returns a modified 3D vector. The function first multiplies the input vector by the negative of the seam vector scaled by the scaling factor and adds a vector of 1.0. This effectively moves the texture coordinates of the input vector away from the seam location, fixing any visible seams.\n\nThese functions are likely used in the larger PlayCanvas engine project to improve the visual quality of 3D models by fixing any visible seams caused by texture mapping. The `fixSeams` and `fixSeamsStatic` functions are used to modify the texture coordinates of the model to fix any seams, while the `calcSeam` function is used to identify the location of any seams. The `applySeam` function is then used to apply the seam fix to the texture coordinates of the model.\n## Questions: \n 1. What does the `fixSeams` function do?\n    \n    The `fixSeams` function takes a 3D vector and scales its components based on the maximum absolute value of the vector's components, with the amount of scaling determined by the `mipmapIndex` or `invRecMipSize` parameter.\n\n2. What is the purpose of the `calcSeam` function?\n    \n    The `calcSeam` function takes a 3D vector and returns a 3D vector indicating which component(s) have the maximum absolute value, with a value of 1.0 indicating the maximum and 0.0 indicating the other components.\n\n3. How is the `applySeam` function used?\n    \n    The `applySeam` function takes a 3D vector, a 3D seam vector, and a scaling factor, and returns a modified 3D vector that is scaled based on the seam vector and scaling factor. It is likely used in conjunction with the `fixSeams` and `calcSeam` functions to correct texture seams in a 3D model.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/fixCubemapSeamsStretch.md"}}],["536",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/fullscreenQuad.js)\n\nThis code is a GLSL shader that is used to render textures onto a 3D object in the PlayCanvas engine. The purpose of this shader is to sample a texture from a 2D texture map and apply it to a 3D object. \n\nThe `varying vec2 vUv0` variable is used to pass the texture coordinates from the vertex shader to the fragment shader. The `uniform sampler2D source` variable is used to sample the texture from the texture map. \n\nThe `main()` function is the entry point for the shader. It sets the `gl_FragColor` variable to the color of the texture at the specified texture coordinates. This is done using the `texture2D()` function, which takes the texture map and the texture coordinates as arguments. \n\nThis shader can be used in the larger PlayCanvas project to apply textures to 3D objects. For example, if we have a 3D model of a car, we can use this shader to apply a texture of a car paint to the model. \n\nHere is an example of how this shader can be used in PlayCanvas:\n\n```javascript\n// Create a material for the 3D object\nvar material = new pc.StandardMaterial();\n\n// Set the shader to the GLSL shader defined above\nmaterial.shader = new pc.Shader('textureShader', {\n    attributes: {\n        aPosition: pc.SEMANTIC_POSITION,\n        aUv0: pc.SEMANTIC_TEXCOORD0\n    },\n    vshader: /* vertex shader code */,\n    fshader: /* fragment shader code */\n});\n\n// Load the texture map\nvar texture = new pc.Texture(app.graphicsDevice, {\n    src: 'car_paint.png'\n});\n\n// Set the texture map to the material\nmaterial.diffuseMap = texture;\n\n// Apply the material to the 3D object\ncar.model.meshInstances[0].material = material;\n```\n\nIn this example, we create a material for a 3D object and set the shader to the GLSL shader defined above. We then load a texture map and set it to the material. Finally, we apply the material to the 3D object. When the scene is rendered, the texture will be applied to the 3D object using the GLSL shader.\n## Questions: \n 1. What is the purpose of this code?\n   - This code is a GLSL shader that samples a texture and sets the output color to the sampled color at the given UV coordinates.\n\n2. What is the expected input for the `source` uniform?\n   - The `source` uniform is expected to be a 2D texture that the shader will sample from.\n\n3. What is the significance of the `varying` keyword used for `vUv0`?\n   - The `varying` keyword indicates that the value of `vUv0` will be interpolated across the vertices of the geometry being rendered, allowing for smooth texture mapping.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/fullscreenQuad.md"}}],["537",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/gamma1_0.js)\n\nThe code above defines a set of functions that perform gamma correction on input and output colors. Gamma correction is a technique used in computer graphics to adjust the brightness and contrast of images to match the way they are perceived by the human eye. \n\nThe `gammaCorrectInput` function takes a single float, vec3, or vec4 color value as input and returns the same value without any modification. This function is used to indicate that the input color has already been gamma-corrected and does not need to be adjusted further.\n\nThe `gammaCorrectOutput` function takes a vec3 color value as input and returns the same value without any modification. This function is used to indicate that the output color should not be gamma-corrected before being displayed.\n\nThe code is written in GLSL (OpenGL Shading Language), which is a high-level language used to write shaders for graphics processing units (GPUs). Shaders are programs that run on the GPU and are used to perform various tasks such as rendering 3D graphics, applying post-processing effects, and performing image processing operations.\n\nIn the context of the PlayCanvas engine, this code may be used in shaders that are used to render 3D scenes. The gamma correction functions can be used to ensure that colors are displayed accurately on different types of displays, such as monitors and televisions. \n\nFor example, the following code snippet shows how the `gammaCorrectInput` function can be used in a shader to adjust the brightness and contrast of a texture:\n\n```\nuniform sampler2D texture;\nvarying vec2 uv;\n\nvoid main() {\n    vec4 color = texture2D(texture, uv);\n    color.rgb = gammaCorrectInput(color.rgb);\n    gl_FragColor = color;\n}\n```\n\nIn this example, the `texture` uniform represents a 2D texture that is being sampled at the current UV coordinate (`uv`). The `color` variable is set to the color value of the texture at the current UV coordinate, and the `gammaCorrectInput` function is used to adjust the brightness and contrast of the color value. The resulting color is then assigned to the `gl_FragColor` output variable, which is used to set the color of the current pixel being rendered.\n## Questions: \n 1. **What is the purpose of this code?** \nThis code defines functions for gamma correction of input and output colors.\n\n2. **What is the expected input and output format for these functions?** \nThe functions accept either a single float or a vector of 3 or 4 floats as input, and return a vector of 3 or 4 floats as output.\n\n3. **Are there any limitations or assumptions made by these functions?** \nThe code does not specify any limitations or assumptions, but it is possible that the functions are designed for a specific color space or display technology.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/gamma1_0.md"}}],["538",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/gamma2_2.js)\n\nThe code above is a set of GLSL functions for gamma correction. Gamma correction is a technique used to adjust the brightness and contrast of images to make them appear more natural on different display devices. \n\nThe `gammaCorrectInput` function takes a color value as input and returns the gamma-corrected version of that color. There are three versions of this function, each taking a different type of input: a single float value, a vec3 vector, or a vec4 vector. The function calls the `decodeGamma` function to perform the gamma correction. \n\nThe `gammaCorrectOutput` function takes a vec3 vector as input and returns the gamma-corrected version of that color. If the `HDR` preprocessor macro is defined, the function simply returns the input color. Otherwise, it applies a gamma correction formula to the input color using the `pow` function. \n\nThese functions are likely used in the PlayCanvas engine to ensure that graphics are displayed correctly on different devices with varying gamma settings. For example, if a user is viewing a 3D scene on a monitor with a high gamma setting, the colors may appear washed out. By applying gamma correction, the colors can be adjusted to appear more natural on that particular display. \n\nHere is an example of how the `gammaCorrectInput` function might be used in a shader:\n\n```\nuniform float u_gamma;\n\nvoid main() {\n    vec4 color = texture2D(u_texture, v_uv);\n    color.rgb = gammaCorrectInput(color.rgb * u_brightness);\n    color.rgb *= u_contrast;\n    gl_FragColor = color;\n}\n```\n\nIn this example, the `gammaCorrectInput` function is used to apply gamma correction to the color value retrieved from a texture. The `u_gamma` uniform variable can be used to adjust the gamma correction value dynamically.\n## Questions: \n 1. What is the purpose of this code?\n- This code provides functions for gamma correction of input and output colors.\n\n2. What is the input and output format for the gamma correction functions?\n- The input format can be a float, vec3, or vec4 color, while the output format depends on the input format and may be a float, vec3, or vec4 color.\n\n3. What is the purpose of the #ifdef HDR preprocessor directive in the gammaCorrectOutput function?\n- The #ifdef HDR directive checks if the HDR (High Dynamic Range) feature is enabled. If it is enabled, the function returns the input color as is. If it is not enabled, the function applies a gamma correction formula to the input color and returns the result.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/gamma2_2.md"}}],["539",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/msdf.js)\n\nThe code provided is a shader code written in GLSL (OpenGL Shading Language) for the PlayCanvas engine. The purpose of this code is to apply multi-channel signed distance field (MSDF) rendering to text in a 3D environment. MSDF is a technique used to render high-quality text in real-time applications. It uses a texture map that stores the distance of each pixel in the font glyph from the edge of the glyph. This information is used to generate smooth edges and sharp corners for the text.\n\nThe code defines a function called `applyMsdf` that takes a color as input and returns a modified color based on the MSDF technique. The function first samples the MSDF texture map to get the signed distance value of the pixel. It then applies smoothing to the value based on the size of the font and the texture on the screen. The smoothing value is used to generate opacity for the text. The function also applies an outline and shadow effect to the text based on the input parameters.\n\nThe code uses several uniform variables to control the intensity, range, and width of the font. It also uses varying variables to pass the outline color, thickness, shadow color, and offset to the fragment shader. The code also checks for the availability of the `GL_OES_standard_derivatives` and `GL2` extensions to determine the smoothing method to use.\n\nThis code can be used in the PlayCanvas engine to render high-quality text in 3D environments. It can be applied to text entities in the scene by setting the material of the entity to use this shader. The uniform variables can be adjusted to control the appearance of the text. For example, the `font_sdfIntensity` variable can be used to adjust the sharpness of the text edges, while the `outline_thickness` variable can be used to adjust the thickness of the text outline. Overall, this code provides a powerful tool for rendering high-quality text in real-time applications.\n## Questions: \n 1. What is the purpose of the `applyMsdf` function?\n    \n    The `applyMsdf` function is used to apply multi-channel signed distance field (MSDF) rendering to text in a texture atlas. It calculates the signed distance value, applies smoothing, and generates opacity for the text.\n\n2. What is the difference between `uniform` and `varying` variables in this code?\n    \n    `uniform` variables are used to pass values from the CPU to the GPU and are constant across all vertices or fragments. `varying` variables are used to pass values from the vertex shader to the fragment shader and can vary across vertices or fragments.\n\n3. What is the purpose of the `USE_FWIDTH` preprocessor directive?\n    \n    The `USE_FWIDTH` preprocessor directive is used to enable the use of the `fwidth` function, which calculates the rate of change of a value with respect to screen space. It is used to calculate smoothing for MSDF rendering.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/msdf.md"}}],["540",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/outputTex2D.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) that is used to render textures in the PlayCanvas engine. The purpose of this code is to sample a texture from a 2D sampler and output the color of the pixel at the given texture coordinates. \n\nThe code starts with a `varying` declaration for `vUv0`, which is a 2D vector that represents the texture coordinates of the current pixel being rendered. This variable is passed from the vertex shader to the fragment shader, allowing the fragment shader to access the texture coordinates of each pixel.\n\nThe `uniform` declaration for `source` is used to specify the texture that will be sampled. This texture is passed to the shader as a uniform variable, allowing it to be set from outside the shader code.\n\nThe `main` function is the entry point of the shader code and is called for each pixel being rendered. Inside the function, the `texture2D` function is used to sample the color of the pixel at the given texture coordinates. The resulting color is then assigned to the `gl_FragColor` variable, which is a built-in variable that represents the color of the current pixel being rendered.\n\nThis shader code can be used in the PlayCanvas engine to render textures on 3D models, UI elements, and other graphical elements. For example, the following code snippet shows how this shader code can be used to render a texture on a 3D model:\n\n```javascript\nconst material = new pc.StandardMaterial();\nconst texture = new pc.Texture(app.graphicsDevice, {\n    src: 'path/to/texture.png'\n});\nmaterial.diffuseMap = texture;\nmaterial.shader = app.assets.get('path/to/shader-code.glsl').resource;\n```\n\nIn this example, a `StandardMaterial` is created and a texture is loaded from a file. The `diffuseMap` property of the material is set to the loaded texture, and the `shader` property is set to the compiled shader code from the file containing the code above. When the 3D model is rendered, the shader code will be used to sample the texture and render it on the model.\n## Questions: \n 1. What is the purpose of this code?\n   This code is a fragment shader that samples a texture and sets the output color to the sampled color.\n\n2. What is the data type of the `source` uniform?\n   The `source` uniform is a 2D texture sampler.\n\n3. What is the meaning of the `/* glsl */` comment at the beginning of the code?\n   The `/* glsl */` comment indicates that this code is written in GLSL (OpenGL Shading Language) syntax.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/outputTex2D.md"}}],["541",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/packDepth.js)\n\nThe code provided is a GLSL function that packs a floating-point value into a vector of four bytes. This is useful for storing depth values in shadow maps, where the depth value needs to be stored in a texture with limited precision. \n\nThe function takes a single argument, a float value representing the depth to be packed. It then performs a series of operations to pack this value into a vec4. \n\nFirst, two constant vec4 values are defined: bit_shift and bit_mask. bit_shift is used to multiply the depth value by a large number, effectively shifting the decimal point to the left. bit_mask is used to mask out the individual bytes of the resulting vec4. \n\nNext, the depth value is multiplied by bit_shift and then passed through the mod function with a vec4 argument of 256. This effectively takes the remainder of the depth value divided by 256 for each byte of the resulting vec4. \n\nFinally, the resulting vec4 is adjusted by subtracting the xxyz components multiplied by bit_mask. This ensures that the values in each byte of the vec4 are between 0 and 255. \n\nThe resulting vec4 is then returned by the function. \n\nThis function is likely used in the larger PlayCanvas engine project to pack depth values for use in shadow maps. It could be used in conjunction with other GLSL functions to create a complete shader program for rendering shadows. \n\nExample usage of this function in a shader program:\n\n```\nuniform float depth;\nvarying vec4 packedDepth;\n\nvoid main() {\n  packedDepth = packFloat(depth);\n  // other shader code for rendering shadows\n}\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code is a GLSL function that packs a float value into a vec4 using multiplication and mod operations.\n\n2. How does this code work?\n    \n    The code uses a combination of multiplication, mod, and division operations to pack a float value into a vec4. It first multiplies the float value by a bit shift vector, then takes the mod of the result with a vector of 256s. It then divides the result by a vector of 255s and subtracts the xxyz components of the result multiplied by a bit mask vector.\n\n3. Where can this code be used?\n    \n    This code can be used in GLSL shaders to pack float values into vec4s, for example when encoding depth values for shadow mapping. It is part of the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/packDepth.md"}}],["542",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/reproject.js)\n\nThe code provided is a GLSL shader that is used to filter and process environment maps. The shader requires several #DEFINEs to be set before it can be used. These #DEFINEs specify the processing function, decode function, encode function, source function, and target function. The shader also requires several uniforms to be set, including the source texture or cubemap, the number of samples to use when filtering, and the size of the source cubemap.\n\nThe shader supports three different source projections: cubemap, equirectangular, and octahedral. The shader provides functions to sample each of these projections. The shader also provides functions to modify the seams of the cubemap and octahedral projections to reduce artifacts.\n\nThe shader provides two processing functions: reproject and prefilter. The reproject function takes a single sample or multiple samples of the environment map and reprojects them onto the target cubemap face. The prefilter function takes pre-generated samples of the environment map and convolves them with the target cubemap face.\n\nThe shader provides several decode and encode functions for converting between different color spaces. The supported decode functions are decodeRGBM, decodeRGBE, decodeGamma, and decodeLinear. The supported encode functions are encodeRGBM, encodeRGBE, encodeGamma, and encodeLinear.\n\nOverall, this shader is an important component of the PlayCanvas engine's rendering pipeline. It allows for the filtering and processing of environment maps, which are an essential component of many 3D scenes. The shader's support for multiple source projections and color spaces makes it a versatile tool for rendering a wide range of scenes.\n## Questions: \n 1. What is the purpose of this shader and what are the required #DEFINEs?\n- The purpose of this shader is to filter and prefilter cubemaps and equirectangular maps. The required #DEFINEs are PROCESS_FUNC, DECODE_FUNC, ENCODE_FUNC, SOURCE_FUNC, and TARGET_FUNC.\n\n2. What are the different supported projections and how are they implemented?\n- The different supported projections are cubemap, equirectangular, and octahedral. They are implemented through various functions such as sampleCubemap, sampleEquirect, sampleOctahedral, getDirectionCubemap, getDirectionEquirect, and getDirectionOctahedral.\n\n3. What is the purpose of the unpackSample function and how is it used in the prefilterSamples and prefilterSamplesUnweighted functions?\n- The unpackSample function is used to extract a sample from a texture containing pre-generated samples. It returns the sample as a vector and the mip level as a float. The prefilterSamples and prefilterSamplesUnweighted functions use this function to decode the samples and calculate the filtered result.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/reproject.md"}}],["543",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/screenDepth.js)\n\nThe code provided is a GLSL shader that is used to retrieve the linear depth of a rendered scene. It is a part of the PlayCanvas engine project and is used to generate linear camera depth for a given world position. \n\nThe shader takes in a texture called `uSceneDepthMap` which is a 2D sampler that contains the depth information of the rendered scene. It also takes in a uniform called `uScreenSize` which is a vec4 that contains the screen size of the rendered scene. Another uniform called `matrix_view` is used to store the view matrix of the camera. \n\nThe shader contains two functions that are used to retrieve the linear depth of the rendered scene. The first function called `getLinearScreenDepth` takes in a vec2 called `uv` which is the UV coordinate of the pixel on the screen. It then uses the `linearizeDepth` function to retrieve the linear depth of the pixel. If the GLSL version is 2.0 or higher, it uses the `linearizeDepth` function to retrieve the linear depth. Otherwise, it uses the `unpackFloat` function to retrieve the depth. \n\nThe `linearizeDepth` function is used to convert the non-linear depth value to a linear depth value. It takes in a float called `z` which is the non-linear depth value of the pixel. It then checks if the camera is orthographic or not. If it is not orthographic, it uses the formula `(camera_params.z * camera_params.y) / (camera_params.y + z * (camera_params.z - camera_params.y))` to convert the depth value to linear. Otherwise, it uses the formula `camera_params.z + z * (camera_params.y - camera_params.z)` to convert the depth value to linear. \n\nThe `unpackFloat` function is used to convert the RGBA depth value to a float value. It takes in a vec4 called `rgbaDepth` which is the RGBA value of the depth. It then uses a bit shift to convert the RGBA value to a float value. \n\nThe second function called `getLinearDepth` takes in a vec3 called `pos` which is the world position of the pixel. It then uses the `matrix_view` uniform to retrieve the linear depth of the pixel. \n\nOverall, this shader is used to retrieve the linear depth of a rendered scene. It is an important part of the PlayCanvas engine project as it is used to generate the depth information required for various rendering techniques such as shadow mapping and depth of field. \n\nExample usage:\n\n```glsl\n// Retrieve the linear depth of the current pixel\nfloat linearDepth = getLinearScreenDepth();\n\n// Retrieve the linear depth of a pixel at a specific UV coordinate\nvec2 uv = vec2(0.5, 0.5);\nfloat linearDepth = getLinearScreenDepth(uv);\n\n// Retrieve the linear depth of a pixel at a specific world position\nvec3 worldPos = vec3(0.0, 0.0, 0.0);\nfloat linearDepth = getLinearDepth(worldPos);\n```\n## Questions: \n 1. What is the purpose of the `uSceneDepthMap` uniform variable?\n    \n    `uSceneDepthMap` is a highp sampler2D uniform variable used to store the depth map of the scene.\n\n2. What is the difference between GL2 and UNPACKFLOAT?\n    \n    GL2 and UNPACKFLOAT are two different methods used to linearize the depth of the scene. GL2 is used for WebGL2, while UNPACKFLOAT is used for WebGL1.\n\n3. What is the purpose of the `getLinearDepth` function?\n    \n    `getLinearDepth` is a function that generates the linear camera depth for a given world position. It does this by multiplying the position vector by the view matrix and returning the negative z-component of the resulting vector.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/screenDepth.md"}}],["544",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/spherical.js)\n\nThis code is a set of helper functions for working with equirectangular textures in the PlayCanvas engine. Equirectangular textures are images that are mapped onto a sphere, and are commonly used for skyboxes and other environmental effects in 3D graphics.\n\nThe first function, `toSpherical`, takes a 3D direction vector and returns a 2D vector representing the spherical coordinates of that direction. The `atan` function is used to calculate the azimuth angle (the angle around the equator), while `asin` is used to calculate the altitude angle (the angle above or below the equator). The resulting vector is returned as a `vec2`.\n\nThe second function, `toSphericalUv`, is similar to `toSpherical`, but returns a 2D texture coordinate instead of a spherical coordinate. The `toSpherical` function is called to calculate the spherical coordinates of the direction vector, which are then mapped to a UV coordinate by dividing by the constants `PI * 2.0` and `PI`, and adding `0.5` to center the texture. Finally, the `y` coordinate is inverted to match the convention used by most graphics APIs.\n\nThese functions can be used to convert between 3D directions and equirectangular texture coordinates, which is useful for rendering skyboxes and other environmental effects. For example, if you have a camera that is looking in a certain direction, you can use `toSpherical` to convert that direction to spherical coordinates, and then use `toSphericalUv` to get the corresponding texture coordinate for the equirectangular skybox texture. This allows you to render the skybox as a background behind your 3D scene, giving the illusion of a vast outdoor environment.\n## Questions: \n 1. What is the purpose of this code?\n    - This code contains helper functions for equirectangular mapping.\n2. What is the meaning of the `/* glsl */` comment at the beginning of the code?\n    - This comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax.\n3. What is the expected input and output of the `toSphericalUv` function?\n    - The `toSphericalUv` function takes a 3D direction vector as input and returns a 2D UV coordinate for equirectangular mapping.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/spherical.md"}}],["545",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/tonemappingAces.js)\n\nThe code provided is a shader function that performs tone mapping on a given color value. Tone mapping is a technique used in computer graphics to convert high dynamic range (HDR) images to low dynamic range (LDR) images that can be displayed on standard monitors. The purpose of this function is to adjust the brightness and contrast of the input color to make it more visually appealing and easier to view on a standard display.\n\nThe function takes in a single parameter, `exposure`, which is a float value representing the exposure level of the input color. The `toneMap` function then applies a series of mathematical operations to the input color to adjust its brightness and contrast. These operations are defined by the variables `tA`, `tB`, `tC`, `tD`, and `tE`, which are all float values.\n\nThe `vec3` data type is used to represent a three-component vector, which is used to store the RGB values of the input color. The `color` parameter is multiplied by the `exposure` value to adjust its brightness, and the resulting value is stored in the `x` variable. The `toneMap` function then applies the following formula to `x`:\n\n```\n(x*(tA*x+tB))/(x*(tC*x+tD)+tE)\n```\n\nThis formula is used to adjust the contrast of the input color and ensure that the output color is within the range of values that can be displayed on a standard monitor. The resulting color value is then returned by the function.\n\nThis shader function can be used in a larger project that involves rendering 3D graphics using the PlayCanvas engine. The function can be applied to the output of a lighting calculation to adjust the brightness and contrast of the final image. For example, the following code snippet shows how the `toneMap` function can be used in a fragment shader to apply tone mapping to the output color:\n\n```\nuniform sampler2D u_diffuseMap;\nuniform float u_exposure;\n\nvarying vec2 v_uv0;\n\nvoid main () {\n    vec4 diffuseColor = texture2D(u_diffuseMap, v_uv0);\n    vec3 finalColor = toneMap(diffuseColor.rgb);\n    gl_FragColor = vec4(finalColor, diffuseColor.a);\n}\n```\n\nIn this example, the `diffuseColor` value is obtained from a texture map and passed to the `toneMap` function to adjust its brightness and contrast. The resulting color value is then used as the final output color of the fragment shader. The `u_exposure` uniform variable is used to control the overall brightness of the output image.\n## Questions: \n 1. **What is the purpose of this code?** \nThis code defines a function called `toneMap` that takes in a color and applies a tone mapping algorithm to it based on certain parameters and an exposure value.\n\n2. **What is the data type of the `exposure` uniform?** \nThe `exposure` uniform is a float data type.\n\n3. **What is the tone mapping algorithm used in this code?** \nThe tone mapping algorithm used in this code is a modified version of the Reinhard tone mapping algorithm, which uses the parameters `tA`, `tB`, `tC`, `tD`, and `tE` to adjust the contrast and brightness of the input color.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/tonemappingAces.md"}}],["546",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/tonemappingAces2.js)\n\nThe code above is a GLSL shader code that implements a tone mapping algorithm called ACES (Academy Color Encoding System) approximation by Stephen Hill. The purpose of this code is to convert high dynamic range (HDR) images to low dynamic range (LDR) images that can be displayed on standard monitors. \n\nThe code defines a uniform float variable called \"exposure\" that is used to control the brightness of the output image. The ACESInputMat and ACESOutputMat are two 3x3 matrices that are used to transform the input color from sRGB color space to AP1 color space and then to output color space. \n\nThe RRTAndODTFit function is used to apply the Reference Rendering Transform (RRT) and Output Device Transform (ODT) to the input color. The RRT is a set of operations that simulate the way the human eye adapts to different lighting conditions, while the ODT is a set of operations that transform the color to the specific characteristics of the output device. \n\nFinally, the toneMap function applies the tone mapping algorithm to the input color. It first scales the color by the exposure value, then applies the ACESInputMat, RRTAndODTFit, and ACESOutputMat to the color. The resulting color is then clamped to the range [0, 1] to ensure that it can be displayed on a monitor. \n\nThis code can be used in the PlayCanvas engine to render HDR images in a way that is suitable for display on standard monitors. It can be used in conjunction with other rendering techniques such as bloom, depth of field, and motion blur to create visually stunning scenes. \n\nExample usage of this code in a PlayCanvas project:\n\n```javascript\n// Create a new material\nvar material = new pc.StandardMaterial();\n\n// Set the shader code to the ACES tone mapping shader\nmaterial.chunks.tonemapping = `\n    uniform float exposure;\n    const mat3 ACESInputMat = mat3(\n        0.59719, 0.35458, 0.04823,\n        0.07600, 0.90834, 0.01566,\n        0.02840, 0.13383, 0.83777\n    );\n    const mat3 ACESOutputMat = mat3(\n         1.60475, -0.53108, -0.07367,\n        -0.10208,  1.10813, -0.00605,\n        -0.00327, -0.07276,  1.07602\n    );\n    vec3 RRTAndODTFit(vec3 v) {\n        vec3 a = v * (v + 0.0245786) - 0.000090537;\n        vec3 b = v * (0.983729 * v + 0.4329510) + 0.238081;\n        return a / b;\n    }\n    vec3 toneMap(vec3 color) {\n        color *= exposure / 0.6;\n        color = color * ACESInputMat;\n        color = RRTAndODTFit(color);\n        color = color * ACESOutputMat;\n        color = clamp(color, 0.0, 1.0);\n        return color;\n    }\n`;\n\n// Set the exposure value to 1.0\nmaterial.setParameter('exposure', 1.0);\n\n// Assign the material to a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines a GLSL shader function called `toneMap` that applies a color grading effect to an input color based on the ACES color space.\n\n2. What is the input to the `toneMap` function?\n    \n    The input to the `toneMap` function is a `vec3` color value.\n\n3. What is the output of the `toneMap` function?\n    \n    The output of the `toneMap` function is a `vec3` color value that has been color graded using the ACES color space and tone mapping techniques.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/tonemappingAces2.md"}}],["547",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/tonemappingFilmic.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) and it defines two functions: `uncharted2Tonemap` and `toneMap`. The purpose of these functions is to apply a tone mapping algorithm to a given color value. \n\nTone mapping is a technique used in computer graphics to map the high dynamic range (HDR) values of an image to a lower dynamic range (LDR) that can be displayed on a standard monitor. The uncharted2Tonemap function is a specific tone mapping algorithm that was developed by John Hable for the game Uncharted 2. It is designed to preserve the contrast and details of the original HDR image while still making it look natural on an LDR display.\n\nThe `uncharted2Tonemap` function takes a vec3 color value as input and applies the uncharted2Tonemap algorithm to it. The algorithm uses several constants (A, B, C, D, E, F) and a formula to map the HDR value to an LDR value. The result is a vec3 color value that has been tone mapped using the uncharted2Tonemap algorithm.\n\nThe `toneMap` function takes a vec3 color value as input and applies both the uncharted2Tonemap algorithm and a white balance correction to it. The white balance correction is achieved by scaling the color value by a factor that ensures that the brightest white value in the image is mapped to a specific value (W). The result is a vec3 color value that has been tone mapped and white balanced.\n\nThis code is likely used in the PlayCanvas engine to apply tone mapping to 3D graphics rendered in real-time. The `toneMap` function could be called in the fragment shader of a material to apply tone mapping to the color of the rendered object. The `exposure` uniform could be used to adjust the overall brightness of the image. The uncharted2Tonemap algorithm is a popular choice for tone mapping in real-time graphics due to its ability to preserve contrast and detail.\n## Questions: \n 1. What is the purpose of this code?\n   This code defines two functions for tone mapping, which is a technique used to convert high dynamic range images to low dynamic range images for display on standard monitors.\n\n2. What are the values of A, B, C, D, E, F, and W used for?\n   These are constants used in the uncharted2Tonemap function to adjust the tone mapping curve.\n\n3. What is the purpose of the \"exposure\" uniform variable?\n   The \"exposure\" variable is used to adjust the brightness of the input color before it is tone mapped.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/tonemappingFilmic.md"}}],["548",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/tonemappingHejl.js)\n\nThe code provided is a shader function that performs tone mapping on a given color value. Tone mapping is a technique used in computer graphics to convert high dynamic range (HDR) images to low dynamic range (LDR) images that can be displayed on standard monitors. The purpose of this function is to adjust the brightness and contrast of the input color value to make it more visually appealing and realistic.\n\nThe function takes in a single parameter, `exposure`, which is a float value that controls the overall brightness of the output color. The input color is first multiplied by the exposure value to adjust its brightness. The function then applies a series of mathematical operations to the color value to perform the tone mapping.\n\nThe tone mapping algorithm used in this function is based on the Reinhard tone mapping operator, which is a popular method for tone mapping HDR images. The algorithm uses a set of constants (A, B, C, D, E, and F) to adjust the contrast and brightness of the input color. These constants are defined as float values in the code and are used in the tone mapping calculation.\n\nThe `vec3` data type is used to represent the color value in the function. The `max` function is used to ensure that the color value is always greater than or equal to zero. The `vec3` constructor is used to create new `vec3` objects with the specified values.\n\nThe output of the function is a `vec3` value that represents the tone mapped color. The tone mapping calculation is performed using a series of mathematical operations that involve multiplication, addition, and division. The final output is a modified color value that has been adjusted to look more visually appealing and realistic.\n\nThis function is likely used in the larger PlayCanvas engine project to perform tone mapping on HDR images used in 3D scenes. It can be used in conjunction with other shader functions to create realistic lighting and shading effects in 3D graphics. Here is an example of how this function can be used in a shader:\n\n```\nuniform sampler2D u_colorMap;\nuniform float u_exposure;\n\nvoid main() {\n    vec4 color = texture2D(u_colorMap, v_uv);\n    vec3 toneMappedColor = toneMap(color.rgb, u_exposure);\n    gl_FragColor = vec4(toneMappedColor, color.a);\n}\n```\n\nIn this example, the `toneMap` function is used to perform tone mapping on the color value obtained from a texture sampler. The `u_exposure` uniform is used to control the overall brightness of the output color. The resulting tone mapped color is then used to set the output color of the shader.\n## Questions: \n 1. What does this code do?\n   This code defines a function called `toneMap` that takes in a color vector and applies tone mapping to it based on the `exposure` uniform value.\n\n2. What is the purpose of the `exposure` uniform?\n   The `exposure` uniform controls the overall brightness of the tone mapping effect applied to the input color.\n\n3. What are the constants `A`, `B`, `C`, `D`, `E`, `F`, and `Scl` used for?\n   These constants are used in the tone mapping calculation to adjust the strength and shape of the effect.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/tonemappingHejl.md"}}],["549",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/tonemappingLinear.js)\n\nThis code exports a function called `toneMap` that takes in a `vec3` color value and returns a modified version of that color based on a uniform float value called `exposure`. The purpose of this function is to apply a tone mapping effect to the input color, which adjusts the brightness and contrast of the image to make it more visually appealing.\n\nThe `toneMap` function multiplies the input color by the `exposure` value, which effectively scales the brightness of the color. This can be used to adjust the overall brightness of an image or to compensate for differences in lighting conditions between different parts of the scene.\n\nThis code is likely part of a larger rendering pipeline in the PlayCanvas engine, which is responsible for rendering 3D scenes in real-time. The `toneMap` function may be used as a post-processing effect that is applied to the final rendered image before it is displayed on the screen. This can be used to improve the visual quality of the image and make it more appealing to the viewer.\n\nHere is an example of how this code might be used in a larger project:\n\n```javascript\n// Create a new material for a 3D object\nconst material = new pc.StandardMaterial();\n\n// Set the diffuse color of the material to red\nmaterial.diffuse.set(1, 0, 0);\n\n// Set the exposure value for the tone mapping effect\nmaterial.setParameter('exposure', 2.0);\n\n// Add the material to a 3D object in the scene\nconst entity = new pc.Entity();\nentity.addComponent('model', {\n    type: 'box',\n    material: material\n});\napp.root.addChild(entity);\n```\n\nIn this example, we create a new `StandardMaterial` for a 3D object and set its diffuse color to red. We also set the `exposure` parameter of the material to 2.0, which will be used by the `toneMap` function to adjust the brightness of the final rendered image. Finally, we add the material to a new `Entity` in the scene and add that entity to the root of the scene graph. When the scene is rendered, the `toneMap` function will be applied to the final image to adjust its brightness based on the `exposure` value we set.\n## Questions: \n 1. What is the purpose of this code?\n   This code defines a GLSL function called `toneMap` that applies an exposure value to a given color.\n\n2. What is the expected input and output of the `toneMap` function?\n   The `toneMap` function takes a vec3 color as input and returns a modified vec3 color with exposure applied.\n\n3. How is the `exposure` value determined or set?\n   The `exposure` value is a uniform float, which means it can be set externally by the application using this code.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/tonemappingLinear.md"}}],["550",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/frag/tonemappingNone.js)\n\nThe code above is a function called `toneMap` that takes in a `vec3` color value and returns the same color value. This function is written in GLSL (OpenGL Shading Language), which is a high-level language used to write shaders for graphics processing units (GPUs).\n\nIn the context of the PlayCanvas engine project, this function may be used in the rendering pipeline to apply tone mapping to the final rendered image. Tone mapping is a technique used to convert high dynamic range (HDR) images to low dynamic range (LDR) images that can be displayed on standard monitors. This is important because HDR images have a wider range of brightness values than standard monitors can display, so tone mapping is used to compress the range of brightness values to fit within the display's capabilities.\n\nThe `toneMap` function in this file is a simple implementation that does not actually perform any tone mapping. Instead, it simply returns the input color value unchanged. This is likely just a placeholder function that can be replaced with a more complex tone mapping function in the future.\n\nHere is an example of how this function might be used in a shader:\n\n```glsl\nuniform sampler2D u_diffuseMap;\nvarying vec2 v_uv;\n\nvec4 applyToneMapping(vec4 color) {\n    vec3 hdrColor = texture2D(u_diffuseMap, v_uv).rgb * color.rgb;\n    vec3 ldrColor = toneMap(hdrColor);\n    return vec4(ldrColor, color.a);\n}\n\nvoid main() {\n    vec4 color = applyToneMapping(vec4(1.0));\n    gl_FragColor = color;\n}\n```\n\nIn this example, the `applyToneMapping` function takes in a color value and applies the diffuse texture map to it before passing it through the `toneMap` function. The resulting color value is then returned as a `vec4` with the alpha value unchanged. This function is then called in the `main` function of the shader to set the final fragment color.\n\nOverall, the `toneMap` function in this file is a small but important piece of the PlayCanvas engine's rendering pipeline that will likely be expanded upon in the future to provide more advanced tone mapping capabilities.\n## Questions: \n 1. **What is the purpose of this function?** \nA smart developer might wonder what the `toneMap` function does and how it fits into the overall functionality of the PlayCanvas engine.\n\n2. **What is the expected input and output of this function?** \nA smart developer might want to know what type of input `color` expects and what type of output the function returns.\n\n3. **Are there any other functions or variables that this code depends on?** \nA smart developer might want to know if there are any other dependencies that this code relies on, such as other functions or variables within the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/frag/tonemappingNone.md"}}],["551",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/vert/fullscreenQuad.js)\n\nThe code above is a GLSL shader code that is used to render 2D graphics in the PlayCanvas engine. The purpose of this code is to define the vertex position and the texture coordinates of a 2D object that is being rendered on the screen. \n\nThe `attribute vec2 vertex_position` defines the position of the vertex in 2D space. The `varying vec2 vUv0` is used to pass the texture coordinates to the fragment shader. \n\nThe `gl_Position` variable is used to set the position of the vertex in 3D space. In this case, it is set to `vec4(vertex_position, 0.5, 1.0)` which means that the vertex is positioned at `vertex_position` in the x and y axis, and at 0.5 in the z axis. The w component is set to 1.0 which means that it is not a directional vector.\n\nThe `vUv0` variable is used to set the texture coordinates of the vertex. It is calculated by multiplying the vertex position by 0.5 and adding 0.5 to it. This ensures that the texture coordinates are in the range of 0 to 1, which is required by the fragment shader.\n\nThis code is used in the larger PlayCanvas engine project to render 2D graphics on the screen. It is used in conjunction with other GLSL shader codes and JavaScript code to create complex 2D and 3D scenes. \n\nHere is an example of how this code can be used in a JavaScript file:\n\n```javascript\nconst shader = new pc.Shader(device, {\n    attributes: {\n        vertex_position: pc.SEMANTIC_POSITION\n    },\n    vshader: /* glsl */`\n        attribute vec2 vertex_position;\n\n        varying vec2 vUv0;\n\n        void main(void)\n        {\n            gl_Position = vec4(vertex_position, 0.5, 1.0);\n            vUv0 = vertex_position.xy*0.5+0.5;\n        }\n    `,\n    fshader: /* glsl */`\n        uniform sampler2D texture_map;\n\n        varying vec2 vUv0;\n\n        void main(void)\n        {\n            gl_FragColor = texture2D(texture_map, vUv0);\n        }\n    `\n});\n```\n\nIn this example, the GLSL shader code is used to create a new `pc.Shader` object that is used to render a texture on a 2D object. The `vshader` property of the `pc.Shader` object is set to the GLSL shader code above. The `fshader` property is set to another GLSL shader code that is used to render the texture on the object. \n\nOverall, this GLSL shader code is an essential part of the PlayCanvas engine project that is used to render 2D graphics on the screen.\n## Questions: \n 1. What is the purpose of the `vertex_position` attribute in this code?\n   - The `vertex_position` attribute is used to define the position of each vertex in the shader program.\n\n2. What does the `vUv0` varying variable represent?\n   - The `vUv0` varying variable represents the texture coordinates of the vertex.\n\n3. What is the significance of the `/* glsl */` comment at the beginning of the code?\n   - The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language) format, which is a high-level language used to write shaders for graphics processing units (GPUs).","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/vert/fullscreenQuad.md"}}],["552",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/vert/msdf.js)\n\nThe code above is a GLSL shader code that defines a function called `unpackMsdfParams()`. This function is responsible for unpacking and decoding two sets of vertex attributes (`vertex_outlineParameters` and `vertex_shadowParameters`) that are used to render text with multi-channel signed distance field (MSDF) fonts. \n\nThe `unpackMsdfParams()` function first extracts the little and big components of the `vertex_outlineParameters` attribute. The little component is obtained by taking the modulus of the attribute with 256, while the big component is obtained by subtracting the little component from the original attribute and dividing the result by 256. The little component encodes the red and blue channels of the outline color, while the big component encodes the green and alpha channels of the outline color. The outline thickness is obtained by dividing the little.z component by 255 and multiplying the result by a constant value of 0.2.\n\nThe function then proceeds to extract the little and big components of the `vertex_shadowParameters` attribute using the same approach as before. The little component encodes the red and blue channels of the shadow color, while the big component encodes the green and alpha channels of the shadow color. The shadow offset is obtained by remapping the z components of the little and big components from the range [0, 254] to the range [-1, 1] and multiplying the result by a constant value of 0.005.\n\nOverall, this code is an essential part of the PlayCanvas engine's text rendering pipeline. It allows the engine to efficiently decode and extract the necessary information from the vertex attributes of MSDF fonts, which are used to render high-quality text with crisp outlines and soft shadows. Here is an example of how this code might be used in a larger project:\n\n```javascript\n// create a new material for text rendering\nconst textMaterial = new pc.StandardMaterial();\n\n// set the shader code for the material\ntextMaterial.shader = new pc.Shader(device, {\n    attributes: {\n        vertex_outlineParameters: pc.SEMANTIC_ATTR0,\n        vertex_shadowParameters: pc.SEMANTIC_ATTR1\n    },\n    vshader: /* glsl */`\n        attribute vec3 vertex_outlineParameters;\n        attribute vec3 vertex_shadowParameters;\n\n        varying vec4 outline_color;\n        varying float outline_thickness;\n        varying vec4 shadow_color;\n        varying vec2 shadow_offset;\n\n        void main() {\n            // unpack the MSDF parameters\n            unpackMsdfParams();\n\n            // pass the parameters to the fragment shader\n            gl_Position = getPosition();\n            outline_color.a = 1.0;\n            shadow_color.a = 1.0;\n            vUv0 = getUv();\n        }\n    `,\n    fshader: /* glsl */`\n        varying vec4 outline_color;\n        varying float outline_thickness;\n        varying vec4 shadow_color;\n        varying vec2 shadow_offset;\n\n        void main() {\n            // render the text with the MSDF parameters\n            gl_FragColor = renderText(outline_color, outline_thickness, shadow_color, shadow_offset);\n        }\n    `\n});\n\n// create a new text entity\nconst textEntity = new pc.Entity();\ntextEntity.addComponent('element', {\n    type: 'text',\n    text: 'Hello, world!',\n    fontAsset: fontAsset,\n    material: textMaterial\n});\n\n// add the text entity to the scene\napp.root.addChild(textEntity);\n``` \n\nIn this example, the `textMaterial` object is created with the GLSL shader code that includes the `unpackMsdfParams()` function. The `textEntity` object is then created with a text component that specifies the font asset and the material to use for rendering the text. When the text is rendered, the `unpackMsdfParams()` function is called to extract the necessary MSDF parameters from the vertex attributes, which are then passed to the fragment shader for rendering the text with crisp outlines and soft shadows.\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n   - This code defines a GLSL shader function called `unpackMsdfParams()` that unpacks outline and shadow parameters for a mesh. It is used to set the color, thickness, and offset of the mesh's outline and shadow.\n2. What are the expected input and output types for this function?\n   - The function takes in two attribute vectors of type `vec3` called `vertex_outlineParameters` and `vertex_shadowParameters`. It outputs four varying variables of type `vec4` and `float` called `outline_color`, `shadow_color`, `outline_thickness`, and `shadow_offset`.\n3. What are the values of `_outlineThicknessScale` and `_shadowOffsetScale` and how do they affect the output of this function?\n   - The value of `_outlineThicknessScale` is 0.2 and it is used to scale the outline thickness value. The value of `_shadowOffsetScale` is 0.005 and it is used to scale the shadow offset value. These values are used to remap the input values to the desired output range.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/vert/msdf.md"}}],["553",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/vert/skinBatchConst.js)\n\nThe code above is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to define a function that returns a 4x4 matrix representing the transformation of a bone in a skeletal animation. \n\nThe function `getBoneMatrix` takes in a float value `i` which represents the index of the bone in the skeleton. The function then reads a 4x3 matrix from the `matrix_pose` uniform variable, which is an array of vec4 values representing the pose of each bone in the skeleton. The matrix is read by accessing the array at the indices `int(3.0 * i)`, `int(3.0 * i + 1.0)`, and `int(3.0 * i + 2.0)`. \n\nThe function then transposes the 4x3 matrix into a 4x4 matrix by adding a fourth column and row of values. The resulting 4x4 matrix is then returned by the function. \n\nThis code is used in the PlayCanvas engine to animate 3D models with skeletal animations. The `vertex_boneIndices` attribute is used to specify which bones in the skeleton affect each vertex in the model. The `matrix_pose` uniform variable is updated each frame to reflect the current pose of each bone in the skeleton. The `getBoneMatrix` function is then called for each affected bone to calculate its transformation matrix. These matrices are then used to transform the vertices of the model to create the final animated result. \n\nExample usage of this code in a PlayCanvas project:\n\n```javascript\n// Create a new material with the shader code\nvar material = new pc.StandardMaterial();\nmaterial.chunks.skinningVertexShader = /* glsl */`\n    attribute float vertex_boneIndices;\n\n    uniform vec4 matrix_pose[BONE_LIMIT * 3];\n\n    mat4 getBoneMatrix(const in float i) {\n        // read 4x3 matrix\n        vec4 v1 = matrix_pose[int(3.0 * i)];\n        vec4 v2 = matrix_pose[int(3.0 * i + 1.0)];\n        vec4 v3 = matrix_pose[int(3.0 * i + 2.0)];\n\n        // transpose to 4x4 matrix\n        return mat4(\n            v1.x, v2.x, v3.x, 0,\n            v1.y, v2.y, v3.y, 0,\n            v1.z, v2.z, v3.z, 0,\n            v1.w, v2.w, v3.w, 1\n        );\n    }\n`;\n\n// Set the material on a model component\nvar model = entity.model.model;\nmodel.meshInstances.forEach(function (meshInstance) {\n    meshInstance.material = material;\n});\n``` \n\nIn the example above, a new `StandardMaterial` is created and the `skinningVertexShader` chunk is replaced with the GLSL code above. The resulting material is then set on a model component attached to an entity in the scene. The `getBoneMatrix` function is called automatically by the PlayCanvas engine for each affected bone in the model to animate it.\n## Questions: \n 1. What is the purpose of the `vertex_boneIndices` attribute and how is it used in this code?\n   - The `vertex_boneIndices` attribute is used to store the indices of the bones that influence each vertex in a mesh. It is used to calculate the transformation matrix for each bone in the `getBoneMatrix` function.\n2. What is the `matrix_pose` uniform and how is it populated?\n   - The `matrix_pose` uniform is an array of 4x3 matrices that represent the pose of each bone in the skeleton. It is populated with the current pose of the skeleton before rendering.\n3. What is the `BONE_LIMIT` constant and how does it affect this code?\n   - The `BONE_LIMIT` constant is a maximum limit on the number of bones that can be used in a skeleton. It is multiplied by 3 in this code to determine the size of the `matrix_pose` array. If the number of bones in the skeleton exceeds this limit, the code may not function correctly.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/vert/skinBatchConst.md"}}],["554",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/vert/skinBatchTex.js)\n\nThe code above is a GLSL shader code that exports a function called `getBoneMatrix`. This function is used to retrieve the transformation matrix of a specific bone in a skeletal animation. The function takes in a float value `i` which represents the index of the bone in the skeleton. \n\nThe function first calculates the position of the bone's transformation matrix in the texture map by using the `texture_poseMapSize` uniform variable which contains the size of the texture map. The `vertex_boneIndices` attribute is used to determine which bone indices affect each vertex in the mesh. \n\nThe function then reads the elements of the 4x3 matrix from the texture map using the `texture2D` function. The matrix is then transposed to a 4x4 matrix and returned as the output of the function. \n\nThis code is part of the PlayCanvas engine project and is used to animate 3D models with skeletal animations. The `getBoneMatrix` function is called by other parts of the engine to retrieve the transformation matrix of each bone in the skeleton for each frame of the animation. This matrix is then used to transform the vertices of the mesh to create the animation effect. \n\nHere is an example of how the `getBoneMatrix` function can be used in the larger project:\n\n```javascript\n// create a new mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n\n// set the vertex bone indices attribute\nmeshInstance.vertexBuffer.addAttribute('vertex_boneIndices', 'float', 4);\n\n// set the texture pose map and size uniforms\nmeshInstance.setParameter('texture_poseMap', poseMap);\nmeshInstance.setParameter('texture_poseMapSize', [poseMap.width, poseMap.height, 1 / poseMap.width, 1 / poseMap.height]);\n\n// animate the mesh\nfunction animate() {\n    for (var i = 0; i < numBones; i++) {\n        var boneMatrix = getBoneMatrix(i);\n        // apply the bone matrix to the mesh vertices\n        // ...\n    }\n}\n``` \n\nIn this example, a new mesh instance is created and the `vertex_boneIndices` attribute is set to the bone indices that affect each vertex in the mesh. The `texture_poseMap` and `texture_poseMapSize` uniforms are also set to the texture map containing the transformation matrices for each bone in the skeleton. \n\nThe `getBoneMatrix` function is then called for each bone in the skeleton to retrieve its transformation matrix. This matrix is then used to transform the vertices of the mesh to create the animation effect.\n## Questions: \n 1. What is the purpose of the `vertex_boneIndices` attribute and how is it used in the code?\n- The `vertex_boneIndices` attribute is used to store the indices of the bones that influence each vertex in a mesh. It is used to calculate the transformation matrix for each bone in the `getBoneMatrix` function.\n\n2. What is the `texture_poseMap` uniform and how is it used in the `getBoneMatrix` function?\n- The `texture_poseMap` uniform is a 2D texture that stores the transformation matrices for each bone in a mesh. In the `getBoneMatrix` function, it is used to read the elements of the 4x3 matrix for a specific bone and transpose them into a 4x4 matrix.\n\n3. What is the purpose of the `texture_poseMapSize` uniform and how is it used in the `getBoneMatrix` function?\n- The `texture_poseMapSize` uniform is a vec4 that stores the size of the `texture_poseMap` texture. It is used in the `getBoneMatrix` function to calculate the x and y coordinates of the texture coordinates for each element of the 4x3 matrix.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/vert/skinBatchTex.md"}}],["555",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/vert/skinConst.js)\n\nThe code above is a GLSL shader code that is used to calculate the final transformation matrix for a skinned mesh. This code is a part of the PlayCanvas engine project and is responsible for calculating the final transformation matrix for a skinned mesh. \n\nThe code starts by defining two attributes, `vertex_boneWeights` and `vertex_boneIndices`, which are used to store the bone weights and indices for each vertex of the mesh. The `matrix_pose` uniform is an array of 4x3 matrices that represent the initial pose of each bone in the skeleton. \n\nThe `getBoneMatrix` function is used to read the 4x3 matrix for a given bone index `i` from the `matrix_pose` uniform. This function takes in the bone index `i` and outputs three vectors `v1`, `v2`, and `v3`, which represent the rows of the 4x3 matrix. \n\nThe `getSkinMatrix` function is the main function that calculates the final transformation matrix for a skinned mesh. This function takes in two vectors, `indices` and `weights`, which represent the bone indices and weights for a single vertex of the mesh. \n\nThe `getSkinMatrix` function first calls the `getBoneMatrix` function four times to get the 4x3 matrices for the four bones that influence the vertex. It then multiplies each of these matrices by their corresponding weight and adds them up to get a final 4x3 matrix. \n\nThe function then calculates the sum of the weights and transposes the 4x3 matrix to a 4x4 matrix by adding a fourth row and column of zeros and a one in the bottom right corner. This final 4x4 matrix represents the transformation matrix for the vertex.\n\nThis code is used in the larger PlayCanvas engine project to animate skinned meshes. The engine uses this code to calculate the transformation matrix for each vertex of the mesh based on the current pose of the skeleton. This allows the engine to smoothly animate the mesh as the skeleton moves. \n\nExample usage of this code in the PlayCanvas engine project:\n\n```javascript\n// create a skinned mesh entity\nvar entity = new pc.Entity();\nentity.addComponent('model', {\n    type: 'asset',\n    asset: skinnedMeshAsset\n});\n\n// animate the skeleton\nvar skeleton = entity.model.model.getGraph().findByName('skeleton');\nskeleton.rotate(0, 0.1, 0);\n\n// update the mesh vertices\nvar meshInstance = entity.model.model.meshInstances[0];\nvar skinInstance = meshInstance.skinInstance;\nvar skinMatrices = skinInstance.matrices;\nfor (var i = 0; i < skinMatrices.length; i++) {\n    var indices = meshInstance.mesh.vertexBuffer.lock().data[i].boneIndices;\n    var weights = meshInstance.mesh.vertexBuffer.lock().data[i].boneWeights;\n    var skinMatrix = getSkinMatrix(indices, weights);\n    skinMatrices.set(skinMatrix, i * 16);\n}\nmeshInstance.mesh.vertexBuffer.unlock();\n```\n## Questions: \n 1. What is the purpose of the `vertex_boneWeights` and `vertex_boneIndices` attributes?\n   \n   These attributes are used to store the bone weights and indices for each vertex in a mesh. They are used to calculate the final transformation matrix for each vertex based on the position of the bones.\n\n2. What is the significance of the `BONE_LIMIT` constant?\n   \n   The `BONE_LIMIT` constant is used to define the maximum number of bones that can be used to animate a mesh. It is multiplied by 3 in the `matrix_pose` uniform to allocate enough space for the 4x3 matrices of each bone.\n\n3. How is the final skinning matrix calculated?\n   \n   The final skinning matrix is calculated by multiplying the 4x3 matrices of each bone by their corresponding weights, adding them up, and then transposing the resulting 3x4 matrix to a 4x4 matrix. The dot product of the weights is also calculated and used to normalize the matrix.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/vert/skinConst.md"}}],["556",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/vert/skinTex.js)\n\nThe code above is a GLSL shader code that is used to calculate the skinning matrix for a 3D model. The skinning matrix is used to animate a 3D model by deforming its vertices based on the movement of its bones. \n\nThe code exports a function called `getSkinMatrix` that takes in two parameters: `indices` and `weights`. `indices` is a vec4 that contains the indices of the four bones that influence the vertex, and `weights` is a vec4 that contains the weights of each bone's influence on the vertex. \n\nThe `getSkinMatrix` function calls another function called `getBoneMatrix` to retrieve the transformation matrix for each bone. The `getBoneMatrix` function takes in a single parameter `index`, which is the index of the bone to retrieve the transformation matrix for. \n\nThe `getBoneMatrix` function calculates the position of the transformation matrix in the texture map by using the `texture_poseMapSize` uniform, which contains the size of the texture map, and the `vertex_boneIndices` attribute, which contains the index of the bone for the current vertex. It then reads the elements of the 4x3 matrix from the texture map and returns them as vec4s. \n\nThe `getSkinMatrix` function multiplies each bone's transformation matrix by its corresponding weight and adds them together to get the final 4x3 matrix. It then calculates the sum of the weights and transposes the 4x3 matrix to a 4x4 matrix by adding a row of zeros and a column of ones. The resulting 4x4 matrix is the skinning matrix that is used to deform the vertex. \n\nThis code is used in the PlayCanvas engine to animate 3D models. It is likely used in conjunction with other code that handles the movement of the bones and the rendering of the model. Here is an example of how this code might be used in a larger project:\n\n```javascript\n// create a new 3D model\nconst model = new PlayCanvas.Model();\n\n// load the model data\nmodel.load('model.json', () => {\n  // get the mesh instance for the model\n  const meshInstance = model.meshInstances[0];\n\n  // get the vertex bone weights and indices for the mesh instance\n  const boneWeights = meshInstance.mesh.vertexBuffer.format.element('vertex_boneWeights').data;\n  const boneIndices = meshInstance.mesh.vertexBuffer.format.element('vertex_boneIndices').data;\n\n  // create a new shader that uses the skinning code\n  const shader = new PlayCanvas.Shader({\n    attributes: {\n      vertex_boneWeights: 'vertex_boneWeights',\n      vertex_boneIndices: 'vertex_boneIndices'\n    },\n    uniforms: {\n      texture_poseMap: 'texture_poseMap',\n      texture_poseMapSize: 'texture_poseMapSize'\n    },\n    vs: `\n      // vertex shader code here\n    `,\n    fs: `\n      ${getSkinMatrix}\n      // fragment shader code here\n    `\n  });\n\n  // set the shader on the mesh instance\n  meshInstance.material.shader = shader;\n\n  // set the bone weights and indices as vertex attributes\n  meshInstance.setParameter('vertex_boneWeights', boneWeights);\n  meshInstance.setParameter('vertex_boneIndices', boneIndices);\n\n  // set the texture map and size uniforms\n  shader.setParameter('texture_poseMap', textureMap);\n  shader.setParameter('texture_poseMapSize', textureMapSize);\n});\n```\n## Questions: \n 1. What is the purpose of the `getBoneMatrix` function?\n    \n    The `getBoneMatrix` function is used to retrieve a 4x3 matrix from a texture based on a given bone index.\n\n2. What is the purpose of the `getSkinMatrix` function?\n    \n    The `getSkinMatrix` function is used to calculate the final 4x4 skinning matrix for a vertex based on its bone weights and indices.\n\n3. What is the format of the `texture_poseMap` and `texture_poseMapSize` uniforms?\n    \n    The `texture_poseMap` uniform is a highp sampler2D used to store a texture containing bone matrices. The `texture_poseMapSize` uniform is a vec4 containing the width, height, and inverse width and height of the texture.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/vert/skinTex.md"}}],["557",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/vert/transform.js)\n\nThe code provided is a GLSL shader code that is used in the PlayCanvas engine project. The code is responsible for rendering 3D models in the engine. The code exports a default function that returns a string of GLSL code. The code is used to calculate the position of vertices in 3D space and convert them to screen space coordinates for rendering.\n\nThe code contains several preprocessor directives that are used to conditionally compile different parts of the code based on the configuration of the engine. For example, the `PIXELSNAP` directive is used to enable pixel snapping, which snaps vertices to a pixel boundary to avoid aliasing artifacts. The `SCREENSPACE` directive is used to enable screen space rendering, which is used for rendering UI elements. The `MORPHING` and `MORPHING_TEXTURE_BASED` directives are used to enable morph target animation, which is a technique used to animate 3D models by blending between different vertex positions.\n\nThe `getModelMatrix` function is used to calculate the model matrix of the 3D model. The model matrix is used to transform the vertices from local space to world space. The `getPosition` function is used to calculate the position of each vertex in screen space coordinates. The function first calculates the local position of the vertex and then applies any morph target animation offsets. The function then calculates the world position of the vertex by multiplying the local position by the model matrix. Finally, the function calculates the screen position of the vertex by multiplying the world position by the view projection matrix.\n\nThe `getWorldPosition` function is used to return the world position of the vertex. This function is used by other parts of the engine to perform collision detection and other calculations that require the world position of the vertices.\n\nOverall, this code is an essential part of the PlayCanvas engine and is used to render 3D models in the engine. The code is highly configurable and can be customized to suit the needs of different projects.\n## Questions: \n 1. What is the purpose of the `#ifdef` statements throughout the code?\n- The `#ifdef` statements are used to conditionally compile certain parts of the code based on whether certain preprocessor macros are defined or not.\n\n2. What is the `getTextureMorphCoords()` function used for?\n- The `getTextureMorphCoords()` function is used to calculate the texture coordinates for a morph target based on the vertex ID and texture parameters.\n\n3. What is the purpose of the `getPosition()` function?\n- The `getPosition()` function is used to calculate the screen position of a vertex based on its local position, model matrix, and view-projection matrix. It also takes into account various optional features such as morphing and pixel snapping.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/vert/transform.md"}}],["558",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/common/vert/transformDecl.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) for the PlayCanvas engine. The purpose of this code is to define the attributes and uniforms required for rendering 3D objects on the screen.\n\nThe `attribute vec3 vertex_position` defines the position of each vertex in 3D space. This attribute is used to create the geometry of the 3D object.\n\nThe `uniform mat4 matrix_model` and `uniform mat4 matrix_viewProjection` are matrices used for transforming the 3D object. The `matrix_model` matrix is used to transform the object from its local space to world space. The `matrix_viewProjection` matrix is used to transform the object from world space to screen space. These matrices are used to create the final position of each vertex on the screen.\n\nThe `vec3 dPositionW` and `mat4 dModelMatrix` are variables used for calculating the position of each vertex in world space. These variables are used in more complex shaders that require additional calculations.\n\nThis code can be used in the larger PlayCanvas project to render 3D objects on the screen. The shader code is compiled and executed on the GPU, which allows for efficient rendering of complex 3D scenes. The attributes and uniforms defined in this code can be used in other shader programs to create different visual effects.\n\nExample usage of this code in PlayCanvas:\n\n```javascript\nvar shader = new pc.Shader(device, {\n    attributes: {\n        vertex_position: pc.SEMANTIC_POSITION\n    },\n    uniforms: {\n        matrix_model: new pc.Mat4(),\n        matrix_viewProjection: new pc.Mat4()\n    },\n    vshader: /* glsl */`\n        attribute vec3 vertex_position;\n\n        uniform mat4 matrix_model;\n        uniform mat4 matrix_viewProjection;\n\n        void main() {\n            gl_Position = matrix_viewProjection * matrix_model * vec4(vertex_position, 1.0);\n        }\n    `,\n    fshader: /* glsl */`\n        void main() {\n            gl_FragColor = vec4(1.0, 1.0, 1.0, 1.0);\n        }\n    `\n});\n```\n\nIn this example, a new shader program is created using the `pc.Shader` constructor. The `attributes` and `uniforms` objects are defined to match the attributes and uniforms in the shader code. The `vshader` and `fshader` properties are set to the vertex and fragment shader code, respectively. This shader program can then be used to render 3D objects on the screen.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code defines a GLSL shader code that includes an attribute for vertex position, as well as uniform matrices for model and view projection.\n\n2. **What is the expected input for the `vertex_position` attribute?**\\\nThe `vertex_position` attribute is expected to be a vec3 data type, which represents the position of a vertex in 3D space.\n\n3. **What is the significance of the `dPositionW` and `dModelMatrix` variables?**\\\nIt is unclear from this code snippet what the purpose of the `dPositionW` and `dModelMatrix` variables are, as they are declared but not used. Further context or additional code would be needed to determine their significance.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/common/vert/transformDecl.md"}}],["559",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lightmapper/frag/bakeDirLmEnd.js)\n\nThis code is a GLSL shader that is used in the PlayCanvas engine to render lighting effects on 3D objects. The purpose of this code is to calculate the final color of a pixel on a surface based on the direction of the light source and the amount of attenuation (reduction in intensity) due to distance from the light source.\n\nThe code first retrieves a texture called `texture_dirLightMap` and samples it at the texture coordinates `vUv1` to get a `vec4` value called `dirLm`. This value represents the color and intensity of the light hitting the surface from the direction of the light source.\n\nThe code then checks if the `bakeDir` variable is greater than 0.5. If it is, this means that the light source is a directional light (as opposed to a point light or spot light), and the code proceeds to calculate the final color of the pixel based on the direction of the light and the amount of attenuation due to distance.\n\nIf `dAtten` (the distance attenuation factor) is greater than a small value of 0.00001, the code applies attenuation to the light by multiplying `dLightDirNormW.xyz` (the normalized direction of the light in world space) by `dAtten`, and adding it to `dirLm.xyz` (the color and intensity of the light from the texture). The resulting color is then normalized, multiplied by 0.5, and added to a vector of (0.5, 0.5, 0.5) to produce the final color of the pixel. The alpha value of the pixel is set to the sum of `dirLm.w` (the alpha value of the light texture) and `dAtten`, and then clamped to a minimum value of 1/255 to avoid transparency artifacts.\n\nIf `dAtten` is less than or equal to 0.00001, this means that the surface is very close to the light source and the attenuation is negligible. In this case, the final color of the pixel is simply set to `dirLm`.\n\nIf `bakeDir` is less than or equal to 0.5, this means that the light source is not a directional light and the code simply sets the final color of the pixel to `dirLm.xyz` (the color and intensity of the light from the texture). The alpha value of the pixel is set to the maximum of `dirLm.w` (the alpha value of the light texture) and either 1/255 or 0, depending on whether `dAtten` is greater than 0.00001 or not.\n\nOverall, this code is an important part of the PlayCanvas engine's rendering pipeline, as it allows for realistic lighting effects to be applied to 3D objects in real-time. Developers can use this code as a starting point for creating their own custom shaders that incorporate different lighting models and effects.\n## Questions: \n 1. What is the purpose of the `texture_dirLightMap` variable?\n    - The `texture_dirLightMap` variable is a texture used to calculate the `dirLm` vector in the code.\n\n2. What is the significance of the `bakeDir` and `dAtten` variables?\n    - The `bakeDir` variable is used to determine whether to apply directional lighting to the fragment. The `dAtten` variable is used to calculate the attenuation of the directional light.\n\n3. What is the expected output of this code?\n    - The expected output of this code is a modified `gl_FragColor` value based on the `dirLm` vector and the directional light attenuation.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lightmapper/frag/bakeDirLmEnd.md"}}],["560",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lightmapper/frag/bakeLmEnd.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) that is used in the PlayCanvas engine project. The purpose of this code is to calculate the final color of a fragment (a pixel on the screen) based on the diffuse light that falls on it. \n\nThe code starts with an `#ifdef` statement that checks if the `LIGHTMAP_RGBM` flag is defined. If it is defined, the code applies gamma correction to the diffuse light by raising it to the power of 0.5. Then, it divides the result by 8.0 to reduce the brightness. The alpha value of the fragment is calculated by taking the maximum value of the red, green, and blue channels of the color, and the value of 1.0/255.0 (which is the smallest possible value for the alpha channel). This value is then clamped between 0.0 and 1.0, and multiplied by 255.0 to convert it to an 8-bit value. Finally, the alpha value is divided by 255.0 to normalize it. The RGB color of the fragment is then divided by the alpha value to get the final color.\n\nIf the `LIGHTMAP_RGBM` flag is not defined, the code simply sets the color of the fragment to the diffuse light color, with an alpha value of 1.0.\n\nThis code is used in the PlayCanvas engine to render 3D scenes with lighting. It is part of the shader program that is executed on the GPU for each fragment of the scene. The `LIGHTMAP_RGBM` flag is typically set by the engine based on the properties of the materials used in the scene. If a material has a lightmap texture that uses RGBM encoding, the flag is set and this code is used to render the material. Otherwise, the simpler code path is used.\n\nHere is an example of how this code might be used in a PlayCanvas script:\n\n```javascript\nvar material = new pc.StandardMaterial();\nmaterial.lightMap = lightMapTexture;\nmaterial.lightMapRgbm = true;\nmaterial.shader = pc.shaderChunks.lightmap;\n```\n\nIn this example, a new material is created with a lightmap texture and the `lightMapRgbm` property set to `true`. This tells the engine to use the RGBM encoding for the lightmap, which in turn sets the `LIGHTMAP_RGBM` flag and causes this code to be used in the shader program. The `shader` property is set to `pc.shaderChunks.lightmap`, which is a predefined string that includes the code for this shader.\n## Questions: \n 1. What is the purpose of the `LIGHTMAP_RGBM` preprocessor directive?\n- The `LIGHTMAP_RGBM` preprocessor directive is used to conditionally compile the code based on whether RGBM lightmaps are being used or not.\n\n2. What does the `dDiffuseLight` variable represent?\n- The `dDiffuseLight` variable represents the diffuse light color.\n\n3. What is the purpose of the `ceil` function call in the code?\n- The `ceil` function call is used to round up the alpha value of the `gl_FragColor` to the nearest multiple of 1/255, which is necessary for correct encoding of the alpha value in certain situations.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lightmapper/frag/bakeLmEnd.md"}}],["561",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lightmapper/frag/bilateralDeNoise.js)\n\nThe code is a shader program that implements a bilateral filter. The bilateral filter is a non-linear, edge-preserving, and noise-reducing smoothing filter for images. It replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels. This weight can be based on a Gaussian distribution. Crucially, the weights depend not only on Euclidean distance of pixels, but also on the radiometric differences (e.g., range differences, such as color intensity, depth distance, etc.). This preserves sharp edges.\n\nThe shader program takes an input texture and applies the bilateral filter to it. The output is a filtered texture. The program uses several helper functions to decode and encode RGBM values, which are used to store high dynamic range (HDR) color values in a low dynamic range (LDR) format. The program also uses a normpdf3 function to calculate the Gaussian weights for the filter.\n\nThe program takes several uniform variables as input, including the source texture, pixel offset, sigmas, bZnorm, and kernel. The pixel offset determines the size of the filter kernel. The sigmas variable contains two values, the first controls the blurriness based on a pixel distance, and the second controls the blurriness based on a pixel similarity (to preserve edges). The bZnorm variable is used to normalize the bilateral factors. The kernel variable contains the weights for the filter kernel.\n\nThe program loops over the texels in the input texture and applies the bilateral filter to each pixel. It skips pixels that were not baked, which allows dilate filter that work on the output of this to work correctly, as it depends on .a being zero to dilate, which the following blur filter would otherwise modify. The program calculates the bilateral factors for each pixel and accumulates the filtered values. Finally, the program encodes the filtered HDR color values into RGBM format and outputs the filtered texture.\n\nExample usage:\n\n```javascript\n// create a new material with the bilateral filter shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.endPS = [\n    'gl_FragColor = texture2D(source, vUv0);',\n    '#ifdef BILATERAL_FILTER',\n    pc.programlib.shaders.bilateralFilterPS,\n    '#endif',\n    'gl_FragColor.a = 1.0;'\n].join('\\n');\nmaterial.chunks.bilateralFilterPS = pc.programlib.shaders.bilateralFilterPS;\n\n// set the uniform variables for the filter\nmaterial.setParameter('pixelOffset', new pc.Vec2(1.0 / texture.width, 1.0 / texture.height));\nmaterial.setParameter('sigmas', new pc.Vec2(1.0, 1.0));\nmaterial.setParameter('bZnorm', 1.0);\nmaterial.setParameter('kernel', new Float32Array([\n    0.000229, 0.000764, 0.002294, 0.006023, 0.013582, 0.027023, 0.047977, 0.075856, 0.105399, 0.126064, 0.126064, 0.105399, 0.075856, 0.047977, 0.027023\n]));\n\n// set the source texture for the filter\nmaterial.setParameter('source', texture);\n\n// apply the material to a mesh\nmeshInstance.material = material;\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code is a shader for a bilateral filter, which is a non-linear, edge-preserving, and noise-reducing smoothing filter for images. It replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels.\n\n2. What is the significance of the `normpdf3` function?\n    \n    The `normpdf3` function calculates the weight of a pixel based on its distance from the center pixel and its radiometric differences. This weight is used to preserve sharp edges in the image.\n\n3. What is the role of the `encodeRGBM` function?\n    \n    The `encodeRGBM` function encodes the filtered pixel color into a compressed format that can be stored in a texture. This format is modified RGBM, which is a variation of RGBM encoding that uses a different exponent bias.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lightmapper/frag/bilateralDeNoise.md"}}],["562",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lightmapper/frag/dilate.js)\n\nThe code provided is a GLSL shader that performs texture sampling with a fallback mechanism. The shader takes in a texture source and a pixel offset as uniforms and outputs a color value for each pixel of the rendered object.\n\nThe purpose of this shader is to handle cases where the texture sampling fails due to the texture being partially or completely transparent. In such cases, the shader samples the texture at neighboring pixels to find a non-transparent color value. The pixel offset uniform is used to determine the distance between neighboring pixels.\n\nThe shader first samples the texture at the current pixel position (vUv0) and checks if the alpha value of the color is greater than 0. If it is, the color value is used as the output. If not, the shader samples the texture at the neighboring pixels in a specific order until a non-transparent color value is found. The order of sampling is designed to prioritize the pixels closest to the current pixel position.\n\nThis shader can be used in various rendering scenarios where textures with transparency are used, such as in particle systems or UI elements. By providing a fallback mechanism, the shader ensures that the rendered object always has a visible color value, even if the texture is partially or completely transparent.\n\nHere is an example of how this shader can be used in a PlayCanvas project:\n\n```javascript\n// create a material with the shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.diffusePS = /* glsl */`\n    varying vec2 vUv0;\n\n    uniform sampler2D source;\n    uniform vec2 pixelOffset;\n\n    void main(void) {\n        vec4 c = texture2D(source, vUv0);\n        c = c.a>0.0? c : texture2D(source, vUv0 - pixelOffset);\n        c = c.a>0.0? c : texture2D(source, vUv0 + vec2(0, -pixelOffset.y));\n        c = c.a>0.0? c : texture2D(source, vUv0 + vec2(pixelOffset.x, -pixelOffset.y));\n        c = c.a>0.0? c : texture2D(source, vUv0 + vec2(-pixelOffset.x, 0));\n        c = c.a>0.0? c : texture2D(source, vUv0 + vec2(pixelOffset.x, 0));\n        c = c.a>0.0? c : texture2D(source, vUv0 + vec2(-pixelOffset.x, pixelOffset.y));\n        c = c.a>0.0? c : texture2D(source, vUv0 + vec2(0, pixelOffset.y));\n        c = c.a>0.0? c : texture2D(source, vUv0 + pixelOffset);\n        gl_FragColor = c;\n    }\n`;\nmaterial.setParameter('source', texture);\nmaterial.setParameter('pixelOffset', new pc.Vec2(1.0 / texture.width, 1.0 / texture.height));\n\n// create a mesh instance with the material\nvar mesh = new pc.Mesh();\n// ... set up mesh data ...\nvar node = new pc.GraphNode();\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n```\n## Questions: \n 1. What is the purpose of this code?\n   \n   This code is a fragment shader that samples a texture and applies a pixel offset to the texture coordinates to check for neighboring pixels with alpha values greater than zero. If a neighboring pixel is found, its color is used for the current pixel. \n\n2. What are the inputs and outputs of this code?\n   \n   The inputs of this code are a texture sampler (`source`) and a pixel offset (`pixelOffset`). The output of this code is the color of the current pixel (`gl_FragColor`).\n\n3. How does this code handle edge cases or pixels with no neighboring pixels with alpha values greater than zero?\n   \n   If no neighboring pixel with alpha value greater than zero is found, the current pixel's color is used (`c`).","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lightmapper/frag/dilate.md"}}],["563",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/TBN.js)\n\nThe code above is a GLSL shader function that calculates the TBN matrix (Tangent, Bitangent, Normal) for a given set of vertex attributes. The TBN matrix is used in 3D graphics to transform normal vectors from object space to tangent space, which is necessary for certain lighting and shading calculations.\n\nThe function takes in three vec3 parameters: tangent, binormal, and normal. These vectors represent the tangent, bitangent, and normal vectors of a vertex in object space. The function then creates a 3x3 matrix called dTBN using these vectors. The matrix is created by normalizing each of the input vectors and placing them as columns in the matrix.\n\nThis function is likely used in the larger PlayCanvas engine project to calculate the TBN matrix for each vertex in a mesh. This matrix can then be used in other shader functions to perform lighting and shading calculations. For example, the TBN matrix can be used to transform a normal vector from object space to tangent space in a normal mapping shader.\n\nHere is an example of how this function might be used in a shader:\n\n```\nattribute vec3 a_position;\nattribute vec3 a_normal;\nattribute vec2 a_uv;\nattribute vec3 a_tangent;\nattribute vec3 a_binormal;\n\nuniform mat4 u_modelMatrix;\nuniform mat4 u_viewMatrix;\nuniform mat4 u_projectionMatrix;\n\nvarying vec2 v_uv;\nvarying vec3 v_tangent;\nvarying vec3 v_binormal;\nvarying vec3 v_normal;\n\nvoid main() {\n    // Transform vertex attributes to world space\n    vec4 worldPosition = u_modelMatrix * vec4(a_position, 1.0);\n    vec3 worldNormal = normalize(mat3(u_modelMatrix) * a_normal);\n    vec3 worldTangent = normalize(mat3(u_modelMatrix) * a_tangent);\n    vec3 worldBinormal = normalize(mat3(u_modelMatrix) * a_binormal);\n\n    // Calculate TBN matrix\n    mat3 TBN;\n    getTBN(worldTangent, worldBinormal, worldNormal);\n    TBN = transpose(inverse(TBN));\n\n    // Transform vertex attributes to view and projection space\n    vec4 viewPosition = u_viewMatrix * worldPosition;\n    gl_Position = u_projectionMatrix * viewPosition;\n\n    // Pass varying variables to fragment shader\n    v_uv = a_uv;\n    v_tangent = TBN * worldTangent;\n    v_binormal = TBN * worldBinormal;\n    v_normal = TBN * worldNormal;\n}\n```\n\nIn this example, the vertex attributes are transformed to world space using the model matrix. The TBN matrix is then calculated using the getTBN function. Finally, the vertex attributes are transformed to view and projection space using the view and projection matrices. The transformed attributes and the TBN matrix are passed as varying variables to the fragment shader for further calculations.\n## Questions: \n 1. What does this code do?\n   - This code defines a function called `getTBN` that takes in three vectors representing the tangent, binormal, and normal of a surface and calculates the corresponding TBN matrix.\n\n2. What is the purpose of the `/* glsl */` comment?\n   - This comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a high-level language used to write shaders for graphics processing units (GPUs).\n\n3. Are there any potential issues with this code?\n   - One potential issue is that the `dTBN` variable is not declared, so it may cause errors if it is not defined elsewhere in the code. Additionally, it is unclear what the intended use of the TBN matrix is, so it may not be suitable for all applications.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/TBN.md"}}],["564",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/TBNObjectSpace.js)\n\nThe code provided is a GLSL shader function that calculates the tangent, binormal, and normal vectors of a given surface. These vectors are commonly used in 3D graphics for various purposes such as normal mapping, bump mapping, and lighting calculations.\n\nThe function takes in three parameters: `tangent`, `binormal`, and `normal`, which are all 3D vectors. The `normal` vector is the surface normal at a given point, while the `tangent` and `binormal` vectors are calculated based on the `normal` vector and another vector `vObjectSpaceUpW`.\n\nThe `T` vector is calculated by taking the cross product of the `normal` vector and `vObjectSpaceUpW`. The `B` vector is then calculated by taking the cross product of the `normal` vector and `T`. If the dot product of `B` with itself is zero, it means that `vObjectSpaceUpW` is parallel to the `normal` vector. In this case, the function calculates a new `B` and `T` vector based on the major component of the `normal` vector.\n\nFinally, the function creates a matrix `dTBN` using the `T`, `B`, and `normal` vectors. This matrix is used to transform vectors from tangent space to object space or vice versa.\n\nThis function is likely used in the PlayCanvas engine to calculate the tangent, binormal, and normal vectors for various surfaces in 3D scenes. These vectors can then be used for lighting calculations, normal mapping, and other visual effects. Here is an example of how this function might be used in a shader:\n\n```\nattribute vec3 aPosition;\nattribute vec3 aNormal;\nattribute vec2 aTexCoord;\n\nvarying vec2 vTexCoord;\nvarying vec3 vTangent;\nvarying vec3 vBinormal;\nvarying vec3 vNormal;\n\nuniform mat4 uModelMatrix;\nuniform mat4 uViewMatrix;\nuniform mat4 uProjectionMatrix;\n\nvoid main() {\n    // Transform position and normal to world space\n    vec4 worldPosition = uModelMatrix * vec4(aPosition, 1.0);\n    vec3 worldNormal = normalize(mat3(uModelMatrix) * aNormal);\n\n    // Calculate tangent, binormal, and normal vectors\n    vec3 tangent;\n    vec3 binormal;\n    getTBN(tangent, binormal, worldNormal);\n\n    // Transform tangent and binormal to world space\n    tangent = normalize(mat3(uModelMatrix) * tangent);\n    binormal = normalize(mat3(uModelMatrix) * binormal);\n\n    // Pass variables to fragment shader\n    vTexCoord = aTexCoord;\n    vTangent = tangent;\n    vBinormal = binormal;\n    vNormal = worldNormal;\n\n    // Transform position to clip space\n    gl_Position = uProjectionMatrix * uViewMatrix * worldPosition;\n}\n```\n## Questions: \n 1. What is the purpose of this code and where is it used in the PlayCanvas engine?\n- This code defines a function called `getTBN` which calculates the tangent, binormal, and normal vectors for a given surface. It is likely used in the rendering pipeline of the PlayCanvas engine to generate the necessary vectors for lighting and shading calculations.\n\n2. What are the input parameters for the `getTBN` function?\n- The `getTBN` function takes in three vec3 parameters: `tangent`, `binormal`, and `normal`. These represent the tangent, binormal, and normal vectors of the surface being rendered.\n\n3. What is the purpose of the conditional statement in the middle of the function?\n- The conditional statement checks if the `B` vector is zero, which can happen if the `vObjectSpaceUpW` vector is parallel to the surface normal. If this is the case, the function calculates a new `B` vector based on the component of the normal vector with the largest magnitude, and recalculates the `T` vector accordingly. This ensures that the `T`, `B`, and `N` vectors are always orthogonal and correctly represent the surface geometry.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/TBNObjectSpace.md"}}],["565",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/TBNderivative.js)\n\nThe code provided is a GLSL shader code that calculates the tangent, binormal, and normal vectors of a pixel on a 3D model's surface. These vectors are used to create a TBN (tangent, binormal, normal) matrix that is used to transform the surface normal from tangent space to world space. This transformation is necessary for lighting calculations in 3D graphics.\n\nThe code starts by defining a uniform variable `tbnBasis` which is used to scale the T and B vectors. The `getTBN` function takes in three vectors: `tangent`, `binormal`, and `normal`. These vectors are calculated using the `dFdx` and `dFdy` functions which return the partial derivatives of the vertex position and UV coordinates. The `cross` function is used to calculate the perpendicular vectors `dp2perp` and `dp1perp` which are then used to calculate the T and B vectors. Finally, the `max` function is used to calculate the denominator of the scale-invariant frame and the `invmax` variable is used to scale the T and B vectors. The `mat3` function is used to construct the TBN matrix.\n\nThis code is used in the PlayCanvas engine to calculate the TBN matrix for each pixel on a 3D model's surface. This matrix is then used to transform the surface normal from tangent space to world space. This transformation is necessary for lighting calculations in 3D graphics. The TBN matrix is also used for other effects such as normal mapping and parallax mapping. \n\nExample usage of this code in PlayCanvas engine:\n\n```javascript\n// create a material for the 3D model\nvar material = new pc.StandardMaterial();\n\n// set the shader code for the material\nmaterial.chunks.transformVS = `\n    uniform float tbnBasis;\n    attribute vec3 vertex_position;\n    attribute vec3 vertex_normal;\n    attribute vec2 vertex_texCoord0;\n    varying vec3 vPositionW;\n    varying vec2 $UV;\n    varying mat3 dTBN;\n\n    void main(void) {\n        gl_Position = getPosition();\n        vPositionW = (getWorldMatrix() * vec4(vertex_position, 1.0)).xyz;\n        $UV = vertex_texCoord0;\n        getTBN(vertex_normal, tangent, binormal, normal);\n        gl_Position = getMVP();\n    }\n`;\n\n// set the TBN basis for the material\nmaterial.setParameter('tbnBasis', 1.0);\n\n// set the material on the 3D model\nmodel.meshInstances[0].material = material;\n```\n## Questions: \n 1. What is the purpose of the `getTBN` function?\n- The `getTBN` function is used to calculate the tangent, binormal, and normal vectors of a pixel triangle.\n\n2. What is the significance of the `tbnBasis` uniform float?\n- The `tbnBasis` uniform float is used to scale the tangent and binormal vectors to ensure they are of equal length.\n\n3. What is the source of the `dFdx` and `dFdy` functions used in the code?\n- The `dFdx` and `dFdy` functions are likely part of the GLSL shading language and are used to calculate the partial derivative of a value with respect to the screen-space x and y coordinates.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/TBNderivative.md"}}],["566",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/TBNfast.js)\n\nThe code above is a GLSL shader function that calculates the TBN matrix (tangent, binormal, normal) for a given set of vertex attributes. The TBN matrix is used in 3D graphics to transform normal vectors from object space to tangent space, which is necessary for certain lighting and shading techniques.\n\nThe function takes in three vec3 parameters: tangent, binormal, and normal. These vectors represent the tangent, binormal, and normal vectors of a vertex in object space. The function then constructs a 3x3 matrix called dTBN using these vectors. This matrix is the TBN matrix for the vertex.\n\nThe TBN matrix is used in various lighting and shading techniques, such as normal mapping and parallax mapping. These techniques require the normal vectors of a mesh to be transformed from object space to tangent space, so that they can be used to calculate lighting and shading effects that are dependent on the orientation of the surface.\n\nHere is an example of how this function might be used in a larger project:\n\n```glsl\n// Vertex shader\nattribute vec3 a_position;\nattribute vec3 a_normal;\nattribute vec2 a_uv;\nattribute vec3 a_tangent;\nattribute vec3 a_binormal;\n\nuniform mat4 u_modelMatrix;\nuniform mat4 u_viewMatrix;\nuniform mat4 u_projectionMatrix;\n\nvarying vec2 v_uv;\nvarying vec3 v_tangent;\nvarying vec3 v_binormal;\nvarying vec3 v_normal;\n\nvoid main() {\n    // Transform vertex attributes to world space\n    vec4 worldPosition = u_modelMatrix * vec4(a_position, 1.0);\n    vec3 worldNormal = normalize(mat3(u_modelMatrix) * a_normal);\n    vec3 worldTangent = normalize(mat3(u_modelMatrix) * a_tangent);\n    vec3 worldBinormal = normalize(mat3(u_modelMatrix) * a_binormal);\n\n    // Transform vertex attributes to view space\n    vec4 viewPosition = u_viewMatrix * worldPosition;\n    vec3 viewNormal = normalize(mat3(u_viewMatrix) * worldNormal);\n    vec3 viewTangent = normalize(mat3(u_viewMatrix) * worldTangent);\n    vec3 viewBinormal = normalize(mat3(u_viewMatrix) * worldBinormal);\n\n    // Calculate TBN matrix\n    getTBN(viewTangent, viewBinormal, viewNormal);\n\n    // Pass vertex attributes to fragment shader\n    v_uv = a_uv;\n    v_tangent = viewTangent;\n    v_binormal = viewBinormal;\n    v_normal = viewNormal;\n\n    // Transform vertex position to clip space\n    gl_Position = u_projectionMatrix * viewPosition;\n}\n```\n\nIn this example, the vertex shader takes in various vertex attributes, including the tangent, binormal, and normal vectors. It then transforms these attributes to world space and view space, and calculates the TBN matrix using the getTBN function. The TBN matrix is then passed to the fragment shader, where it is used to transform normal vectors from object space to tangent space for lighting and shading calculations.\n\nOverall, the getTBN function is a crucial part of many advanced lighting and shading techniques in 3D graphics, and is used extensively in the PlayCanvas engine.\n## Questions: \n 1. What does the `/* glsl */` comment indicate in the code?\n   - The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a high-level shading language used for graphics programming.\n\n2. What is the purpose of the `getTBN` function?\n   - The `getTBN` function is used to calculate the TBN (tangent, binormal, normal) matrix from the given tangent, binormal, and normal vectors.\n\n3. What is the data type of the `dTBN` variable?\n   - The data type of the `dTBN` variable is not specified in the code snippet, so it is unclear what type it is. It could be a global variable or a parameter passed in from another function.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/TBNfast.md"}}],["567",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/ambientConstant.js)\n\nThe code above is a GLSL shader function that adds ambient lighting to a 3D scene. The function takes in a world normal vector as a parameter and updates the diffuse light value by adding the global ambient light value. \n\nIn a 3D scene, ambient lighting is the light that is present in the environment and is not coming from any specific light source. This function allows for the addition of ambient lighting to a scene by updating the diffuse light value. \n\nThis code can be used in the larger PlayCanvas engine project to create more realistic and dynamic lighting in 3D scenes. For example, if a scene is set in a dark room, the ambient lighting can be set to a low value to create a more realistic and moody atmosphere. On the other hand, if a scene is set in a bright outdoor environment, the ambient lighting can be set to a higher value to simulate the natural light present in the environment. \n\nHere is an example of how this function can be used in a shader:\n\n```\nuniform vec3 worldNormal;\nuniform vec3 light_globalAmbient;\n\nvoid main() {\n    vec3 dDiffuseLight = vec3(0.0);\n    addAmbient(worldNormal);\n    // other lighting calculations\n    // ...\n    gl_FragColor = vec4(dDiffuseLight, 1.0);\n}\n```\n\nIn this example, the `addAmbient` function is called with the `worldNormal` vector and the `light_globalAmbient` value is added to the `dDiffuseLight` value. This updated `dDiffuseLight` value is then used in other lighting calculations to determine the final color of the fragment. \n\nOverall, this GLSL shader function is a useful tool for adding ambient lighting to 3D scenes in the PlayCanvas engine project.\n## Questions: \n 1. What does the `/* glsl */` comment indicate in the code?\n   - The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax.\n\n2. What is the purpose of the `addAmbient` function?\n   - The `addAmbient` function adds the global ambient light to the diffuse light value.\n\n3. What is the data type of the `worldNormal` parameter in the `addAmbient` function?\n   - The `worldNormal` parameter is of type `vec3`, which represents a 3-component vector in GLSL.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/ambientConstant.md"}}],["568",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/ambientEnv.js)\n\nThe code above is a GLSL shader code that is used to add ambient lighting to a 3D scene. The code is a part of the PlayCanvas engine project and is used to render 3D graphics in real-time. \n\nThe code starts with an export statement that exports the code as a default module. The code defines a uniform sampler2D variable called texture_envAtlas that is used to sample the environment map. The environment map is a texture that is used to simulate the reflection of the environment on the surface of the 3D object. \n\nThe addAmbient function takes a vec3 worldNormal as an input parameter. The worldNormal is the normal vector of the surface of the 3D object. The function first rotates the worldNormal vector using the cubeMapRotate function. The cubeMapRotate function is used to rotate the normal vector to the orientation of the environment map. The function then calculates the direction vector of the reflected light using the rotated normal vector. \n\nThe function then calculates the spherical coordinates of the direction vector using the toSphericalUv function. The toSphericalUv function is used to convert the direction vector to spherical coordinates. The function then maps the spherical coordinates to the texture coordinates of the environment map using the mapUv function. The mapUv function is used to map the spherical coordinates to the texture coordinates of the environment map. \n\nThe function then samples the environment map using the texture2D function and the texture coordinates calculated in the previous step. The sampled color is then decoded using the $DECODE function. The $DECODE function is a built-in function in the PlayCanvas engine that is used to decode the color value from the texture. The decoded color value is then passed to the processEnvironment function. The processEnvironment function is a built-in function in the PlayCanvas engine that is used to process the environment map and calculate the diffuse lighting. \n\nIn summary, the code above is a GLSL shader code that is used to add ambient lighting to a 3D scene. The code uses an environment map to simulate the reflection of the environment on the surface of the 3D object. The code calculates the direction vector of the reflected light and maps it to the texture coordinates of the environment map. The code then samples the environment map and decodes the color value. The decoded color value is then passed to the processEnvironment function to calculate the diffuse lighting.\n## Questions: \n 1. What is the purpose of the `addAmbient` function?\n    \n    The `addAmbient` function is used to add ambient lighting to a scene based on a given world normal.\n\n2. What is the significance of the `texture_envAtlas` uniform and how is it used in the code?\n    \n    The `texture_envAtlas` uniform is a sampler2D used to sample a texture atlas containing environment maps. It is used to retrieve the environment map for a given direction in the `addAmbient` function.\n\n3. What is the purpose of the `mapUv` function and how is it used in the code?\n    \n    The `mapUv` function is used to map a direction vector to a UV coordinate in the environment map atlas. It takes in a spherical UV coordinate and a vector representing the size of the atlas and returns the corresponding UV coordinate in the atlas. This UV coordinate is then used to sample the environment map in the `addAmbient` function.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/ambientEnv.md"}}],["569",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/ambientSH.js)\n\nThis code is a shader function that calculates ambient lighting for a 3D scene using spherical harmonics. The purpose of this code is to add ambient lighting to a scene by calculating the color of the ambient light based on the orientation of the surface normal at each point in the scene. \n\nThe `ambientSH` uniform is an array of 9 vec3 values that represent the coefficients of the spherical harmonics used to calculate the ambient lighting. These coefficients are precomputed and stored in the `ambientSH` array, and are used to calculate the color of the ambient light at each point in the scene.\n\nThe `addAmbient` function takes a `worldNormal` vec3 as input, which represents the surface normal of the point in the scene where the ambient lighting is being calculated. The `cubeMapRotate` function is called to rotate the surface normal to the appropriate orientation for the spherical harmonics calculation.\n\nThe color of the ambient light is then calculated using the `ambientSH` coefficients and the rotated surface normal. The resulting color is added to the `dDiffuseLight` variable, which is used to calculate the final color of the scene.\n\nThis code is an important part of the PlayCanvas engine, as it provides a way to add realistic ambient lighting to 3D scenes. It is used in conjunction with other shaders and lighting techniques to create a visually appealing and realistic scene. \n\nExample usage of this code would be to include it in a shader program for a 3D model in a game or simulation. The `ambientSH` coefficients would be precomputed and passed to the shader as a uniform variable. The `addAmbient` function would be called for each point in the scene where ambient lighting is needed, and the resulting color would be added to the `dDiffuseLight` variable to calculate the final color of the scene.\n## Questions: \n 1. What is the purpose of the `ambientSH` uniform variable?\n- The `ambientSH` uniform variable is used to store the coefficients of the ambient lighting in a spherical harmonics basis.\n\n2. What does the `addAmbient` function do?\n- The `addAmbient` function calculates the ambient lighting contribution based on the world normal and the coefficients stored in `ambientSH`, and adds it to the `dDiffuseLight` variable after processing it with the `processEnvironment` function.\n\n3. What is the significance of the `/* glsl */` comment at the beginning of the code?\n- The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax, which is a C-like language used to write shaders for graphics processing units (GPUs).","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/ambientSH.md"}}],["570",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/aoDiffuseOcc.js)\n\nThe code above is a shader function written in GLSL (OpenGL Shading Language) that is used to occlude diffuse lighting. The function takes in a single parameter, `ao`, which stands for ambient occlusion. Ambient occlusion is a shading technique used to simulate the soft shadows that occur when objects are close to each other. \n\nThe purpose of this function is to modify the diffuse lighting of a 3D object by multiplying it with the ambient occlusion value. This results in a darker appearance of the object in areas where ambient occlusion is high, and a brighter appearance in areas where it is low. \n\nThis function is likely used in the larger PlayCanvas engine project to enhance the realism of 3D scenes by simulating the way light interacts with objects in the real world. It can be used in conjunction with other shader functions to create complex lighting effects. \n\nHere is an example of how this function could be used in a shader:\n\n```\nuniform float ao; // ambient occlusion value\nvarying vec3 vDiffuseLight; // diffuse lighting value\n\nvoid main() {\n    occludeDiffuse(ao); // call the occludeDiffuse function\n    gl_FragColor = vec4(vDiffuseLight, 1.0); // set the fragment color to the modified diffuse lighting value\n}\n```\n\nIn this example, the `occludeDiffuse` function is called with the `ao` value passed in as a uniform variable. The resulting modified diffuse lighting value is then used to set the fragment color. \n\nOverall, this function is a useful tool for creating realistic lighting effects in 3D scenes and is likely an important part of the PlayCanvas engine's shader library.\n## Questions: \n 1. What does the `/* glsl */` comment indicate in this code?\n   - The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a high-level shading language used for rendering graphics in OpenGL and WebGL.\n\n2. What does the `occludeDiffuse` function do?\n   - The `occludeDiffuse` function takes in a float value `ao` and multiplies it with the `dDiffuseLight` variable, which is likely used to adjust the diffuse lighting in a 3D scene based on the ambient occlusion value.\n\n3. What is the purpose of this code within the PlayCanvas engine?\n   - Without additional context, it is difficult to determine the specific purpose of this code within the PlayCanvas engine. However, it appears to be a GLSL shader function that could be used for rendering graphics in a 3D scene.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/aoDiffuseOcc.md"}}],["571",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/aoSpecOcc.js)\n\nThis code is a GLSL shader function that calculates the specular occlusion of a material. The function takes in several parameters, including the glossiness of the material, the ambient occlusion (AO) value, the world normal vector, and the view direction vector. \n\nThe first step of the function is to calculate the specular power using the glossiness value. This is done by raising 2 to the power of the glossiness multiplied by 11.0. The result is used to approximate the specular occlusion of the material.\n\nNext, the function calculates the specular occlusion using the world normal and view direction vectors, as well as the AO value. This is done by taking the dot product of the world normal and view direction vectors, adding the AO value, and raising the result to the power of 0.01 times the specular power. The result is then saturated and subtracted from 1.0 plus the AO value. The final result is then mixed with 1.0 using the material_occludeSpecularIntensity uniform value.\n\nFinally, the function applies the specular occlusion to the dSpecularLight and dReflection variables, which are used for directional light and reflection calculations, respectively. If the LIT_SHEEN flag is defined, the function also applies the specular occlusion to the sSpecularLight and sReflection variables, which are used for sheen calculations.\n\nOverall, this function is an important part of the PlayCanvas engine's shader system, as it allows for accurate and efficient calculation of specular occlusion in materials. It can be used in conjunction with other shader functions to create realistic lighting and shading effects in 3D scenes. \n\nExample usage:\n\n```glsl\nuniform float glossiness;\nuniform float ambientOcclusion;\nuniform vec3 worldNormal;\nuniform vec3 viewDirection;\n\nvoid main() {\n  occludeSpecular(glossiness, ambientOcclusion, worldNormal, viewDirection);\n}\n```\n## Questions: \n 1. What does the `material_occludeSpecularIntensity` uniform do?\n    \n    The `material_occludeSpecularIntensity` uniform controls the intensity of the specular occlusion effect applied to the material.\n\n2. What is the purpose of the `occludeSpecular` function?\n    \n    The `occludeSpecular` function calculates an approximated specular occlusion value based on the gloss, ambient occlusion, world normal, and view direction of a material. It then applies this value to the diffuse and specular light and reflection calculations.\n\n3. What is the source of the formula used in the `occludeSpecular` function?\n    \n    The formula used in the `occludeSpecular` function is based on a presentation by Tri-Ace on real-time physically based rendering implementation.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/aoSpecOcc.md"}}],["572",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/aoSpecOccConst.js)\n\nThe code provided is a GLSL shader function called `occludeSpecular`. This function is used to calculate the specular occlusion of a material based on its glossiness, ambient occlusion (AO), world normal, and view direction. \n\nThe function first calculates the specular power using the glossiness value passed in as a parameter. It does this by raising 2 to the power of the glossiness multiplied by 11. This value is then used to calculate the specular occlusion, which is a measure of how much the specular reflection is occluded by the surrounding environment. \n\nThe specular occlusion is calculated using the formula from a presentation by Tri-Ace, which takes into account the dot product of the world normal and view direction, as well as the ambient occlusion value. The result is then clamped between 0 and 1 using the `saturate` function. \n\nFinally, the specular occlusion value is used to modify the diffuse specular light and reflection values. If the `LIT_SHEEN` preprocessor macro is defined, it also modifies the sheen specular light and reflection values. \n\nThis function is likely used in the larger PlayCanvas engine project to calculate the lighting and reflections of materials in a scene. It is specifically used to calculate the specular occlusion, which is an important factor in determining the appearance of shiny or reflective materials. \n\nHere is an example of how this function might be used in a shader:\n\n```\nuniform float u_gloss;\nuniform float u_ao;\nvarying vec3 v_normal;\nvarying vec3 v_viewDir;\n\nvoid main() {\n  // calculate specular occlusion\n  occludeSpecular(u_gloss, u_ao, v_normal, v_viewDir);\n\n  // calculate final color using modified specular light and reflection values\n  vec3 color = dAlbedo * (dDiffuseLight + dSpecularLight) + dReflection;\n  gl_FragColor = vec4(color, 1.0);\n}\n```\n## Questions: \n 1. What does this code do?\n   \n   This code defines a function called `occludeSpecular` that calculates an approximated specular occlusion from ambient occlusion (AO) and modifies some variables related to specular lighting and reflection.\n\n2. What is the purpose of the `exp2` and `pow` functions used in this code?\n   \n   The `exp2` function is used to calculate the specular power based on the gloss value, and the `pow` function is used to calculate the specular occlusion based on the dot product of the world normal and view direction vectors, the AO value, and the specular power.\n\n3. What is the significance of the `saturate` function used in this code?\n   \n   The `saturate` function is used to clamp the specular occlusion value between 0 and 1, ensuring that it does not exceed the valid range.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/aoSpecOccConst.md"}}],["573",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/aoSpecOccConstSimple.js)\n\nThe code above is a function called `occludeSpecular` that is written in GLSL (OpenGL Shading Language). This function is used to calculate the occlusion of specular lighting in a 3D scene. \n\nThe function takes four parameters: `gloss`, `ao`, `worldNormal`, and `viewDir`. `gloss` is a float value that represents the glossiness of the material. `ao` is a float value that represents the ambient occlusion of the material. `worldNormal` is a vec3 value that represents the normal vector of the surface in world space. `viewDir` is a vec3 value that represents the direction of the view in world space.\n\nThe function first multiplies the diffuse specular light and reflection by the ambient occlusion value. This is done to simulate the effect of light being blocked by nearby objects or surfaces. \n\n#ifdef LIT_SHEEN is a preprocessor directive that checks if the LIT_SHEEN flag is defined. If it is defined, then the function also multiplies the specular light and reflection of the sheen by the ambient occlusion value. This is done to simulate the effect of light being reflected off of a surface with a sheen.\n\nThis function is likely used in the larger PlayCanvas engine project to calculate the lighting and shading of 3D objects in a scene. It is specifically used to calculate the occlusion of specular lighting, which is an important aspect of realistic lighting in 3D graphics. \n\nHere is an example of how this function might be used in a shader:\n\n```\nvoid main() {\n    // calculate lighting and shading\n    occludeSpecular(gloss, ao, worldNormal, viewDir);\n    // output final color\n    gl_FragColor = vec4(color, 1.0);\n}\n```\n\nIn this example, the `occludeSpecular` function is called to calculate the occlusion of specular lighting before the final color is outputted.\n## Questions: \n 1. What does the `occludeSpecular` function do?\n- The `occludeSpecular` function takes in parameters for gloss, ambient occlusion, world normal, and view direction, and modifies the specular and reflection values of the directional and sheen lights based on the ambient occlusion value.\n\n2. What is the purpose of the `glsl` tag before the template literal?\n- The `glsl` tag indicates that the template literal contains GLSL code, which is a shading language used for graphics programming.\n\n3. What is the significance of the `#ifdef LIT_SHEEN` preprocessor directive?\n- The `#ifdef LIT_SHEEN` directive checks if the `LIT_SHEEN` macro is defined, and if it is, it includes the code block that modifies the specular and reflection values of the sheen light. If the macro is not defined, the code block is excluded.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/aoSpecOccConstSimple.md"}}],["574",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/aoSpecOccSimple.js)\n\nThe code above is a GLSL shader function that is used to calculate the occlusion of specular highlights on a material. The function takes in four parameters: gloss, ao, worldNormal, and viewDir. \n\nThe gloss parameter is a float value that represents the glossiness of the material. The ao parameter is also a float value that represents the ambient occlusion of the material. The worldNormal parameter is a vec3 value that represents the normal vector of the surface in world space. The viewDir parameter is also a vec3 value that represents the direction of the camera in world space.\n\nThe function calculates the specular occlusion by mixing the ambient occlusion value with a material-specific occlusion intensity value. This is done using the mix() function, which takes in three parameters: a, b, and c. The function returns a value that is a linear interpolation between a and b, based on the value of c. In this case, the value of c is the material_occludeSpecularIntensity uniform, which is a float value that is set externally.\n\nThe specular occlusion value is then used to modify the dSpecularLight and dReflection values, which are used to calculate the diffuse and reflection components of the material's lighting. If the LIT_SHEEN flag is defined, the sSpecularLight and sReflection values are also modified.\n\nOverall, this function is used to calculate the occlusion of specular highlights on a material, which is an important aspect of realistic lighting in 3D graphics. It is likely used in conjunction with other shader functions to create a complete material shader for the PlayCanvas engine. \n\nExample usage:\n\n```glsl\nuniform float gloss;\nuniform float ao;\nuniform vec3 worldNormal;\nuniform vec3 viewDir;\n\nvoid main() {\n  occludeSpecular(gloss, ao, worldNormal, viewDir);\n  // other shader calculations\n}\n```\n## Questions: \n 1. **What is the purpose of this code?** \nThis code defines a function called `occludeSpecular` that adjusts the specular lighting and reflection based on the glossiness, ambient occlusion, and a uniform called `material_occludeSpecularIntensity`.\n\n2. **What is the data type of the `worldNormal` and `viewDir` parameters?** \nThe `worldNormal` and `viewDir` parameters are both of type `vec3`, which represents a 3-component vector in 3D space.\n\n3. **What is the significance of the `#ifdef LIT_SHEEN` preprocessor directive?** \nThe `#ifdef LIT_SHEEN` directive checks if a macro called `LIT_SHEEN` has been defined, and if so, it includes the code block that adjusts the sheen specular lighting and reflection. If `LIT_SHEEN` is not defined, that code block is excluded from the function.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/aoSpecOccSimple.md"}}],["575",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/base.js)\n\nThe code above is a GLSL shader code that defines three functions: `square()`, `saturate()` and `saturate(vec3 x)`. The purpose of this code is to provide utility functions that can be used in other shaders within the PlayCanvas engine project.\n\nThe `square()` function takes a single float parameter and returns the square of that parameter. This function can be used to calculate the square of a value in other shaders. For example, if we want to calculate the distance between two points in a 3D space, we can use the `square()` function to calculate the square of the difference between the x, y, and z coordinates of the two points.\n\nThe `saturate()` function takes a single float parameter and returns a value between 0.0 and 1.0. If the input value is less than 0.0, the function returns 0.0. If the input value is greater than 1.0, the function returns 1.0. Otherwise, the function returns the input value. This function can be used to clamp a value between 0.0 and 1.0 in other shaders. For example, if we want to ensure that a color value is within the range of 0.0 to 1.0, we can use the `saturate()` function to clamp the color value.\n\nThe `saturate(vec3 x)` function takes a vec3 parameter and returns a vec3 value with each component clamped between 0.0 and 1.0. This function can be used to clamp a color vector between 0.0 and 1.0 in other shaders. For example, if we want to ensure that the red, green, and blue components of a color vector are within the range of 0.0 to 1.0, we can use the `saturate(vec3 x)` function to clamp the color vector.\n\nOverall, this code provides useful utility functions that can be used in other shaders within the PlayCanvas engine project. By providing these functions, the code helps to simplify the development of shaders and ensure that they are consistent across the project.\n## Questions: \n 1. What is the purpose of the `view_position` and `light_globalAmbient` uniforms?\n- `view_position` is likely used to determine the position of the camera/view in the scene, while `light_globalAmbient` may be used to set the overall ambient lighting in the scene.\n\n2. What is the purpose of the `square` and `saturate` functions?\n- `square` is a simple function that returns the square of a given number. `saturate` is a function that clamps a given value between 0 and 1, ensuring that it stays within a certain range.\n\n3. What is the expected input and output of the `saturate` function that takes a `vec3` parameter?\n- The `saturate` function that takes a `vec3` parameter is likely used to clamp the values of a color vector between 0 and 1, ensuring that the color stays within a certain range. The expected input is a `vec3` color vector, and the output is also a `vec3` color vector with each component clamped between 0 and 1.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/base.md"}}],["576",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/baseNineSliced.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) used in the PlayCanvas engine project. The purpose of this code is to define a nine-sliced texture that can be used to create scalable UI elements such as buttons, panels, and windows. \n\nThe `#define NINESLICED` statement at the beginning of the code indicates that this shader code is specifically designed for nine-sliced textures. Nine-sliced textures are textures that can be divided into nine sections, where the four corners remain unchanged, the four edges are stretched or tiled, and the center is repeated or stretched to fill the remaining space. This technique is commonly used in UI design to create scalable elements that can adapt to different screen sizes and resolutions.\n\nThe `varying` statements define two variables that will be interpolated across the vertices of the mesh: `vMask` and `vTiledUv`. These variables will be used to mask the texture and tile it across the mesh.\n\nThe `uniform` statements define four variables that will be passed to the shader from the application: `innerOffset`, `outerScale`, and `atlasRect`. `innerOffset` is a vector that defines the offset of the inner rectangle of the nine-sliced texture. `outerScale` is a vector that defines the scale of the outer edges of the nine-sliced texture. `atlasRect` is a vector that defines the position and size of the texture in the texture atlas.\n\nThe `vec2` statement defines a variable `nineSlicedUv` that will be used to calculate the UV coordinates of the nine-sliced texture.\n\nOverall, this code is an essential part of the PlayCanvas engine project as it enables the creation of scalable UI elements using nine-sliced textures. Developers can use this code to create custom shaders for their UI elements and pass the necessary parameters to the shader to achieve the desired effect. \n\nExample usage of this code in a PlayCanvas project:\n\n```javascript\n// Create a new material for a UI element\nvar material = new pc.StandardMaterial();\n\n// Set the shader code for the material\nmaterial.chunks.base = `\n    #define NINESLICED\n    varying vec2 vMask;\n    varying vec2 vTiledUv;\n    uniform mediump vec4 innerOffset;\n    uniform mediump vec2 outerScale;\n    uniform mediump vec4 atlasRect;\n    vec2 nineSlicedUv;\n    // custom shader code goes here\n`;\n\n// Set the necessary parameters for the shader\nmaterial.setParameter('innerOffset', new pc.Vec4(0.1, 0.1, 0.1, 0.1));\nmaterial.setParameter('outerScale', new pc.Vec2(0.2, 0.2));\nmaterial.setParameter('atlasRect', new pc.Vec4(0, 0, 256, 256));\n\n// Assign the material to a UI element\nuiElement.element.material = material;\n```\n## Questions: \n 1. What is the purpose of the `#define NINESLICED` statement at the beginning of the code?\n- The `#define NINESLICED` statement is a preprocessor directive that indicates the use of nine-slice scaling in the shader.\n\n2. What do the `innerOffset`, `outerScale`, and `atlasRect` uniforms represent?\n- `innerOffset` is a vector that specifies the offset of the inner region of the nine-slice scaling. `outerScale` is a vector that specifies the scale of the outer regions. `atlasRect` is a vector that specifies the texture atlas rectangle.\n\n3. How is the `nineSlicedUv` variable used in the shader?\n- The `nineSlicedUv` variable is not used in this code snippet. It is likely used in other parts of the shader to calculate the UV coordinates for the nine-slice scaling.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/baseNineSliced.md"}}],["577",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/baseNineSlicedTiled.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) for the PlayCanvas engine. The purpose of this code is to define certain variables and macros that will be used in rendering a nine-sliced and tiled texture. \n\nThe `#define` statements are used to define two macros, `NINESLICED` and `NINESLICETILED`, which will be used to enable the nine-sliced and tiled texture rendering modes respectively. \n\nThe `varying` keyword is used to declare two variables, `vMask` and `vTiledUv`, which will be used to pass data from the vertex shader to the fragment shader. \n\nThe `uniform` keyword is used to declare three variables, `innerOffset`, `outerScale`, and `atlasRect`, which will be used to pass data from the application to the shader. `innerOffset` is a vector of four medium-precision floating-point values that specifies the offset of the inner part of the nine-sliced texture. `outerScale` is a vector of two medium-precision floating-point values that specifies the scale of the outer part of the nine-sliced texture. `atlasRect` is a vector of four medium-precision floating-point values that specifies the texture atlas rectangle that contains the nine-sliced texture. \n\nThe `vec2` keyword is used to declare a variable `nineSlicedUv`, which will be used to store the UV coordinates of the nine-sliced texture. \n\nOverall, this code sets up the necessary variables and macros for rendering a nine-sliced and tiled texture in the PlayCanvas engine. An example of how this code may be used in the larger project is in the rendering of UI elements such as buttons and panels, where nine-sliced textures are commonly used to ensure that the texture scales properly without distorting the corners.\n## Questions: \n 1. What is the purpose of the `NINESLICED` and `NINESLICETILED` defines?\n   - These defines are likely used to enable or disable nine-slicing and tiling functionality in the shader.\n2. What do the `innerOffset`, `outerScale`, and `atlasRect` uniforms represent?\n   - `innerOffset` likely represents the offset of the inner content within a nine-sliced sprite. `outerScale` may represent the scaling of the outer edges of a nine-sliced sprite. `atlasRect` may represent the texture atlas coordinates of the sprite.\n3. How is the `nineSlicedUv` variable used in the shader?\n   - It is unclear from this code snippet how `nineSlicedUv` is used in the shader. Further context would be needed to answer this question.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/baseNineSlicedTiled.md"}}],["578",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/biasConst.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) and it defines a function called `getShadowBias`. The purpose of this function is to calculate the shadow bias value for a given resolution and maximum bias. \n\nIn computer graphics, shadow bias is a technique used to prevent shadow acne, which is a visual artifact that occurs when the shadow map resolution is not high enough or when the shadow map is too close to the surface it is casting shadows on. Shadow bias is added to the depth value of the shadow map to offset it from the surface, thus preventing the shadow acne. \n\nThe `getShadowBias` function takes two parameters: `resolution` and `maxBias`. The `resolution` parameter is the resolution of the shadow map, and the `maxBias` parameter is the maximum bias value that can be used. The function simply returns the `maxBias` value, which means that it does not actually calculate the shadow bias based on the resolution. This is likely a placeholder function that can be replaced with a more sophisticated implementation in the future.\n\nThis code is likely used in the PlayCanvas engine to generate shaders for rendering 3D scenes with shadows. The `getShadowBias` function can be called from other shader code to calculate the shadow bias value for a given resolution and maximum bias. For example, the following code snippet shows how the `getShadowBias` function can be used in a shader:\n\n```\nuniform float shadowMapResolution;\nuniform float shadowMaxBias;\n\nvoid main() {\n    float shadowBias = getShadowBias(shadowMapResolution, shadowMaxBias);\n    // use the shadowBias value to offset the depth value of the shadow map\n    // and prevent shadow acne\n}\n```\n\nOverall, this code is a small but important part of the PlayCanvas engine that helps to improve the visual quality of rendered shadows in 3D scenes.\n## Questions: \n 1. What is the purpose of the `#define SHADOWBIAS` line?\n   - This line is defining a preprocessor macro called `SHADOWBIAS`, which may be used elsewhere in the code to enable/disable certain features or behaviors related to shadow rendering.\n\n2. What is the expected input and output of the `getShadowBias` function?\n   - The `getShadowBias` function takes in two float parameters: `resolution` and `maxBias`. It then returns the value of `maxBias`, which suggests that this function is used to calculate or retrieve a maximum value for a shadow bias parameter.\n\n3. What is the significance of the `/* glsl */` comment at the beginning of the code?\n   - This comment indicates that the code is written in the GLSL (OpenGL Shading Language) syntax, which is used to write shaders for graphics rendering. The comment may be used by tools or editors to provide syntax highlighting or other features specific to GLSL.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/biasConst.md"}}],["579",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/blurVSM.js)\n\nThe code above is a GLSL shader that is used in the PlayCanvas engine project. The shader is responsible for computing the moments of a texture, which can be used for various post-processing effects such as blurring or bloom. \n\nThe shader takes in a texture called \"source\" and a pixel offset value. The pixel offset value is used to sample the texture at different locations to compute the moments. The number of samples is determined by the \"SAMPLES\" constant, which is not defined in this code snippet but is likely defined elsewhere in the project. \n\nThe moments are computed by iterating over each sample and accumulating the color values of the texture at that sample location. If the \"GAUSS\" preprocessor constant is defined, the color values are multiplied by a weight value that is also defined elsewhere in the project. If \"GAUSS\" is not defined, the color values are simply added together. \n\nAfter all the samples have been processed, the moments are divided by the number of samples if \"GAUSS\" is not defined. Finally, the moments are outputted as a vec4 color value. If the \"PACKED\" preprocessor constant is defined, the moments are encoded as two float values and packed into the xy and zw components of the output color value. Otherwise, the moments are outputted as separate x, y, and z components of the color value. \n\nOverall, this shader is a crucial component of the PlayCanvas engine's post-processing pipeline. It allows for the computation of moments that can be used for various visual effects, and the ability to pack the moments into a single color value allows for efficient memory usage. Below is an example of how this shader might be used in a post-processing effect:\n\n```javascript\nconst shader = new pc.Shader(device, {\n    attributes: {...},\n    vshader: {...},\n    fshader: /* glsl */`\n        // include the code from the shader above\n        void main() {\n            // set the uniforms\n            ...\n            // render the texture to a render target\n            ...\n            // apply the post-processing effect using the moments computed by the shader\n            ...\n        }\n    `\n});\n```\n## Questions: \n 1. What is the purpose of the `SAMPLES` variable used in the code?\n- `SAMPLES` is used to determine the number of samples to take in the loop that calculates the `moments` vector.\n\n2. What is the difference between the `GAUSS` and `PACKED` preprocessor directives?\n- `GAUSS` is used to apply a Gaussian filter to the texture, while `PACKED` is used to encode and decode floating point values into a two-component vector.\n\n3. What is the output of this code?\n- The output of this code is a `vec4` color value that represents the `moments` vector, either as separate `x`, `y`, and `z` components or as encoded `x` and `y` values in the case of `PACKED`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/blurVSM.md"}}],["580",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/clusteredLightCookies.js)\n\nThe code provided is a set of functions that are used to retrieve a cookie texture for use in clustered lighting. A cookie texture is a texture that is used to modify the light emitted by a light source. The texture is applied to the light source and the resulting light is then projected onto the scene. The functions in this code are used to retrieve the cookie texture and apply it to the light source.\n\nThe first function, `_getCookieClustered`, takes in a texture, a UV coordinate, an intensity value, a boolean value indicating whether the texture is RGB or not, and a cookie channel. The function returns a vec3 value that represents the modified light. The function first retrieves the pixel value from the texture at the given UV coordinate. The intensity value is then used to mix the pixel value with a vec4 value of 1.0. The resulting pixel value is then either returned as an RGB value or as a dot product of the pixel value and the cookie channel.\n\nThe second function, `getCookie2DClustered`, takes in a texture, a transformation matrix, a world position, an intensity value, a boolean value indicating whether the texture is RGB or not, and a cookie channel. The function returns a vec3 value that represents the modified light. The function first applies the transformation matrix to the world position to get the projected position. The function then calls `_getCookieClustered` with the texture, the projected position, the intensity value, the boolean value indicating whether the texture is RGB or not, and the cookie channel.\n\nThe third function, `getCookieCubeClustered`, takes in a texture, a direction vector, an intensity value, a boolean value indicating whether the texture is RGB or not, a cookie channel, a shadow texture resolution, a shadow edge pixel value, and an omni atlas viewport. The function returns a vec3 value that represents the modified light. The function first retrieves the UV coordinate from the cubemap atlas using the direction vector, the shadow texture resolution, the shadow edge pixel value, and the omni atlas viewport. The function then calls `_getCookieClustered` with the texture, the UV coordinate, the intensity value, the boolean value indicating whether the texture is RGB or not, and the cookie channel.\n\nOverall, these functions are used to retrieve and apply a cookie texture to a light source in a clustered lighting system. The functions take in various parameters to modify the light emitted by the light source. These functions are likely used in a larger project that involves rendering a 3D scene with lighting.\n## Questions: \n 1. What does the `_getCookieClustered` function do?\n    \n    The `_getCookieClustered` function takes in a texture, UV coordinates, intensity, a boolean value, and a cookie channel and returns either the RGB value of the pixel or the dot product of the pixel and the cookie channel, depending on the value of the boolean.\n\n2. What is the purpose of the `getCookie2DClustered` function?\n\n    The `getCookie2DClustered` function takes in a texture, a transformation matrix, a world position, intensity, a boolean value, and a cookie channel, and returns the result of calling `_getCookieClustered` with the appropriate parameters.\n\n3. What is the purpose of the `getCookieCubeClustered` function?\n\n    The `getCookieCubeClustered` function takes in a texture, a direction vector, intensity, a boolean value, a cookie channel, a shadow texture resolution, shadow edge pixels, and an omni atlas viewport, and returns the result of calling `_getCookieClustered` with the appropriate parameters. It is specifically designed for clustered omni lights with the cookie texture stored in the cookie atlas.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/clusteredLightCookies.md"}}],["581",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/clusteredLightShadows.js)\n\nThe code above is a shader code that implements different functions for sampling shadows in a clustered omni and spot light setup. The code is written in GLSL and is used in the PlayCanvas engine project. \n\nThe code is divided into two main sections, one for clustered omni sampling and the other for clustered spot sampling. Each section has three functions that implement different levels of Percentage Closer Filtering (PCF) for shadows. \n\nThe first function in each section, `getShadowOmniClusteredPCF1` and `getShadowSpotClusteredPCF1`, implements no filtering and returns a binary value indicating whether a pixel is in shadow or not. The second function in each section, `getShadowOmniClusteredPCF3` and `getShadowSpotClusteredPCF3`, implements PCF3x3 filtering and returns a value between 0 and 1 indicating the level of shadowing. The third function in each section, `getShadowOmniClusteredPCF5` and `getShadowSpotClusteredPCF5`, implements PCF5x5 filtering and returns a value between 0 and 1 indicating the level of shadowing. \n\nThe functions take as input a shadow map, a set of shadow parameters, an omni or spot light viewport, a number of shadow edge pixels, and a light direction. The shadow map is a texture that contains the depth values of the scene from the point of view of the light. The shadow parameters contain information about the resolution of the shadow map and the distance between the light and the objects in the scene. The omni or spot light viewport is a set of coordinates that define the area of the shadow map that corresponds to the light. The number of shadow edge pixels is used to adjust the size of the viewport to avoid edge artifacts. The light direction is used to compute the depth of the shadow map at a given point. \n\nThe functions use a set of helper functions to compute the coordinates of the shadow map texels that correspond to a given point in the scene. These helper functions are not included in the code above. \n\nThe code is used in the PlayCanvas engine project to implement shadow mapping for clustered omni and spot lights. The code is executed on the GPU and is called from the main rendering pipeline. The functions are used to compute the level of shadowing for each pixel in the scene that is affected by a clustered omni or spot light. The resulting shadow map is then combined with the color buffer to produce the final image. \n\nExample usage:\n\n```glsl\n// get the level of shadowing for a clustered omni light using PCF5x5 filtering\nfloat shadowLevel = getShadowOmniClusteredPCF5(shadowMap, shadowParams, omniViewport, shadowEdgePixels, lightDir);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code is for implementing clustered shadow mapping with different levels of PCF filtering for both omni and spot lights using an atlas.\n\n2. What is the difference between the `getShadowOmniClusteredPCF1`, `getShadowOmniClusteredPCF3`, and `getShadowOmniClusteredPCF5` functions?\n- These functions differ in the level of PCF filtering used for omni lights. `getShadowOmniClusteredPCF1` uses no filtering, `getShadowOmniClusteredPCF3` uses a 3x3 PCF filter, and `getShadowOmniClusteredPCF5` uses a 5x5 PCF filter.\n\n3. What is the purpose of the `CLUSTER_SHADOW_TYPE_PCF1`, `CLUSTER_SHADOW_TYPE_PCF3`, and `CLUSTER_SHADOW_TYPE_PCF5` defines?\n- These defines are used to conditionally compile the code for different levels of PCF filtering. `CLUSTER_SHADOW_TYPE_PCF1` is used for no filtering, `CLUSTER_SHADOW_TYPE_PCF3` is used for 3x3 PCF filtering, and `CLUSTER_SHADOW_TYPE_PCF5` is used for 5x5 PCF filtering.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/clusteredLightShadows.md"}}],["582",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/clusteredLightUtils.js)\n\nThe code provided is a set of two functions that are used to convert a direction vector into cubemap face coordinates and texture coordinates for a cubemap face stored within a texture atlas. \n\nThe first function, `getCubemapFaceCoordinates`, takes a direction vector `dir` and returns the corresponding cubemap face index in the range of [0..5] and the UV coordinates within the face in the range of [0..1]. Additionally, an offset to a tile in the atlas within a 3x3 subdivision is provided. The function first calculates the absolute values of the direction vector components and then determines which face of the cubemap the vector is pointing towards. If the absolute value of the z-component is greater than or equal to the absolute values of the x and y components, the vector is pointing towards the front or back face of the cubemap. If the absolute value of the y-component is greater than or equal to the absolute value of the x-component, the vector is pointing towards the top or bottom face of the cubemap. Otherwise, the vector is pointing towards the left or right face of the cubemap. Based on the determined face, the function sets the face index, calculates the UV coordinates, and sets the tile offset. The UV coordinates are then scaled and offset to the range of [0..1] and returned along with the face index and tile offset.\n\nThe second function, `getCubemapAtlasCoordinates`, takes an omniAtlasViewport, shadowEdgePixels, shadowTextureResolution, and a direction vector dir. The function first calls `getCubemapFaceCoordinates` to get the UV coordinates and face index for the direction vector. It then moves the UV coordinates inwards to compensate for the larger field of view when rendering shadows into the atlas. The function then scales the UV coordinates to the cube face area within the viewport and offsets them into the face of the atlas (3x3 grid) and the atlas viewport. The final UV coordinates are returned.\n\nThese functions are likely used in the PlayCanvas engine to convert direction vectors into cubemap face coordinates and texture coordinates for rendering cubemaps and shadows. The `getCubemapFaceCoordinates` function is used to determine which face of the cubemap a direction vector is pointing towards, which is necessary for rendering cubemaps. The `getCubemapAtlasCoordinates` function is used to convert direction vectors into texture coordinates for a cubemap face stored within a texture atlas, which is necessary for rendering shadows.\n## Questions: \n 1. What does this code do?\n- This code contains two functions that convert an unnormalized direction vector to cubemap face and texture coordinates within the face, and to texture coordinates for a cubemap face stored within a texture atlas.\n\n2. What are the input parameters for the `getCubemapAtlasCoordinates` function?\n- The `getCubemapAtlasCoordinates` function takes in four parameters: `omniAtlasViewport`, `shadowEdgePixels`, `shadowTextureResolution`, and `dir`. \n\n3. What is the purpose of the `tileOffset` variable in the `getCubemapFaceCoordinates` function?\n- The `tileOffset` variable provides an offset to a tile in the atlas within a 3x3 subdivision.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/clusteredLightUtils.md"}}],["583",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/combine.js)\n\nThe code provided is a function called `combineColor` that takes in three parameters: `albedo`, `sheenSpecularity`, and `clearcoatSpecularity`. The function returns a `vec3` value that represents the combined color of the input parameters.\n\nThe purpose of this function is to calculate the final color of a material based on various lighting and shading models. The function takes into account different lighting models such as diffuse, specular, reflections, sheen, and clearcoat. The function uses pre-defined variables such as `dDiffuseLight`, `dSpecularLight`, `dReflection`, `sSpecularLight`, `sReflection`, `ccFresnel`, `ccSpecularLight`, and `ccReflection` to calculate the final color.\n\nThe function first calculates the diffuse lighting contribution to the final color. If the `LIT_OLD_AMBIENT` flag is defined, the function subtracts the global ambient light from the diffuse light and multiplies it by the material's ambient color. Otherwise, it multiplies the diffuse light by the albedo color. The function then adds the specular lighting contribution if the `LIT_SPECULAR` flag is defined. It also adds the reflection contribution if the `LIT_REFLECTIONS` flag is defined.\n\nIf the `LIT_SHEEN` flag is defined, the function calculates the sheen contribution to the final color. It first calculates a scaling factor based on the maximum value of the `sheenSpecularity` vector and multiplies it by a constant value. It then multiplies the diffuse color by the scaling factor and adds the sheen specular and reflection contributions multiplied by the `sheenSpecularity` vector.\n\nIf the `LIT_CLEARCOAT` flag is defined, the function calculates the clearcoat contribution to the final color. It first calculates a scaling factor based on the fresnel term and the `clearcoatSpecularity` value. It then multiplies the diffuse color by the scaling factor and adds the clearcoat specular and reflection contributions multiplied by the `clearcoatSpecularity` value.\n\nOverall, this function is an important part of the PlayCanvas engine as it calculates the final color of a material based on various lighting and shading models. It can be used in conjunction with other functions and classes in the engine to create visually appealing 3D scenes and games. Here is an example usage of the `combineColor` function:\n\n```\nconst albedo = vec3(1, 0.5, 0.2);\nconst sheenSpecularity = vec3(0.2, 0.4, 0.6);\nconst clearcoatSpecularity = 0.8;\n\nconst finalColor = combineColor(albedo, sheenSpecularity, clearcoatSpecularity);\n```\n## Questions: \n 1. What does this code do?\n   - This code defines a function called `combineColor` that takes in three parameters (`albedo`, `sheenSpecularity`, and `clearcoatSpecularity`) and returns a `vec3` value.\n2. What is the purpose of the `#ifdef` statements in this code?\n   - The `#ifdef` statements are used to conditionally include or exclude certain parts of the code based on whether certain preprocessor macros are defined or not.\n3. What is the significance of the calculations involving `sheenScaling` and `clearCoatScaling`?\n   - `sheenScaling` and `clearCoatScaling` are used to adjust the contribution of the sheen and clearcoat components to the final color value based on their respective specularity values.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/combine.md"}}],["584",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/cookie.js)\n\nThe code above is a set of functions that provide light cookie functionality for non-clustered lights in the PlayCanvas engine. Light cookies are textures that are used to modify the appearance of a light source. They can be used to create various lighting effects such as shadows, reflections, and color grading.\n\nThe first four functions, `getCookie2D`, `getCookie2DClip`, `getCookie2DXform`, and `getCookie2DClipXform`, are used to retrieve a 2D light cookie texture. The `sampler2D` parameter represents the texture that is used as the light cookie. The `mat4` parameter represents the transform matrix that is used to transform the position of the light source. The `float` parameter represents the intensity of the light source. The `vec4` parameter represents the cookie matrix, and the `vec2` parameter represents the cookie offset.\n\nThe `getCookie2D` function retrieves the light cookie texture without any clipping or transformation. The `getCookie2DClip` function retrieves the light cookie texture with clipping, which means that if the projected position of the light source is outside the screen, the function returns a black color. The `getCookie2DXform` function retrieves the light cookie texture with transformation, which means that the cookie texture is transformed using the cookie matrix and offset. The `getCookie2DClipXform` function retrieves the light cookie texture with both clipping and transformation.\n\nThe last function, `getCookieCube`, is used to retrieve a cube light cookie texture. The `samplerCube` parameter represents the cube texture that is used as the light cookie. The `mat4` parameter represents the transform matrix that is used to transform the position of the light source. The `float` parameter represents the intensity of the light source. This function retrieves the light cookie texture by using the `textureCube` function, which returns a color value from the cube texture based on the direction of the light source.\n\nOverall, these functions provide a way to retrieve light cookie textures for non-clustered lights in the PlayCanvas engine. They can be used to create various lighting effects and modify the appearance of a light source.\n## Questions: \n 1. What is the purpose of this code?\n- This code provides light cookie functionality for non-clustered lights in the PlayCanvas engine.\n\n2. What are the input parameters for each of the functions?\n- Each function takes in a texture sampler, a transformation matrix, a float intensity value, and additional parameters for the getCookie2DXform and getCookie2DClipXform functions.\n\n3. What is the difference between the getCookie2D and getCookie2DClip functions?\n- The getCookie2DClip function includes a check to ensure that the projected position is within the bounds of the screen before returning the texture value, while the getCookie2D function does not have this check.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/cookie.md"}}],["585",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/cubeMapProjectBox.js)\n\nThe code above is a GLSL shader code that exports a function called `cubeMapProject`. This function takes two uniform vectors as input, `envBoxMin` and `envBoxMax`, which represent the minimum and maximum bounds of a cube map. The purpose of this function is to project a given normal direction vector onto the cube map and return the normalized vector that points to the corresponding position on the cube map.\n\nThe `cubeMapProject` function first rotates the input normal direction vector using another function called `cubeMapRotate`. This is likely done to ensure that the cube map is oriented correctly with respect to the scene. The rotated vector is then used to calculate the intersection points of the vector with the six faces of the cube map. This is done by dividing the difference between the cube map bounds and the current position by the rotated vector. The resulting vectors represent the distances along the x, y, and z axes to the intersection points of the vector with the cube map.\n\nNext, the function calculates the minimum and maximum values of these intersection distances along each axis. This is done by comparing the signs of the x, y, and z components of the rotated vector and selecting the appropriate minimum and maximum values. The minimum of these three values is then used to calculate the position on the cube map that corresponds to the input normal direction vector. This is done by adding the product of the rotated vector and the minimum distance to the current position.\n\nFinally, the function calculates the center position of the cube map and returns the normalized vector that points from the current position to the calculated position on the cube map. This normalized vector can be used to sample the cube map and retrieve the corresponding color value for the input normal direction vector.\n\nOverall, this function is an important part of the PlayCanvas engine's rendering pipeline as it allows for efficient and accurate projection of normal direction vectors onto cube maps. It can be used in various rendering techniques such as environment mapping, reflection mapping, and skybox rendering. Here is an example of how this function can be used in a shader:\n\n```glsl\nuniform samplerCube envMap;\n\nvoid main() {\n    vec3 normal = normalize(vNormalW);\n    vec3 envDir = cubeMapProject(normal);\n    vec4 envColor = texture(envMap, envDir);\n    // use envColor to shade the current fragment\n}\n```\n## Questions: \n 1. What is the purpose of the `envBoxMin` and `envBoxMax` uniform variables?\n- These uniform variables are used to define the minimum and maximum bounds of an environment box.\n\n2. What is the `cubeMapRotate` function and where is it defined?\n- The code references a `cubeMapRotate` function, but it is not defined in this code snippet. A smart developer might want to know where this function is defined and what it does.\n\n3. What is the expected input and output of the `cubeMapProject` function?\n- A smart developer might want to know what type of input the `cubeMapProject` function expects and what type of output it returns. Based on the code, it appears that the function takes a `vec3` as input and returns a normalized `vec3`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/cubeMapProjectBox.md"}}],["586",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/cubeMapProjectNone.js)\n\nThe code above is a function called `cubeMapProject` that takes in a `vec3` direction and returns a transformed `vec3` direction. This function is written in GLSL, which is a shading language used for graphics programming. \n\nThe purpose of this function is to project a direction onto a cube map. A cube map is a texture that is used to simulate reflections or environment maps in 3D graphics. It is made up of six square images that represent the six faces of a cube. The `cubeMapProject` function takes in a direction vector and applies a rotation to it using the `cubeMapRotate` function. This rotation is necessary because the cube map is projected onto a cube, and the direction vector needs to be transformed to match the orientation of the cube faces.\n\nThis function is likely used in the larger PlayCanvas engine project to handle cube map projections for various graphics effects. For example, it could be used to simulate reflections on a shiny surface or to create a skybox for a game environment. \n\nHere is an example of how this function could be used in a shader:\n\n```\nuniform samplerCube cubeMap;\nvarying vec3 worldNormal;\n\nvoid main() {\n  vec3 reflectionDir = cubeMapProject(reflect(-normalize(vPosition), worldNormal));\n  vec4 reflectionColor = textureCube(cubeMap, reflectionDir);\n  gl_FragColor = reflectionColor;\n}\n```\n\nIn this example, the `cubeMapProject` function is used to project a reflection direction onto a cube map texture. The `reflect` function calculates the reflection direction based on the surface normal and the view direction. The resulting reflection direction is then passed into `cubeMapProject` to transform it into the correct orientation for the cube map. Finally, the `textureCube` function is used to sample the cube map texture at the projected direction, and the resulting color is output as the fragment color.\n## Questions: \n 1. What does the `/* glsl */` comment indicate in this code?\n   - The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a high-level shading language used for graphics programming.\n\n2. What is the purpose of the `cubeMapProject` function?\n   - The `cubeMapProject` function takes a direction vector and returns the result of passing it through the `cubeMapRotate` function. It is likely used for projecting a texture onto a cube map.\n\n3. Where is the `cubeMapRotate` function defined?\n   - The `cubeMapRotate` function is not defined in this code snippet, so a smart developer might want to look for its definition elsewhere in the PlayCanvas engine codebase.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/cubeMapProjectNone.md"}}],["587",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/cubeMapRotate.js)\n\nThis code is a shader function that is used to rotate a cubemap texture. The function takes in a reference direction vector and returns a rotated direction vector. The rotation is performed using a matrix, which is passed in as a uniform variable. \n\nThe code is written in GLSL, which is a shading language used for graphics programming. The function is exported as a default module, which means it can be imported and used in other parts of the project. \n\nThe function first checks if the CUBEMAP_ROTATION flag is defined. If it is, the reference direction vector is multiplied by the cubeMapRotationMatrix. This matrix is a 3x3 matrix that represents the rotation to be applied to the cubemap texture. If the flag is not defined, the function simply returns the reference direction vector without any rotation. \n\nThis function can be used in the larger project to apply a rotation to a cubemap texture. Cubemap textures are often used in 3D graphics to create reflections and environment maps. By rotating the cubemap texture, the reflections and environment maps can be adjusted to match the orientation of the object being rendered. \n\nHere is an example of how this function could be used in a shader:\n\n```\nuniform samplerCube cubemap;\nvarying vec3 worldNormal;\n\nvoid main() {\n  vec3 rotatedNormal = cubeMapRotate(worldNormal);\n  vec4 color = textureCube(cubemap, rotatedNormal);\n  gl_FragColor = color;\n}\n```\n\nIn this example, the shader takes in a cubemap texture and a world normal vector. The world normal vector is passed through the cubeMapRotate function to apply the rotation. The resulting rotated normal vector is then used to sample the cubemap texture using the textureCube function. The resulting color is then output as the final fragment color.\n## Questions: \n 1. What is the purpose of the `CUBEMAP_ROTATION` preprocessor directive?\n- The `CUBEMAP_ROTATION` preprocessor directive is used to conditionally compile code that rotates a cube map based on a matrix provided by the `cubeMapRotationMatrix` uniform.\n\n2. What is the input and output of the `cubeMapRotate` function?\n- The `cubeMapRotate` function takes a `vec3` parameter `refDir` representing a reference direction and returns a `vec3` representing the rotated direction.\n\n3. How is this code intended to be used within the PlayCanvas engine?\n- This code is likely intended to be used as part of a shader program that renders a cube map texture, allowing for dynamic rotation of the cube map based on the `cubeMapRotationMatrix` uniform.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/cubeMapRotate.md"}}],["588",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/end.js)\n\nThis code is a GLSL shader that is used in the PlayCanvas engine to render a 3D scene. The shader takes in several arguments, including the albedo (base color) of the object being rendered, the specularity of the sheen and clearcoat layers, and the amount of emission (light) emitted by the object. \n\nThe first line of the shader combines the albedo color with the specularity of the sheen and clearcoat layers to determine the final color of the object. This is done using the `combineColor` function, which is likely defined elsewhere in the PlayCanvas engine.\n\nThe second line of the shader adds the emission color to the final color of the object. This is done using the `+=` operator, which adds the value of `litShaderArgs.emission` to the existing value of `gl_FragColor.rgb`.\n\nThe third line of the shader applies fog to the final color of the object using the `addFog` function. This function is likely defined elsewhere in the PlayCanvas engine and applies a fog effect to the color based on the distance of the object from the camera.\n\nThe final three lines of the shader are conditional statements that only execute if the `HDR` flag is not defined. These lines apply tone mapping and gamma correction to the final color of the object. Tone mapping is a technique used to map the high dynamic range (HDR) colors of a scene to a lower dynamic range suitable for display on a standard monitor. Gamma correction is a technique used to adjust the brightness and contrast of an image to match the characteristics of a display device.\n\nOverall, this shader is an important component of the PlayCanvas engine's rendering pipeline. It takes in various parameters that describe the appearance of an object and applies various effects to produce a final color that is suitable for display on a screen. Developers using the PlayCanvas engine can customize this shader or create their own shaders to achieve different visual effects in their projects.\n## Questions: \n 1. What does the `combineColor` function do and what are its arguments?\n- The `combineColor` function combines the albedo, sheen specularity, and clearcoat specularity to produce a final color. Its arguments are `litShaderArgs.albedo`, `litShaderArgs.sheen.specularity`, and `litShaderArgs.clearcoat.specularity`.\n2. What is the purpose of the `addFog` function and how does it affect the output?\n- The `addFog` function adds fog to the final color by modifying the RGB values of `gl_FragColor`. It is applied after the emission is added to the color.\n3. What is the purpose of the `#ifndef HDR` preprocessor directive and how does it affect the output?\n- The `#ifndef HDR` preprocessor directive checks if the `HDR` flag is not defined. If it is not defined, the `toneMap` and `gammaCorrectOutput` functions are applied to the final color to adjust its brightness and color balance. If the `HDR` flag is defined, these functions are not applied.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/end.md"}}],["589",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/extension.js)\n\nThis code exports a default string that contains a GLSL shader code. GLSL stands for OpenGL Shading Language, which is a high-level language used to write shaders for graphics processing units (GPUs). \n\nIn the context of the PlayCanvas engine project, this code may be used to define the visual appearance of 3D objects in a scene. Shaders are used to manipulate the way light interacts with objects, creating effects such as reflections, shadows, and transparency. \n\nHere is an example of how this code may be used in a PlayCanvas project:\n\n```javascript\nconst material = new pc.StandardMaterial();\nmaterial.shader = new pc.Shader({\n    attributes: {\n        aPosition: pc.SEMANTIC_POSITION,\n        aNormal: pc.SEMANTIC_NORMAL,\n        aUv0: pc.SEMANTIC_TEXCOORD0\n    },\n    vshader: /* glsl */`\n        attribute vec3 aPosition;\n        attribute vec3 aNormal;\n        attribute vec2 aUv0;\n\n        uniform mat4 matrix_model;\n        uniform mat4 matrix_viewProjection;\n\n        varying vec3 vNormal;\n        varying vec2 vUv0;\n\n        void main() {\n            gl_Position = matrix_viewProjection * matrix_model * vec4(aPosition, 1.0);\n            vNormal = aNormal;\n            vUv0 = aUv0;\n        }\n    `,\n    fshader: /* glsl */`\n        precision mediump float;\n\n        varying vec3 vNormal;\n        varying vec2 vUv0;\n\n        void main() {\n            gl_FragColor = vec4(vNormal, 1.0);\n        }\n    `\n});\n```\n\nIn this example, a new `StandardMaterial` is created and assigned a new `Shader` instance. The `Shader` instance is defined using GLSL code, which is passed as a template literal using the `/* glsl */` tag. The `vshader` property defines the vertex shader code, which is responsible for transforming the vertices of the 3D object. The `fshader` property defines the fragment shader code, which is responsible for determining the color of each pixel of the 3D object. \n\nOverall, this code is a crucial part of the PlayCanvas engine project, as it allows developers to create visually stunning 3D scenes by defining custom shaders.\n## Questions: \n 1. What is the purpose of the `export default` statement in this code?\n   - The `export default` statement is used to export the code as the default export of the module.\n2. What does the `/* glsl */` comment indicate in this code?\n   - The `/* glsl */` comment indicates that the code is written in the GLSL shader language.\n3. What is the expected usage of this code within the PlayCanvas engine?\n   - Without additional context, it is unclear how this code is intended to be used within the PlayCanvas engine. Further documentation or code examples may be necessary to understand its purpose.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/extension.md"}}],["590",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/falloffInvSquared.js)\n\nThe code provided is a set of two functions that calculate the falloff of a light source in a 3D environment. The first function, `getFalloffWindow`, takes in two parameters: `lightRadius`, which is the radius of the light source, and `lightDir`, which is the direction of the light source. The function calculates the squared distance between the light source and the object it is illuminating, and then calculates the inverse radius of the light source. It then returns the square of the result of the `saturate` function applied to the difference between 1 and the square of the squared distance multiplied by the square of the inverse radius. This calculation is used to determine the intensity of the light at a given point in the scene, with the falloff increasing as the distance from the light source increases.\n\nThe second function, `getFalloffInvSquared`, also takes in `lightRadius` and `lightDir` as parameters. It calculates the squared distance between the light source and the object it is illuminating, and then calculates the inverse radius of the light source. It then multiplies the result of the `saturate` function applied to the difference between 1 and the square of the squared distance multiplied by the square of the inverse radius by 16. This calculation is used to determine the intensity of the light at a given point in the scene, with the falloff increasing more rapidly as the distance from the light source increases compared to the `getFalloffWindow` function.\n\nThese functions are likely used in the larger PlayCanvas engine project to calculate the falloff of light sources in a 3D environment. This is an important aspect of creating realistic lighting in a scene, as it allows for the intensity of light to decrease as the distance from the light source increases. The `getFalloffWindow` function may be used for light sources that have a more gradual falloff, while the `getFalloffInvSquared` function may be used for light sources that have a more rapid falloff. These functions may be called by other functions or scripts within the PlayCanvas engine project to determine the intensity of light at a given point in the scene. \n\nExample usage of `getFalloffWindow`:\n\n```\nconst lightRadius = 10;\nconst lightDir = new pc.Vec3(0, 1, 0);\nconst distanceFromLight = 5;\nconst intensity = getFalloffWindow(lightRadius, lightDir) / (distanceFromLight * distanceFromLight);\n```\n\nExample usage of `getFalloffInvSquared`:\n\n```\nconst lightRadius = 10;\nconst lightDir = new pc.Vec3(0, 1, 0);\nconst distanceFromLight = 5;\nconst intensity = getFalloffInvSquared(lightRadius, lightDir) / (distanceFromLight * distanceFromLight);\n```\n## Questions: \n 1. What is the purpose of this code?\n   - This code calculates the falloff of a light source based on its radius and direction.\n\n2. What are the inputs required for this code to work?\n   - This code requires two inputs: the radius of the light source and the direction of the light.\n\n3. What is the output of this code?\n   - This code returns a float value that represents the falloff of the light source.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/falloffInvSquared.md"}}],["591",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/falloffLinear.js)\n\nThe code above is a GLSL shader function that calculates the linear falloff of a light source. The function takes two parameters: the radius of the light source and the direction of the light. \n\nThe `getFalloffLinear` function first calculates the distance between the light source and the point being lit by taking the length of the light direction vector. It then calculates the falloff factor by subtracting the distance from the light radius and dividing by the light radius. This gives a value between 0 and 1, where 0 means the point is outside the light radius and 1 means the point is at the center of the light source. \n\nThe `max` function is used to ensure that the falloff factor is never negative. This is because negative values would cause the light to appear brighter as the distance from the light source increases, which is not physically accurate. \n\nThis function can be used in the larger PlayCanvas engine project to calculate the lighting of a scene. It is particularly useful for point lights, which have a finite radius and emit light in all directions. By using this function, the engine can accurately calculate the brightness of each point in the scene based on its distance from the light source. \n\nHere is an example of how this function might be used in a shader:\n\n```\nuniform vec3 lightPosition;\nuniform float lightRadius;\n\nvoid main() {\n    vec3 lightDir = normalize(lightPosition - vPosition);\n    float falloff = getFalloffLinear(lightRadius, lightDir);\n    vec3 diffuse = ...; // calculate diffuse lighting\n    vec3 specular = ...; // calculate specular lighting\n    vec3 color = falloff * (diffuse + specular);\n    gl_FragColor = vec4(color, 1.0);\n}\n```\n\nIn this example, `lightPosition` and `lightRadius` are uniform variables that define the position and radius of the light source. `vPosition` is the position of the current fragment being shaded. The `getFalloffLinear` function is called to calculate the falloff factor based on the distance between the light source and the fragment. The diffuse and specular lighting calculations are then combined with the falloff factor to produce the final color of the fragment.\n## Questions: \n 1. What does this code do?\n   This code defines a GLSL function that calculates the linear falloff of a light based on its radius and direction.\n\n2. How can this code be used in the PlayCanvas engine?\n   This code can be used in the PlayCanvas engine to calculate the falloff of lights in a 3D scene, which can improve the realism and accuracy of lighting.\n\n3. Are there any limitations or performance considerations when using this code?\n   Depending on the number of lights and the complexity of the scene, calculating the falloff for each light can be computationally expensive. Developers may need to optimize their code or use alternative techniques to improve performance.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/falloffLinear.md"}}],["592",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/float-unpacking.js)\n\nThe code provided is a set of functions that perform float unpacking. The purpose of this code is to convert packed float data into its original floating-point representation. This is done by reversing the process of float packing, which is a technique used to compress floating-point data into a smaller size for storage or transmission.\n\nThe functions in this code are designed to work with data that has been packed using the float-packing.js library. The library packs floating-point data into a smaller size by converting it into a series of bytes. The packed data can then be transmitted or stored more efficiently. The functions in this code are used to reverse this process and convert the packed data back into its original floating-point representation.\n\nThe `bytes2float2`, `bytes2float3`, and `bytes2float4` functions take in a vector of packed data and return the corresponding floating-point value. The `bytes2floatRange2`, `bytes2floatRange3`, and `bytes2floatRange4` functions are similar, but they also take in a minimum and maximum value and return the floating-point value within that range.\n\nThe `mantissaExponent2Float` function takes in a packed vector of data and returns the corresponding floating-point value. This function is specifically designed to work with data that has been packed using the float-packing.js library. The function first uses the `bytes2floatRange3` function to convert the packed data into a floating-point value within the range of -1.0 to 1.0. It then uses the fourth component of the packed data to calculate the exponent and applies it to the floating-point value using the `exp2` function.\n\nOverall, this code provides a set of functions that are essential for unpacking packed floating-point data. These functions are used in the larger PlayCanvas engine project to efficiently transmit and store floating-point data. Below is an example of how the `bytes2float2` function can be used to unpack packed data:\n\n```\nconst packedData = vec2(0.5, 0.25); // packed data\nconst unpackedData = bytes2float2(packedData); // unpacked data\nconsole.log(unpackedData); // output: 0.5009804\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code provides functions for converting byte data to floating point values and for mapping those values to a specified range.\n\n2. What is the input and output of the `mantissaExponent2Float` function?\n    \n    The `mantissaExponent2Float` function takes a vec4 input and returns a single float value. The input is expected to be a packed representation of a floating point value, with the first three components representing the mantissa and the fourth component representing the exponent.\n\n3. What is the relationship between the `bytes2float` functions and the `float-packing.js` file mentioned in the comments?\n    \n    The `bytes2float` functions are described as being \"complimentary to float-packing.js\", suggesting that the `float-packing.js` file provides a way to pack floating point values into byte data, while these functions provide a way to unpack them back into floating point values.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/float-unpacking.md"}}],["593",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/fogExp.js)\n\nThe code above is a GLSL shader code that defines a function called `addFog`. This function takes in a `vec3` color value and returns a new color value with fog added to it. The fog effect is achieved by calculating the distance of the current fragment from the camera and using that distance to calculate a fog factor. This fog factor is then used to blend the original color with a fog color.\n\nThe `uniform` variables `fog_color` and `fog_density` are used to control the color and density of the fog effect. The `dBlendModeFogFactor` variable is used to control the blending mode of the fog effect.\n\nThis code is likely used in the PlayCanvas engine to add a fog effect to 3D scenes. The `addFog` function can be called in a shader program to add fog to the rendered scene. The `fog_color` and `fog_density` uniforms can be set to control the color and density of the fog effect. The `dBlendModeFogFactor` variable can be set to control the blending mode of the fog effect.\n\nHere is an example of how this code might be used in a PlayCanvas project:\n\n```javascript\n// create a new material with the fog shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.fog = `\n    uniform vec3 fog_color;\n    uniform float fog_density;\n    float dBlendModeFogFactor = 1.0;\n\n    vec3 addFog(vec3 color) {\n        float depth = gl_FragCoord.z / gl_FragCoord.w;\n        float fogFactor = exp(-depth * fog_density);\n        fogFactor = clamp(fogFactor, 0.0, 1.0);\n        return mix(fog_color * dBlendModeFogFactor, color, fogFactor);\n    }\n`;\n\n// set the fog color and density\nmaterial.setParameter('fog_color', new pc.Color(0.5, 0.5, 0.5));\nmaterial.setParameter('fog_density', 0.1);\n\n// apply the material to a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n``` \n\nIn this example, a new `StandardMaterial` is created with the `fog` shader code added to its `chunks` property. The `fog_color` and `fog_density` uniforms are set to control the color and density of the fog effect. Finally, the material is applied to a `MeshInstance` to render a mesh with fog added to it.\n## Questions: \n 1. What does this code do?\n   This code defines a function called `addFog` that takes in a color and applies fog to it based on the depth of the fragment and the fog density and color specified by uniforms.\n\n2. What is the purpose of the `dBlendModeFogFactor` variable?\n   The purpose of `dBlendModeFogFactor` is to allow for different blending modes to be applied to the fog color and the original color. It is multiplied by the fog color before being mixed with the original color.\n\n3. What is the expected input for the `fog_color` and `fog_density` uniforms?\n   The `fog_color` uniform is expected to be a vec3 representing the color of the fog, while the `fog_density` uniform is expected to be a float representing the density of the fog. These values are used to calculate the fog factor applied to the color in the `addFog` function.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/fogExp.md"}}],["594",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/fogExp2.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) that adds fog to a 3D scene. It exports a function called `addFog` that takes a color as input and returns a new color with fog added to it. \n\nThe function first calculates the depth of the current fragment (pixel) in the scene by dividing the z-coordinate of the fragment by its w-coordinate. This gives a value between 0 and 1, where 0 is the near plane and 1 is the far plane of the camera's view frustum. \n\nNext, the function calculates the fog factor using an exponential decay function that depends on the depth of the fragment and the fog density. The fog density determines how quickly the fog accumulates as the depth increases. The `clamp` function is used to ensure that the fog factor is between 0 and 1. \n\nFinally, the function uses the `mix` function to blend the original color with the fog color based on the fog factor. The `dBlendModeFogFactor` variable is used to control the intensity of the fog. \n\nThis code can be used in the PlayCanvas engine to add fog to a 3D scene. The shader can be attached to a material and applied to a mesh or a group of meshes. The fog color and density can be set using uniform variables. The `addFog` function can also be modified to add additional effects, such as color grading or noise. \n\nExample usage:\n\n```javascript\n// create a material with the fog shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.fog = `\n    uniform vec3 fog_color;\n    uniform float fog_density;\n    float dBlendModeFogFactor = 1.0;\n    vec3 addFog(vec3 color) {\n        // ... shader code here ...\n    }\n`;\nmaterial.uniforms.fog_color = new pc.Color(1, 1, 1);\nmaterial.uniforms.fog_density = 0.1;\n\n// apply the material to a mesh\nvar mesh = new pc.Mesh();\n// ... set up mesh vertices and indices ...\nvar entity = new pc.Entity();\nentity.addComponent('model', {\n    type: 'mesh',\n    mesh: mesh,\n    material: material\n});\n```\n## Questions: \n 1. What does this code do?\n   This code defines a function called `addFog` that takes in a color and applies fog to it based on the distance from the camera.\n\n2. What are the inputs to the `addFog` function?\n   The `addFog` function takes in a `vec3` color value.\n\n3. What are the outputs of the `addFog` function?\n   The `addFog` function returns a `vec3` color value with fog applied to it.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/fogExp2.md"}}],["595",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/fogLinear.js)\n\nThis code is a GLSL shader that adds fog to a 3D scene. It takes in three uniform variables: `fog_color`, `fog_start`, and `fog_end`. `fog_color` is the color of the fog, while `fog_start` and `fog_end` define the start and end distances of the fog from the camera. \n\nThe `addFog` function calculates the distance of the current fragment from the camera using `gl_FragCoord.z` and `gl_FragCoord.w`. It then calculates the `fogFactor` based on the distance of the fragment from the camera and the `fog_start` and `fog_end` values. The `clamp` function is used to ensure that `fogFactor` is always between 0 and 1. Finally, the `mix` function is used to blend the `fog_color` with the original color of the fragment based on the `fogFactor`.\n\nThis shader can be used in a larger project to add atmospheric effects to a 3D scene. For example, it can be used to simulate fog, mist, or haze. The `fog_start` and `fog_end` values can be adjusted to control the density and distance of the fog. The `fog_color` can be changed to create different colored fog effects. \n\nHere is an example of how this shader can be used in a PlayCanvas project:\n\n```javascript\n// Create a material with the fog shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.fog = `\n    uniform vec3 fog_color;\n    uniform float fog_start;\n    uniform float fog_end;\n    float dBlendModeFogFactor = 1.0;\n\n    vec3 addFog(vec3 color) {\n        float depth = gl_FragCoord.z / gl_FragCoord.w;\n        float fogFactor = (fog_end - depth) / (fog_end - fog_start);\n        fogFactor = clamp(fogFactor, 0.0, 1.0);\n        return mix(fog_color * dBlendModeFogFactor, color, fogFactor);\n    }\n`;\n\n// Set the fog color and distance\nmaterial.setParameter('fog_color', new pc.Color(0.5, 0.5, 0.5));\nmaterial.setParameter('fog_start', 10);\nmaterial.setParameter('fog_end', 50);\n\n// Apply the material to a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n``` \n\nIn this example, a new `StandardMaterial` is created with the `fog` shader code added to the `chunks` property. The `setParameter` function is used to set the `fog_color`, `fog_start`, and `fog_end` values. Finally, the material is applied to a `MeshInstance` which is attached to a node in the scene.\n## Questions: \n 1. What is the purpose of this code?\n   - This code defines a function called `addFog` that takes in a color and applies fog to it based on the distance from the camera.\n\n2. What are the inputs to the `addFog` function?\n   - The `addFog` function takes in a `vec3` color as its input.\n\n3. What are the outputs of the `addFog` function?\n   - The `addFog` function returns a `vec3` color with fog applied to it based on the distance from the camera.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/fogLinear.md"}}],["596",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/fogNone.js)\n\nThis code defines a GLSL shader function that adds fog to a given color. The function is exported as the default export of the module. \n\nThe `addFog` function takes a `vec3` color as input and returns a modified `vec3` color with fog added. However, the function currently does not actually add any fog and simply returns the original color. \n\nThe `dBlendModeFogFactor` variable is also defined, but it is not used in this function. It is likely used in other parts of the PlayCanvas engine to control the amount of fog applied to a scene. \n\nThis code is likely used in the larger PlayCanvas engine project to add fog effects to 3D scenes. Developers can import this module and use the `addFog` function in their own shaders to add fog to their scenes. For example, a developer could use this function in a fragment shader to add fog to a skybox:\n\n```\nuniform samplerCube skybox;\nvarying vec3 vWorldPosition;\n\nvoid main() {\n  vec4 color = textureCube(skybox, vWorldPosition);\n  vec3 foggedColor = addFog(color.rgb);\n  gl_FragColor = vec4(foggedColor, color.a);\n}\n```\n\nIn this example, the `addFog` function is used to modify the color of the skybox based on the amount of fog in the scene. The resulting color is then output as the fragment color.\n## Questions: \n 1. What is the purpose of the `dBlendModeFogFactor` variable?\n   - It is unclear from the provided code what the `dBlendModeFogFactor` variable is used for or how it affects the code.\n2. How is the `addFog` function intended to be used?\n   - The `addFog` function is defined but does not currently modify the input `color` parameter. It is unclear how it is intended to be used or what its purpose is.\n3. What is the significance of the `/* glsl */` comment at the beginning of the code?\n   - The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax, which is used for writing shaders in graphics programming.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/fogNone.md"}}],["597",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/fresnelSchlick.js)\n\nThe code provided is a glsl shader function that calculates the Fresnel effect for a given surface. The Fresnel effect is the phenomenon where the reflectivity of a surface changes based on the angle of incidence of the incoming light. This effect is commonly seen in materials such as glass, water, and metal.\n\nThe function takes in several parameters, including the cosine of the angle between the surface normal and the incoming light, the glossiness of the surface, and the specularity of the surface. If the `LIT_IRIDESCENCE` flag is defined, the function also takes in additional parameters related to iridescence.\n\nThe function first calculates the Fresnel term using Schlick's approximation, which is a simplified model for calculating the Fresnel effect. The Fresnel term is then used to interpolate between the surface's specularity and a blurred version of the surface based on the glossiness. This creates the illusion of a smooth, reflective surface.\n\nIf the `LIT_IRIDESCENCE` flag is defined, the function also applies an iridescence effect to the surface. Iridescence is the phenomenon where the color of a surface changes based on the viewing angle. The function uses the `mix` function to blend the surface's Fresnel term with an iridescence Fresnel term based on the intensity of the iridescence effect.\n\nThe function `getFresnelCC` is a simplified version of the main `getFresnel` function that only calculates the Fresnel term and returns a constant value for the ambient reflectivity of the surface.\n\nThis code is likely used in the PlayCanvas engine to create realistic lighting and shading effects for 3D models. It can be used in conjunction with other shaders and materials to create a wide variety of surface appearances. Here is an example of how this function could be used in a shader:\n\n```\nuniform float glossiness;\nuniform vec3 specularity;\nvarying vec3 surfaceNormal;\nvarying vec3 surfaceToCamera;\n\nvoid main() {\n    float cosTheta = dot(surfaceNormal, surfaceToCamera);\n    vec3 fresnel = getFresnel(cosTheta, glossiness, specularity);\n    // use fresnel to calculate final color of the surface\n}\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines two functions for calculating the Fresnel effect in computer graphics, one with additional parameters for iridescence.\n\n2. What is the input and output of the `getFresnel` function?\n- The `getFresnel` function takes in a cosine theta value, a gloss value, and a specularity vector, and optionally an iridescenceFresnel vector and an IridescenceArgs object. It returns a vector representing the Fresnel effect with the given parameters.\n\n3. What is the purpose of the `getFresnelCC` function?\n- The `getFresnelCC` function calculates a simplified version of the Fresnel effect with a fixed constant value of 0.04 added to the result.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/fresnelSchlick.md"}}],["598",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/iridescenceDiffraction.js)\n\nThe code provided is a shader code that calculates iridescence effect on a material. Iridescence is a phenomenon where the color of an object appears to change when viewed from different angles. This effect is caused by the interference of light waves that are reflected from the surface of the object. The shader code calculates the iridescence effect by simulating the interference of light waves that are reflected from the surface of the material.\n\nThe shader code exports a function that takes in a cosine of the angle between the surface normal and the view direction, a specularity value, and an iridescence argument. The function returns a color value that represents the iridescence effect on the material. The iridescence argument contains the thickness of the iridescent layer and the refractive index of the material.\n\nThe shader code contains several helper functions that are used to calculate the iridescence effect. The `iridescence_iorToFresnel` function calculates the Fresnel reflectance of a material given its refractive index. The `iridescence_fresnelToIor` function calculates the refractive index of a material given its Fresnel reflectance. The `iridescence_sensitivity` function calculates the sensitivity of the material to changes in the thickness of the iridescent layer. The `iridescence_fresnel` function calculates the Fresnel reflectance of a material given the cosine of the angle between the surface normal and the incident light direction.\n\nThe `calcIridescence` function is the main function that calculates the iridescence effect on the material. The function first calculates the refractive index of the iridescent layer based on the thickness of the layer and the refractive index of the material. It then calculates the Fresnel reflectance of the material and the iridescent layer based on the angle between the surface normal and the incident light direction. The function then calculates the sensitivity of the material to changes in the thickness of the iridescent layer and the interference of light waves that are reflected from the surface of the material. Finally, the function returns the color value that represents the iridescence effect on the material.\n\nThe `getIridescence` function is a wrapper function that calls the `calcIridescence` function with the appropriate arguments and returns the resulting color value.\n\nOverall, this shader code is an essential part of the PlayCanvas engine that enables developers to create materials with iridescence effect. The code can be used to create a wide range of materials, including metals, plastics, and fabrics, that exhibit iridescence effect when viewed from different angles.\n## Questions: \n 1. What is the purpose of this code?\n- This code defines functions for calculating iridescence effects in materials.\n\n2. What are the inputs and outputs of the `calcIridescence` function?\n- The inputs are `outsideIor` (refractive index of the medium outside the material), `cosTheta` (angle between the surface normal and the incoming light), `base_f0` (base color of the material), and `iridescenceThickness` (thickness of the iridescent layer). The output is a color value representing the iridescence effect.\n\n3. What is the significance of the `material_iridescenceRefractionIndex` uniform variable?\n- This variable is used in the `calcIridescence` function to determine the refractive index of the iridescent layer. Its value can be set externally to control the strength of the iridescence effect.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/iridescenceDiffraction.md"}}],["599",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/lightDiffuseLambert.js)\n\nThe code above is a function that calculates the diffuse lighting of a given surface. It takes in four parameters: `worldNormal`, `viewDir`, `lightDir`, and `lightDirNorm`. \n\n`worldNormal` is a vector that represents the normal of the surface in world space. `viewDir` is a vector that represents the direction from the surface to the viewer's position. `lightDir` is a vector that represents the direction from the surface to the light source. Finally, `lightDirNorm` is a normalized vector that represents the direction of the light source.\n\nThe function calculates the diffuse lighting by taking the dot product of the `worldNormal` and the negated `lightDirNorm`. The negation is necessary because the dot product returns the cosine of the angle between the two vectors, and we want to calculate the angle between the `worldNormal` and the direction that the light is coming from. The `max` function is used to ensure that the result is always positive, as negative values would indicate that the surface is facing away from the light source.\n\nThis function is likely used in the larger PlayCanvas engine project to calculate the lighting of 3D models in a scene. It can be used in conjunction with other lighting functions to create realistic lighting effects. For example, the result of this function could be multiplied by the color of the light source to create a diffuse lighting effect on the surface. \n\nHere is an example of how this function could be used in a shader:\n\n```\nvec3 worldNormal = normalize(vNormal);\nvec3 viewDir = normalize(vViewDir);\nvec3 lightDir = normalize(uLightPos - vPosition);\nvec3 lightDirNorm = normalize(lightDir);\n\nfloat diffuse = getLightDiffuse(worldNormal, viewDir, lightDir, lightDirNorm);\nvec3 lightColor = vec3(1.0, 1.0, 1.0); // white light\nvec3 diffuseColor = lightColor * diffuse;\n\ngl_FragColor = vec4(diffuseColor, 1.0);\n```\n\nIn this example, `vNormal` and `vViewDir` are vertex shader outputs that represent the surface normal and the direction from the surface to the viewer's position, respectively. `uLightPos` is a uniform variable that represents the position of the light source. The result of `getLightDiffuse` is multiplied by the color of the light source to create a diffuse lighting effect, which is then used as the final color of the fragment.\n## Questions: \n 1. What does this code do?\n   - This code exports a GLSL function that calculates the diffuse lighting contribution of a light source on a surface given the surface normal, view direction, and light direction.\n\n2. What are the input parameters for this function?\n   - The input parameters for this function are `worldNormal`, which is a vec3 representing the surface normal in world space, `viewDir`, which is a vec3 representing the view direction in world space, `lightDir`, which is a vec3 representing the direction of the light source in world space, and `lightDirNorm`, which is a vec3 representing the normalized direction of the light source in world space.\n\n3. What is the expected output of this function?\n   - The expected output of this function is a float value representing the diffuse lighting contribution of the light source on the surface, which is calculated using the dot product between the surface normal and the negated normalized light direction, clamped to a minimum value of 0.0.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/lightDiffuseLambert.md"}}],["600",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/lightDirPoint.js)\n\nThe code above is a function that calculates the direction and position of a light source in a 3D space. The function takes in a parameter `lightPosW`, which is the position of the light source in world space. \n\nThe function first calculates the direction of the light source by subtracting the position of the fragment being rendered (`vPositionW`) from the position of the light source (`lightPosW`). This direction vector is stored in the variable `dLightDirW`. \n\nNext, the function normalizes the direction vector using the `normalize()` function, which returns a vector with the same direction but a magnitude of 1. This normalized direction vector is stored in the variable `dLightDirNormW`. \n\nFinally, the function stores the position of the light source in the variable `dLightPosW`. \n\nThis function is likely used in the larger PlayCanvas engine project to calculate the lighting for a 3D scene. By knowing the direction and position of the light source, the engine can calculate how much light should be applied to each fragment being rendered. \n\nHere is an example of how this function might be used in the PlayCanvas engine:\n\n```javascript\n// create a new instance of the PlayCanvas engine\nconst app = new pc.Application(canvas);\n\n// create a new entity to represent a 3D object in the scene\nconst entity = new pc.Entity();\nentity.addComponent('model', {\n    type: 'box'\n});\n\n// add the entity to the scene\napp.root.addChild(entity);\n\n// set the position of the entity\nentity.setPosition(0, 0, 0);\n\n// create a new light source entity\nconst light = new pc.Entity();\nlight.addComponent('light', {\n    type: 'point'\n});\n\n// add the light source to the scene\napp.root.addChild(light);\n\n// set the position of the light source\nlight.setPosition(5, 5, 5);\n\n// calculate the direction and position of the light source for each fragment being rendered\nentity.model.meshInstances.forEach((meshInstance) => {\n    meshInstance.material.onUpdateShader = function (options) {\n        options.uniforms.lightDirPoint = new pc.Vec4(light.getPosition().x, light.getPosition().y, light.getPosition().z, 1);\n        options.uniforms.update();\n    };\n});\n```\n\nIn this example, we create a new entity to represent a 3D box in the scene, and we add it to the root of the scene graph. We also create a new entity to represent a point light source, and we add it to the root of the scene graph. \n\nWe then use the `getLightDirPoint()` function to calculate the direction and position of the light source for each fragment being rendered. We do this by iterating over each mesh instance in the entity's model, and setting the `lightDirPoint` uniform in the material's shader to the direction and position of the light source. \n\nBy doing this, the PlayCanvas engine can calculate the lighting for each fragment being rendered based on the direction and position of the light source, resulting in a more realistic and visually appealing 3D scene.\n## Questions: \n 1. **What does this code do?** \nThis code defines a function called `getLightDirPoint` that calculates the direction and position of a directional light source in world space.\n\n2. **What is the purpose of the `/* glsl */` comment?** \nThis comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a high-level language used to write shaders for graphics processing units (GPUs).\n\n3. **What are the inputs and outputs of this function?** \nThe input of this function is a 3D vector representing the position of the light source in world space (`lightPosW`). There are no explicit outputs, but the function sets the values of three variables (`dLightDirW`, `dLightDirNormW`, and `dLightPosW`) that can be used in subsequent calculations.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/lightDirPoint.md"}}],["601",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/lightSheen.js)\n\nThe code above is a GLSL shader code that defines three functions for calculating the sheen effect on a material. The sheen effect is a type of specular highlight that appears on certain materials, such as silk or velvet, and is caused by the reflection of light on the fibers of the material.\n\nThe first function, `sheenD`, calculates the distribution of the sheen effect on the surface of the material. It takes in the surface normal, the halfway vector between the view direction and the light direction, and the roughness of the material as inputs. It first calculates the inverse of the roughness squared, which is used to control the spread of the sheen effect. It then calculates the cosine squared of the angle between the normal and the halfway vector, which is clamped to a minimum of 0.0. This value is then squared and subtracted from 1.0 to get the sine squared of the angle. This value is then raised to the power of half of the inverse roughness and multiplied by a factor of 2.0 plus the inverse roughness. Finally, the result is divided by 2.0 times PI to normalize it.\n\nThe second function, `sheenV`, calculates the visibility of the sheen effect between the view direction and the light direction. It takes in the surface normal, the view direction, and the light direction as inputs. It first calculates the maximum of the dot product between the normal and the view direction and a small value of 0.000001 to avoid division by zero. It then calculates the maximum of the dot product between the normal and the light direction and the same small value. It adds these two values together and subtracts their product from it, and then divides the result by 4.0 to normalize it.\n\nThe third function, `getLightSpecularSheen`, combines the results of the previous two functions to calculate the specular sheen contribution of a light source. It takes in the halfway vector, the surface normal, the view direction, the normalized light direction, and the sheen glossiness as inputs. It first calculates the distribution of the sheen effect using the `sheenD` function, and then calculates the visibility of the sheen effect using the `sheenV` function. It multiplies these two values together to get the final result.\n\nThis code can be used in a larger project that involves rendering materials with the sheen effect. It can be incorporated into a shader program that calculates the lighting and shading of a scene, and used to add the sheen effect to certain materials. For example, it can be used to render a silk dress or a velvet cushion with a realistic sheen effect. The `getLightSpecularSheen` function can be called for each light source in the scene to calculate the contribution of the sheen effect to the final color of the material.\n## Questions: \n 1. What is the purpose of this code?\n   \n   This code defines three functions for calculating sheen specular highlights in a physically-based rendering (PBR) system.\n\n2. What are the input parameters for each function?\n   \n   The `sheenD` function takes a surface normal, a half-vector, and a roughness value as input. The `sheenV` function takes a surface normal, a view direction, and a light direction as input. The `getLightSpecularSheen` function takes a half-vector, a surface normal, a view direction, a normalized light direction, and a sheen gloss value as input.\n\n3. What is the expected output of each function?\n   \n   The `sheenD` function returns a float value representing the sheen specular distribution for a given surface normal, half-vector, and roughness value. The `sheenV` function returns a float value representing the sheen specular visibility term for a given surface normal, view direction, and light direction. The `getLightSpecularSheen` function returns a float value representing the sheen specular contribution from a single light source for a given half-vector, surface normal, view direction, normalized light direction, and sheen gloss value.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/lightSheen.md"}}],["602",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/lightSpecularAnisoGGX.js)\n\nThe code provided is a GLSL shader code that calculates the specular lighting of a material using the Anisotropic GGX model. The purpose of this code is to provide a realistic and accurate way of calculating the specular highlights of a material based on its roughness and anisotropy properties. \n\nThe `calcLightSpecular` function takes in several parameters, including the glossiness of the material, the world normal, the view direction, the half vector, the normalized light direction, and the TBN matrix. It first calculates the roughness and anisotropy values based on the glossiness property. It then calculates the tangent and bitangent vectors of the material using the TBN matrix. \n\nNext, it calculates the NoH, ToH, and BoH values, which represent the dot products of the world normal with the half vector, the tangent vector with the half vector, and the bitangent vector with the half vector, respectively. These values are used to calculate the anisotropic distribution function (D) of the material. \n\nThe function then calculates the dot products of the view direction and the light direction with the tangent and bitangent vectors, as well as the world normal. These values are used to calculate the lambdaV and lambdaL values, which represent the mean free path of the view and light directions, respectively. Finally, the function calculates the geometry attenuation factor (G) based on these values. \n\nThe `getLightSpecular` function simply calls the `calcLightSpecular` function with the provided parameters and returns the result. \n\nThis code is likely used in the larger PlayCanvas engine project to provide realistic lighting for 3D models and scenes. It can be used in conjunction with other shaders and materials to create a visually appealing and accurate representation of a scene. \n\nExample usage of this code would involve passing in the necessary parameters to the `getLightSpecular` function, such as the half vector, the reflection direction, the world normal, the view direction, the normalized light direction, and the glossiness of the material. The function would then return the specular lighting value for that particular pixel or vertex.\n## Questions: \n 1. What is the purpose of this code?\n- This code calculates the specular lighting for a material using an anisotropic GGX model.\n\n2. What are the inputs and outputs of the `calcLightSpecular` function?\n- The inputs are `gloss` (specular glossiness), `worldNormal` (surface normal in world space), `viewDir` (view direction in world space), `h` (half vector between view and light directions), `lightDirNorm` (normalized light direction in world space), and `tbn` (tangent space matrix). The output is the specular lighting value.\n\n3. What is the role of the `material_anisotropy` variable in this code?\n- The `material_anisotropy` variable is used to control the anisotropy of the specular highlight. It is multiplied by the roughness value to determine the anisotropy value used in the calculation.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/lightSpecularAnisoGGX.md"}}],["603",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/lightSpecularBlinn.js)\n\nThe code provided is a GLSL shader function that calculates the specular lighting contribution for a given surface point. The function is called `getLightSpecular` and takes in several parameters, including the half-vector `h`, the reflection direction `reflDir`, the surface normal `worldNormal`, the view direction `viewDir`, the normalized light direction `lightDirNorm`, and the glossiness value `gloss`. The function returns a float value that represents the specular lighting contribution for the given surface point.\n\nThe `calcLightSpecular` function is called within `getLightSpecular` and is responsible for calculating the specular power value based on the glossiness input. The specular power is then used to calculate the specular lighting contribution using the Blinn-Phong lighting model. The Blinn-Phong model is an energy-conserving lighting model that approximates the behavior of real-world materials by using a combination of diffuse and specular lighting components.\n\nThe `getLightSpecular` function is likely used in the larger PlayCanvas engine project to calculate the specular lighting contribution for each surface point in a scene. This function would be called for each light source in the scene and the resulting specular lighting contributions would be combined to produce the final lighting for the scene.\n\nHere is an example of how the `getLightSpecular` function might be used in a shader program:\n\n```glsl\nuniform vec3 lightDir;\nuniform vec3 viewDir;\nuniform vec3 worldNormal;\nuniform float gloss;\n\nvoid main() {\n    vec3 reflDir = reflect(-viewDir, worldNormal);\n    vec3 h = normalize(lightDir + viewDir);\n    mat3 tbn = mat3(worldNormal, cross(worldNormal, vec3(0.0, 0.0, 1.0)), cross(worldNormal, vec3(0.0, 1.0, 0.0)));\n    float specular = getLightSpecular(h, reflDir, worldNormal, viewDir, normalize(lightDir), gloss, tbn);\n    // combine specular with other lighting components to produce final color\n}\n```\n\nIn this example, the `getLightSpecular` function is called with the appropriate parameters to calculate the specular lighting contribution for a given surface point. The resulting specular value is then combined with other lighting components to produce the final color for the surface point.\n## Questions: \n 1. What is the purpose of this code?\n- This code calculates the specular lighting for a material using the Blinn-Phong model.\n\n2. What are the parameters required for the `getLightSpecular` function?\n- The `getLightSpecular` function requires the half vector `h`, reflection direction `reflDir`, world normal `worldNormal`, view direction `viewDir`, normalized light direction `lightDirNorm`, glossiness `gloss`, and a TBN matrix `tbn`.\n\n3. Why is there a hack for the `specPow` variable on Mac OS X?\n- Calling `pow` with a zero exponent on Mac OS X generates artifacts, so the `specPow` variable is biased up slightly to avoid this issue.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/lightSpecularBlinn.md"}}],["604",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/lightSpecularPhong.js)\n\nThe code provided is a GLSL shader code that calculates the specular lighting of a 3D object. The code exports a function called `getLightSpecular` that takes in several parameters including the half vector `h`, reflection direction `reflDir`, world normal `worldNormal`, view direction `viewDir`, normalized light direction `lightDirNorm`, glossiness `gloss`, and a transformation matrix `tbn`. \n\nThe `getLightSpecular` function calls another function called `calcLightSpecular` which calculates the specular lighting intensity based on the glossiness of the material and the angle between the reflection direction and the light direction. The `calcLightSpecular` function takes in the glossiness, reflection direction, and normalized light direction as parameters and returns the specular lighting intensity.\n\nThe purpose of this code is to provide a way to calculate the specular lighting of a 3D object in a shader program. This function can be used in a larger project that involves rendering 3D objects with realistic lighting effects. The `getLightSpecular` function can be called for each light source in the scene to calculate the total specular lighting intensity for the object.\n\nHere is an example of how this code can be used in a larger project:\n\n```glsl\n// vertex shader\nattribute vec3 aPosition;\nattribute vec3 aNormal;\n\nuniform mat4 uModelMatrix;\nuniform mat4 uViewMatrix;\nuniform mat4 uProjectionMatrix;\n\nvarying vec3 vNormal;\nvarying vec3 vViewDir;\nvarying vec3 vReflDir;\n\nvoid main() {\n    vec4 worldPos = uModelMatrix * vec4(aPosition, 1.0);\n    vec4 viewPos = uViewMatrix * worldPos;\n    gl_Position = uProjectionMatrix * viewPos;\n\n    vec3 worldNormal = normalize(mat3(uModelMatrix) * aNormal);\n    vNormal = worldNormal;\n\n    vec3 viewDir = normalize(-viewPos.xyz);\n    vViewDir = viewDir;\n\n    vec3 reflDir = reflect(viewDir, worldNormal);\n    vReflDir = reflDir;\n}\n\n// fragment shader\nprecision highp float;\n\nuniform vec3 uSpecularColor;\nuniform float uGlossiness;\n\nvarying vec3 vNormal;\nvarying vec3 vViewDir;\nvarying vec3 vReflDir;\n\nfloat calcLightSpecular(float gloss, vec3 reflDir, vec3 lightDirNorm) {\n    float specPow = gloss;\n    return pow(max(dot(reflDir, -lightDirNorm), 0.0), specPow + 0.0001);\n}\n\nfloat getLightSpecular(vec3 h, vec3 reflDir, vec3 worldNormal, vec3 viewDir, vec3 lightDirNorm, float gloss, mat3 tbn) {\n    return calcLightSpecular(gloss, reflDir, lightDirNorm);\n}\n\nvoid main() {\n    vec3 worldNormal = normalize(vNormal);\n    vec3 viewDir = normalize(vViewDir);\n    vec3 reflDir = normalize(vReflDir);\n\n    vec3 lightDir = normalize(vec3(1.0, 1.0, 1.0));\n    vec3 lightDirNorm = normalize(lightDir);\n\n    float specular = getLightSpecular(vec3(0.0), reflDir, worldNormal, viewDir, lightDirNorm, uGlossiness, mat3(1.0));\n    vec3 specularColor = uSpecularColor * specular;\n\n    gl_FragColor = vec4(specularColor, 1.0);\n}\n```\n\nIn this example, the vertex shader calculates the world normal, view direction, and reflection direction for each vertex of the 3D object. These values are passed to the fragment shader where the `getLightSpecular` function is called to calculate the specular lighting intensity for a single light source. The final color of the fragment is determined by multiplying the specular color with the specular lighting intensity. This process is repeated for each light source in the scene to calculate the total specular lighting intensity for the object.\n## Questions: \n 1. What does this code do?\n    \n    This code calculates the specular lighting for a given gloss value and light direction, using the hack to avoid artifacts on Mac OS X.\n\n2. What is the input and output of the `calcLightSpecular` function?\n    \n    The input of the `calcLightSpecular` function is the gloss value, reflection direction, and normalized light direction. The output is the specular lighting value.\n\n3. What is the purpose of the `getLightSpecular` function and how does it use the `calcLightSpecular` function?\n    \n    The `getLightSpecular` function calculates the specular lighting for a given half vector, reflection direction, world normal, view direction, light direction, gloss value, and TBN matrix. It uses the `calcLightSpecular` function to calculate the specular lighting value based on the gloss value, reflection direction, and normalized light direction. However, it does not use the other input parameters in this function.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/lightSpecularPhong.md"}}],["605",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/lightmapAdd.js)\n\nThe code provided is a GLSL shader function called `addLightMap`. This function takes in several parameters, including `lightmap`, `dir`, `worldNormal`, `viewDir`, `reflectionDir`, `gloss`, `specularity`, `vertexNormal`, and `tbn`. These parameters are used to calculate the diffuse lighting of a 3D object in a scene.\n\nThe `lightmap` parameter is a texture that contains precomputed lighting information for the object. The `dir` parameter is the direction of the light source, while `worldNormal` is the normal vector of the object's surface in world space. `viewDir` is the direction from the camera to the object, while `reflectionDir` is the direction of the reflected light. `gloss` is the glossiness of the object's surface, while `specularity` is the color of the specular highlights on the surface. `vertexNormal` is the normal vector of the object's surface at the current vertex, while `tbn` is the tangent, bitangent, and normal matrix that transforms the surface normal from object space to world space.\n\nThe function then adds the `lightmap` value to the `dDiffuseLight` variable, which is used to accumulate the diffuse lighting for the object.\n\nOptionally, if the `LIT_IRIDESCENCE` flag is defined, the function also takes in `iridescenceFresnel` and `iridescence` parameters. These are used to calculate the iridescence effect on the object's surface.\n\nOverall, this function is an important part of the PlayCanvas engine's rendering pipeline, as it calculates the diffuse lighting for 3D objects in a scene. It can be used in conjunction with other shader functions to create realistic lighting effects for games and other interactive applications. Here is an example of how this function might be used in a shader:\n\n```\n#pragma glslify: addLightMap = require('path/to/addLightMap.glsl')\n\nvoid main() {\n  vec3 lightmap = texture2D(uLightMap, vTexCoord).rgb;\n  vec3 dir = normalize(uLightDir);\n  vec3 worldNormal = normalize(vWorldNormal);\n  vec3 viewDir = normalize(vViewDir);\n  vec3 reflectionDir = reflect(-dir, worldNormal);\n  float gloss = uGloss;\n  vec3 specularity = uSpecularity;\n  vec3 vertexNormal = normalize(vNormal);\n  mat3 tbn = mat3(vTangent, vBitangent, vNormal);\n\n  addLightMap(lightmap, dir, worldNormal, viewDir, reflectionDir, gloss, specularity, vertexNormal, tbn);\n}\n```\n## Questions: \n 1. What does this code do?\n   This code defines a function called `addLightMap` that takes in various parameters related to lighting and adds the `lightmap` value to `dDiffuseLight`.\n\n2. What is the purpose of the `#if defined(LIT_IRIDESCENCE)` section?\n   This section is a preprocessor directive that checks if the `LIT_IRIDESCENCE` macro is defined. If it is defined, then the code within the `#if` block will be included in the final code. Otherwise, it will be excluded.\n\n3. What is the data type of the `tbn` parameter?\n   The `tbn` parameter is of type `mat3`, which is a 3x3 matrix of floating point values. It is used to transform vectors from tangent space to world space.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/lightmapAdd.md"}}],["606",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/lightmapDirAdd.js)\n\nThe code provided is a GLSL shader function called `addLightMap` that calculates the lighting for a given fragment in a 3D scene. The function takes in several parameters, including the lightmap color, the direction of the light source, the surface normal, the view direction, the reflection direction, the glossiness, the specularity, and the TBN matrix. \n\nThe function first checks if the light direction is close to zero, in which case it adds the lightmap color to the diffuse light. Otherwise, it calculates the diffuse light by taking the dot product of the light direction and the vertex normal, and then dividing it by the maximum of the dot product of the light direction and the view direction and a small value of 0.01. This value is then multiplied by the lightmap color and added to the diffuse light. \n\nNext, the function calculates the specular light by computing the half vector between the light direction and the view direction, and then passing it along with the reflection direction, surface normal, view direction, light direction, glossiness, and TBN matrix to the `getLightSpecular` function. The result is then multiplied by the fresnel term, which is computed using the dot product of the view direction and the half vector, the glossiness, and the specularity. If the `LIT_IRIDESCENCE` macro is defined, the fresnel term is also multiplied by the iridescence fresnel and iridescence arguments. Finally, the specular light is added to the total specular light.\n\nThis function is likely used in the larger PlayCanvas engine project to calculate the lighting for each fragment in a 3D scene. It is called by other shader functions that are responsible for rendering different types of materials, such as diffuse, specular, and transparent materials. By providing the necessary parameters, this function can accurately calculate the lighting for each fragment, which is essential for creating realistic and immersive 3D graphics. \n\nExample usage:\n\n```glsl\nuniform vec3 lightColor;\nuniform vec3 lightDirection;\nuniform vec3 surfaceNormal;\nuniform vec3 viewDirection;\nuniform vec3 reflectionDirection;\nuniform float glossiness;\nuniform vec3 specularity;\nuniform vec3 vertexNormal;\nuniform mat3 TBN;\n\nvec3 lightmap = lightColor * computeShadowFactor(); // computeShadowFactor() is a function that returns the shadow factor for the current fragment\nvec3 iridescenceFresnel = vec3(0.1, 0.2, 0.3);\nIridescenceArgs iridescence = computeIridescence(); // computeIridescence() is a function that returns the iridescence arguments for the current fragment\n\naddLightMap(lightmap, lightDirection, surfaceNormal, viewDirection, reflectionDirection, glossiness, specularity, vertexNormal, TBN, iridescenceFresnel, iridescence);\n```\n## Questions: \n 1. What is the purpose of this code and where is it used in the PlayCanvas engine?\n- This code defines a function called `addLightMap` which is likely used to add lighting effects to a 3D scene in the PlayCanvas engine.\n\n2. What are the parameters of the `addLightMap` function and what do they represent?\n- The function takes in several parameters including `lightmap`, `dir`, `worldNormal`, `viewDir`, `reflectionDir`, `gloss`, `specularity`, `vertexNormal`, and `tbn`. These parameters likely represent various properties of the lighting and materials being used in the scene.\n\n3. What is the purpose of the conditional statement in the function and what does it do?\n- The conditional statement checks if the dot product of `dir` and itself is less than 0.0001. If it is, then `lightmap` is added to `dDiffuseLight`. Otherwise, the function calculates various lighting values and adds them to `dDiffuseLight` and `dSpecularLight`. The purpose of this conditional statement is likely to optimize the function by skipping unnecessary calculations when `dir` is close to zero.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/lightmapDirAdd.md"}}],["607",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/metalnessModulate.js)\n\nThe code above is a shader function that is used to modify the metalness of a material in the PlayCanvas engine. The function takes in a uniform float value called `material_f0` and an `inout` variable called `litShaderArgs` of type `LitShaderArguments`. \n\nThe `getMetalnessModulate` function first calculates the dielectric F0 value by multiplying the `material_f0` with the `specularity` value of the `litShaderArgs`. The `specularity` value is then modified by using the `mix` function to interpolate between the dielectric F0 value and the `albedo` value of the `litShaderArgs` based on the `metalness` value of the `litShaderArgs`. The `albedo` value is also modified by multiplying it with `1.0 - litShaderArgs.metalness`.\n\nThis function is used in the larger PlayCanvas engine project to modify the metalness of a material in a shader. The `LitShaderArguments` variable is a struct that contains various properties of a material such as `albedo`, `specularity`, `metalness`, etc. The `getMetalnessModulate` function is called in the shader code to modify the `LitShaderArguments` variable and thus modify the metalness of the material.\n\nHere is an example of how this function can be used in a shader:\n\n```\nuniform float material_f0;\n\nvarying vec3 vAlbedo;\nvarying vec3 vSpecularity;\nvarying float vMetalness;\n\nvoid main() {\n    LitShaderArguments litShaderArgs;\n    litShaderArgs.albedo = vAlbedo;\n    litShaderArgs.specularity = vSpecularity;\n    litShaderArgs.metalness = vMetalness;\n\n    getMetalnessModulate(litShaderArgs);\n\n    // use the modified litShaderArgs to render the material\n    // ...\n}\n```\n\nIn the example above, the `getMetalnessModulate` function is called with the `LitShaderArguments` variable initialized with the `vAlbedo`, `vSpecularity`, and `vMetalness` values passed in as varying variables. The modified `litShaderArgs` variable can then be used to render the material in the shader.\n## Questions: \n 1. What is the purpose of the `getMetalnessModulate` function?\n- The `getMetalnessModulate` function modifies the `specularity` and `albedo` properties of the `LitShaderArguments` object based on the `material_f0` and `metalness` values.\n\n2. What is the data type of the `material_f0` uniform?\n- The `material_f0` uniform is a float data type.\n\n3. What does the `mix` function do in this code?\n- The `mix` function performs a linear interpolation between two values (`dielectricF0` and `litShaderArgs.albedo`) based on the `metalness` value.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/metalnessModulate.md"}}],["608",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/outputAlpha.js)\n\nThis code is a fragment shader written in GLSL (OpenGL Shading Language) and exported as a default module. The purpose of this shader is to set the alpha (transparency) value of the fragment's color to the value of the `opacity` property passed in as an argument. \n\nIn the context of the PlayCanvas engine, this shader can be used to render transparent objects in a 3D scene. For example, if a developer wants to create a glass object that is partially transparent, they can apply this shader to the object's material and set the `opacity` property to a value between 0 and 1. \n\nHere is an example of how this shader can be used in a PlayCanvas project:\n\n```javascript\n// create a new material\nvar material = new pc.StandardMaterial();\n\n// set the material's shader to the default fragment shader\nmaterial.fragmentShader = pc.shaderChunks.defaultFragment;\n\n// set the material's opacity to 0.5 (50% transparent)\nmaterial.opacity = 0.5;\n\n// create a new entity with a model component\nvar entity = new pc.Entity();\nentity.addComponent('model', {\n    type: 'box',\n    material: material\n});\n\n// add the entity to the scene\napp.root.addChild(entity);\n```\n\nIn this example, a new `StandardMaterial` is created and its `fragmentShader` property is set to the default fragment shader. The `opacity` property is then set to 0.5 to make the material 50% transparent. Finally, a new entity is created with a `model` component that uses the material, and the entity is added to the scene.\n\nOverall, this code is a simple but important part of the PlayCanvas engine's rendering pipeline, allowing developers to create transparent objects in their 3D scenes.\n## Questions: \n 1. What is the purpose of this code?\n   - This code sets the alpha value of the fragment color to the opacity value passed in as an argument.\n\n2. What is the data type of `litShaderArgs.opacity`?\n   - It is not clear from this code snippet what the data type of `litShaderArgs.opacity` is. It could be a float, integer, or some other data type.\n\n3. Where is this code used within the PlayCanvas engine?\n   - Without additional context, it is unclear where this code is used within the PlayCanvas engine. It could be part of a shader program or used in some other context.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/outputAlpha.md"}}],["609",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/outputAlphaOpaque.js)\n\nThis code exports a default GLSL fragment shader that sets the alpha value of the output color to 1.0, making it fully opaque. \n\nIn the context of the PlayCanvas engine, this shader could be used to render objects with solid colors or textures without any transparency. It may be applied to materials used in 3D models or UI elements. \n\nHere is an example of how this shader could be used in a PlayCanvas project:\n\n```javascript\n// create a new material with the opaque shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.diffusePS = /* glsl */`\n    gl_FragColor.rgb = texture2D(uDiffuseMap, vUv0).rgb;\n    gl_FragColor.a = 1.0;\n`;\n\n// assign the material to a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n```\n\nIn this example, a new `StandardMaterial` is created and its `diffusePS` property is set to the opaque shader code. This material is then assigned to a `MeshInstance` which is attached to a node in the scene. When the scene is rendered, the mesh will be drawn with fully opaque colors. \n\nOverall, this code provides a simple and reusable shader for rendering opaque objects in PlayCanvas projects.\n## Questions: \n 1. What is the purpose of this code?\n   \n   This code sets the alpha value of the output color to 1.0 in a GLSL fragment shader.\n\n2. What is the expected input to this code?\n   \n   This code is a standalone GLSL fragment shader and does not have any input dependencies. \n\n3. Can this code be used in other rendering engines or is it specific to PlayCanvas?\n   \n   This code is not specific to PlayCanvas and can be used in any rendering engine that supports GLSL fragment shaders.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/outputAlphaOpaque.md"}}],["610",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/outputAlphaPremul.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) that is used to modify the color and opacity of a fragment in a 3D scene. The purpose of this code is to apply lighting effects to the fragment based on the values passed in the `litShaderArgs` object. \n\nThe first line of the code multiplies the RGB values of the `gl_FragColor` variable (which represents the color of the fragment) by the `opacity` value passed in `litShaderArgs`. This means that the fragment's color will be dimmed or brightened based on the opacity value. \n\nThe second line of the code sets the alpha value of `gl_FragColor` to the `opacity` value passed in `litShaderArgs`. This means that the fragment's transparency will be adjusted based on the opacity value. \n\nThis code can be used in the larger PlayCanvas engine project to create custom lighting effects for 3D scenes. For example, if a developer wants to create a scene where certain objects are dimmed or brightened based on their distance from a light source, they can use this code to modify the fragment colors accordingly. \n\nHere is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// create a new material with a custom shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.customFragmentShader = /* glsl */`\n    // custom lighting code here\n`;\n\n// set the material's opacity value\nmaterial.opacity = 0.5;\n\n// apply the material to a mesh instance\nvar meshInstance = new pc.MeshInstance(graphicsDevice, mesh, material);\n``` \n\nIn this example, the developer creates a new material with a custom shader that includes the code shown above. They can then set the material's opacity value to adjust the transparency of the mesh instance. When the scene is rendered, the custom shader will be used to modify the fragment colors and opacity based on the values passed in `litShaderArgs`.\n## Questions: \n 1. What is the purpose of the `litShaderArgs` variable?\n- It is unclear from this code snippet what `litShaderArgs` is and how it is defined. It may be necessary to look at other parts of the code to understand its purpose.\n\n2. What does the `/* glsl */` comment indicate?\n- This comment likely indicates that the code following it is written in GLSL (OpenGL Shading Language), which is used to write shaders for graphics processing.\n\n3. What is the expected behavior of this code?\n- Based on the code, it appears that the RGB values of `gl_FragColor` are being multiplied by the `opacity` value from `litShaderArgs`, and then the alpha value of `gl_FragColor` is set to the same `opacity` value. However, without more context it is unclear what the overall effect of this code is on the rendering process.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/outputAlphaPremul.md"}}],["611",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflDir.js)\n\nThe code above is a shader function that calculates the reflection direction of a given view direction and world normal. It takes in four parameters: `worldNormal`, `viewDir`, `gloss`, and `tbn`. \n\n`worldNormal` is a 3D vector representing the normal of the surface in world space. `viewDir` is a 3D vector representing the direction from the camera to the surface. `gloss` is a float value representing the glossiness of the surface, which affects the sharpness of the reflection. `tbn` is a 3x3 matrix representing the tangent, bitangent, and normal vectors of the surface.\n\nThe function uses the `reflect` function to calculate the reflection direction of `viewDir` with respect to `worldNormal`. It then normalizes the resulting vector and stores it in a variable called `dReflDirW`.\n\nThis function is likely used in the larger PlayCanvas engine project to generate reflections on surfaces in a 3D scene. It can be used in a shader program to calculate the reflection vector for each pixel on a reflective surface. This reflection vector can then be used to sample the environment map and generate a reflection for that pixel.\n\nHere is an example of how this function could be used in a shader program:\n\n```\nuniform samplerCube envMap;\n\nvoid main() {\n    vec3 worldNormal = normalize(vNormal);\n    vec3 viewDir = normalize(vViewDir);\n    mat3 tbn = mat3(vTangent, vBitangent, vNormal);\n\n    getReflDir(worldNormal, viewDir, gloss, tbn);\n\n    vec3 reflColor = texture(envMap, dReflDirW).rgb;\n    gl_FragColor = vec4(reflColor, 1.0);\n}\n```\n\nIn this example, `envMap` is a cube map texture representing the environment surrounding the reflective surface. `vNormal`, `vViewDir`, `vTangent`, `vBitangent`, and `vNormal` are vertex attributes passed in from the vertex shader. The `getReflDir` function is called to calculate the reflection direction, which is then used to sample the environment map and generate the final reflection color for the pixel.\n## Questions: \n 1. What does this code do?\n   This code defines a function called `getReflDir` that takes in a world normal vector, a view direction vector, a gloss value, and a transformation matrix, and calculates a reflected direction vector.\n\n2. What is the purpose of the `/* glsl */` comment?\n   This comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a high-level language used to write shaders for graphics processing units (GPUs).\n\n3. What is the data type of `dReflDirW`?\n   It is not clear from this code snippet what the data type of `dReflDirW` is. It is likely defined elsewhere in the codebase or passed in as a parameter to the function.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflDir.md"}}],["612",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflDirAniso.js)\n\nThis code exports a GLSL shader function called `getReflDir` that calculates the reflection direction of a given view direction and surface normal, taking into account the material's glossiness and anisotropy. The function takes four parameters: `worldNormal`, `viewDir`, `gloss`, and `tbn`.\n\n`worldNormal` is a 3D vector representing the surface normal in world space. `viewDir` is a 3D vector representing the direction from the camera to the surface point being shaded. `gloss` is a float value between 0 and 1 representing the glossiness of the material, where 0 is completely matte and 1 is completely reflective. `tbn` is a 3x3 matrix representing the tangent, bitangent, and normal vectors of the surface in tangent space.\n\nThe function first calculates the roughness of the material based on the glossiness value. It then calculates the anisotropy of the material by multiplying the roughness by a global `material_anisotropy` value. If `material_anisotropy` is positive, the anisotropic direction is set to the second column of the `tbn` matrix, otherwise it is set to the first column. \n\nThe function then calculates the anisotropic tangent and normal vectors by taking the cross product of the anisotropic direction and the view direction. It then calculates a \"bent normal\" vector by interpolating between the surface normal and the anisotropic normal based on the anisotropy value. Finally, it calculates the reflection direction by reflecting the negative of the view direction around the bent normal.\n\nThis function is likely used in the larger PlayCanvas engine project to calculate the reflection direction of a surface for use in various shading and lighting calculations. It may be used in conjunction with other shader functions to create realistic lighting and reflections in 3D scenes. Here is an example of how this function might be used in a shader:\n\n```\nvec3 reflDir = vec3(0.0);\ngetReflDir(worldNormal, viewDir, gloss, tbn);\nreflDir = dReflDirW;\n```\n\nIn this example, `worldNormal`, `viewDir`, `gloss`, and `tbn` are all variables representing the surface normal, view direction, glossiness, and tangent space of the surface being shaded. The `getReflDir` function is called with these variables, and the resulting reflection direction is stored in the `reflDir` variable for use in other shader calculations.\n## Questions: \n 1. What does this code do?\n   \n   This code defines a function called `getReflDir` that calculates the reflection direction of a given view direction and world normal, taking into account material gloss, anisotropy, and a TBN matrix.\n\n2. What is the purpose of the `/* glsl */` comment at the beginning of the code?\n\n   This comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax, which is a C-like language used for programming shaders in graphics applications.\n\n3. What is the `dReflDirW` variable used for?\n\n   It is unclear from this code snippet what the `dReflDirW` variable is used for, as it is not defined or used anywhere else in the code. It is possible that it is a typo or a variable that is used in another part of the code.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflDirAniso.md"}}],["613",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflectionCC.js)\n\nThe code above is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to add a clear coat reflection to a 3D model. \n\nThe code is written in GLSL, which is a shading language used for creating shaders that run on the GPU. The code is exported as a default module, which means that it can be imported and used in other parts of the project.\n\nThe code defines a function called `addReflectionCC` that takes two parameters: `reflDir` and `gloss`. `reflDir` is a vector that represents the direction of the reflection, and `gloss` is a float that represents the glossiness of the reflection. \n\nThe function checks if the `LIT_CLEARCOAT` flag is defined. If it is, the function adds a clear coat reflection to the `ccReflection` variable by calling the `calcReflection` function with the `reflDir` and `gloss` parameters. \n\nThis code is used in the larger PlayCanvas engine project to add realistic reflections to 3D models. The `addReflectionCC` function can be called from other parts of the project to add clear coat reflections to a model. For example, the following code could be used to add a clear coat reflection to a model:\n\n```\n// create a material for the model\nvar material = new pc.StandardMaterial();\n\n// set the clear coat reflection flag\nmaterial.litClearcoat = true;\n\n// set the clear coat glossiness\nmaterial.clearcoatGlossiness = 0.5;\n\n// set the clear coat reflection direction\nvar reflectionDir = new pc.Vec3(0, 1, 0);\n\n// add the clear coat reflection\naddReflectionCC(reflectionDir, material.clearcoatGlossiness);\n``` \n\nOverall, this code is an important part of the PlayCanvas engine project that enables developers to create realistic 3D models with clear coat reflections.\n## Questions: \n 1. What is the purpose of the `#ifdef LIT_CLEARCOAT` preprocessor directive?\n   - The `#ifdef LIT_CLEARCOAT` directive is used to conditionally compile the code block that follows it only if the `LIT_CLEARCOAT` macro is defined.\n\n2. What does the `addReflectionCC` function do?\n   - The `addReflectionCC` function adds the result of calculating the reflection of a given direction and glossiness to the `ccReflection` variable.\n\n3. What is the `calcReflection` function and where is it defined?\n   - The `calcReflection` function is not defined in this code snippet and must be defined elsewhere in the codebase. It is used to calculate the reflection of a given direction and glossiness.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflectionCC.md"}}],["614",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflectionCube.js)\n\nThe code above is a GLSL shader code that is used to calculate and add reflections to a 3D object in the PlayCanvas engine. The code exports a function that takes in two uniform variables, a samplerCube texture_cubeMap and a float material_reflectivity. \n\nThe calcReflection function takes in a reflection direction vector and a gloss value and returns a color value that represents the reflection of the object in the given direction. The function first projects the reflection direction onto the cube map using the cubeMapProject function and then fixes any seams that may appear in the cube map using the fixSeams function. The lookup vector is then negated in the x-axis and used to sample the texture_cubeMap using the textureCube function. The resulting color value is then decoded using the DECODE function and returned.\n\nThe addReflection function takes in a reflection direction vector and a gloss value and adds the calculated reflection color to the dReflection variable. The dReflection variable is a vec4 that represents the final color of the object after all the lighting calculations have been performed. The material_reflectivity value is used to control the strength of the reflection.\n\nThis code can be used in the larger PlayCanvas engine project to add realistic reflections to 3D objects in a scene. The shader code can be attached to a material and applied to a mesh to create a reflective surface. The material_reflectivity value can be adjusted to control the strength of the reflection and the gloss value can be adjusted to control the sharpness of the reflection. \n\nExample usage:\n\n```javascript\n// create a new material\nvar material = new pc.StandardMaterial();\n\n// set the texture_cubeMap and material_reflectivity uniforms\nmaterial.setParameter('texture_cubeMap', cubeMapTexture);\nmaterial.setParameter('material_reflectivity', 0.5);\n\n// attach the shader code to the material\nmaterial.chunks.reflectPS = shaderCode;\n\n// apply the material to a mesh\nmeshInstance.material = material;\n```\n## Questions: \n 1. What is the purpose of the `calcReflection` function?\n- The `calcReflection` function calculates the reflection of a given direction vector based on a provided cube map texture and gloss value.\n\n2. What is the `addReflection` function used for?\n- The `addReflection` function adds the reflection of a given direction vector to the `dReflection` variable, taking into account the material's reflectivity value.\n\n3. What is the meaning of the `/* glsl */` comment at the beginning of the code?\n- The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax, which is used for writing shaders in WebGL and other graphics applications.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflectionCube.md"}}],["615",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflectionEnv.js)\n\nThe code is a GLSL shader that calculates reflections for a 3D environment. It takes in a reflection direction and a gloss value as inputs and outputs a color value that represents the reflection of the environment at that direction. The shader uses an environment atlas texture that contains pre-rendered images of the environment from different angles. The atlas is accessed using texture2D() function and the texture_envAtlas uniform variable.\n\nThe shinyMipLevel() function calculates the mip level of the atlas texture to use for the shiny reflection. It takes in the equirectangular coordinates of the reflection direction and calculates the second derivative of the coordinates to handle the discontinuity at the azimuthal edge. It then calculates the maximum of both sets of derivatives and uses it to calculate the mip level using a logarithmic function.\n\nThe calcReflection() function takes in the reflection direction and gloss value and calculates the reflection color. It first projects the reflection direction onto a cube map using the cubeMapProject() function and then converts it to spherical coordinates using the toSphericalUv() function. It then calculates the roughness level based on the gloss value and performs a manual mipmap lookup to access the shiny reflection if the roughness level is 0, or the rough reflection otherwise. It uses the mapShinyUv() and mapRoughnessUv() functions to map the spherical coordinates to the atlas texture coordinates. It then samples the atlas texture using texture2D() function and decodes the linear color value using the $DECODE() function. It mixes the linear color values based on the roughness level and returns the processed environment color.\n\nThe addReflection() function takes in the reflection direction and gloss value and adds the reflection color to the dReflection variable with the material_reflectivity factor. This function is likely called by other parts of the PlayCanvas engine to render reflections in the 3D environment.\n\nOverall, this code is an essential part of the PlayCanvas engine that enables realistic reflections in the 3D environment. It uses advanced techniques such as manual mipmap lookup and logarithmic functions to achieve high-quality results. Developers can use this code as a reference to implement reflections in their own projects or modify it to suit their specific needs.\n## Questions: \n 1. What is the purpose of the `texture_envAtlas` uniform and how is it used in the code?\n   \n   The `texture_envAtlas` uniform is a 2D texture sampler used to store environment maps. It is used to calculate reflections and is accessed multiple times in the `calcReflection` function.\n\n2. What is the significance of the `material_reflectivity` uniform and how does it affect the output of the `addReflection` function?\n\n   The `material_reflectivity` uniform is a float value that determines the strength of the reflection. It is added to the `dReflection` vector in the `addReflection` function to create the final reflection output.\n\n3. What is the purpose of the `shinyMipLevel` function and how is it used in the code?\n\n   The `shinyMipLevel` function calculates the mip level for shiny reflections based on the equirectangular coordinates. It is used in the `calcReflection` function to perform manual mipmap lookup for the shiny (top level) reflection.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflectionEnv.md"}}],["616",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflectionEnvHQ.js)\n\nThe code is a GLSL shader that calculates reflections for a 3D scene. It takes in a cube map texture and a material reflectivity value as inputs, and outputs a color value that represents the reflection of the scene in the given direction. The shader uses an environment atlas texture to simulate different levels of roughness in the reflections.\n\nThe `calcReflection` function takes in a reflection direction vector and a gloss value, and returns a color value that represents the reflection of the scene in that direction. It first projects the reflection direction onto the cube map using the `cubeMapProject` function, and then converts the resulting direction vector into spherical coordinates using the `toSphericalUv` function. It then calculates the roughness level based on the gloss value, and uses this level to sample two roughness values from the environment atlas texture. These values are then blended together based on the fractional part of the roughness level, and the resulting color is passed through the `processEnvironment` function to apply any additional effects.\n\nThe `addReflection` function takes in a reflection direction vector and a gloss value, and adds the resulting reflection color to the `dReflection` variable. This variable is likely used elsewhere in the larger project to apply reflections to the scene.\n\nOverall, this code is an important part of the PlayCanvas engine's rendering pipeline, as it allows for realistic reflections to be calculated and applied to the scene. It demonstrates the use of cube maps and environment atlases to simulate reflections with varying levels of roughness, and provides a useful example of how GLSL shaders can be used to create complex visual effects in real-time applications.\n## Questions: \n 1. What does this code do?\n   \n   This code defines GLSL shader code for calculating reflections and adding them to a material in the PlayCanvas engine. It uses a texture atlas and a cube map to calculate the reflection based on the glossiness of the material.\n\n2. What is the purpose of the `texture_envAtlas` uniform?\n   \n   The `texture_envAtlas` uniform is used to sample a texture atlas that contains pre-filtered environment maps at different roughness levels. These maps are used to calculate the reflection of the material based on its glossiness.\n\n3. What is the role of the `addReflection` function?\n   \n   The `addReflection` function is used to add the calculated reflection to the material. It takes a reflection direction and a glossiness value as input, and uses the `calcReflection` function to calculate the reflection color. The resulting color is then added to the material's diffuse reflection using the `dReflection` variable.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflectionEnvHQ.md"}}],["617",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflectionSheen.js)\n\nThe code provided is a GLSL shader function that adds a reflection sheen effect to a 3D object. The function takes in three parameters: `worldNormal`, `viewDir`, and `gloss`. \n\n`worldNormal` is a vector that represents the surface normal of the object in world space. `viewDir` is a vector that represents the direction from the camera to the object. `gloss` is a scalar value that represents the glossiness of the object's surface.\n\nThe function first calculates the dot product of `worldNormal` and `viewDir` and stores it in a variable called `NoV`. It then calculates `alphaG` by squaring the `gloss` value.\n\nNext, the function calculates two values `a` and `b` using an analytical approximation. The values of `a` and `b` depend on the value of `gloss`. If `gloss` is less than 0.25, then `a` and `b` are calculated using one set of coefficients. Otherwise, a different set of coefficients is used. \n\nFinally, the function calculates `DG` using the values of `a`, `b`, `NoV`, and `gloss`. `DG` represents the amount of reflection sheen to be added to the object. The function then calls another function called `calcReflection` to calculate the reflection of the object and multiplies it by `DG`. The result is then added to a variable called `sReflection`.\n\nThis function is likely used in a larger project that involves rendering 3D objects with realistic lighting and shading effects. The function is called for each pixel of the object's surface during the rendering process to add a reflection sheen effect. The `sReflection` variable is likely used to accumulate the reflection sheen effect for each pixel and is combined with other lighting and shading effects to produce the final rendered image.\n## Questions: \n 1. What does this code do?\n   \n   This code defines a function called `addReflectionSheen` that calculates the sheen effect on a reflective surface based on the glossiness of the material.\n\n2. What is the purpose of the `sReflection` variable?\n   \n   The code does not provide information about the `sReflection` variable. It is likely defined elsewhere in the code and used to accumulate the reflection value calculated by this function.\n\n3. What is the significance of the conditional statements in the function?\n   \n   The conditional statements are used to approximate the sheen values analytically instead of using a lookup table (LUT). The values are calculated differently based on the glossiness of the material, with different coefficients used for glossiness values below or above 0.25.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflectionSheen.md"}}],["618",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflectionSphere.js)\n\nThe code above is a GLSL shader code that is used to calculate and add reflections to a 3D object in the PlayCanvas engine. The shader code exports a function that takes in a reflection direction and a gloss value and adds the calculated reflection to the object's material.\n\nThe shader code starts by defining a uniform matrix variable called `matrix_view` which represents the view matrix of the camera. This matrix is used to transform the reflection direction vector into view space. The shader also defines two other uniform variables: `texture_sphereMap`, which is a 2D texture used to store the environment map, and `material_reflectivity`, which is a float value that determines the strength of the reflection.\n\nThe `calcReflection` function takes in a reflection direction vector and a gloss value. It first transforms the reflection direction vector into view space by multiplying it with the view matrix. It then calculates the texture coordinates of the environment map by projecting the reflection direction vector onto a unit sphere and mapping the resulting coordinates to the texture space. Finally, it decodes the color value from the environment map texture using the calculated texture coordinates and returns the resulting color value.\n\nThe `addReflection` function takes in a reflection direction vector and a gloss value. It calls the `calcReflection` function to calculate the reflection color and adds it to the object's material by updating the `dReflection` variable. The `dReflection` variable is a built-in variable in the PlayCanvas engine that represents the accumulated reflection color of the object.\n\nOverall, this shader code is an essential part of the PlayCanvas engine's rendering pipeline as it allows for the rendering of realistic reflections on 3D objects. It can be used in various scenarios, such as rendering reflective surfaces like mirrors, water, or metallic objects. Below is an example of how this shader code can be used in a PlayCanvas project:\n\n```javascript\n// create a new material\nvar material = new pc.StandardMaterial();\n\n// set the shader code to the material\nmaterial.chunks.reflectPS = /* glsl */`\n    // the shader code here\n`;\n\n// set the environment map texture to the material\nmaterial.setParameter('texture_sphereMap', envMapTexture);\n\n// set the reflectivity value to the material\nmaterial.setParameter('material_reflectivity', 0.5);\n\n// assign the material to a 3D object\nmyObject.model.meshInstances[0].material = material;\n```\n## Questions: \n 1. What is the purpose of the `texture_sphereMap` uniform and how is it used in the code?\n   \n   The `texture_sphereMap` uniform is a 2D texture sampler used to calculate the reflection of a given direction. It is used in the `calcReflection` function to decode the texture at a specific UV coordinate.\n\n2. What is the significance of the `material_reflectivity` uniform and how does it affect the output of the `addReflection` function?\n   \n   The `material_reflectivity` uniform is a float value that determines the strength of the reflection. It is used in the `addReflection` function to add the calculated reflection to the `dReflection` vector with the specified reflectivity.\n\n3. How does the `calcReflection` function calculate the reflection direction and what is the purpose of the `gloss` parameter?\n   \n   The `calcReflection` function calculates the reflection direction by transforming the input reflection direction using the `matrix_view` uniform and then mapping it to a UV coordinate on the `texture_sphereMap`. The `gloss` parameter is used to control the sharpness of the reflection by adjusting the size of the UV coordinate mapping.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflectionSphere.md"}}],["619",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/reflectionSphereLow.js)\n\nThe code above is a GLSL shader code that is used to calculate and add reflections to a 3D object in the PlayCanvas engine. The shader code takes in two uniform variables: `texture_sphereMap` and `material_reflectivity`. The `texture_sphereMap` is a 2D texture that represents the environment map, which is used to reflect the surrounding environment onto the object. The `material_reflectivity` is a float value that determines the strength of the reflection.\n\nThe `calcReflection` function takes in two parameters: `reflDir` and `gloss`. The `reflDir` is the reflection direction vector, which is calculated based on the surface normal and the view direction. The `gloss` parameter is the glossiness of the material, which determines how sharp or blurry the reflection will be. The function first calculates the UV coordinates of the environment map based on the reflection direction vector. It then decodes the color value from the environment map texture using the calculated UV coordinates and returns the resulting color as a vec3.\n\nThe `addReflection` function takes in two parameters: `reflDir` and `gloss`. It calls the `calcReflection` function to calculate the reflection color and adds it to the `dReflection` variable, which is a vec4 that accumulates the reflection color over multiple reflection passes. The `material_reflectivity` value is used to determine the strength of the reflection and is added to the alpha channel of the `dReflection` variable.\n\nOverall, this shader code is used to add reflections to a 3D object in the PlayCanvas engine. It can be used in combination with other shader codes to create realistic materials and lighting effects. Here is an example of how this shader code can be used in a PlayCanvas project:\n\n```javascript\n// create a material with the reflection shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.reflectPS = /* glsl */`\n    uniform sampler2D texture_sphereMap;\n    uniform float material_reflectivity;\n\n    vec3 calcReflection(vec3 reflDir, float gloss) {\n        vec3 reflDirV = vNormalV;\n\n        vec2 sphereMapUv = reflDirV.xy * 0.5 + 0.5;\n        return $DECODE(texture2D(texture_sphereMap, sphereMapUv));\n    }\n\n    void addReflection(vec3 reflDir, float gloss) {   \n        dReflection += vec4(calcReflection(reflDir, gloss), material_reflectivity);\n    }\n`;\nmaterial.update();\n\n// set the environment map texture\nvar texture = new pc.Texture();\ntexture.loadFromUrl('path/to/texture.png', function() {\n    material.setParameter('texture_sphereMap', texture);\n});\n\n// set the material reflectivity\nmaterial.setParameter('material_reflectivity', 0.5);\n\n// assign the material to a 3D object\nvar entity = new pc.Entity();\nentity.addComponent('model', {\n    type: 'box',\n    material: material\n});\n```\n## Questions: \n 1. What is the purpose of the `texture_sphereMap` uniform and how is it used in the code?\n   - The `texture_sphereMap` uniform is a sampler2D used to sample a texture for calculating reflections. It is used in the `calcReflection` function to decode the texture and calculate the reflection direction.\n\n2. What is the `material_reflectivity` uniform and how does it affect the output of the `addReflection` function?\n   - The `material_reflectivity` uniform is a float value that determines the strength of the reflection. It is used in the `addReflection` function to add the calculated reflection to the `dReflection` variable.\n\n3. What is the purpose of the `calcReflection` function and how is it used in the code?\n   - The `calcReflection` function calculates the reflection direction based on the input reflection direction and glossiness. It is used in the `addReflection` function to calculate the reflection and add it to the `dReflection` variable.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/reflectionSphereLow.md"}}],["620",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/refractionCube.js)\n\nThe code above is a GLSL shader code that is used to add refraction to a material in the PlayCanvas engine. The purpose of this code is to simulate the effect of light passing through a transparent material, such as glass or water, and bending as it enters and exits the material. \n\nThe `refract2` function takes in a view vector, a surface normal, and an index of refraction (IOR) value. It calculates the refracted vector using Snell's law and returns it. This function is used to calculate the direction of the refracted light ray as it passes through the material.\n\nThe `addRefraction` function takes in several parameters, including the world normal, view direction, thickness, gloss, specularity, albedo, and transmission. It first calculates the reflection direction using the `refract2` function and the material's refraction index. It then calls the `addReflection` function to add the reflection to the material. Finally, it mixes the diffuse light with the reflection and applies the transmission value to simulate the effect of light passing through the material.\n\nThis code is used in the PlayCanvas engine to create realistic materials with refraction, such as glass or water. It can be used in combination with other shader code to create complex materials with various properties. For example, the `iridescenceFresnel` and `IridescenceArgs` parameters can be used to add iridescence to the material, which is the phenomenon of colors changing as the angle of view changes. \n\nHere is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// create a material with refraction\nvar material = new pc.StandardMaterial();\nmaterial.refractionIndex = 1.5; // set the material's refraction index\nmaterial.chunks.refraction = /* glsl */`\n    addRefraction(vNormalW, -normalize(vPositionW), 0.1, 0.5, vec3(1), vec3(1), 0.8);\n`; // add the refraction code to the material's shader\n\n// assign the material to a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n``` \n\nIn this example, a new `StandardMaterial` is created and the `refractionIndex` property is set to 1.5 to simulate the refraction of light passing through glass. The `chunks.refraction` property is then set to the GLSL code above to add the refraction effect to the material's shader. Finally, the material is assigned to a `MeshInstance` to be rendered in the scene.\n## Questions: \n 1. What does the `refract2` function do?\n   - The `refract2` function calculates the refraction vector of a given view vector and normal, based on the material's index of refraction.\n2. What is the purpose of the `addRefraction` function?\n   - The `addRefraction` function adds refraction to the material's lighting calculation, based on various input parameters such as thickness, gloss, and transmission.\n3. What is the `LIT_IRIDESCENCE` preprocessor directive used for?\n   - The `LIT_IRIDESCENCE` preprocessor directive is used to conditionally include additional input parameters related to iridescence, if the feature is enabled.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/refractionCube.md"}}],["621",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/refractionDynamic.js)\n\nThe code above is a shader function that adds refraction to a 3D object in the PlayCanvas engine. Refraction is the bending of light as it passes through a medium, and this function simulates that effect on a material. The function takes in several parameters, including the world normal of the object, the view direction, the thickness of the material, the glossiness, the specularity, the albedo, and the transmission. \n\nThe function first extracts the scale of the object from its model transform and calculates the refraction vector based on the thickness and scale of the object. It then calculates the point of refraction and projects it to texture space so that it can be sampled. The function uses the built-in `getGrabScreenPos` function to convert the screen position to grab texture UV coordinates. \n\nThe function then calculates the refraction color by sampling the scene color map at the UV coordinates. If the `SUPPORTS_TEXLOD` flag is defined, the function uses the index of refraction (IOR) and roughness to select the appropriate mip level. Otherwise, it simply samples the texture at the given UV coordinates. \n\nThe final step is to calculate the transmittance, which is the final refraction color. If the material has an attenuation distance, the function calculates the attenuation and applies it to the length of the refraction vector. Otherwise, it simply uses the refraction color. The function then applies the fresnel effect on the refraction and mixes it with the diffuse light based on the transmission parameter. \n\nThis function is used in the larger PlayCanvas engine project to simulate the refraction of light on materials. It can be used to create realistic glass, water, or other transparent materials. Developers can customize the parameters of the function to achieve the desired effect. For example, they can adjust the thickness, glossiness, and specularity of the material to create different types of glass. They can also adjust the transmission parameter to control the amount of refraction. \n\nHere is an example of how this function can be used in a PlayCanvas project:\n\n```javascript\n// create a new material with refraction\nvar material = new pc.StandardMaterial();\nmaterial.refraction = true;\nmaterial.refractionIndex = 1.5; // set the index of refraction\nmaterial.attenuationDistance = 10; // set the attenuation distance\nmaterial.attenuation = new pc.Vec3(1, 1, 1); // set the attenuation color\n\n// set the material on a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n```\n## Questions: \n 1. What does this code do?\n    \n    This code defines a function","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/refractionDynamic.md"}}],["622",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowCascades.js)\n\nThe code above is a GLSL shader code that is used to generate shadow maps for a 3D scene. The code defines two functions: `getShadowCascadeMatrix` and `fadeShadow`. \n\nThe `getShadowCascadeMatrix` function is responsible for selecting a shadow projection matrix based on cascade distances. The function takes in an array of `shadowMatrixPalette` which contains the projection matrices for each cascade, an array of `shadowCascadeDistances` which contains the distances for each cascade, and the number of cascades to be used `shadowCascadeCount`. \n\nThe function first calculates the depth of the current fragment in the range of 0 to the far plane. It then loops through the `shadowCascadeDistances` array to find the index of the cascade that the fragment belongs to. The index is determined by comparing the depth of the fragment to the distances of each cascade. Once the index is found, it is limited to the actual number of cascades used. Finally, the function selects the shadow matrix for the cascade based on the index. \n\nThe `fadeShadow` function is responsible for fading out the shadow when the fragment is past the shadow distance. This is done to enforce a straight line instead of a corner of the shadow which moves when the camera rotates. The function takes in an array of `shadowCascadeDistances` and calculates the depth of the current fragment. If the depth is greater than the distance of the last cascade, the `z` component of the `dShadowCoord` vector is set to a large negative value to remove the shadow. \n\nOverall, these functions are used to generate and manipulate shadow maps for a 3D scene. The `getShadowCascadeMatrix` function is used to select the appropriate shadow matrix for each fragment based on its depth, while the `fadeShadow` function is used to fade out the shadow when the fragment is past the shadow distance. These functions are likely used in conjunction with other shaders and rendering techniques to create realistic shadows in the scene. \n\nExample usage of `getShadowCascadeMatrix`:\n```\n// define shadow matrices and distances\nmat4 shadowMatrixPalette[4];\nfloat shadowCascadeDistances[4] = {10.0, 20.0, 30.0, 40.0};\nfloat shadowCascadeCount = 4.0;\n\n// call getShadowCascadeMatrix to select shadow matrix for current fragment\ngetShadowCascadeMatrix(shadowMatrixPalette, shadowCascadeDistances, shadowCascadeCount);\n\n// use cascadeShadowMat to generate shadow map for current fragment\n```\n\nExample usage of `fadeShadow`:\n```\n// define shadow distances\nfloat shadowCascadeDistances[4] = {10.0, 20.0, 30.0, 40.0};\n\n// call fadeShadow to fade out shadow if fragment is past shadow distance\nfadeShadow(shadowCascadeDistances);\n```\n## Questions: \n 1. What is the purpose of the `getShadowCascadeMatrix` function?\n    \n    The `getShadowCascadeMatrix` function selects a shadow projection matrix based on cascade distances.\n\n2. What is the purpose of the `fadeShadow` function?\n    \n    The `fadeShadow` function removes the shadow if the pixel is past the shadow distance, enforcing a straight line instead of a corner of shadow which moves when the camera rotates.\n\n3. What is the significance of the `maxCascades` constant?\n    \n    The `maxCascades` constant sets the maximum number of cascades that can be used for shadow mapping.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowCascades.md"}}],["623",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowCommon.js)\n\nThe code provided is a GLSL shader function called `normalOffsetPointShadow`. This function is used to calculate the position of a shadow in a point light source scenario. \n\nThe function takes in several parameters, including `shadowParams`, `lightPos`, `lightDir`, `lightDirNorm`, and `normal`. `shadowParams` is a vec4 that contains information about the shadow, including its size and distance. `lightPos` is the position of the light source, `lightDir` is the direction of the light, `lightDirNorm` is the normalized direction of the light, and `normal` is the normal vector of the surface being shaded.\n\nThe function first calculates the `distScale` by taking the length of the `lightDir` vector. It then calculates the `wPos` vector by adding the `vPositionW` (the world position of the vertex being shaded) to the product of `normal`, `shadowParams.y`, and a clamped value based on the dot product of `normal` and `-lightDirNorm`. This calculation is then multiplied by `distScale`. \n\nFinally, the function calculates the `dir` vector by subtracting `lightPos` from `wPos`, and sets `lightDir` to be equal to `dir`.\n\nThis function is likely used in the larger PlayCanvas engine project to calculate the position of shadows in point light source scenarios. It may be used in conjunction with other shader functions to create realistic lighting effects in 3D scenes. \n\nExample usage of this function in a shader program:\n\n```\nuniform vec4 shadowParams;\nuniform vec3 lightPos;\nvarying vec3 vPositionW;\nvarying vec3 vNormal;\n\nvoid main() {\n  vec3 lightDir = normalize(lightPos - vPositionW);\n  normalOffsetPointShadow(shadowParams, lightPos, lightDir, normalize(lightDir), vNormal);\n  // other shading calculations\n}\n```\n## Questions: \n 1. What does this code do?\n   This code defines a function called `normalOffsetPointShadow` that calculates the direction of a point light source's shadow based on the position of the light, the surface normal, and a set of shadow parameters.\n\n2. What are the inputs and outputs of this function?\n   The function takes in a set of shadow parameters, the position of the light source, the original direction of the light, the surface normal, and modifies the direction of the light to point towards the calculated shadow direction.\n\n3. How is the shadow direction calculated?\n   The shadow direction is calculated by first scaling the original light direction by its distance from the surface, then offsetting the surface position along the surface normal by a factor of the shadow parameter and the cosine of the angle between the surface normal and the negative light direction. The resulting offset position is then subtracted from the light position to get the new shadow direction.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowCommon.md"}}],["624",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowCoord.js)\n\nThe code provided is a set of functions that calculate shadow coordinates for different types of light sources in a 3D scene. The functions are written in GLSL, which is a shading language used for graphics programming. \n\nThe first two functions, `_getShadowCoordOrtho` and `_getShadowCoordPersp`, are helper functions that calculate the shadow coordinates for an orthographic and a perspective light source, respectively. Both functions take in a `shadowMatrix` which is a matrix that transforms the world coordinates of a vertex into the light's view space. The `shadowParams` parameter is a vector or a 4-component vector that contains information about the light source, such as its position, direction, and intensity. The `wPos` parameter is the world position of the vertex that needs to be shadowed. \n\nThe `getShadowCoordOrtho` and `getShadowCoordPersp` functions are the main functions that are called to calculate the shadow coordinates for a vertex. They take in the same parameters as their corresponding helper functions, but instead of `wPos`, they take in the `vPositionW` variable, which is the world position of the current vertex being processed. These functions call their corresponding helper functions to calculate the shadow coordinates and store them in the `dShadowCoord` variable.\n\nThe last two functions, `getShadowCoordPerspNormalOffset` and `getShadowCoordOrthoNormalOffset`, are similar to the previous two functions, but they also take in the `lightDirNorm` and `normal` parameters. These parameters are used to calculate the offset of the shadow coordinates based on the surface normal and the direction of the light source. This is useful for creating more realistic shadows that take into account the shape of the object being shadowed.\n\nOverall, these functions are an essential part of the PlayCanvas engine's rendering pipeline. They allow for the calculation of accurate shadow coordinates for different types of light sources, which is crucial for creating realistic 3D scenes. Here is an example of how these functions might be used in a larger project:\n\n```glsl\nuniform mat4 shadowMatrix;\nuniform vec3 shadowParams;\nuniform vec4 shadowParamsPersp;\nuniform vec3 lightPos;\nuniform vec3 lightDir;\nuniform vec3 lightDirNorm;\nuniform vec3 normal;\n\nvoid main() {\n  // Calculate shadow coordinates for an orthographic light source\n  getShadowCoordOrtho(shadowMatrix, shadowParams);\n\n  // Calculate shadow coordinates for a perspective light source\n  getShadowCoordPersp(shadowMatrix, shadowParamsPersp, lightPos, lightDir);\n\n  // Calculate shadow coordinates with normal offset for a perspective light source\n  getShadowCoordPerspNormalOffset(shadowMatrix, shadowParamsPersp, lightPos, lightDir, lightDirNorm, normal);\n\n  // Calculate shadow coordinates with normal offset for an orthographic light source\n  getShadowCoordOrthoNormalOffset(shadowMatrix, shadowParams, lightPos, lightDir, lightDirNorm, normal);\n\n  // Use the shadow coordinates to sample the shadow map and calculate the final color of the pixel\n  // ...\n}\n```\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n   - This code defines functions for calculating shadow coordinates in orthographic and perspective projections, with and without normal offset. It is used in the PlayCanvas engine for rendering shadows in 3D scenes.\n2. What is the role of the `shadowParams` parameter in the `getShadowCoord` functions?\n   - The `shadowParams` parameter contains information about the shadow map, such as its size, resolution, and filtering mode. It is used to adjust the shadow coordinates and apply a shadow bias to prevent self-shadowing artifacts.\n3. What is the significance of the `dShadowCoord` variable and how is it computed?\n   - The `dShadowCoord` variable stores the final shadow coordinates in the range [0,1], which are used to sample the shadow map and determine the amount of shadowing at a given pixel. It is computed by transforming the world position of the pixel into the shadow map space, and adjusting the z-coordinate to avoid depth bias issues.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowCoord.md"}}],["625",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowCoordPerspZbuffer.js)\n\nThe code provided is a GLSL shader code that contains three functions: `_getShadowCoordPerspZbuffer`, `getShadowCoordPerspZbufferNormalOffset`, and `getShadowCoordPerspZbuffer`. These functions are used to calculate the shadow coordinates for a perspective projection with a z-buffer. \n\nThe `_getShadowCoordPerspZbuffer` function takes in a `shadowMatrix`, `shadowParams`, and `wPos` as parameters. The `shadowMatrix` is a matrix that transforms the world coordinates to shadow map coordinates. The `shadowParams` is a vector that contains the parameters for the shadow map, including the bias and the strength. The `wPos` is the world position of the vertex. The function first transforms the `wPos` to the shadow map coordinates using the `shadowMatrix`. Then, it divides the `xyz` components of the resulting vector by its `w` component to obtain the normalized device coordinates. Finally, it sets the `dShadowCoord` variable to the normalized device coordinates. \n\nThe `getShadowCoordPerspZbufferNormalOffset` function takes in a `shadowMatrix`, `shadowParams`, and `normal` as parameters. The `normal` is the normal vector of the vertex. The function first calculates the world position of the vertex by adding the `normal` vector multiplied by the `y` component of the `shadowParams` vector to the `vPositionW` vector. Then, it calls the `_getShadowCoordPerspZbuffer` function with the `shadowMatrix`, `shadowParams`, and the calculated `wPos`. \n\nThe `getShadowCoordPerspZbuffer` function takes in a `shadowMatrix` and `shadowParams` as parameters. It calls the `_getShadowCoordPerspZbuffer` function with the `shadowMatrix`, `shadowParams`, and the `vPositionW` vector. \n\nThese functions are used to calculate the shadow coordinates for a perspective projection with a z-buffer. They are likely used in the rendering pipeline of the PlayCanvas engine to generate shadows for 3D objects in a scene. For example, the `getShadowCoordPerspZbuffer` function may be called in the vertex shader to calculate the shadow coordinates for each vertex, which can then be used in the fragment shader to determine the amount of shadowing for each pixel. \n\nExample usage:\n\n```glsl\nuniform mat4 shadowMatrix;\nuniform vec4 shadowParams;\n\nvoid main() {\n  getShadowCoordPerspZbuffer(shadowMatrix, shadowParams);\n  // use dShadowCoord to determine shadowing\n}\n```\n## Questions: \n 1. What does this code do?\n    \n    This code defines three functions related to calculating shadow coordinates for a perspective z-buffer shadow map.\n\n2. What are the input parameters for each function?\n    \n    Each function takes a `shadowMatrix` (a matrix used to transform the position into shadow map space), `shadowParams` (a vec4 containing shadow map parameters), and either a `wPos` (a vec3 world position) or `normal` (a vec3 surface normal).\n\n3. What is the purpose of the `getShadowCoordPerspZbufferNormalOffset` function?\n    \n    This function calculates the shadow coordinate for a position offset by a given surface normal, which can be used to apply a depth bias to prevent self-shadowing artifacts.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowCoordPerspZbuffer.md"}}],["626",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowEVSM.js)\n\nThe code above is a GLSL shader code that defines three functions used for calculating Variance Shadow Mapping (VSM) in the PlayCanvas engine. VSM is a technique used in computer graphics to improve the quality of shadows in real-time rendering. \n\nThe first function, `VSM$`, takes in a texture, texture coordinates, resolution, Z value, VSM bias, and exponent. It returns the calculated EVSM (Exponential Variance Shadow Mapping) value using the moments of the texture. The moments are obtained by sampling the texture at the given texture coordinates and extracting the RGB values. The moments are then used to calculate the EVSM value using the `calculateEVSM` function, which is not defined in this code snippet.\n\nThe second function, `getShadowVSM$`, takes in a shadow map, shadow coordinates, shadow parameters, exponent, and light direction. It returns the VSM value calculated using the `VSM$` function. The shadow coordinates are transformed into screen space, and the shadow parameters are used to adjust the bias and scale of the shadow map. The light direction is used to calculate the length of the shadow coordinate in the z-axis.\n\nThe third function, `getShadowSpotVSM$`, is similar to `getShadowVSM$` but is used for spotlights. It takes in a shadow map, shadow coordinates, shadow parameters, exponent, and light direction. The difference is that the length of the shadow coordinate in the z-axis is calculated using the length of the light direction multiplied by the shadow parameter's w value, plus the shadow parameter's z value.\n\nThese functions are used in the PlayCanvas engine to calculate VSM shadows for different types of lights. They are called from other parts of the engine, such as the rendering pipeline, to generate shadows for objects in the scene. The calculated shadows are then applied to the objects to improve the realism of the rendered scene. \n\nExample usage of these functions in the PlayCanvas engine:\n\n```javascript\n// create a new light entity\nvar light = new pc.Entity();\nlight.addComponent('light', {\n    type: 'directional',\n    color: new pc.Color(1, 1, 1),\n    castShadows: true,\n    shadowResolution: 2048\n});\n\n// create a new material with VSM shadows\nvar material = new pc.StandardMaterial();\nmaterial.shadowMap = light.light.shadowMap;\nmaterial.shadowBias = light.light.shadowBias;\nmaterial.shadowDistance = light.light.shadowDistance;\nmaterial.shadowResolution = light.light.shadowResolution;\nmaterial.chunks.shadow = `\n    float shadow = getShadowVSM$(shadowMap, shadowCoord, vec3(shadowResolution, shadowBias, shadowDistance), 50.0, lightDir);\n    dDiffuseLight += shadow;\n`;\n\n// assign the material to a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n``` \n\nIn the example above, a new light entity is created with the `castShadows` property set to true. A new material is also created with the `shadowMap`, `shadowBias`, `shadowDistance`, and `shadowResolution` properties set to the corresponding values of the light entity. The `chunks.shadow` property is also set to include the GLSL code for calculating the VSM shadow using the `getShadowVSM$` function. Finally, the material is assigned to a mesh instance, which is then rendered in the scene with VSM shadows.\n## Questions: \n 1. What does the `VSM$` function do?\n    - The `VSM$` function takes in a texture, texture coordinates, resolution, Z value, VSM bias, and exponent, and returns the result of the `calculateEVSM` function applied to the moments of the texture at the given coordinates.\n2. What is the difference between `getShadowVSM$` and `getShadowSpotVSM$`?\n    - `getShadowVSM$` takes in a shadow map, shadow coordinates, shadow parameters, exponent, and light direction, and returns the result of calling `VSM$` with the appropriate arguments. `getShadowSpotVSM$` is similar, but also takes into account the length of the light direction and an additional offset value.\n3. What is the purpose of the `exponent` parameter in these functions?\n    - The `exponent` parameter is used in the calculation of the EVSM (Exponential Variance Shadow Map) value, which is a technique for improving the quality of shadows in real-time rendering. The exponent controls the shape of the distribution of shadow values, with higher values resulting in sharper, more defined shadows.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowEVSM.md"}}],["627",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowEVSMn.js)\n\nThe code provided is a GLSL shader code that implements Variance Shadow Mapping (VSM) algorithm. VSM is a technique used in computer graphics to generate soft shadows in real-time rendering. The code provides two functions, `getShadowVSM$` and `getShadowSpotVSM$`, that take in a shadow map, shadow coordinates, shadow parameters, exponent, and light direction as input and return a float value that represents the soft shadow value.\n\nThe `VSM$` function calculates the EVSM (Exponential Variance Shadow Mapping) value for a given pixel. It takes in a texture, texture coordinates, resolution, Z value, VSM bias, and exponent as input. The function first calculates the pixel size based on the resolution and subtracts it from the texture coordinates. It then samples the texture at four neighboring points and calculates the moments of the samples. The moments are then used to calculate the EVSM value using the `calculateEVSM` function.\n\nThe `getShadowVSM$` and `getShadowSpotVSM$` functions are used to calculate the soft shadow value for a point light and a spot light, respectively. Both functions take in a shadow map, shadow coordinates, shadow parameters, exponent, and light direction as input. The shadow coordinates are transformed into texture coordinates and passed to the `VSM$` function to calculate the EVSM value.\n\nThis code is a part of the PlayCanvas engine project and can be used to generate soft shadows in real-time rendering. The `getShadowVSM$` and `getShadowSpotVSM$` functions can be called from other parts of the engine to calculate the soft shadow value for different types of lights. The code can be used in games, simulations, and other real-time rendering applications that require soft shadows. \n\nExample usage:\n\n```glsl\n// create a shadow map texture\nsampler2D shadowMap = createShadowMap();\n\n// calculate the shadow value for a point light\nvec3 shadowCoord = calculateShadowCoord(pointLightPosition);\nvec3 shadowParams = calculateShadowParams(pointLightNear, pointLightFar);\nfloat shadowValue = getShadowVSM$(shadowMap, shadowCoord, shadowParams, 32.0, pointLightDirection);\n\n// calculate the shadow value for a spot light\nvec3 shadowCoord = calculateShadowCoord(spotLightPosition);\nvec4 shadowParams = calculateShadowParams(spotLightNear, spotLightFar, spotLightAngle);\nfloat shadowValue = getShadowSpotVSM$(shadowMap, shadowCoord, shadowParams, 32.0, spotLightDirection);\n```\n## Questions: \n 1. What is the purpose of the `VSM$` function?\n    \n    The `VSM$` function calculates the Exponential Variance Shadow Mapping (EVSM) value for a given texture, texture coordinates, resolution, Z value, VSM bias, and exponent.\n\n2. What is the difference between `getShadowVSM$` and `getShadowSpotVSM$` functions?\n    \n    The `getShadowVSM$` function calculates the EVSM value for a given shadow map, shadow coordinate, shadow parameters, exponent, and light direction. The `getShadowSpotVSM$` function calculates the EVSM value for a given shadow map, shadow coordinate, shadow parameters, exponent, and light direction, but also takes into account the length of the light direction and a shadow parameter value.\n\n3. What is the expected input format for the `tex` parameter in the `VSM$` function?\n    \n    The `tex` parameter in the `VSM$` function is expected to be a `sampler2D` texture.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowEVSMn.md"}}],["628",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowStandard.js)\n\nThe code provided is a shader code written in GLSL (OpenGL Shading Language) for the PlayCanvas engine. The code contains functions that are used for shadow mapping in the engine. Shadow mapping is a technique used in computer graphics to create shadows in a 3D scene. The technique involves rendering the scene from the perspective of a light source and storing the depth values of the scene in a texture called a shadow map. The shadow map is then used to determine whether a point in the scene is in shadow or not.\n\nThe code contains functions for both direct/spot sampling and omni sampling. The direct/spot sampling functions are used for directional and spot lights, while the omni sampling function is used for point lights.\n\nThe `lessThan2` function is a helper function that returns a vector that is clamped between 0 and 1. This function is used to soften the shadows.\n\nThe `unpackFloat` function is used to unpack the depth value from the RGBA value stored in the shadow map. The function takes a `vec4` value and returns a `float` value.\n\nThe `getShadowPCF3x3` function is used for direct/spot sampling in GL1 and GL2. The function takes a shadow map, shadow coordinate, and shadow parameters as input and returns a float value. The function uses a 3x3 PCF (Percentage Closer Filtering) kernel to determine whether a point is in shadow or not. The function calculates the shadow kernel by sampling the shadow map at different offsets and comparing the depth values with the current depth value. The function then returns the average of the shadow values.\n\nThe `getShadowSpotPCF3x3` function is similar to the `getShadowPCF3x3` function but is used for spot lights.\n\nThe `_getShadowPCF3x3` function is a helper function used by both `getShadowPCF3x3` and `getShadowSpotPCF3x3` functions. The function takes a shadow map, shadow coordinate, and shadow parameters as input and returns a float value. The function calculates the shadow kernel in the same way as the `getShadowPCF3x3` function.\n\nThe `_xgetShadowPCF3x3` function is used for direct/spot sampling in GL1. The function takes a depth kernel, shadow coordinate, shadow map, and shadow parameters as input and returns a float value. The function calculates the shadow kernel in the same way as the `_getShadowPCF3x3` function.\n\nThe `_getShadowPoint` function is used for omni sampling. The function takes a shadow map, shadow parameters, and light direction as input and returns a float value. The function calculates the shadow kernel by sampling the shadow map at different offsets and comparing the depth values with the current depth value. The function then returns the average of the shadow values.\n\nIn summary, the code provides functions for shadow mapping in the PlayCanvas engine. The functions use different techniques to determine whether a point is in shadow or not, depending on the type of light used. The functions are written in GLSL and are used in the engine to create realistic shadows in a 3D scene.\n## Questions: \n 1. What does the `lessThan2` function do and how is it used in this code?\n   \n   The `lessThan2` function returns a vector that clamps the difference between two input vectors multiplied by 1000.0 between 0.0 and 1.0. It is used in the `_getShadowPoint` function to compare depth values to a shadow depth threshold.\n\n2. What is the purpose of the `UNPACKFLOAT` macro and how is it used in this code?\n   \n   The `UNPACKFLOAT` macro defines a function that unpacks a float value from a vec4 RGBA color. It is used in the `_getShadowPCF3x3` function to extract depth values from the shadow map texture.\n\n3. What is the difference between the `getShadowPCF3x3` and `getShadowSpotPCF3x3` functions?\n   \n   The `getShadowPCF3x3` function is used for directional light sources, while the `getShadowSpotPCF3x3` function is used for spot light sources. The latter takes a vec4 shadowParams input, while the former takes a vec3 shadowParams input.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowStandard.md"}}],["629",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowStandardGL2.js)\n\nThe code above is a GLSL shader function that calculates the percentage-closer filtering (PCF) for a 5x5 shadow map. The function takes in three parameters: the shadow map, the shadow coordinate, and the shadow parameters. \n\nThe shadow map is a texture that contains the depth values of the scene from the light's point of view. The shadow coordinate is a vector that represents the position of the fragment in the shadow map. The shadow parameters are a vector that contains the size of the shadow map and the bias value used to prevent self-shadowing artifacts.\n\nThe function first calculates the z value of the shadow coordinate and the UV coordinates of the shadow map. It then calculates the base UV coordinates and the s and t values used for bilinear interpolation. The function then calculates the weights and coordinates for the nine texels used in the PCF filter. The weights are calculated using a cubic B-spline function. The function then samples the shadow map at the nine texels and calculates the sum of the weighted values. The sum is then divided by the total number of texels (144) and clamped to the range [0, 1]. The final value is returned as the shadow factor.\n\nThe function also includes two wrapper functions that call the _getShadowPCF5x5 function with different parameters. The getShadowPCF5x5 function takes in the same parameters as _getShadowPCF5x5 but passes the shadow map through the SHADOWMAP_PASS macro. The getShadowSpotPCF5x5 function takes in a vec4 shadowParams parameter and passes the xyz components to _getShadowPCF5x5.\n\nThis function is used in the PlayCanvas engine to calculate the shadow factor for a 5x5 shadow map. It is likely used in the rendering pipeline to determine the amount of shadowing for a given fragment. Developers can use this function in their own shaders to implement PCF filtering for their shadow maps. For example, a developer could use this function in a fragment shader to calculate the shadow factor for a directional light. \n\nExample usage in a fragment shader:\n\n```\nuniform sampler2D shadowMap;\nuniform vec3 shadowCoord;\nuniform vec3 shadowParams;\n\nvoid main() {\n    float shadowFactor = getShadowPCF5x5(shadowMap, shadowCoord, shadowParams);\n    gl_FragColor = vec4(1.0, 1.0, 1.0, 1.0) * shadowFactor;\n}\n```\n## Questions: \n 1. What is the purpose of this code?\n   \n   This code is a GLSL shader function that calculates the percentage-closer filtering (PCF) for a 5x5 shadow map. It takes in a shadow map texture, shadow coordinates, and shadow parameters as inputs and returns the filtered shadow value.\n\n2. What is the algorithm used for the PCF filtering?\n   \n   The algorithm used for the PCF filtering is described in the comments of the code and is based on the method outlined in this article: http://the-witness.net/news/2013/09/shadow-mapping-summary-part-1/. It involves sampling the shadow map at multiple locations around the current pixel and averaging the results to get a smoother shadow transition.\n\n3. What are the differences between the `getShadowPCF5x5` and `getShadowSpotPCF5x5` functions?\n   \n   The `getShadowPCF5x5` function takes in a 3-component shadow parameter vector, while the `getShadowSpotPCF5x5` function takes in a 4-component shadow parameter vector. The fourth component is assumed to be the spot light cone angle, which is not used in this function. Otherwise, both functions call the same `_getShadowPCF5x5` function to perform the PCF filtering.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowStandardGL2.md"}}],["630",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowVSM8.js)\n\nThe code provided is a GLSL shader code that is used for calculating Variance Shadow Mapping (VSM) for a given scene. VSM is a technique used in computer graphics to create soft shadows in real-time rendering. The purpose of this code is to provide a set of functions that can be used to calculate VSM for different types of light sources, such as directional and spotlights.\n\nThe first function `calculateVSM8` takes in three parameters: `moments`, `Z`, and `vsmBias`. `moments` is a vector that contains the first two moments of the depth values of the scene. `Z` is the depth value of the current fragment being rendered, and `vsmBias` is a user-defined bias value that is used to adjust the sharpness of the shadows. The function calculates the minimum variance of the depth values and returns the upper bound of the Chebyshev approximation of the variance.\n\nThe second function `decodeFloatRG` takes in a vector `rg` and decodes it into a floating-point value. This function is used to decode the two-component texture coordinates that are used to store the depth values in the shadow map.\n\nThe third function `VSM8` is the main function that calculates the VSM for a given texture, texture coordinates, resolution, depth value, bias, and exponent. It first samples the texture at the given texture coordinates and decodes the depth values using the `decodeFloatRG` function. It then calls the `calculateVSM8` function to calculate the variance and returns the upper bound of the Chebyshev approximation of the variance.\n\nThe fourth function `getShadowVSM8` is a helper function that takes in a shadow map, shadow coordinates, shadow parameters, exponent, and light direction. It calculates the VSM for a directional light source by calling the `VSM8` function with the appropriate parameters.\n\nThe fifth function `getShadowSpotVSM8` is another helper function that takes in a shadow map, shadow coordinates, shadow parameters, exponent, and light direction. It calculates the VSM for a spotlight by calling the `VSM8` function with the appropriate parameters.\n\nOverall, this code provides a set of functions that can be used to calculate VSM for different types of light sources in a real-time rendering engine. These functions can be used in conjunction with other rendering techniques to create high-quality, realistic shadows in a scene.\n## Questions: \n 1. What is the purpose of the `calculateVSM8` function?\n    \n    The `calculateVSM8` function calculates the Variance Shadow Map (VSM) for a given set of moments, depth, and bias.\n\n2. What is the purpose of the `decodeFloatRG` function?\n    \n    The `decodeFloatRG` function decodes a float value from a vec2 by dividing the y component by 255 and adding the x component.\n\n3. What is the difference between `getShadowVSM8` and `getShadowSpotVSM8` functions?\n    \n    The `getShadowVSM8` function calculates the VSM for a directional light, while the `getShadowSpotVSM8` function calculates the VSM for a spotlight by taking into account the light direction and spot parameters.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowVSM8.md"}}],["631",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/shadowVSM_common.js)\n\nThe code above is a GLSL shader code that contains several functions for calculating the Variance Shadow Mapping (VSM) technique. VSM is a shadow mapping algorithm that aims to reduce shadow aliasing artifacts by storing the depth and variance of the scene in a texture. The code is a part of the PlayCanvas engine project and can be used to implement VSM in 3D graphics applications.\n\nThe first function, `linstep`, is a linear interpolation function that returns a value between 0 and 1 based on the input value `v` and the range between `a` and `b`. This function is used to rescale the variance value in the `reduceLightBleeding` function.\n\nThe `reduceLightBleeding` function takes two parameters, `pMax` and `amount`, and returns a rescaled value of `pMax`. The function removes the tail of the variance distribution between 0 and `amount` and linearly rescales the remaining values between `amount` and 1. This function is used to reduce light bleeding artifacts that occur when the shadow map resolution is low.\n\nThe `chebyshevUpperBound` function calculates the probabilistic upper bound of the variance distribution using the Chebyshev inequality. The function takes four parameters, `moments`, `mean`, `minVariance`, and `lightBleedingReduction`. The `moments` parameter is a vector that contains the first two moments of the variance distribution, `mean` is the mean depth value, `minVariance` is the minimum variance value, and `lightBleedingReduction` is the amount of light bleeding reduction applied to the variance value. The function first computes the variance of the distribution and then calculates the probabilistic upper bound using the Chebyshev inequality. The `reduceLightBleeding` function is called to reduce light bleeding artifacts before calculating the upper bound. The function returns a value between 0 and 1 that represents the upper bound of the variance distribution.\n\nThe `calculateEVSM` function calculates the VSM value for a given pixel. The function takes four parameters, `moments`, `Z`, `vsmBias`, and `exponent`. The `moments` parameter is a vector that contains the first three moments of the variance distribution, `Z` is the depth value of the pixel, `vsmBias` is the VSM bias value, and `exponent` is the exponent value used to warp the depth value. The function first warps the depth value using the `exponent` value and updates the first two moments of the variance distribution using the warped depth value. The function then calculates the minimum variance value and calls the `chebyshevUpperBound` function to calculate the probabilistic upper bound of the variance distribution. The function returns a value between 0 and 1 that represents the VSM value of the pixel.\n\nOverall, this code provides a set of functions that can be used to implement the VSM technique in 3D graphics applications. The `calculateEVSM` function can be called for each pixel in the scene to calculate the VSM value, which can then be used to generate soft shadows with reduced aliasing artifacts.\n## Questions: \n 1. What is the purpose of the `reduceLightBleeding` function?\n- The `reduceLightBleeding` function removes the [0, amount] tail and linearly rescales (amount, 1] to reduce light bleeding.\n\n2. What is the `chebyshevUpperBound` function computing?\n- The `chebyshevUpperBound` function computes the probabilistic upper bound using the one-tailed Chebyshev method.\n\n3. What is the `calculateEVSM` function doing?\n- The `calculateEVSM` function calculates the Exponential Variance Shadow Mapping (EVSM) using the given moments, depth, bias, and exponent.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/shadowVSM_common.md"}}],["632",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/spot.js)\n\nThe code provided is a GLSL shader function that calculates the spot effect of a light source. The function takes in four parameters: `lightSpotDir`, `lightInnerConeAngle`, `lightOuterConeAngle`, and `lightDirNorm`. \n\n`lightSpotDir` is a normalized vector that represents the direction of the light source's spot. `lightInnerConeAngle` and `lightOuterConeAngle` are the inner and outer angles of the light source's cone, respectively. `lightDirNorm` is a normalized vector that represents the direction of the light source.\n\nThe function calculates the cosine of the angle between `lightDirNorm` and `lightSpotDir` using the `dot` function. This value is then used as the input for the `smoothstep` function, which returns a value between 0 and 1 based on the input and the two threshold values (`lightOuterConeAngle` and `lightInnerConeAngle`). \n\nThe purpose of this function is to determine the intensity of the light source's spot effect on a given point in the scene. This can be used in the larger PlayCanvas engine project to create more realistic lighting effects in 3D scenes. \n\nHere is an example of how this function could be used in a PlayCanvas project:\n\n```javascript\n// create a new light source\nvar light = new pc.Entity();\nlight.addComponent(\"light\", {\n    type: \"spot\",\n    innerConeAngle: 30,\n    outerConeAngle: 45,\n    range: 10,\n    color: new pc.Color(1, 1, 1),\n    intensity: 1\n});\n\n// calculate the spot effect of the light on a point in the scene\nvar spotEffect = getSpotEffect(light.getDirection(), light.innerConeAngle, light.outerConeAngle, pointNormal);\n\n// apply the spot effect to the point's color\nvar pointColor = new pc.Color(1, 0, 0);\npointColor.mulScalar(spotEffect * light.intensity * light.color);\n```\n## Questions: \n 1. What does this code do?\n   This code defines a GLSL function that calculates the spot effect of a light source based on its direction and cone angles.\n\n2. How is this code used in the PlayCanvas engine?\n   This code may be used in shaders that implement lighting effects in PlayCanvas, where it can be called to calculate the contribution of a spot light to the final color of a pixel.\n\n3. What are the inputs and outputs of this function?\n   The function takes in the direction of the light source, its inner and outer cone angles, and the normalized direction vector of the surface being lit. It returns a float value between 0 and 1 that represents the intensity of the light at the given surface point.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/spot.md"}}],["633",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/start.js)\n\nThis code is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to define the main function of the shader, which is responsible for rendering the scene. The main function is called once per pixel and is responsible for calculating the color of the pixel based on the lighting and other factors.\n\nThe code starts by initializing the dReflection variable to a vec4(0). This variable is used to store the reflection of the object being rendered. The code then checks if the LIT_CLEARCOAT flag is defined. If it is defined, the ccSpecularLight and ccReflection variables are initialized to vec3(0). These variables are used to store the specular lighting and reflection of the object being rendered.\n\nThe purpose of this code is to set the initial values of the variables used in the shader. These variables are used to store information about the object being rendered, such as its reflection and lighting. The code is part of a larger project, the PlayCanvas engine, which is a game engine that allows developers to create 3D games and applications. The shader code is used to render the graphics in the game or application, and is an important part of the engine's functionality.\n\nHere is an example of how this code might be used in the larger project:\n\n```javascript\n// Create a new material for an object\nvar material = new pc.StandardMaterial();\n\n// Set the shader code for the material\nmaterial.chunks.customFragmentShader = /* glsl */`\n    void main(void) {\n        dReflection = vec4(0);\n\n        #ifdef LIT_CLEARCOAT\n        ccSpecularLight = vec3(0);\n        ccReflection = vec3(0);\n        #endif\n    }\n`;\n\n// Set other properties of the material\nmaterial.diffuse = new pc.Color(1, 1, 1);\n\n// Assign the material to an object\nvar entity = new pc.Entity();\nentity.addComponent('model', {\n    type: 'box'\n});\nentity.model.material = material;\n``` \n\nIn this example, a new material is created for an object in the game. The shader code is set for the material, including the code from the example above. Other properties of the material are set, such as the diffuse color. Finally, the material is assigned to a 3D model of a box, which is attached to an entity in the game. When the game is rendered, the shader code is executed for each pixel in the box, and the resulting color is displayed on the screen.\n## Questions: \n 1. What is the purpose of the `dReflection` variable being set to `vec4(0)` in the `main` function?\n    \n    Answer: It is unclear from this code snippet what the `dReflection` variable is used for or why it is being set to `vec4(0)` in the `main` function.\n\n2. What is the `LIT_CLEARCOAT` preprocessor directive and how does it affect the code within the `ifdef` block?\n    \n    Answer: It is unclear from this code snippet what the `LIT_CLEARCOAT` preprocessor directive is or how it affects the code within the `ifdef` block. \n\n3. What is the purpose of the `ccSpecularLight` and `ccReflection` variables being set to `vec3(0)` in the `ifdef` block?\n    \n    Answer: It is unclear from this code snippet what the `ccSpecularLight` and `ccReflection` variables are used for or why they are being set to `vec3(0)` in the `ifdef` block.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/start.md"}}],["634",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/startNineSliced.js)\n\nThis code exports a GLSL shader code that performs a simple transformation on a texture's UV coordinates. The purpose of this code is to implement a \"nine-slice\" scaling technique commonly used in UI design. \n\nThe `nineSlicedUv` variable is assigned the value of the input UV coordinates `vUv0`. The `y` component of `nineSlicedUv` is then inverted by subtracting it from 1.0. This effectively flips the texture vertically, which is necessary for the nine-slice technique to work properly.\n\nThe nine-slice technique involves dividing a texture into nine sections and scaling each section independently to fit a UI element of any size. The four corners of the texture remain unscaled, while the four edges are scaled only in one direction. The center section is scaled in both directions to fill the remaining space. By using this technique, UI elements can be resized without distorting the texture or losing detail.\n\nIn the larger PlayCanvas engine project, this code would likely be used in conjunction with other shaders and rendering techniques to create UI elements. For example, a UI button could use this shader to display a texture that scales properly when the button is resized. \n\nHere is an example of how this code could be used in a PlayCanvas script:\n\n```\nvar material = new pc.StandardMaterial();\nmaterial.chunks.transformVS = /* glsl */`\n    nineSlicedUv = vUv0;\n    nineSlicedUv.y = 1.0 - nineSlicedUv.y;\n`;\n```\n\nIn this example, a new `StandardMaterial` is created and the `transformVS` chunk is replaced with the `nineSlicedUv` code. This would apply the nine-slice transformation to the material's texture coordinates. The material could then be applied to a UI element to achieve the desired scaling effect.\n## Questions: \n 1. **What is the purpose of this code?** \nThis code appears to be modifying the `nineSlicedUv` variable by setting its `y` value to `1.0` minus its original `y` value. It is unclear what the purpose of this modification is without additional context.\n\n2. **What is the data type of `vUv0` and `nineSlicedUv`?** \nWithout additional context, it is unclear what the data type of `vUv0` and `nineSlicedUv` are. They could be floats, vectors, or matrices, among other possibilities.\n\n3. **What is the significance of the `/* glsl */` comment at the beginning of the code?** \nThe `/* glsl */` comment indicates that this code is written in GLSL (OpenGL Shading Language), which is a high-level shading language used for rendering graphics in OpenGL and WebGL applications. It is possible that this code is being used to modify the appearance of a 3D object in a PlayCanvas scene.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/startNineSliced.md"}}],["635",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/startNineSlicedTiled.js)\n\nThis code is a GLSL shader that is used for nine-slice scaling of textures in the PlayCanvas engine. Nine-slice scaling is a technique used to scale an image in a way that preserves the corners and edges of the image while stretching the middle portion. This is useful for creating UI elements that can be resized without distorting the edges or corners.\n\nThe code takes in several variables, including a mask value, an offset value, and a tiled UV value. It then calculates the size and scale of the tiles based on the offset value, and uses this information to clamp the UV coordinates of the texture. The resulting UV coordinates are then multiplied by an atlas rectangle value to get the final nine-sliced UV coordinates.\n\nThe resulting nine-sliced UV coordinates are then used to sample the texture in the fragment shader. The mask value is used to blend between the original UV coordinates and the nine-sliced UV coordinates, depending on whether a pixel is inside or outside the nine-slice area. Finally, the Y coordinate of the resulting UV coordinates is inverted to account for the difference in coordinate systems between OpenGL and WebGL.\n\nHere is an example of how this code might be used in the PlayCanvas engine:\n\n```javascript\n// Create a new material with the nine-slice shader\nvar material = new pc.StandardMaterial();\nmaterial.chunks.base = pc.shaderChunks.base;\nmaterial.chunks.diffusePS = pc.shaderChunks.diffusePSNineSlice;\n\n// Set the material's texture and nine-slice parameters\nmaterial.diffuseMap = texture;\nmaterial.innerOffset = new pc.Vec4(10, 10, 10, 10);\nmaterial.atlasRect = new pc.Vec4(0, 0, 1, 1);\n\n// Create a new entity with a sprite component\nvar entity = new pc.Entity();\nentity.addComponent(\"sprite\", {\n    type: pc.SPRITETYPE_SIMPLE,\n    material: material,\n    spriteAsset: sprite\n});\n\n// Add the entity to the scene\napp.root.addChild(entity);\n``` \n\nIn this example, a new material is created with the nine-slice shader, and the material's texture and nine-slice parameters are set. A new entity is then created with a sprite component, and the material is assigned to the sprite component. Finally, the entity is added to the scene. This allows the sprite to be resized without distorting the edges or corners of the texture.\n## Questions: \n 1. What is the purpose of the `tileMask` variable?\n   - The `tileMask` variable is used to determine which parts of the texture should be tiled and which parts should be stretched.\n2. What is the significance of the `nineSlicedUv` variable?\n   - The `nineSlicedUv` variable is used to calculate the UV coordinates for a nine-sliced texture, which is a technique used to stretch a texture while preserving its edges.\n3. How is the `clampedUv` variable calculated?\n   - The `clampedUv` variable is calculated by mixing two UV coordinates based on the `tileSize` and `tileScale` variables, and then scaling and offsetting the result based on the `atlasRect` variable.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/startNineSlicedTiled.md"}}],["636",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/storeEVSM.js)\n\nThis code is a shader program written in GLSL (OpenGL Shading Language) that is used for rendering shadows in the PlayCanvas engine. The shader takes in a depth value and applies a technique called Variance Shadow Mapping (VSM) to create soft shadows.\n\nThe first line of the code initializes a variable called \"exponent\" with a value of VSM_EXPONENT. This value is used to control the softness of the shadows.\n\nThe next two lines of code manipulate the depth value by scaling it from the range of [0,1] to [-1,1] and then applying an exponential function to it. This is done to improve the accuracy of the shadow map and reduce the occurrence of shadow acne (artifacts that occur when the depth values are too close together).\n\nFinally, the resulting depth value is used to calculate the final color of the pixel. The color is a combination of the depth value, the depth squared, and a constant value of 1 for the alpha channel. This creates a smooth gradient of shadow intensity that can be used to render realistic shadows in the game.\n\nThis shader program is likely used in conjunction with other rendering techniques to create a complete shadow rendering system in the PlayCanvas engine. It can be applied to any object in the game that needs to cast or receive shadows, and can be customized by adjusting the value of the \"exponent\" variable to achieve the desired level of softness. \n\nExample usage:\n\n```javascript\n// create a material with the VSM shadow shader\nvar shadowMaterial = new pc.StandardMaterial();\nshadowMaterial.chunks.shadow = PlayCanvasVSMShader;\n\n// apply the material to an object that needs to cast shadows\nvar shadowCaster = new pc.Entity();\nshadowCaster.addComponent(\"model\", {\n    type: \"box\"\n});\nshadowCaster.model.material = shadowMaterial;\nshadowCaster.setLocalPosition(0, 1, 0);\n\n// apply the material to an object that receives shadows\nvar shadowReceiver = new pc.Entity();\nshadowReceiver.addComponent(\"model\", {\n    type: \"plane\"\n});\nshadowReceiver.model.material = shadowMaterial;\nshadowReceiver.setLocalPosition(0, 0, 0);\n```\n## Questions: \n 1. **What is the purpose of the `VSM_EXPONENT` variable?** \nThe `VSM_EXPONENT` variable is used to control the exponent value in the depth calculation.\n\n2. **What is the expected input for the `depth` variable?** \nThe `depth` variable is expected to be a value between 0 and 1, representing the depth of a pixel in the scene.\n\n3. **What is the output of this code?** \nThe output of this code is a `vec4` color value, with the first component representing the depth value, the second component representing the depth squared, and the last two components set to 1.0.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/storeEVSM.md"}}],["637",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/frag/viewDir.js)\n\nThe code provided is a GLSL shader function that calculates the view direction of a camera in a 3D scene. The function is called `getViewDir()` and takes no parameters. \n\nThe `normalize()` function is used to ensure that the resulting view direction vector has a length of 1, which is important for certain calculations in 3D graphics. The view direction is calculated by subtracting the camera position (`view_position`) from the world position of the current pixel (`vPositionW`). \n\nThis code is likely used in the larger PlayCanvas engine project to render 3D scenes with a camera. The view direction is an important parameter for many rendering techniques, such as lighting and shading. By calculating the view direction in a shader function, the engine can efficiently perform these calculations for each pixel in the scene. \n\nHere is an example of how this code might be used in a PlayCanvas project:\n\n```javascript\n// Create a camera entity\nvar camera = new pc.Entity();\ncamera.addComponent(\"camera\", {\n    clearColor: new pc.Color(0.1, 0.1, 0.1)\n});\n\n// Create a material with a shader that uses getViewDir()\nvar material = new pc.StandardMaterial();\nmaterial.shader = new pc.Shader({\n    attributes: {...},\n    vshader: /* vertex shader code */,\n    fshader: /* fragment shader code that includes getViewDir() */\n});\n\n// Create a mesh instance with the material and add it to the scene\nvar mesh = new pc.Mesh();\nvar node = new pc.GraphNode();\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\napp.scene.addModel(meshInstance);\n```\n\nIn this example, a camera entity is created and a material is created with a shader that includes the `getViewDir()` function. The material is then applied to a mesh instance and added to the scene. When the scene is rendered, the shader function is called for each pixel in the mesh, allowing the engine to efficiently calculate the view direction for each pixel.\n## Questions: \n 1. What is the purpose of this code and how is it used within the PlayCanvas engine?\n   - This code defines a function called `getViewDir()` that calculates the normalized view direction vector. It is likely used in rendering calculations within the engine.\n2. What are the inputs and outputs of this function?\n   - The function takes in the `view_position` and `vPositionW` variables and calculates the `dViewDirW` output vector.\n3. Are there any potential performance implications of using this function?\n   - It is unclear without more context, but if this function is called frequently during rendering calculations, it could potentially impact performance. It would be important to analyze the specific use case and optimize as necessary.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/frag/viewDir.md"}}],["638",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/base.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) used in the PlayCanvas engine project. The purpose of this code is to define the attributes and uniforms used in the rendering pipeline of 3D objects. \n\nThe `attribute` keyword is used to define the input variables that are passed to the vertex shader. In this code, there are six attributes defined: `vertex_position`, `vertex_normal`, `vertex_tangent`, `vertex_texCoord0`, `vertex_texCoord1`, and `vertex_color`. These attributes are used to define the position, normal, tangent, texture coordinates, and color of each vertex in the 3D object.\n\nThe `uniform` keyword is used to define the variables that are constant across all vertices and are passed to the shader from the CPU. In this code, there are three uniforms defined: `matrix_viewProjection`, `matrix_model`, and `matrix_normal`. These uniforms are used to define the view-projection matrix, model matrix, and normal matrix used in the rendering pipeline.\n\nThe `vec3` and `vec4` keywords are used to define vectors of 3 and 4 components respectively. The `mat4` and `mat3` keywords are used to define 4x4 and 3x3 matrices respectively.\n\nThe `dPositionW`, `dModelMatrix`, and `dNormalMatrix` variables are not used in this code but are likely defined for use in other parts of the PlayCanvas engine project.\n\nOverall, this code is an essential part of the rendering pipeline in the PlayCanvas engine project. It defines the attributes and uniforms used in the vertex shader, which is responsible for transforming the vertices of 3D objects from object space to screen space. By defining these variables, the shader can apply lighting, textures, and other effects to the 3D object during rendering. \n\nExample usage of this code would be in defining a material for a 3D object in the PlayCanvas engine. The material would specify the shader to use, and the attributes and uniforms to pass to the shader. The shader would then use this code to define the variables used in the rendering pipeline.\n## Questions: \n 1. **What is the purpose of this code?** \nThis code defines a set of attributes and uniforms for use in a shader program.\n\n2. **What is the data type of the variables defined in this code?** \nThe variables defined in this code are of various types, including vec3, vec4, mat3, and mat4.\n\n3. **How is this code used within the PlayCanvas engine?** \nThis code is likely used as part of a larger shader program within the PlayCanvas engine, which would use these attributes and uniforms to render 3D graphics.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/base.md"}}],["639",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/baseNineSliced.js)\n\nThis code is defining a GLSL shader program that is used for rendering a 9-sliced image. The `#define NINESLICED` statement indicates that this shader is specifically designed for rendering 9-sliced images, which are images that can be stretched or shrunk without distorting the edges. \n\nThe `varying` statements define two variables that will be passed from the vertex shader to the fragment shader. `vMask` is a 2D vector that represents the mask that will be applied to the image, and `vTiledUv` is a 2D vector that represents the texture coordinates of the image after it has been tiled. \n\nThe `uniform` statements define four variables that will be passed to the shader from the application. `innerOffset` is a 4D vector that represents the offset of the inner part of the image from the edges. `outerScale` is a 2D vector that represents the scale of the outer part of the image. `atlasRect` is a 4D vector that represents the rectangle in the texture atlas that contains the image. \n\nThis shader program can be used in the larger PlayCanvas engine project to render 9-sliced images. To use this shader, the application would need to pass in the appropriate values for the `innerOffset`, `outerScale`, and `atlasRect` uniforms, as well as the `vMask` and `vTiledUv` varying variables. \n\nHere is an example of how this shader might be used in the PlayCanvas engine:\n\n```javascript\n// create a new material\nvar material = new pc.StandardMaterial();\n\n// set the shader to the 9-sliced shader\nmaterial.shader = pc.shaderChunks.ninesliced;\n\n// set the values for the uniforms\nmaterial.setParameter('innerOffset', new pc.Vec4(0.1, 0.1, 0.1, 0.1));\nmaterial.setParameter('outerScale', new pc.Vec2(0.5, 0.5));\nmaterial.setParameter('atlasRect', new pc.Vec4(0, 0, 0.5, 0.5));\n\n// set the texture for the material\nmaterial.diffuseMap = texture;\n\n// create a new entity with a sprite component\nvar entity = new pc.Entity();\nentity.addComponent('sprite', {\n    type: 'sprite',\n    spriteAsset: spriteAsset,\n    material: material\n});\n\n// add the entity to the scene\napp.root.addChild(entity);\n``` \n\nIn this example, a new material is created and the 9-sliced shader is set as the shader for the material. The values for the `innerOffset`, `outerScale`, and `atlasRect` uniforms are set, and a texture is assigned to the material. Finally, a new entity is created with a sprite component, and the material is assigned to the sprite component. The entity is then added to the scene.\n## Questions: \n 1. What is the purpose of the `#define NINESLICED` statement at the beginning of the code?\n- The `#define NINESLICED` statement is a preprocessor directive that defines a macro named `NINESLICED`. It is likely used to enable or disable certain features or behaviors in the code.\n\n2. What do the `varying` and `uniform` keywords mean in this code?\n- The `varying` keyword is used to declare variables that are passed from the vertex shader to the fragment shader. The `uniform` keyword is used to declare variables that are constant across all vertices or fragments.\n\n3. What is the purpose of the `mediump` keyword in the `uniform` declarations?\n- The `mediump` keyword is a precision qualifier that specifies the precision of the variable. In this case, it is likely used to indicate that the variables have medium precision, which can help optimize performance on certain hardware.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/baseNineSliced.md"}}],["640",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/end.js)\n\nThis code exports a default string that contains a GLSL shader code. GLSL (OpenGL Shading Language) is a high-level shading language used to write shaders for graphics processing units (GPUs). \n\nIn the context of the PlayCanvas engine project, this code may be used to define the visual appearance of 3D objects in a scene. Shaders are used to define how light interacts with the surface of an object, determining its color, reflectivity, and other visual properties. \n\nHere is an example of how this code may be used in a PlayCanvas project:\n\n```javascript\nimport { Shader } from 'playcanvas';\n\nconst myShader = new Shader(device, {\n    attributes: {\n        aPosition: pc.SEMANTIC_POSITION,\n        aNormal: pc.SEMANTIC_NORMAL,\n        aUv0: pc.SEMANTIC_TEXCOORD0\n    },\n    vshader: /* glsl */`\n        attribute vec3 aPosition;\n        attribute vec3 aNormal;\n        attribute vec2 aUv0;\n\n        uniform mat4 matrix_model;\n        uniform mat4 matrix_viewProjection;\n\n        varying vec3 vNormal;\n        varying vec2 vUv0;\n\n        void main() {\n            gl_Position = matrix_viewProjection * matrix_model * vec4(aPosition, 1.0);\n            vNormal = aNormal;\n            vUv0 = aUv0;\n        }\n    `,\n    fshader: /* glsl */`\n        precision mediump float;\n\n        varying vec3 vNormal;\n        varying vec2 vUv0;\n\n        void main() {\n            gl_FragColor = vec4(vNormal, 1.0);\n        }\n    `\n});\n```\n\nIn this example, a new shader is created using the `Shader` class from the PlayCanvas engine. The `vshader` and `fshader` properties are set to GLSL code strings that define the vertex and fragment shaders, respectively. The `gl_Position` variable in the vertex shader is used to set the position of each vertex in the scene, while the `gl_FragColor` variable in the fragment shader is used to set the color of each pixel on the object's surface. \n\nOverall, this code is a crucial part of the PlayCanvas engine's rendering pipeline, allowing developers to define the visual appearance of 3D objects in their projects.\n## Questions: \n 1. What is the purpose of the `export default` statement in this code?\n   - The `export default` statement is used to export the code as the default export of the module.\n2. What does the `/* glsl */` comment indicate in this code?\n   - The `/* glsl */` comment indicates that the code is written in the GLSL shader language.\n3. What is the expected usage of this code within the PlayCanvas engine?\n   - Without additional context, it is unclear how this code is intended to be used within the PlayCanvas engine. Further documentation or code examples may be necessary to understand its purpose.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/end.md"}}],["641",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/extension.js)\n\nThis code exports a default string that contains a GLSL shader code. GLSL (OpenGL Shading Language) is a high-level shading language used to write shaders for graphics processing units (GPUs). \n\nIn the context of the PlayCanvas engine project, this code may be used to define the visual appearance of 3D objects in a scene. Shaders are used to define how light interacts with the surface of an object, and can be used to create effects such as reflections, shadows, and transparency. \n\nHere is an example of how this code may be used in a PlayCanvas project:\n\n```javascript\n// create a new material\nconst material = new pc.StandardMaterial();\n\n// set the shader code for the material\nmaterial.chunks.diffusePS = `\n    uniform sampler2D texture_diffuseMap;\n    varying vec2 vUv0;\n\n    void main(void) {\n        gl_FragColor = texture2D(texture_diffuseMap, vUv0);\n    }\n`;\n\n// set the diffuse texture for the material\nmaterial.diffuseMap = diffuseTexture;\n\n// assign the material to a mesh instance\nmeshInstance.material = material;\n```\n\nIn this example, a new `StandardMaterial` is created and the `diffusePS` chunk of the shader code is set to a GLSL code that samples a diffuse texture. The diffuse texture is then assigned to the material, and the material is assigned to a `MeshInstance` that represents a 3D object in the scene. \n\nOverall, this code plays an important role in defining the visual appearance of 3D objects in a PlayCanvas project.\n## Questions: \n 1. What is the purpose of the `export default` statement?\n   - The `export default` statement is used to export a single value from a module as the default export.\n\n2. What does the `/* glsl */` comment indicate?\n   - The `/* glsl */` comment indicates that the code is written in the GLSL shader language, which is used to create graphics shaders.\n\n3. What is the expected use case for this code?\n   - Without additional context, it is difficult to determine the expected use case for this code. However, it is likely that this code is part of a larger project that involves creating and rendering graphics using the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/extension.md"}}],["642",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/instancing.js)\n\nThis code exports a GLSL shader code as a default module. The shader code defines four attributes, `instance_line1`, `instance_line2`, `instance_line3`, and `instance_line4`, each of which is a 4-component vector of floating-point values. These attributes are used to define the geometry of a 3D object in the PlayCanvas engine.\n\nThe `attribute` keyword in GLSL is used to define per-vertex data that is passed from the CPU to the GPU. In this case, the four attributes define the four lines that make up the edges of a 3D object. The `vec4` type specifies a 4-component vector, which is used to represent a point in 3D space along with an additional value (such as color or texture coordinates).\n\nThe purpose of this code is to provide a basic shader module that can be used as a starting point for defining the geometry of 3D objects in the PlayCanvas engine. By defining the four lines that make up the edges of an object, developers can create complex shapes and structures that can be rendered in real-time using WebGL.\n\nHere is an example of how this code might be used in a larger project:\n\n```javascript\nimport { Shader } from 'playcanvas';\n\nconst myShader = new Shader(device, {\n    attributes: {\n        instance_line1: { type: 'vec4' },\n        instance_line2: { type: 'vec4' },\n        instance_line3: { type: 'vec4' },\n        instance_line4: { type: 'vec4' }\n    },\n    vertexShader: /* glsl */`\n        attribute vec4 instance_line1;\n        attribute vec4 instance_line2;\n        attribute vec4 instance_line3;\n        attribute vec4 instance_line4;\n\n        void main() {\n            // Use the four attributes to define the geometry of the object\n            // ...\n        }\n    `,\n    fragmentShader: /* glsl */`\n        void main() {\n            // Define the color of the object\n            // ...\n        }\n    `\n});\n\n// Use the shader to render a 3D object\n// ...\n```\n\nIn this example, a new `Shader` object is created using the `instance_line1`, `instance_line2`, `instance_line3`, and `instance_line4` attributes. The `vertexShader` function uses these attributes to define the geometry of the object, while the `fragmentShader` function defines the color of the object. The resulting shader can then be used to render a 3D object in the PlayCanvas engine.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code exports a GLSL shader code as a default module. It defines four attributes for a 2D shape instance's four lines.\n\n2. **What are the data types of the attributes defined in this code?**\\\nThe attributes defined in this code are all of type `vec4`, which represents a 4-component vector in GLSL.\n\n3. **Where is this code used in the PlayCanvas engine?**\\\nWithout additional context, it is unclear where this code is used in the PlayCanvas engine. It could potentially be used in a variety of places, such as rendering 2D shapes or defining custom materials.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/instancing.md"}}],["643",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/normal.js)\n\nThe code provided is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to calculate the normal vector of a vertex in a 3D model. The normal vector is a vector that is perpendicular to the surface of the model at a given point. It is used to calculate the lighting and shading of the model.\n\nThe code starts by defining a uniform variable called `morphNormalTex`. This variable is used to store a texture that contains morph offsets for the normal vector. If the `MORPHING_TEXTURE_BASED_NORMAL` flag is defined, the code will apply the morph offset from the texture to the normal vector.\n\nThe `getNormal()` function is then defined. This function calculates the normal vector of a vertex based on the current state of the model. The function first checks if the `SKIN` or `INSTANCING` flags are defined. If either of these flags is defined, the function calculates the normal matrix based on the current state of the model. If neither of these flags is defined, the function uses the `matrix_normal` variable to calculate the normal matrix.\n\nThe function then calculates the temporary normal vector based on the `vertex_normal` variable. If the `MORPHING` flag is defined, the function adds the morph offsets to the temporary normal vector. The morph offsets are calculated based on the `morph_weights_a` and `morph_weights_b` variables and the `morph_nrm0` to `morph_nrm7` variables.\n\nFinally, if the `MORPHING_TEXTURE_BASED_NORMAL` flag is defined, the function calculates the morph offset from the texture and adds it to the temporary normal vector. The function then returns the normalized normal vector by multiplying the temporary normal vector by the normal matrix.\n\nThis code is used in the PlayCanvas engine project to calculate the normal vectors of vertices in 3D models. The normal vectors are used to calculate the lighting and shading of the models. The code is flexible and can handle different types of models, including models with morph offsets for the normal vector.\n## Questions: \n 1. What is the purpose of the `getNormal()` function?\n- The `getNormal()` function returns the normal vector of a vertex, taking into account various factors such as skinning, instancing, and morphing.\n\n2. What is the `MORPHING_TEXTURE_BASED_NORMAL` preprocessor directive used for?\n- The `MORPHING_TEXTURE_BASED_NORMAL` preprocessor directive is used to indicate that the normal vector of a vertex should be adjusted based on a morph offset from a texture.\n\n3. What are the `morph_weights_a` and `morph_weights_b` variables used for?\n- The `morph_weights_a` and `morph_weights_b` variables are used to weight the contribution of each morph target to the final normal vector of a vertex, depending on which set of morph targets is being used.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/normal.md"}}],["644",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/normalInstanced.js)\n\nThis code exports a GLSL function called `getNormal()`. The purpose of this function is to calculate the normal vector of a vertex in a 3D model. The normal vector is a vector that is perpendicular to the surface of the model at that vertex. This is an important calculation in 3D graphics because it is used to determine how light interacts with the surface of the model.\n\nThe function takes no arguments and returns a `vec3` (a 3D vector). It first creates a `mat3` (a 3x3 matrix) called `dNormalMatrix` using the `instance_line1`, `instance_line2`, and `instance_line3` vectors. These vectors are likely related to the position and orientation of the model in 3D space. The `mat3` is created by using the `xyz` components of each vector as the rows of the matrix.\n\nNext, the function multiplies the `dNormalMatrix` by the `vertex_normal` vector. The `vertex_normal` vector is likely a vector that is associated with each vertex in the model and represents the direction that the surface of the model is facing at that vertex. Multiplying the `dNormalMatrix` by the `vertex_normal` vector transforms the `vertex_normal` vector into the coordinate space of the model.\n\nFinally, the function normalizes the resulting vector and returns it. Normalizing a vector means scaling it so that its length is 1, while preserving its direction. This is important because it ensures that the normal vector is always the same length, regardless of the size of the model.\n\nThis code is likely used in the larger PlayCanvas engine project to calculate normal vectors for 3D models. It may be used in shaders or other parts of the engine that need to interact with the surface of a model. Here is an example of how this function might be used in a shader:\n\n```glsl\nuniform mat4 modelMatrix;\nuniform mat4 viewMatrix;\nuniform mat4 projectionMatrix;\n\nattribute vec3 position;\nattribute vec3 normal;\n\nvarying vec3 vNormal;\n\nvoid main() {\n    // Transform the position and normal vectors into world space\n    vec4 worldPosition = modelMatrix * vec4(position, 1.0);\n    vec3 worldNormal = normalize(mat3(modelMatrix) * normal);\n\n    // Transform the position into clip space\n    gl_Position = projectionMatrix * viewMatrix * worldPosition;\n\n    // Calculate the normal vector in world space\n    vNormal = worldNormal;\n}\n```\n\nIn this example, the `getNormal()` function is not used directly, but its functionality is similar. The `normal` attribute is transformed into world space using the `modelMatrix`, and then normalized. The resulting vector is passed to the fragment shader as a varying variable called `vNormal`. The fragment shader can then use this normal vector to calculate lighting and other effects.\n## Questions: \n 1. What does the `/* glsl */` comment indicate in this code?\n   - The `/* glsl */` comment indicates that this code is written in GLSL (OpenGL Shading Language), which is a language used to write shaders for graphics processing units (GPUs).\n\n2. What is the purpose of the `getNormal()` function?\n   - The `getNormal()` function calculates and returns the normal vector of a vertex using the `dNormalMatrix` and `vertex_normal` variables.\n\n3. What are the `instance_line1`, `instance_line2`, and `instance_line3` variables used for?\n   - The `instance_line1`, `instance_line2`, and `instance_line3` variables are used to create a matrix (`dNormalMatrix`) that is used to transform the `vertex_normal` vector into the normal vector of the vertex. It is unclear from this code snippet what the values of these variables represent.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/normalInstanced.md"}}],["645",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/normalSkinned.js)\n\nThe code above is a GLSL shader function that calculates the normal vector of a vertex in a 3D model. The function takes no arguments and returns a vec3 (a 3D vector) representing the normal vector of the vertex. \n\nThe function first creates a mat3 (a 3x3 matrix) called dNormalMatrix using the first three columns of the dModelMatrix. The dModelMatrix is a 4x4 matrix that represents the transformation of the vertex from model space to world space. The first three columns of the dModelMatrix represent the rotation and scaling of the model, which is why they are used to create the dNormalMatrix. \n\nNext, the function multiplies the dNormalMatrix by the vertex_normal vector, which is a vec3 representing the normal vector of the vertex in model space. This multiplication transforms the normal vector from model space to world space. Finally, the function normalizes the resulting vector and returns it.\n\nThis code is likely used in the larger PlayCanvas engine project to calculate the normal vectors of vertices in 3D models. Normal vectors are important for lighting calculations, as they determine how light interacts with the surface of the model. By calculating the normal vectors in a shader function like this, the PlayCanvas engine can efficiently perform lighting calculations on 3D models in real-time. \n\nHere is an example of how this code might be used in a PlayCanvas project:\n\n```javascript\n// create a new material for a 3D model\nvar material = new pc.StandardMaterial();\n\n// set the shader function for calculating normals\nmaterial.chunks.normal = `\n    vec3 getNormal() {\n        dNormalMatrix = mat3(dModelMatrix[0].xyz, dModelMatrix[1].xyz, dModelMatrix[2].xyz);\n        return normalize(dNormalMatrix * vertex_normal);\n    }\n`;\n\n// create a new mesh instance for the 3D model\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n\n// add the mesh instance to the scene\napp.scene.addModel(model).addMeshInstances([meshInstance]);\n```\n\nIn this example, we create a new StandardMaterial for a 3D model and set the `chunks.normal` property to the GLSL shader function defined above. This tells the PlayCanvas engine to use this function to calculate the normal vectors for the model. We then create a new MeshInstance for the model and add it to the scene. The PlayCanvas engine will use the shader function to calculate the normal vectors for each vertex in the model, which will be used in lighting calculations.\n## Questions: \n 1. What is the purpose of the `/* glsl */` comment at the beginning of the code?\n   - The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax, which is used for writing shaders in WebGL and other graphics applications.\n\n2. What does the `getNormal()` function do?\n   - The `getNormal()` function calculates the normal vector of a vertex in 3D space by multiplying the vertex's normal vector with the transpose of the inverse of the model matrix.\n\n3. What is the data type of `vertex_normal` and where does it come from?\n   - The data type of `vertex_normal` is not specified in this code snippet, but it is likely a vec3 (3-component vector) since it is being multiplied with a mat3 (3x3 matrix). The source of `vertex_normal` is not shown in this code and would need to be determined from other parts of the program.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/normalSkinned.md"}}],["646",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/start.js)\n\nThis code is a GLSL shader program that sets the position of a vertex in a 3D space. The `export default` keyword indicates that this code is being exported as the default export of the module. The `/* glsl */` comment is a hint for code editors to recognize this as GLSL code and provide syntax highlighting.\n\nThe `void main(void)` function is the entry point of the shader program. It is called once for each vertex in the mesh that this shader is applied to. The `gl_Position` variable is a built-in variable in GLSL that represents the position of the vertex in clip space. The `getPosition()` function is a custom function that returns the position of the vertex in world space. The `gl_Position` variable is set to the value returned by `getPosition()`, which means that the vertex will be positioned at the same location in clip space as it is in world space.\n\nThis code is likely part of a larger project that uses the PlayCanvas engine to render 3D graphics. The shader program is compiled and executed on the GPU to transform the vertices of a mesh into clip space. This is an essential step in the rendering pipeline, as it determines the final position of each vertex on the screen. The `getPosition()` function may be defined elsewhere in the project, and could take into account various factors such as the position of the camera, lighting, and other scene parameters.\n\nHere is an example of how this shader program might be used in a PlayCanvas project:\n\n```javascript\n// Create a new material with the shader program\nvar material = new pc.StandardMaterial();\nmaterial.chunks.vertex = /* glsl */`\n    void main(void) {\n        gl_Position = getPosition();\n    }\n`;\n\n// Create a new mesh with some vertices\nvar mesh = new pc.Mesh();\nmesh.setPositions([0, 0, 0, 1, 0, 0, 0, 1, 0]);\n\n// Create a new entity with the mesh and material\nvar entity = new pc.Entity();\nentity.addComponent(\"model\", {\n    type: \"mesh\",\n    mesh: mesh,\n    material: material\n});\n\n// Add the entity to the scene\napp.root.addChild(entity);\n``` \n\nIn this example, a new material is created with the shader program, and a new mesh is created with some vertices. An entity is then created with the mesh and material, and added to the scene. When the scene is rendered, the shader program is executed for each vertex in the mesh, and the final position of each vertex is determined based on the `getPosition()` function.\n## Questions: \n 1. What is the purpose of this code?\n   - This code is a GLSL shader that sets the position of a vertex in a 3D space.\n\n2. What is the expected input for the `getPosition()` function?\n   - It is unclear from this code what the `getPosition()` function does or what input it expects. Further documentation or context is needed.\n\n3. What is the expected output of this code?\n   - The expected output of this code is a vertex position that will be used in rendering a 3D scene. However, the specific behavior and context of this code is not clear without additional information.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/start.md"}}],["647",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/tangentBinormal.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) and it defines two functions: `getTangent()` and `getBinormal()`. These functions are used to calculate the tangent and binormal vectors of a 3D model's surface at a given vertex. \n\nThe `getTangent()` function takes the `vertex_tangent` attribute as input and multiplies it by the `dNormalMatrix` matrix to transform it into world space. The resulting vector is then normalized and returned as the tangent vector. The tangent vector is a vector that lies in the plane of the surface at the vertex and is perpendicular to the surface normal. It is commonly used in normal mapping to calculate the direction of the surface detail.\n\nThe `getBinormal()` function takes the `vNormalW` and `vTangentW` attributes as input and calculates the cross product between them. The resulting vector is then multiplied by the `vertex_tangent.w` attribute and returned as the binormal vector. The binormal vector is a vector that lies in the plane of the surface at the vertex and is perpendicular to both the surface normal and the tangent vector. It is commonly used in normal mapping to calculate the direction of the surface detail.\n\nThese functions are likely used in the larger PlayCanvas engine project to generate normal maps for 3D models. Normal maps are textures that store surface detail information in the RGB channels of each texel. By using the tangent and binormal vectors calculated by these functions, the engine can calculate the direction of the surface detail and apply it to the model's surface during rendering. \n\nHere is an example of how these functions might be used in a shader:\n\n```\nuniform sampler2D normalMap;\n\nvarying vec3 vNormalW;\nvarying vec3 vTangentW;\nattribute vec4 vertex_tangent;\n\nvoid main() {\n    vec3 tangent = getTangent();\n    vec3 binormal = getBinormal();\n    vec3 normal = texture2D(normalMap, uv).rgb * 2.0 - 1.0;\n    normal = normalize(tangent * normal.r + binormal * normal.g + vNormalW * normal.b);\n    // use the calculated normal vector for lighting calculations\n}\n```\n\nIn this example, the `normalMap` texture is sampled to get the surface detail information. The tangent and binormal vectors are calculated using the `getTangent()` and `getBinormal()` functions, and the surface normal is reconstructed using these vectors and the sampled normal map. The resulting normal vector is then used for lighting calculations.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code defines two functions, `getTangent()` and `getBinormal()`, which are likely used for calculating tangent and binormal vectors in a 3D graphics application.\n\n2. **What is the input to these functions?**\\\nThe functions use variables `dNormalMatrix`, `vertex_tangent`, `vNormalW`, and `vTangentW`, which are likely defined elsewhere in the code. The specific data types and values of these variables would need to be known to understand the input to these functions.\n\n3. **What is the output of these functions?**\\\n`getTangent()` returns a normalized 3D vector, while `getBinormal()` returns the cross product of two other vectors multiplied by a scalar value. The purpose of these output values would depend on how they are used in the larger application.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/tangentBinormal.md"}}],["648",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/uv0.js)\n\nThe code provided is a shader code written in GLSL (OpenGL Shading Language) and exported as a default module. The code exports a function called `getUv0()` which returns a 2D vector representing the texture coordinates of a vertex. The function is conditionally compiled based on the presence of a preprocessor macro `NINESLICED`. \n\nIf the macro is defined, the function calculates the texture coordinates for a nine-sliced sprite. Nine-sliced sprites are used to create scalable UI elements such as buttons, panels, and windows. The sprite is divided into nine regions, and the corners are left unscaled while the middle regions are scaled to fit the desired size. The `innerOffset` variable contains the offsets for the inner regions of the sprite. The `vertex_position` variable contains the position of the vertex in the local space of the sprite. The `vertex_texCoord0` variable contains the original texture coordinates of the vertex. \n\nThe function first calculates the UV coordinates for the unscaled corners of the sprite by using the `vertex_position` variable. It then calculates the UV coordinates for the scaled regions of the sprite by using the `innerOffset` variable. The `clamp()` function is used to ensure that the offset values are within the range of 0 to 1. The final UV coordinates are calculated by multiplying the original texture coordinates with the scaled offset values. The `atlasRect` variable contains the texture atlas coordinates of the sprite. The final UV coordinates are transformed to the texture atlas space by scaling and translating them using the `atlasRect` variable. The `vMask` variable is set to the original texture coordinates of the vertex.\n\nIf the `NINESLICED` macro is not defined, the function simply returns the original texture coordinates of the vertex.\n\nThis code is used in the PlayCanvas engine to render nine-sliced sprites in the UI system. The `getUv0()` function is called by the shader program for each vertex of the sprite. The function calculates the texture coordinates for the vertex based on the position of the vertex and the texture atlas coordinates of the sprite. The resulting texture coordinates are used to sample the texture and render the sprite on the screen. \n\nExample usage:\n\n```glsl\nuniform vec4 atlasRect;\nuniform vec4 innerOffset;\n\nvarying vec2 vMask;\n\nattribute vec3 vertex_position;\nattribute vec2 vertex_texCoord0;\n\nvoid main() {\n    vec2 uv = getUv0();\n    gl_Position = vec4(vertex_position, 1.0);\n}\n``` \n\nIn this example, the `getUv0()` function is called in the vertex shader to calculate the texture coordinates for each vertex of the sprite. The resulting UV coordinates are stored in the `uv` variable and used to sample the texture in the fragment shader. The `atlasRect` and `innerOffset` uniforms are passed to the shader program to provide the texture atlas coordinates and the offsets for the inner regions of the sprite. The `vMask` varying variable is used to pass the original texture coordinates of the vertex to the fragment shader.\n## Questions: \n 1. What is the purpose of the `NINESLICED` preprocessor directive?\n- The `NINESLICED` preprocessor directive is used to conditionally compile code that calculates UV coordinates for a nine-sliced sprite.\n\n2. What is the significance of the `innerOffset` vector?\n- The `innerOffset` vector is used to offset the inner vertices of a nine-sliced sprite. It contains four values that represent the amount to offset the left, top, right, and bottom edges of the sprite.\n\n3. What is the purpose of the `vMask` variable?\n- The `vMask` variable is used to pass the texture coordinates of the vertex to the fragment shader. It is set to the `vertex_texCoord0.xy` value in the `getUv0()` function.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/uv0.md"}}],["649",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/uv1.js)\n\nThe code above is a GLSL shader function that returns a 2D vector representing the texture coordinates of a vertex. The function is named `getUv1()` and is exported as the default function of the module. \n\nIn the PlayCanvas engine, shaders are used to define how 3D models are rendered on the screen. This particular shader function is used to retrieve the texture coordinates of a vertex, which are used to map a 2D texture onto a 3D model. \n\nThe function takes no arguments and simply returns the `vertex_texCoord1` variable, which is assumed to be a 2D vector representing the texture coordinates of the vertex. \n\nHere is an example of how this function might be used in a PlayCanvas shader:\n\n```glsl\nuniform sampler2D texture;\nvarying vec2 vUv1;\n\nvoid main() {\n    vec4 color = texture2D(texture, vUv1);\n    gl_FragColor = color;\n}\n```\n\nIn this example, the `getUv1()` function is used to retrieve the texture coordinates of each vertex and pass them to the fragment shader as a varying variable named `vUv1`. The fragment shader then uses these texture coordinates to sample the texture and set the color of the pixel being rendered. \n\nOverall, this code is a small but important part of the PlayCanvas engine's rendering pipeline, allowing developers to easily map textures onto 3D models.\n## Questions: \n 1. What is the purpose of this code?\n   - This code exports a GLSL function that returns a vec2 representing the texture coordinates of a vertex.\n\n2. How is this code used within the PlayCanvas engine?\n   - It is unclear from this code alone how it is used within the PlayCanvas engine. It may be used as part of a shader program or in some other context.\n\n3. Are there any dependencies or requirements for using this code?\n   - It is unclear from this code alone if there are any dependencies or requirements for using it. It may require certain libraries or be dependent on other code within the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/uv1.md"}}],["650",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/lit/vert/viewNormal.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) and it defines a uniform variable `matrix_view` of type `mat4` which represents the view matrix. The view matrix is used to transform the world coordinates into view coordinates, which is the coordinate system relative to the camera. \n\nThe `getViewNormal()` function returns the normal vector of the current vertex in view space. It does this by multiplying the world normal vector `vNormalW` with the upper-left 3x3 submatrix of the view matrix `matrix_view`. This is because the normal vector needs to be transformed by the inverse transpose of the model-view matrix to maintain its perpendicularity to the surface after transformation. \n\nThis code is a part of the PlayCanvas engine project and is used to render 3D graphics. The view matrix is an essential component of the rendering pipeline as it determines the position and orientation of the camera in the scene. The `getViewNormal()` function is used to calculate the normal vector of the vertex in view space, which is used in lighting calculations to determine the amount of light reflected off the surface. \n\nHere is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// create a new material\nvar material = new pc.StandardMaterial();\n\n// set the shader code\nmaterial.chunks.viewNormal = /* glsl */`\n    #ifndef VIEWMATRIX\n    #define VIEWMATRIX\n    uniform mat4 matrix_view;\n    #endif\n\n    vec3 getViewNormal() {\n        return mat3(matrix_view) * vNormalW;\n    }\n`;\n\n// set the material on a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n``` \n\nIn this example, a new material is created and the `chunks.viewNormal` property is set to the shader code. This will ensure that the `getViewNormal()` function is included in the shader code when the material is used to render a mesh instance.\n## Questions: \n 1. **What is the purpose of the `#ifndef VIEWMATRIX` and `#define VIEWMATRIX` statements?**\\\nThese statements are used to ensure that the `matrix_view` uniform is only defined once in the code, preventing any potential errors that could arise from multiple definitions.\n\n2. **What is the data type of `matrix_view`?**\\\nBased on the `mat4` keyword used to define it, `matrix_view` is likely a 4x4 matrix data type.\n\n3. **What is the purpose of the `getViewNormal()` function?**\\\nThe `getViewNormal()` function returns the normal vector of the current view, which is calculated by multiplying the `vNormalW` vector by the `matrix_view` uniform after converting it to a 3x3 matrix.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/lit/vert/viewNormal.md"}}],["651",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle.js)\n\nThe code above is a GLSL shader that is used to render particle effects in the PlayCanvas engine. The shader takes in two textures, `colorMap` and `colorParam`, and uses them to calculate the final color of each particle. \n\nThe `texCoordsAlphaLife` varying is used to pass in the texture coordinates, alpha value, and life value of each particle. These values are used to sample the `colorMap` and `colorParam` textures to get the color and ramp values for each particle. \n\nThe `graphSampleSize` and `graphNumSamples` uniforms are used to control the size and number of samples taken from the ramp texture. These samples are used to interpolate the ramp value for each particle based on its alpha value. \n\nThe `CAMERAPLANES` define is used to pass in the camera parameters to the shader. The `softening` and `colorMult` uniforms are used to control the softening and color multiplication of the final particle color. \n\nThe `saturate` function is a helper function that clamps a value between 0 and 1. The `unpackFloat` function is another helper function that is used to unpack a float value from a RGBA color value. \n\nThe `main` function is the entry point of the shader. It first samples the `colorMap` and `colorParam` textures using the `texCoordsAlphaLife` varying. It then applies gamma correction to the sampled color values using the `gammaCorrectInput` function (not shown in this code). \n\nThe `ramp` value is then multiplied by the `colorMult` uniform and added to the alpha value of the particle. The `rgb` value of the particle is then multiplied by the `ramp.rgb` value to get the final color value. The `a` value of the particle is multiplied by the `ramp.a` value to get the final alpha value. \n\nOverall, this shader is an important part of the PlayCanvas engine's particle system. It allows for the creation of complex particle effects by using textures to control the color and ramp values of each particle.\n## Questions: \n 1. What is the purpose of the `gammaCorrectInput` function?\n    - The `gammaCorrectInput` function is used to apply gamma correction to the input texture.\n\n2. What is the significance of the `softening` uniform variable?\n    - The `softening` uniform variable is used to control the amount of softening applied to the texture.\n\n3. What is the purpose of the `unpackFloat` function?\n    - The `unpackFloat` function is used to unpack a float value from a vec4 RGBA depth value.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle.md"}}],["652",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleInputFloat.js)\n\nThe code provided is a GLSL shader function that reads input data from a texture and assigns it to various variables. This function is likely used in the PlayCanvas engine to control the behavior of particles in a particle system.\n\nThe function takes in a single float parameter, `uv`, which represents the texture coordinate to sample from. The function then uses this parameter to sample two different colors from the `particleTexIN` texture. The first color is sampled at the `uv` coordinate and a fixed `0.25` value for the y-coordinate. The second color is sampled at the same `uv` coordinate but with a `0.75` y-coordinate value.\n\nThe function then assigns the `xyz` components of the first color to the `inPos` variable, which likely represents the position of the particle. The `xyz` components of the second color are assigned to the `inVel` variable, which likely represents the velocity of the particle. The `w` component of the first color is used to calculate the `inAngle` variable, which likely represents the angle of the particle. If the `w` component is less than 0, the value is negated before subtracting 1000. This suggests that negative values represent angles in the opposite direction. Finally, the `w` component of the second color is used to set the `inLife` variable, which likely represents the remaining lifespan of the particle.\n\nOverall, this function is an important part of the PlayCanvas engine's particle system. It allows the engine to read input data from a texture and assign it to various variables that control the behavior of individual particles. Here is an example of how this function might be used in a larger shader program:\n\n```glsl\nuniform sampler2D particleTexIN;\nvarying float vUV;\n\nattribute vec3 aPosition;\n\nvarying vec3 inPos;\nvarying vec3 inVel;\nvarying float inAngle;\nvarying bool inShow;\nvarying float inLife;\n\nvoid main() {\n    readInput(vUV);\n\n    // Use inPos, inVel, inAngle, inShow, and inLife to control particle behavior\n\n    gl_Position = vec4(aPosition, 1.0);\n}\n```\n## Questions: \n 1. What is the purpose of this code and where is it used in the PlayCanvas engine?\n- This code defines a function called `readInput` that reads input data from a texture and assigns values to various variables. It is likely used in a particle system or other graphics-related feature of the PlayCanvas engine.\n\n2. What is the data type of the `particleTexIN` variable and where is it defined?\n- The data type of `particleTexIN` is not specified in this code snippet, so a developer may need to look elsewhere in the PlayCanvas engine codebase to find its definition and data type.\n\n3. What is the significance of the value 0.25 and 0.75 in the `texture2D` function calls?\n- The values 0.25 and 0.75 are used as the y-coordinate of the texture coordinate passed to the `texture2D` function. Without more context, it is unclear what these values represent or why they were chosen.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleInputFloat.md"}}],["653",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleInputRgba8.js)\n\nThe code is a GLSL shader that is used to read input data for particle effects in the PlayCanvas engine. The purpose of this code is to decode the input data from a texture and convert it into usable values for the particle system. \n\nThe code starts by defining some constants and uniforms that are used throughout the shader. The `inBoundsSize` and `inBoundsCenter` uniforms define the size and center of the bounding box for the particle system. The `maxVel` uniform defines the maximum velocity of the particles. \n\nThe `decodeFloatRG` and `decodeFloatRGBA` functions are used to decode floating point values from the input texture. The `readInput` function is the main function that reads the input texture and converts the data into usable values. \n\nThe input texture is read using the `texture2D` function, which takes a UV coordinate and returns a color value. The texture is read at four different UV coordinates to get the position, velocity, angle, and life of each particle. \n\nThe `decodeFloatRG` function is used to decode the position and angle values from the texture. The position is stored in the red and green channels of the first texture read, and the angle is stored in the blue and alpha channels of the second texture read. The `decodeFloatRGBA` function is used to decode the life value from the fourth texture read. \n\nThe decoded values are then converted into usable values for the particle system. The position is scaled and translated to fit within the bounding box defined by `inBoundsSize` and `inBoundsCenter`. The velocity is scaled to fit within the maximum velocity defined by `maxVel`. The angle is converted from a value between 0 and 1 to a value between 0 and 2. The `inShow` value is set to true if the particle should be visible, based on the alpha value of the velocity texture. \n\nFinally, the `inLife` value is calculated based on the decoded life value and the maximum and minimum possible life values for the particle system. This value is used to determine when the particle should be removed from the system. \n\nOverall, this code is an important part of the PlayCanvas engine's particle system. It allows the engine to read input data from a texture and convert it into usable values for the particle system. This code is likely used in conjunction with other shaders and code to create a complete particle system.\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n    \n    This code is a GLSL shader code used for particle simulation in the PlayCanvas engine. It reads input data from a texture and decodes it to set particle position, velocity, angle, visibility, and life.\n\n2. What is the significance of the constants defined in this code, such as PI2 and maxVel?\n    \n    PI2 is a constant used to convert angle from radians to degrees. maxVel is a uniform variable used to scale the velocity of particles. \n\n3. What is the format of the input texture used in this code and how is it structured?\n    \n    The input texture is a 2D texture with a height of 32 pixels and a width equal to the number of particles. Each row of the texture contains 4 pixels, each representing a different attribute of the particle (position, angle, velocity, and life) encoded as RGBA values. The decodeFloatRG and decodeFloatRGBA functions are used to extract the values from the texture.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleInputRgba8.md"}}],["654",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleOutputFloat.js)\n\nThe code provided is a GLSL shader function called `writeOutput()`. This function is responsible for writing the output color of a fragment (a pixel on the screen) during the rendering process. \n\nThe function first checks if the `y` coordinate of the current fragment is less than 1.0. If it is, it sets the output color to a combination of the `outPos` vector (which likely represents the position of the fragment in 3D space) and the `outAngle` value (which is added to 1000.0 and multiplied by a `visMode` variable). This combination is packed into a `vec4` vector and assigned to the `gl_FragColor` output variable. \n\nIf the `y` coordinate of the fragment is greater than or equal to 1.0, the output color is set to a combination of the `outVel` vector (which likely represents the velocity of the fragment) and the `outLife` value (which likely represents the remaining lifespan of the fragment). This combination is also packed into a `vec4` vector and assigned to the `gl_FragColor` output variable. \n\nThis function is likely used in a larger rendering pipeline to determine the final color of each fragment in a scene. It may be called multiple times during the rendering process, depending on the complexity of the scene and the number of fragments that need to be processed. \n\nHere is an example of how this function might be used in a larger GLSL shader program:\n\n```\n// Define the writeOutput function\nvoid writeOutput() {\n    if (gl_FragCoord.y<1.0) {\n        gl_FragColor = vec4(outPos, (outAngle + 1000.0) * visMode);\n    } else {\n        gl_FragColor = vec4(outVel, outLife);\n    }\n}\n\n// Main shader function\nvoid main() {\n    // Perform some calculations to determine the values of outPos, outAngle, outVel, and outLife\n    // ...\n\n    // Call the writeOutput function to set the output color of the current fragment\n    writeOutput();\n}\n```\n## Questions: \n 1. What is the purpose of this code and where is it used in the PlayCanvas engine?\n- This code is a GLSL shader function called \"writeOutput\" that writes output to the screen. It is likely used in the rendering pipeline of the PlayCanvas engine.\n\n2. What are the inputs and outputs of this function?\n- The function does not have any explicit inputs, but it uses the built-in variable \"gl_FragCoord\" to determine the y-coordinate of the current fragment. The function outputs a color value to \"gl_FragColor\".\n\n3. What is the significance of the \"visMode\" variable in the calculation of the output color?\n- The \"visMode\" variable is a multiplier that affects the output color's alpha channel. It is likely used to control the visibility of the output based on some external factor.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleOutputFloat.md"}}],["655",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleOutputRgba8.js)\n\nThe code above is a GLSL shader code that is used to write output data for a particle system. The output data is written to the screen as colors. The purpose of this code is to encode the particle system data into colors that can be displayed on the screen. \n\nThe code exports a function called `writeOutput()` which is responsible for encoding the particle system data into colors. The function takes no arguments and is called by the particle system to write the output data. \n\nThe `writeOutput()` function first applies a transformation to the particle system position, angle, and velocity data. The transformation is done using the `outBoundsMul` and `outBoundsAdd` uniforms. These uniforms are used to scale and translate the particle system data to fit within the screen bounds. \n\nNext, the function encodes the particle system data into colors. The encoding is done using two functions called `encodeFloatRG()` and `encodeFloatRGBA()`. These functions take a float value and encode it into a color value. The `encodeFloatRG()` function encodes a float value into a two-component color value (red and green channels). The `encodeFloatRGBA()` function encodes a float value into a four-component color value (red, green, blue, and alpha channels). \n\nFinally, the function sets the output color value based on the type of particle system data being encoded. The output color value is set to the encoded position data for the first row of pixels, encoded angle data for the second row of pixels, encoded velocity data for the third row of pixels, and encoded lifetime data for the fourth row of pixels. \n\nOverall, this code is an important part of the PlayCanvas engine as it is responsible for rendering particle systems. The code allows for the encoding of particle system data into colors that can be displayed on the screen. This is an essential part of rendering particle systems as it allows for the visualization of particle system data. \n\nExample usage of this code would be as follows:\n\n```javascript\nimport particleShader from 'path/to/particleShader.glsl';\n\n// create a WebGL context and compile the shader\nconst gl = canvas.getContext('webgl');\nconst shader = gl.createShader(gl.FRAGMENT_SHADER);\ngl.shaderSource(shader, particleShader);\ngl.compileShader(shader);\n\n// create a particle system and set the shader program\nconst particleSystem = new ParticleSystem();\nparticleSystem.setShaderProgram(gl.createProgram());\ngl.attachShader(particleSystem.getShaderProgram(), shader);\ngl.linkProgram(particleSystem.getShaderProgram());\n\n// update and render the particle system\nparticleSystem.update();\nparticleSystem.render();\n```\n## Questions: \n 1. What is the purpose of the `encodeFloatRG` and `encodeFloatRGBA` functions?\n- These functions are used to encode floating point values into RGBA or RG color values for output.\n\n2. What is the purpose of the `writeOutput` function?\n- This function is responsible for writing the final output values to the `gl_FragColor` variable, which is used to render particles.\n\n3. What do the `outBoundsMul` and `outBoundsAdd` uniform variables represent?\n- These variables are used to transform the output position of particles by scaling and translating them before they are rendered.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleOutputRgba8.md"}}],["656",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleUpdaterAABB.js)\n\nThe code provided is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to calculate the spawn position of particles and add initial velocity to them. \n\nThe `calcSpawnPosition` function takes in two uniform variables, `spawnBounds` and `spawnPosInnerRatio`, and two parameters, `inBounds` and `rndFactor`. It calculates the spawn position of particles based on the given bounds and a random factor. The `spawnBounds` uniform variable is a 3x3 matrix that defines the bounds of the particle emitter. The `spawnPosInnerRatio` uniform variable is a vector that defines the inner ratio of the spawn position. The `inBounds` parameter is the position of the particle in the bounds. The `rndFactor` parameter is a random factor that is used to add some randomness to the spawn position.\n\nThe function first calculates the position of the particle relative to the center of the bounds. It then calculates the absolute value of the position and finds the maximum value of the x, y, and z components. It then calculates the edge of the spawn position by multiplying the maximum position with the `spawnPosInnerRatio` vector. Finally, it calculates the final position of the particle by multiplying the edge with the sign of the position.\n\nThe `addInitialVelocity` function takes in two parameters, `localVelocity` and `inBounds`. It subtracts the `initialVelocity` value from the z component of the `localVelocity` vector. This function is used to add initial velocity to the particles.\n\nOverall, this code is used to calculate the spawn position of particles and add initial velocity to them. It is an important part of the PlayCanvas engine project as it is used to create particle effects in games and other applications. Here is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// create a particle system\nvar particleSystem = new pc.ParticleSystem();\n\n// set the spawn bounds and inner ratio\nparticleSystem.spawnBounds = new pc.Mat3().setIdentity();\nparticleSystem.spawnPosInnerRatio = new pc.Vec3(0.5, 0.5, 0.5);\n\n// set the initial velocity\nparticleSystem.initialVelocity = 0.1;\n\n// add the GLSL shader code\nparticleSystem.shader = `\n    uniform mat3 spawnBounds;\n    uniform vec3 spawnPosInnerRatio;\n\n    vec3 calcSpawnPosition(vec3 inBounds, float rndFactor) {\n        // code here\n    }\n\n    void addInitialVelocity(inout vec3 localVelocity, vec3 inBounds) {\n        // code here\n    }\n`;\n\n// add the particle system to the scene\napp.root.addChild(particleSystem.entity);\n```\n## Questions: \n 1. What is the purpose of the `calcSpawnPosition` function?\n- The `calcSpawnPosition` function calculates the spawn position of an object within a given boundary and with a random factor.\n\n2. What is the purpose of the `addInitialVelocity` function?\n- The `addInitialVelocity` function subtracts a specified initial velocity from the local velocity of an object.\n\n3. What is the significance of the `/* glsl */` comment at the beginning of the code?\n- The `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language), a C-like language used to write shaders for graphics processing units (GPUs).","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleUpdaterAABB.md"}}],["657",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleUpdaterEnd.js)\n\nThis code exports a GLSL shader function called \"writeOutput\" using the default export syntax. GLSL (OpenGL Shading Language) is a C-like language used to write shaders for graphics processing units (GPUs). \n\nThe purpose of this code is to provide a way to write output from a shader program. In the context of the PlayCanvas engine, this could be used to render graphics on a canvas or display them on a screen. \n\nHere is an example of how this code might be used in a larger project:\n\n```javascript\nimport writeOutput from 'path/to/writeOutput.glsl';\n\n// create a WebGL context\nconst gl = canvas.getContext('webgl');\n\n// create a shader program\nconst program = gl.createProgram();\nconst vertexShader = gl.createShader(gl.VERTEX_SHADER);\nconst fragmentShader = gl.createShader(gl.FRAGMENT_SHADER);\n\n// compile the shaders\ngl.shaderSource(vertexShader, vertexShaderSource);\ngl.compileShader(vertexShader);\ngl.shaderSource(fragmentShader, fragmentShaderSource);\ngl.compileShader(fragmentShader);\n\n// attach the shaders to the program\ngl.attachShader(program, vertexShader);\ngl.attachShader(program, fragmentShader);\n\n// link the program\ngl.linkProgram(program);\n\n// use the program\ngl.useProgram(program);\n\n// set up the output buffer\nconst outputBuffer = gl.createBuffer();\ngl.bindBuffer(gl.ARRAY_BUFFER, outputBuffer);\ngl.bufferData(gl.ARRAY_BUFFER, new Float32Array([0, 0, 0, 1]), gl.STATIC_DRAW);\n\n// set up the writeOutput function\nconst writeOutputLocation = gl.getUniformLocation(program, 'writeOutput');\ngl.uniform1i(writeOutputLocation, 1);\n\n// render the graphics\ngl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);\n\n// call the writeOutput function to write the output\nwriteOutput();\n```\n\nIn this example, the GLSL shader function \"writeOutput\" is imported from a file located at 'path/to/writeOutput.glsl'. The function is then used in the WebGL rendering pipeline to write the output of the shader program to the screen. \n\nOverall, this code provides a way to write output from a GLSL shader program and is an important part of the PlayCanvas engine's graphics rendering capabilities.\n## Questions: \n 1. What is the purpose of the `writeOutput()` function?\n   - It is not clear from this code snippet what the `writeOutput()` function does or what its intended purpose is.\n\n2. What language is this code written in?\n   - This code appears to be written in GLSL (OpenGL Shading Language), which is a C-like language used to write shaders for graphics processing.\n\n3. What is the context or location of this code within the PlayCanvas engine?\n   - Without additional information or context, it is unclear where this code is located within the PlayCanvas engine or how it fits into the overall architecture of the engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleUpdaterEnd.md"}}],["658",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleUpdaterInit.js)\n\nThis code is a GLSL shader code that defines a set of uniform variables and varying variables used in the PlayCanvas engine. The purpose of this code is to define the properties of particles that are used in particle systems. \n\nThe `varying vec2 vUv0` variable is used to pass the texture coordinates of the particle to the fragment shader. \n\nThe `uniform highp sampler2D` variables are used to store the textures used for the particle system. The `particleTexIN` variable is the texture used for the particle itself, while the `internalTex0`, `internalTex1`, `internalTex2`, and `internalTex3` variables are used for internal calculations. \n\nThe `uniform mat3 emitterMatrix` and `emitterMatrixInv` variables are used to transform the particle's position and velocity based on the emitter's position and orientation. The `emitterScale` variable is used to scale the particle's size. \n\nThe `uniform vec3 emitterPos` variable is used to set the emitter's position, while the `frameRandom` variable is used to add randomness to the particle system. The `localVelocityDivMult` and `velocityDivMult` variables are used to control the particle's velocity. \n\nThe `delta` variable is used to calculate the time elapsed since the last frame, while the `rate`, `rateDiv`, `lifetime`, `numParticles`, `rotSpeedDivMult`, `radialSpeedDivMult`, and `seed` variables are used to control the particle's behavior. \n\nThe `startAngle`, `startAngle2`, and `initialVelocity` variables are used to set the particle's initial angle and velocity. \n\nThe `graphSampleSize` and `graphNumSamples` variables are used to control the particle's visual appearance. \n\nThe `vec3 inPos`, `inVel`, `float inAngle`, `bool inShow`, and `float inLife` variables are used to store the particle's current position, velocity, angle, visibility, and remaining lifetime. \n\nThe `vec3 outPos`, `outVel`, `float outAngle`, `bool outShow`, and `float outLife` variables are used to store the particle's next position, velocity, angle, visibility, and remaining lifetime. \n\nOverall, this code defines the properties of particles used in the PlayCanvas engine's particle system. These properties can be adjusted to create a wide range of particle effects, such as explosions, smoke, and fire.\n## Questions: \n 1. What is the purpose of this code and how is it used within the PlayCanvas engine?\n- This code defines a GLSL shader program for particle effects within the PlayCanvas engine. It is used to render particles with various properties such as position, velocity, angle, and lifetime.\n\n2. What are the meanings and expected data types of the various uniform variables declared in this code?\n- The uniform variables represent various properties of the particle system, such as the texture used for the particles, the emitter position and scale, the rate of particle emission, the lifetime of particles, and various multipliers/divisors for velocity and rotation speed. The expected data types are mostly floats and vec3s.\n\n3. How does the particle system handle visualization modes and what are the possible values for the \"visMode\" variable?\n- The \"visMode\" variable is used to determine how particles are visualized, with possible values of 0 (points), 1 (billboards), and 2 (stretched billboards). The particle system likely uses this variable to determine how to render particles based on the desired visual effect.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleUpdaterInit.md"}}],["659",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleUpdaterNoRespawn.js)\n\nThe code snippet provided is a GLSL shader code that is used to control the lifetime of particles in a particle system. The purpose of this code is to ensure that particles are removed from the system once they have exceeded their lifetime. \n\nThe code achieves this by checking if the current lifetime of the particle is greater than or equal to the maximum lifetime. If this condition is true, the code subtracts the maximum lifetime from the current lifetime and sets the visibility mode of the particle to -1.0. This effectively hides the particle from view and marks it for removal from the system.\n\nThis code is likely used in conjunction with other GLSL shader code and JavaScript code to create and manage a particle system in the PlayCanvas engine. The particle system may be used to create various visual effects such as explosions, smoke, fire, and more. \n\nHere is an example of how this code may be used in a larger project:\n\n```javascript\n// Create a new particle system\nconst particleSystem = new pc.ParticleSystem();\n\n// Set the maximum lifetime of particles to 5 seconds\nparticleSystem.maxLifetime = 5;\n\n// Set the particle rate to 10 particles per second\nparticleSystem.particleRate = 10;\n\n// Set the shader code for particle lifetime control\nparticleSystem.shader = /* glsl */`\n    if (outLife >= lifetime) {\n        outLife -= max(lifetime, (numParticles - 1.0) * particleRate);\n        visMode = -1.0;\n    }\n`;\n\n// Add the particle system to the scene\napp.root.addChild(particleSystem.entity);\n```\n\nIn this example, we create a new particle system and set its maximum lifetime and particle rate. We then set the shader code for particle lifetime control using the code snippet provided. Finally, we add the particle system to the scene. \n\nOverall, this code plays an important role in managing the lifetime of particles in a particle system and is a crucial component of creating various visual effects in the PlayCanvas engine.\n## Questions: \n 1. What is the purpose of this code?\n   - This code appears to be a GLSL shader code that updates the lifetime and visibility mode of particles in a particle system.\n\n2. What are the inputs and outputs of this code?\n   - It is unclear what the inputs and outputs of this code are without additional context. It is possible that `outLife` and `visMode` are outputs, but it is unclear what the inputs are.\n\n3. How does this code fit into the overall PlayCanvas engine project?\n   - Without additional context, it is unclear how this code fits into the overall PlayCanvas engine project. It is possible that this code is part of a larger particle system module or shader library.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleUpdaterNoRespawn.md"}}],["660",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleUpdaterOnStop.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) and it is exported as a default module. The purpose of this code is to set the visibility mode of an object based on its life value. \n\nThe code first checks if the life value of the object is less than zero. If it is, then the visibility mode is set to -1.0, which means the object is invisible. If the life value is greater than or equal to zero, then the visibility mode remains unchanged. \n\nThis code can be used in the larger PlayCanvas engine project to create visual effects for objects in a scene. For example, if an object represents a character in a game and its life value decreases as it takes damage, this code can be used to gradually make the character invisible as its life value approaches zero. \n\nHere is an example of how this code can be used in a shader:\n\n```\nuniform float outLife; // life value of the object\nvarying float visMode; // visibility mode of the object\n\nvoid main() {\n  // set the visibility mode based on the life value\n  visMode = outLife < 0.0? -1.0: visMode;\n  // set the color of the object\n  gl_FragColor = vec4(1.0, 1.0, 1.0, 1.0);\n}\n```\n\nIn this example, the `outLife` uniform variable represents the life value of the object, and the `visMode` varying variable represents the visibility mode of the object. The `main()` function sets the visibility mode using the code from the original example, and then sets the color of the object to white. \n\nOverall, this code is a small but important part of the PlayCanvas engine project, as it allows developers to create dynamic visual effects for objects in a scene based on their life values.\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code is setting the value of `visMode` based on whether `outLife` is less than 0.0 or not.\n\n2. What is the data type of `visMode` and `outLife`?\n    \n    The code does not provide information about the data type of `visMode` and `outLife`. It is possible that they are defined in another part of the code.\n\n3. What is the context in which this code is used?\n    \n    Without additional information about the context in which this code is used, it is difficult to determine its significance and how it fits into the overall functionality of the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleUpdaterOnStop.md"}}],["661",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleUpdaterRespawn.js)\n\nThis code is a GLSL shader code that is likely used in the PlayCanvas engine to control the visibility of particles in a particle system. The purpose of this code is to determine when a particle should be visible or not based on its lifetime and the particle rate. \n\nThe code starts by checking if the particle's lifetime has exceeded the maximum lifetime. If it has, then the code subtracts the maximum lifetime from the particle's remaining lifetime (outLife) and sets the visibility mode (visMode) to 1.0. This means that the particle is now visible. \n\nIf the particle's remaining lifetime is less than 0.0, then the visibility mode is set to 1.0 regardless of whether the particle has exceeded its maximum lifetime or not. This ensures that the particle is visible if it has not yet exceeded its maximum lifetime but has already reached the end of its lifetime. \n\nThis code is likely used in conjunction with other code in the PlayCanvas engine to create particle effects in games or other interactive applications. For example, the code may be used to create a particle system that simulates the explosion of a bomb. The particles would be invisible until the bomb explodes, at which point the particles would become visible and start to disperse. \n\nOverall, this code is an important part of the PlayCanvas engine's particle system functionality and helps to create dynamic and engaging visual effects in games and other interactive applications.\n## Questions: \n 1. What is the purpose of this code?\n    - This code appears to be a GLSL shader code that updates the visibility mode of particles based on their lifetime and particle rate.\n\n2. What are the inputs and outputs of this code?\n    - The inputs and outputs of this code are not explicitly stated, but it can be inferred that the code takes in variables for particle lifetime, number of particles, and particle rate, and outputs a visibility mode value.\n\n3. How is this code used within the PlayCanvas engine?\n    - It is unclear how this code is specifically used within the PlayCanvas engine without further context. It may be used as part of a particle system or visual effect.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleUpdaterRespawn.md"}}],["662",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleUpdaterSphere.js)\n\nThe code provided is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to calculate the spawn position and initial velocity of particles in a particle system. \n\nThe `calcSpawnPosition` function takes in two uniform variables, `spawnBoundsSphere` and `spawnBoundsSphereInnerRatio`, which are used to calculate the spawn position of particles. The function first generates a random number using the `fract` function and the `rndFactor` parameter. It then normalizes the input bounds and calculates a random radius `r` based on the `spawnBoundsSphereInnerRatio` and the random number generated earlier. Finally, it returns the spawn position of the particle based on whether the `LOCAL_SPACE` flag is defined or not. If it is defined, the spawn position is returned as `norm * r * spawnBoundsSphere`, otherwise, it is returned as `emitterPos + norm * r * spawnBoundsSphere`.\n\nThe `addInitialVelocity` function takes in two parameters, `localVelocity` and `inBounds`, and adds an initial velocity to the `localVelocity` vector. The initial velocity is calculated by normalizing the input bounds and multiplying it by the `initialVelocity` uniform variable.\n\nOverall, this code is used to generate the initial spawn position and velocity of particles in a particle system. It can be used in conjunction with other GLSL shader code to create complex particle effects. Here is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// Create a particle system entity\nvar particleSystem = new pc.Entity();\nparticleSystem.addComponent('particlesystem', {\n    // Set the particle system properties\n    // ...\n});\n\n// Add the GLSL shader code to the particle system\nparticleSystem.particlesystem.shader = `\n    // Import the GLSL code from the file\n    import \"particle-shader.glsl\";\n\n    // Set the spawn bounds and initial velocity uniforms\n    uniform float spawnBoundsSphere;\n    uniform float spawnBoundsSphereInnerRatio;\n    uniform vec3 emitterPos;\n    uniform float initialVelocity;\n\n    // Calculate the spawn position and initial velocity of particles\n    vec3 spawnPosition = calcSpawnPosition(particlePosition, randomFactor);\n    vec3 initialVelocity = vec3(0);\n    addInitialVelocity(initialVelocity, particlePosition);\n\n    // Set the particle position and velocity\n    particlePosition = spawnPosition;\n    particleVelocity = initialVelocity;\n`;\n```\n## Questions: \n 1. What is the purpose of the `calcSpawnPosition` function?\n- The `calcSpawnPosition` function calculates a random spawn position within a given bounds sphere, based on a random factor and a ratio for the inner radius of the sphere.\n\n2. What is the significance of the `spawnBoundsSphere` and `spawnBoundsSphereInnerRatio` uniform variables?\n- `spawnBoundsSphere` is the radius of the bounds sphere used for spawning particles, while `spawnBoundsSphereInnerRatio` is the ratio of the inner radius of the sphere to the outer radius. These variables are used in the `calcSpawnPosition` function to calculate a random spawn position within the bounds sphere.\n\n3. What does the `addInitialVelocity` function do?\n- The `addInitialVelocity` function adds an initial velocity to a given local velocity vector, based on a given bounds vector. The velocity is calculated as the normalized difference between the bounds vector and the center point of the bounds.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleUpdaterSphere.md"}}],["663",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particleUpdaterStart.js)\n\nThe code provided is a GLSL shader that is used to simulate particle effects in the PlayCanvas engine. The shader takes in various inputs such as the particle's position, velocity, and lifetime, and outputs the updated position, velocity, and angle of the particle. \n\nThe `saturate` function is a utility function that clamps a value between 0 and 1. The `unpack3NFloats` function takes in a float value and unpacks it into three normalized floats between 0 and 1. The `tex1Dlod_lerp` function takes in a texture and a texture coordinate, and performs a level-of-detail texture lookup with linear interpolation. It returns the interpolated color value and the unpacked normalized float value from the texture's alpha channel. \n\nThe `hash41` function generates a random value between 0 and 1 based on a given input value. It uses a 4D vector of hash values to generate the random value. \n\nThe `main` function is the entry point of the shader. It first checks if the particle's x-coordinate is greater than the total number of particles, and discards the particle if it is. It then reads in the input values such as the particle's position, velocity, and lifetime. The `visMode` variable is set based on whether the particle should be visible or not. \n\nThe `rndFactor` variable is generated using the `hash41` function and is used to add randomness to the particle's rate, velocity, and rotation speed. The `outLife` variable is calculated by adding the `delta` value to the particle's current lifetime. The `nlife` variable is a normalized value between 0 and 1 that represents the particle's current lifetime as a percentage of its total lifetime. \n\nThe `localVelocity`, `velocity`, and `params` variables are obtained by performing a level-of-detail texture lookup using the `tex1Dlod_lerp` function. These variables represent the particle's local velocity, global velocity, and other parameters such as rotation speed. The `radialParams` variable is also obtained using `tex1Dlod_lerp` and represents the particle's radial velocity and speed. \n\nThe `respawn` variable is set to true if the particle's lifetime has expired or if it has reached its maximum lifetime. If `respawn` is true, the particle's position and angle are recalculated based on the `rndFactor` value. \n\nThe `radialVel` variable is calculated based on the particle's position relative to the emitter's position. If the `LOCAL_SPACE` flag is defined, the `radialVel` variable is calculated based on the particle's local position. The `localVelocity`, `velocity`, and `rotSpeed` variables are modified based on the `rndFactor` value. \n\nFinally, the `outVel` and `outPos` variables are calculated based on the particle's current position, velocity, and angle. If the `LOCAL_SPACE` flag is defined, the `outVel` variable is calculated based on the particle's local position. \n\nOverall, this shader is an essential component of the PlayCanvas engine's particle system. It allows for the creation of dynamic and visually appealing particle effects by simulating the behavior of individual particles. The `tex1Dlod_lerp` function is particularly useful for interpolating between different levels of detail in the particle's texture, allowing for smooth transitions between different particle states. The `hash41` function adds an element of randomness to the particle's behavior, making each particle unique.\n## Questions: \n 1. What is the purpose of the `saturate` function?\n   \n   The `saturate` function clamps a float value between 0.0 and 1.0 and returns the result. It is likely used to ensure that values stay within a certain range.\n\n2. What is the `hash41` function used for?\n   \n   The `hash41` function generates a random vector based on a given float value. It is likely used to introduce randomness into the particle system.\n\n3. What is the purpose of the `tex1Dlod_lerp` function?\n   \n   The `tex1Dlod_lerp` function performs a texture lookup and linear interpolation based on a given texture coordinate and an output vector. It is likely used to sample from internal textures to determine particle behavior.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particleUpdaterStart.md"}}],["664",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_blendAdd.js)\n\nThis code is written in GLSL, which is a shading language used for graphics programming. It is a part of the PlayCanvas engine project and is used to define the blending mode and fog factor for rendering 3D graphics.\n\nThe code starts by setting the `dBlendModeFogFactor` variable to 0.0. This variable is used to determine the blending mode for the rendered objects. The blending mode determines how the colors of the objects are combined with the colors of the background. In this case, the blending mode is not affected by the fog factor.\n\nNext, the code applies gamma correction to the `rgb` variable. Gamma correction is a technique used to adjust the brightness and contrast of an image. It is used to ensure that the colors are displayed correctly on different types of displays. The `max(a, 0.0)` function is used to ensure that the input value is not negative.\n\nFinally, the code checks if the sum of the red, green, and blue components of the `rgb` variable is less than 0.000001. If it is, the fragment is discarded. This is done to optimize the rendering process by discarding fragments that are not visible.\n\nOverall, this code is used to define the blending mode and fog factor for rendering 3D graphics in the PlayCanvas engine. It is a small part of a larger system that is responsible for rendering complex 3D scenes in real-time. Here is an example of how this code might be used in a larger project:\n\n```javascript\n// Set the blending mode and fog factor\nshader.setBlendMode(dBlendModeFogFactor);\n\n// Render the scene\nrenderer.render(scene, camera);\n```\n## Questions: \n 1. **What is the purpose of this code?** \nThis code appears to be a GLSL shader code that modifies the RGB color values based on a gamma correction input and discards the pixel if the sum of RGB values is too small.\n\n2. **What is the significance of the `dBlendModeFogFactor` variable?** \nIt is unclear from this code snippet what the `dBlendModeFogFactor` variable is used for or how it is defined. Further context or documentation may be needed to understand its purpose.\n\n3. **What is the expected input and output of this code?** \nIt is unclear from this code snippet what the expected input and output of this code is, or how it fits into the larger PlayCanvas engine project. Further context or documentation may be needed to understand its role in the project.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_blendAdd.md"}}],["665",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_blendMultiply.js)\n\nThis code is a GLSL shader code that is used to modify the color of a rendered object. The purpose of this code is to adjust the color of an object based on a given alpha value and to discard the object if the resulting color is too bright.\n\nThe first line of the code exports the shader code as a default export. The code is written in GLSL, which is a shading language used for rendering graphics on the GPU. The code uses the `mix` function to blend the original color of the object with a new color based on the alpha value. The `vec3` function is used to create a vector of three values, which represent the red, green, and blue components of the color. The `1.0` value passed to the `vec3` function represents the maximum value for each component, which means that the original color is fully visible.\n\nThe `if` statement checks if the sum of the red, green, and blue components of the resulting color is greater than `2.99`. If the sum is greater than this value, the `discard` keyword is used to discard the object, which means that it will not be rendered.\n\nThis code can be used in the larger PlayCanvas engine project to modify the appearance of rendered objects based on a given alpha value. For example, it can be used to create a fade effect when transitioning between scenes or to adjust the brightness of an object based on its distance from the camera. Here is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// Create a new material with the shader code\nvar material = new pc.StandardMaterial();\nmaterial.chunks.diffusePS = /* glsl */`\n    rgb = mix(vec3(1.0), rgb, vec3(a));\n    if (rgb.r + rgb.g + rgb.b > 2.99) discard;\n`;\n\n// Set the alpha value of the material\nmaterial.opacity = 0.5;\n\n// Assign the material to an object\nvar entity = new pc.Entity();\nentity.addComponent('model', {\n    type: 'box',\n    material: material\n});\n``` \n\nIn this example, a new `StandardMaterial` is created with the shader code. The `opacity` property of the material is set to `0.5`, which means that the resulting color will be a blend of the original color and white. The material is then assigned to a new `Entity` object, which has a `model` component that renders a box. When the object is rendered, the shader code will adjust the color of the box based on the alpha value and discard it if the resulting color is too bright.\n## Questions: \n 1. What does the `mix` function do in this code?\n   - The `mix` function is used to interpolate between two values based on a third value, and in this code it is used to mix the `rgb` color with a vector of `1.0` and `a`.\n\n2. What is the purpose of the `discard` keyword in this code?\n   - The `discard` keyword is used to discard the current fragment, which means that it will not be drawn to the screen. In this code, it is used to discard fragments where the sum of the red, green, and blue values of the `rgb` color is greater than `2.99`.\n\n3. What type of code is this (`glsl`, `javascript`, etc.) and how is it being used in the PlayCanvas engine?\n   - This code is written in `glsl`, which is a shading language used to write shaders for graphics processing units (GPUs). It is being used in the PlayCanvas engine to define the behavior of materials and lighting in 3D scenes.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_blendMultiply.md"}}],["666",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_blendNormal.js)\n\nThis code is a GLSL shader code that checks if a given value 'a' is less than 0.01. If it is, then the code discards the current fragment. This code is used in the PlayCanvas engine to control the rendering of objects in a scene. \n\nGLSL (OpenGL Shading Language) is a high-level shading language used to write shaders for graphics processing units (GPUs). Shaders are programs that run on the GPU and are responsible for rendering objects in a scene. The PlayCanvas engine uses GLSL shaders to render objects in a scene.\n\nThe code exports a GLSL shader as a default module. This means that other modules in the PlayCanvas engine can import this code and use it in their own shaders. For example, a module that renders a specific type of object may use this code to discard fragments that are too small to be visible.\n\nHere is an example of how this code can be used in a PlayCanvas engine module:\n\n```javascript\nimport discardSmallFragments from './discardSmallFragments.glsl';\n\nconst myShader = /* glsl */`\n  void main() {\n    // some shader code here\n    ${discardSmallFragments}\n    // more shader code here\n  }\n`;\n```\n\nIn this example, the 'myShader' module imports the 'discardSmallFragments' module and uses it in its own shader code. When the 'myShader' module is used to render an object, it will discard fragments that are too small to be visible.\n\nOverall, this code is a small but important part of the PlayCanvas engine's rendering pipeline. It allows developers to control the rendering of objects in a scene by discarding fragments that are too small to be visible.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code is a GLSL shader code that discards fragments (pixels) if the value of variable 'a' is less than 0.01.\n\n2. **Where is this code used in the PlayCanvas engine?**\\\nWithout additional context, it is difficult to determine where this code is used in the PlayCanvas engine. It could be used in various parts of the engine that require GLSL shaders.\n\n3. **What are the possible consequences of using this code?**\\\nUsing this code could result in some fragments being discarded, which could affect the visual appearance of the rendered scene. It is important to ensure that the value of variable 'a' is set appropriately to avoid unintended consequences.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_blendNormal.md"}}],["667",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_end.js)\n\nThis code is a shader program written in GLSL (OpenGL Shading Language) that is used to render graphics in the PlayCanvas engine. The purpose of this code is to apply various post-processing effects to the rendered image before it is displayed on the screen.\n\nThe first line of the code exports the shader program as a default module. The next three lines apply different effects to the RGB color values of the rendered image. The `addFog` function adds a fog effect to the image, which can be used to simulate atmospheric conditions or to create a sense of depth in the scene. The `toneMap` function adjusts the brightness and contrast of the image to make it more visually appealing. The `gammaCorrectOutput` function applies gamma correction to the image, which is a technique used to adjust the brightness and contrast of the image based on the characteristics of the display device.\n\nFinally, the `gl_FragColor` variable is set to a `vec4` value that contains the modified RGB color values and an alpha value (`a`). This variable is used to set the color of the fragment (i.e. pixel) that is being rendered.\n\nThis code is an important part of the PlayCanvas engine because it allows developers to apply post-processing effects to their rendered images without having to write complex shader programs from scratch. For example, a developer could use this code to add a fog effect to a scene by simply including this shader program in their project and calling the `addFog` function. This makes it easier for developers to create visually appealing graphics without having to spend a lot of time on low-level graphics programming.\n## Questions: \n 1. What is the purpose of the `addFog` function being called on `rgb`?\n   - The `addFog` function is likely used to add atmospheric fog to the rendered scene, affecting the color of `rgb`.\n\n2. What does the `toneMap` function do to `rgb`?\n   - The `toneMap` function likely adjusts the brightness and contrast of `rgb` to improve the overall visual quality of the rendered scene.\n\n3. What is the significance of `gl_FragColor` being set to `vec4(rgb, a)`?\n   - `gl_FragColor` is likely the output color of the fragment shader, and setting it to `vec4(rgb, a)` assigns the final color of the rendered fragment, including any modifications made by the previous functions.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_end.md"}}],["668",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_halflambert.js)\n\nThe code above is a GLSL shader code that performs a mathematical operation on a vector. The purpose of this code is to calculate the negative and positive normal vectors of a given input vector. \n\nThe input vector is represented by the variable `normal`, which is a 3D vector. The code then calculates the negative normal vector by multiplying the input vector by 0.5 and adding 0.5 to it. This operation scales the input vector to the range of 0 to 1, and then shifts it up by 0.5. The resulting vector is then squared, which effectively inverts the direction of the vector. This is stored in the variable `negNormal`.\n\nThe positive normal vector is calculated in a similar way, but with a negative sign added to the input vector before the scaling and shifting operations. This results in a vector that is also scaled to the range of 0 to 1, but with a different direction than the negative normal vector. This is stored in the variable `posNormal`.\n\nThis code may be used in the larger PlayCanvas engine project to calculate the normal vectors of 3D models. Normal vectors are important in 3D graphics because they determine how light interacts with the surface of a model. By calculating both the positive and negative normal vectors, this code can be used to create more realistic lighting effects on 3D models.\n\nHere is an example of how this code may be used in a larger GLSL shader program:\n\n```glsl\nvarying vec3 vNormal;\n\nvoid main() {\n  vec3 negNormal = vNormal*0.5+0.5;\n  vec3 posNormal = -vNormal*0.5+0.5;\n  negNormal *= negNormal;\n  posNormal *= posNormal;\n  \n  // Use the normal vectors to calculate lighting\n  // ...\n}\n```\n\nIn this example, the `vNormal` variable is a 3D vector that represents the normal of a vertex in a 3D model. The code above is used to calculate the positive and negative normal vectors of this vertex, which can then be used to calculate lighting effects in the rest of the shader program.\n## Questions: \n 1. **What is the purpose of this code?** \nThis code appears to be manipulating a normal vector by scaling it and adding a constant value. It is unclear what the intended use case is without additional context.\n\n2. **What is the data type of the input variable `normal`?** \nThe code assumes that `normal` is a `vec3` data type, but it is unclear where this variable is defined or how it is being used in the larger context of the project.\n\n3. **What is the expected output of this code?** \nWithout additional context, it is unclear what the expected output of this code is or how it will be used in the larger project.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_halflambert.md"}}],["669",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_lambert.js)\n\nThis code is a GLSL shader code that takes in a normal vector and returns two modified normal vectors. The purpose of this code is to ensure that the normal vectors are always pointing in the positive direction, which is important for lighting calculations in 3D graphics.\n\nThe first line of code creates a new vector called `negNormal` by taking the maximum value between the input `normal` vector and a vector of all zeros. This effectively sets any negative values in the `normal` vector to zero, ensuring that the resulting vector only has positive values.\n\nThe second line of code creates a new vector called `posNormal` by taking the maximum value between the negated input `normal` vector and a vector of all zeros. This effectively sets any positive values in the `normal` vector to zero, ensuring that the resulting vector only has negative values.\n\nThese modified normal vectors can then be used in lighting calculations to ensure that the lighting behaves correctly regardless of the orientation of the surface. For example, if a surface has a normal vector that is pointing away from the light source, the lighting calculations would be incorrect without this modification.\n\nHere is an example of how this code might be used in a larger project:\n\n```glsl\n// vertex shader code\nattribute vec3 aPosition;\nattribute vec3 aNormal;\n\nuniform mat4 uModelMatrix;\nuniform mat4 uViewMatrix;\nuniform mat4 uProjectionMatrix;\n\nvarying vec3 vNormal;\n\nvoid main() {\n  // transform the position and normal vectors\n  vec4 worldPosition = uModelMatrix * vec4(aPosition, 1.0);\n  vec4 viewPosition = uViewMatrix * worldPosition;\n  gl_Position = uProjectionMatrix * viewPosition;\n  vNormal = normalize(mat3(uModelMatrix) * aNormal);\n  // modify the normal vector to ensure it is always pointing in the positive direction\n  vec3 negNormal = max(vNormal, vec3(0.0));\n  vec3 posNormal = max(-vNormal, vec3(0.0));\n  // pass the modified normal vectors to the fragment shader\n  gl_Position = vec4(negNormal, 1.0);\n  gl_Position = vec4(posNormal, 1.0);\n}\n```\n\nIn this example, the modified normal vectors are passed to the fragment shader where they can be used in lighting calculations. By ensuring that the normal vectors are always pointing in the positive direction, the lighting calculations will be correct regardless of the orientation of the surface.\n## Questions: \n 1. What is the purpose of this code?\n   - This code appears to be a GLSL shader code that calculates the negative and positive values of a given normal vector.\n\n2. What is the data type of the variables 'normal', 'negNormal', and 'posNormal'?\n   - Based on the usage of the 'vec3' keyword, it can be inferred that 'normal', 'negNormal', and 'posNormal' are all 3-dimensional vectors.\n\n3. Can this code be used in any context or is it specific to a certain application or platform?\n   - Without additional context, it is unclear whether this code is platform or application-specific. It is possible that it is designed to work with the PlayCanvas engine, but further investigation would be needed to confirm this.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_lambert.md"}}],["670",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_lighting.js)\n\nThe code above is a GLSL shader code that calculates the lighting of a 3D object based on a light cube. The light cube is an array of six vectors that represent the direction and intensity of light coming from six different directions. \n\nThe code first calculates the contribution of each direction of light to the final color of the object. This is done by multiplying the intensity of each direction of light by the dot product of the surface normal of the object and the direction of the light. The dot product is positive if the surface normal and the light direction are pointing in the same direction, and negative if they are pointing in opposite directions. \n\nThe resulting six values are then combined into a single vector by adding them together. This vector represents the total contribution of all six directions of light to the final color of the object. \n\nFinally, the code multiplies the original color of the object by this combined light vector to get the final color of the object with lighting applied. \n\nThis code is likely used in the larger PlayCanvas engine project to render 3D objects with realistic lighting. It is a part of the shader code that is applied to each object during rendering. The light cube is likely generated based on the position and intensity of light sources in the scene, and the surface normal of each object is calculated based on its geometry. \n\nHere is an example of how this code might be used in a larger shader program:\n\n```glsl\nuniform vec3 lightCube[6];\nvarying vec3 surfaceNormal;\nvarying vec3 color;\n\nvoid main() {\n  vec3 negNormal = max(surfaceNormal, 0.0) * -1.0;\n  vec3 posNormal = max(surfaceNormal, 0.0);\n  vec3 light = negNormal.x*lightCube[0] + posNormal.x*lightCube[1] +\n                        negNormal.y*lightCube[2] + posNormal.y*lightCube[3] +\n                        negNormal.z*lightCube[4] + posNormal.z*lightCube[5];\n  gl_FragColor = vec4(color * light, 1.0);\n}\n```\n\nIn this example, the shader program takes in a light cube as a uniform variable, along with the surface normal and color of the object as varying variables. The code then calculates the combined light vector using the same method as in the original code, and multiplies it by the object color to get the final color of the object with lighting applied. The result is output as the fragment color.\n## Questions: \n 1. What is the purpose of the `lightCube` variable?\n    - The `lightCube` variable is used to calculate the `light` vector in the code, which is then used to modify the `rgb` value.\n\n2. What is the data type of the `rgb` variable?\n    - The data type of the `rgb` variable is not specified in this code snippet, so it is unclear what type of data it represents.\n\n3. What is the significance of the `negNormal` and `posNormal` variables?\n    - The `negNormal` and `posNormal` variables are used to calculate the `light` vector based on the normal vector of the surface being lit. It is unclear from this code snippet how these variables are defined or calculated.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_lighting.md"}}],["671",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_normalMap.js)\n\nThis code is a GLSL shader code that is used to calculate the normal vector of a particle in a particle system. The normal vector is an important property of a surface that is used in lighting calculations to determine how light interacts with the surface. \n\nThe code takes in a normal map texture and a texture coordinate (texCoordsAlphaLife) as input. The texture2D function is used to sample the normal map texture at the given texture coordinate. The resulting color value is then converted to a 3D vector (xyz) and normalized to ensure that its length is 1.0. \n\nNext, the normalized vector is scaled by 2.0 and then subtracted by 1.0. This operation maps the range of the vector from [0, 1] to [-1, 1]. This is a common technique used to convert texture data to a range that can be used in calculations. \n\nFinally, the resulting vector is transformed by a ParticleMat matrix to convert it from local space to world space. The resulting vector is the normal vector of the particle in world space. \n\nThis code is likely used in the larger PlayCanvas engine project to render particle systems with realistic lighting. The normal vector calculated by this code can be used in lighting calculations to determine how light interacts with the particle surface. \n\nExample usage of this code in a particle system shader:\n\n```\n// Vertex shader\nattribute vec3 vertex_position;\nattribute vec2 vertex_texCoordAlphaLife;\n\nuniform mat4 matrix_model;\nuniform mat4 matrix_viewProjection;\n\nvarying vec2 texCoordsAlphaLife;\n\nvoid main() {\n    // Transform vertex position to world space\n    vec4 worldPos = matrix_model * vec4(vertex_position, 1.0);\n    gl_Position = matrix_viewProjection * worldPos;\n    \n    // Pass texture coordinate to fragment shader\n    texCoordsAlphaLife = vertex_texCoordAlphaLife;\n}\n\n// Fragment shader\nuniform sampler2D normalMap;\nuniform mat4 ParticleMat;\n\nvarying vec2 texCoordsAlphaLife;\n\nvoid main() {\n    // Calculate normal vector of particle\n    vec3 normalMap = normalize(texture2D(normalMap, vec2(texCoordsAlphaLife.x, 1.0 - texCoordsAlphaLife.y)).xyz * 2.0 - 1.0);\n    vec3 normal = ParticleMat * normalMap;\n    \n    // Use normal vector in lighting calculations\n    // ...\n}\n```\n## Questions: \n 1. What is the purpose of the `normalize` function in the first line of code?\n    - The `normalize` function is used to ensure that the resulting vector has a length of 1, which is important for calculating accurate lighting and shading effects.\n\n2. What is the `ParticleMat` variable and how is it used in the second line of code?\n    - `ParticleMat` is likely a matrix used to transform the normal map vector into world space. It is multiplied by the normalized normal map vector to obtain the final normal vector.\n\n3. What is the significance of the `/* glsl */` comment at the beginning of the code?\n    - This comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a specialized language used for writing shaders that run on the GPU. The comment is used by tools and editors to provide syntax highlighting and other features specific to GLSL.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_normalMap.md"}}],["672",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/frag/particle_soft.js)\n\nThis code is a GLSL shader that is used to render particles in a 3D scene. The purpose of this code is to calculate the depth difference between the particle and the camera, and then use that depth difference to adjust the particle's alpha value. This creates a softening effect that makes the particles blend more naturally with the scene.\n\nThe `getLinearScreenDepth()` function is used to calculate the depth of the current pixel on the screen. This value is then compared to the depth of the particle (`vDepth`) to calculate the depth difference. The `saturate()` function is used to clamp the depth difference to a value between 0 and 1.\n\nThe `softening` variable is used to control the strength of the effect. A higher value will result in a more pronounced softening effect, while a lower value will result in a more subtle effect.\n\nThe `a` variable represents the alpha value of the particle. By multiplying it by the depth difference, the alpha value is adjusted based on the particle's depth relative to the camera. This creates the softening effect that blends the particles with the scene.\n\nThis code is likely used in the larger PlayCanvas engine project to render particles in a more realistic and natural way. By adjusting the alpha value based on depth, the particles are able to blend more seamlessly with the scene, creating a more immersive experience for the user.\n\nExample usage:\n\n```glsl\nuniform float softening;\n\nvoid main() {\n  float depth = getLinearScreenDepth();\n  float particleDepth = vDepth;\n  float depthDiff = saturate(abs(particleDepth - depth) * softening);\n  a *= depthDiff;\n}\n```\n\nIn this example, the `softening` uniform variable is used to control the strength of the effect. The `getLinearScreenDepth()` function and `vDepth` variable are assumed to be defined elsewhere in the shader. The resulting `a` value is then used to set the alpha value of the particle.\n## Questions: \n 1. What is the purpose of the `getLinearScreenDepth()` function?\n   - The `getLinearScreenDepth()` function is used to retrieve the linear depth value of the current pixel on the screen.\n\n2. What is the meaning of the `vDepth` variable?\n   - The `vDepth` variable likely represents the depth value of a particle in the scene.\n\n3. What does the `softening` variable do?\n   - The `softening` variable is used to control the strength of the effect that modifies the alpha value of the particle (`a`) based on the difference between the particle depth and the screen depth.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/frag/particle_soft.md"}}],["673",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle.js)\n\nThe code provided is a collection of GLSL functions that are used for rendering particles in the PlayCanvas engine. The functions are used to manipulate the position, rotation, and scaling of particles, as well as to calculate their lifetimes and alpha values.\n\nThe `unpack3NFloats` function takes a single float value and returns a vec3 containing the three components of the float. This function is used to unpack the alphaDiv, scaleDiv, and paramDiv values in the `main` function.\n\nThe `saturate` function takes a float value and returns a clamped value between 0.0 and 1.0. This function is used to clamp the scaleDiv value in the `main` function.\n\nThe `tex1Dlod_lerp` function takes a texture and a texture coordinate and returns a vec4 containing the interpolated color value of the texture at the given coordinate. This function is used to sample the internalTex2 texture in the `main` function.\n\nThe `rotate` function takes a vec2 representing the x and y coordinates of a particle and a float representing the rotation angle, and returns a vec2 representing the rotated coordinates. This function is used to rotate particles around their center point.\n\nThe `billboard` function takes a vec3 representing the position of a particle and a vec2 representing the x and y coordinates of the particle, and returns a vec3 representing the position of the particle in screen space. This function is used to position particles in screen space.\n\nThe `customFace` function takes a vec3 representing the position of a particle and a vec2 representing the x and y coordinates of the particle, and returns a vec3 representing the position of the particle in world space. This function is used to position particles on a custom face.\n\nThe `safeNormalize` function takes a vec2 and returns a normalized vec2. This function is used to normalize the velocity vector of particles.\n\nThe `main` function is the main particle rendering function. It takes the particle vertex data and calculates the position, rotation, and scaling of each particle. It also calculates the lifetime and alpha value of each particle. Finally, it sets the texture coordinates and alpha values for each particle and calculates the final position of each particle.\n\nOverall, these GLSL functions are used to manipulate the position, rotation, and scaling of particles in the PlayCanvas engine. They are used in the larger project to create particle effects for games and other interactive applications.\n## Questions: \n 1. What does the `tex1Dlod_lerp` function do?\n- The `tex1Dlod_lerp` function performs a linear interpolation between two texture samples based on a given texture coordinate and a number of samples.\n\n2. What is the purpose of the `billboard` function?\n- The `billboard` function calculates the position of a particle in screen space or world space based on its instance coordinates and quad coordinates.\n\n3. What is the role of the `safeNormalize` function?\n- The `safeNormalize` function normalizes a 2D vector while avoiding division by zero errors by checking if the length of the vector is greater than a small threshold value.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle.md"}}],["674",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particleAnimFrameClamp.js)\n\nThis code is a GLSL shader code that defines a float variable called `animFrame`. The purpose of this code is to calculate the current frame of an animation based on the texture coordinates and animation parameters provided. \n\nThe `texCoordsAlphaLife` variable is a vec4 that contains the texture coordinates (x and y) and alpha and life values (z and w) for a given pixel. The `animTexParams` variable is a vec3 that contains the animation parameters, specifically the starting frame (x), the number of frames per row (y), and the total number of frames (z) in the animation texture. \n\nThe `floor` function is used to round down the result of multiplying the texture coordinate's y value by the number of frames per row. This gives us the row of the current frame in the animation texture. We then add the starting frame value to this result to get the current frame number. \n\nThe `min` function is used to ensure that the current frame number does not exceed the total number of frames in the animation texture. This is important because if the current frame number exceeds the total number of frames, the animation will loop back to the beginning. \n\nOverall, this code is used to calculate the current frame of an animation in a GLSL shader. It can be used in conjunction with other shader code to create complex animations and effects in the PlayCanvas engine. \n\nExample usage:\n\n```glsl\nuniform sampler2D animationTexture;\nvarying vec4 texCoordsAlphaLife;\n\n// Define animTexParams vec3 here\n\nfloat animFrame = min(floor(texCoordsAlphaLife.w * animTexParams.y) + animTexParams.x, animTexParams.z);\nvec2 animTexCoords = vec2((animFrame + 0.5) / animTexParams.y, 0.5);\n\nvec4 animationColor = texture2D(animationTexture, animTexCoords);\n```\n## Questions: \n 1. What is the purpose of the `texCoordsAlphaLife` variable in this code?\n   - The `texCoordsAlphaLife` variable is used to calculate the `animFrame` value, which is used for animation.\n\n2. What do the values of `animTexParams` represent?\n   - `animTexParams` is an array of three values, where the first value represents the starting frame of the animation, the second value represents the number of frames per row in the animation texture, and the third value represents the total number of frames in the animation texture.\n\n3. What is the data type of the `animFrame` variable?\n   - The `animFrame` variable is of type `float`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particleAnimFrameClamp.md"}}],["675",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particleAnimFrameLoop.js)\n\nThe code snippet is written in GLSL, which is a shading language used for graphics programming. The purpose of this code is to calculate the current animation frame based on the texture coordinates, alpha value, and animation parameters. \n\nThe code starts by declaring a float variable called `animFrame`. The `floor` function is then used to round down the result of the `mod` function. The `mod` function calculates the remainder of the division of `texCoordsAlphaLife.w * animTexParams.y + animTexParams.x` by `animTexParams.z + 1.0`. \n\nThe `texCoordsAlphaLife` variable is a vec4 that contains the texture coordinates, alpha value, and life value. The `animTexParams` variable is a vec3 that contains the animation parameters, including the number of frames in the animation, the starting frame, and the frame rate. \n\nThis code is likely used in a larger project that involves animating textures. It could be used in a game engine, for example, to animate sprites or other graphical elements. The `animFrame` variable could be used to determine which frame of the animation to display at any given time. \n\nHere is an example of how this code could be used in a larger project:\n\n```glsl\nuniform sampler2D animTexture;\nvarying vec4 texCoordsAlphaLife;\nuniform vec3 animTexParams;\n\nvoid main() {\n  float animFrame = floor(mod(texCoordsAlphaLife.w * animTexParams.y + animTexParams.x, animTexParams.z + 1.0));\n  vec2 animTexCoords = vec2((animFrame + 0.5) / animTexParams.z, 0.5);\n  vec4 animColor = texture2D(animTexture, animTexCoords);\n  gl_FragColor = animColor * texCoordsAlphaLife;\n}\n```\n\nIn this example, the GLSL code is used in a fragment shader to animate a texture. The `animTexture` uniform contains the texture to animate. The `texCoordsAlphaLife` varying contains the texture coordinates, alpha value, and life value for each fragment. The `animTexParams` uniform contains the animation parameters. \n\nThe `animFrame` variable is calculated using the code from the original snippet. The `animTexCoords` variable is then calculated based on the current animation frame. The `texture2D` function is used to sample the texture at the current animation frame. Finally, the `gl_FragColor` variable is set to the sampled color multiplied by the alpha and life values from `texCoordsAlphaLife`.\n## Questions: \n 1. What is the purpose of the `texCoordsAlphaLife` variable in this code?\n   - The `texCoordsAlphaLife` variable is used to calculate the `animFrame` value.\n\n2. What is the significance of the `animTexParams` variable?\n   - The `animTexParams` variable is used in the calculation of the `animFrame` value, specifically to determine the range of animation frames available.\n\n3. What is the expected output of this code?\n   - The expected output of this code is a float value representing the current animation frame based on the input texture coordinates and animation parameters.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particleAnimFrameLoop.md"}}],["676",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particleAnimTex.js)\n\nThis code is a GLSL shader that is used for animating textures in the PlayCanvas engine. The purpose of this code is to calculate the texture coordinates for a specific frame of an animation based on the animation index and the number of tiles in the animation texture.\n\nThe code starts by declaring a variable called `animationIndex` which will hold the index of the current animation frame. The value of this variable is calculated based on the `animTexIndexParams` and `animTexParams` variables. If `animTexIndexParams.y` is equal to 1.0, then the animation index is calculated using the formula `floor((animTexParams.w + 1.0) * rndFactor3.z) * (animTexParams.z + 1.0)`. Otherwise, the animation index is simply `animTexIndexParams.x * (animTexParams.z + 1.0)`.\n\nOnce the animation index is calculated, the code then calculates the texture coordinates for the current frame of the animation. The `atlasX` and `atlasY` variables are used to calculate the position of the current frame in the animation texture. The `atlasX` variable is calculated by adding the animation index to the current frame and multiplying by the `animTexTilesParams.x` variable. The `atlasY` variable is calculated by subtracting the result of `floor(atlasX + 1.0)` from 1.0 and multiplying by the `animTexTilesParams.y` variable. Finally, the `atlasX` variable is set to the fractional part of itself using the `fract` function.\n\nThe final step in the code is to update the `texCoordsAlphaLife.xy` variable with the new texture coordinates. The `texCoordsAlphaLife.xy` variable is multiplied by the `animTexTilesParams.xy` variable to scale the texture coordinates to the correct size. The `atlasX` and `atlasY` variables are then added to the `texCoordsAlphaLife.xy` variable to set the new texture coordinates.\n\nOverall, this code is an important part of the PlayCanvas engine's animation system. It allows developers to easily animate textures by specifying the animation index and the number of tiles in the animation texture. Here is an example of how this code might be used in a PlayCanvas project:\n\n```javascript\n// Create a material with an animated texture\nvar material = new pc.StandardMaterial();\nmaterial.diffuseMap = texture;\nmaterial.chunks.diffusePS = `\n    uniform vec4 animTexParams;\n    uniform vec4 animTexIndexParams;\n    uniform vec4 animTexTilesParams;\n    uniform vec4 rndFactor3;\n    varying vec2 texCoordsAlphaLife;\n\n    ${animationShaderCode}\n\n    void main() {\n        ${getDiffuseColorCode}\n        ${getAlphaCode}\n        ${getLifeCode}\n\n        // Animate the texture\n        ${animateTextureCode}\n\n        gl_FragColor = vec4(diffuseColor.rgb, alpha);\n    }\n`;\nmaterial.update();\n\n// Set the animation parameters\nmaterial.setParameter('animTexParams', new pc.Vec4(512, 512, 4, 4));\nmaterial.setParameter('animTexIndexParams', new pc.Vec4(0, 1, 0, 0));\nmaterial.setParameter('animTexTilesParams', new pc.Vec4(0.25, 0.25, 4, 4));\nmaterial.setParameter('rndFactor3', new pc.Vec4(0.1, 0.2, 0.3, 0.4));\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code is likely used to calculate texture coordinates for a sprite animation in a shader program.\n\n2. What are the inputs required for this code to work correctly?\n    \n    This code requires several input parameters, including `animTexIndexParams`, `animTexParams`, `animTexTilesParams`, `animFrame`, and `rndFactor3.z`.\n\n3. What is the expected output of this code?\n    \n    The expected output of this code is a modified set of texture coordinates (`texCoordsAlphaLife.xy`) that correspond to a specific frame of a sprite animation.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particleAnimTex.md"}}],["677",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_TBN.js)\n\nThe code above is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to create a rotation matrix that is used to rotate particles in a 3D space. \n\nThe code starts by defining a 3x3 rotation matrix called `rot3`. This matrix is created using the `mat3` function and is initialized with values from the `rotMatrix` array. The `rotMatrix` array is a 2D array that contains the rotation values for the x and y axes. The `mat3` function is used to create a 3x3 matrix from the values in the `rotMatrix` array.\n\nNext, the code creates a new matrix called `ParticleMat`. This matrix is also a 3x3 matrix and is initialized using the `mat3` function. The values for this matrix are calculated by multiplying the `rot3` matrix with another matrix that is created using the `matrix_viewInverse` array. The `matrix_viewInverse` array is a 3x3 matrix that contains the inverse of the view matrix. \n\nThe `ParticleMat` matrix is used to rotate particles in a 3D space. This is done by multiplying the position of each particle by the `ParticleMat` matrix. This will rotate the particle in the direction specified by the `rotMatrix` array.\n\nHere is an example of how this code can be used in the PlayCanvas engine project:\n\n```javascript\n// Create a new particle system\nvar particleSystem = new pc.ParticleSystem();\n\n// Set the rotation matrix for the particle system\nparticleSystem.rotationMatrix = /* glsl */`\n    mat3 rot3 = mat3(rotMatrix[0][0], rotMatrix[0][1], 0.0, rotMatrix[1][0], rotMatrix[1][1], 0.0, 0.0, 0.0, 1.0);\n    ParticleMat = mat3(-matrix_viewInverse[0].xyz, -matrix_viewInverse[1].xyz, matrix_viewInverse[2].xyz) * rot3;\n`;\n\n// Add particles to the particle system\nparticleSystem.addParticles(particles);\n\n// Update the particle system\nparticleSystem.update();\n```\n\nIn this example, a new particle system is created and the rotation matrix is set using the GLSL shader code. Particles are then added to the particle system and the system is updated. The particles will be rotated in the direction specified by the `rotMatrix` array.\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code is likely used to calculate a particle matrix based on a rotation matrix and the inverse view matrix.\n\n2. What is the data type of `ParticleMat`?\n    \n    Based on the code, `ParticleMat` is likely a matrix of type `mat3`.\n\n3. What is the significance of the `/* glsl */` comment at the beginning of the code?\n    \n    The `/* glsl */` comment indicates that this code is written in GLSL (OpenGL Shading Language) syntax and should be treated as such by any tools or editors processing the code.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_TBN.md"}}],["678",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_billboard.js)\n\nThis code is written in GLSL, which is a shading language used for graphics programming. It is exporting a function that takes in two parameters: `quadXY` and `inAngle`. \n\nThe first line of the code is rotating the `quadXY` vector by the `inAngle` amount using a `rotMatrix`. This is likely used to orient the quad in a specific direction based on the angle provided. \n\nThe second line of the code is using a `billboard` function to calculate the position of a particle in 3D space. The `particlePos` parameter is the position of the particle in 3D space, and `quadXY` is the rotated quad vector from the previous line. The `billboard` function is likely used to ensure that the particle always faces the camera, regardless of its position in 3D space. \n\nOverall, this code is likely used in the PlayCanvas engine to render particles in a 3D space. The `quadXY` vector is rotated to orient the particle in a specific direction, and the `billboard` function is used to ensure that the particle always faces the camera. \n\nExample usage of this code in the PlayCanvas engine could be:\n\n```javascript\nconst quadXY = new pc.Vec2(1, 0); // create a new quad vector\nconst inAngle = 45; // set the angle to rotate the quad by\nconst rotMatrix = new pc.Mat4().setFromAxisAngle(pc.Vec3.UP, inAngle); // create a rotation matrix\nconst particlePos = new pc.Vec3(0, 0, 0); // set the position of the particle in 3D space\n\n// rotate the quad vector and calculate the position of the particle\nconst localPos = billboard(particlePos, rotate(quadXY, inAngle, rotMatrix));\n```\n## Questions: \n 1. What is the purpose of the `rotate` function being called on `quadXY`?\n   - The `rotate` function is being used to rotate `quadXY` by `inAngle` degrees using `rotMatrix`.\n2. What is the `billboard` function doing with `particlePos` and `quadXY`?\n   - The `billboard` function is using `particlePos` and `quadXY` to calculate the position of a billboarded particle in 3D space.\n3. What is the overall purpose of this code and how does it fit into the PlayCanvas engine?\n   - This code is likely part of a larger system for rendering particles in 3D space using billboarding techniques. It may be used in various parts of the engine, such as particle systems or special effects.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_billboard.md"}}],["679",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_cpu.js)\n\nThe code is a GLSL shader that is used to render particles in the PlayCanvas engine. The shader takes in various attributes and uniforms that define the properties of the particles, such as their position, velocity, lifetime, and texture coordinates. \n\nThe `attribute` variables `particle_vertexData`, `particle_vertexData2`, `particle_vertexData3`, `particle_vertexData4`, and `particle_vertexData5` represent the per-particle data that is passed to the shader. These attributes are used to calculate the position, velocity, rotation, and texture coordinates of each particle. \n\nThe `uniform` variables represent the global properties of the particle system, such as the view and projection matrices, the number of particles, the lifetime of each particle, and the texture samplers. \n\nThe `varying` variable `texCoordsAlphaLife` is used to pass the texture coordinates, alpha value, and remaining lifetime of each particle to the fragment shader. \n\nThe `rotate` function takes in a 2D vector and a rotation angle and returns the rotated vector and the rotation matrix. This function is used to rotate the particles based on their initial angle. \n\nThe `billboard` function takes in the instance coordinates and a 2D vector and returns the position of the particle in world space. This function is used to orient the particles towards the camera. \n\nThe `customFace` function takes in the instance coordinates and a 2D vector and returns the position of the particle based on the face tangent and binormal vectors. This function is used to orient the particles based on a custom face direction. \n\nOverall, this shader is an essential part of the PlayCanvas engine's particle system. It takes in the per-particle data and global properties and calculates the position, rotation, and texture coordinates of each particle. The `rotate`, `billboard`, and `customFace` functions are used to orient the particles in different ways. The resulting particles are then rendered using the fragment shader.\n## Questions: \n 1. What is the purpose of the `PlayCanvas engine` project?\n- Unfortunately, the given code snippet does not provide any information about the purpose of the PlayCanvas engine project. Further investigation is needed.\n\n2. What do the attributes `particle_vertexData`, `particle_vertexData2`, `particle_vertexData3`, `particle_vertexData4`, and `particle_vertexData5` represent?\n- These attributes represent various properties of particles, such as world position, life, angle, scale, alpha, velocity, and particle ID. The exact meaning of each attribute may depend on the value of the `USE_MESH` property.\n\n3. What is the purpose of the functions `rotate`, `billboard`, and `customFace`?\n- These functions are used to calculate the position of particles based on their properties and the current view. `rotate` rotates a 2D vector by a given angle, `billboard` calculates the position of a particle in screen space, and `customFace` calculates the position of a particle based on custom face vectors.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_cpu.md"}}],["680",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_cpu_end.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) that is used in the PlayCanvas engine project. The purpose of this code is to transform the position of particles in a particle system to their final position on the screen.\n\nThe code takes in two variables, `localPos` and `particle_vertexData2.y`. `localPos` is the local position of the particle in the particle system, while `particle_vertexData2.y` is a scaling factor for the particle. The code multiplies `localPos` by `particle_vertexData2.y * emitterScale` to scale the particle's position based on its size. The result is then added to `particlePos`, which is the position of the particle system in world space.\n\nThe final position of the particle is then calculated by multiplying the transformed `localPos` by the `matrix_viewProjection` matrix, which is the product of the view and projection matrices. The resulting position is assigned to `gl_Position`, which is a built-in variable in GLSL that represents the final position of the vertex on the screen.\n\nThis code is used in the larger PlayCanvas engine project to render particle systems in 3D space. The shader code is applied to each particle in the system to transform its position and render it on the screen. Here is an example of how this code might be used in the PlayCanvas engine:\n\n```javascript\n// Create a new particle system\nvar particleSystem = new pc.ParticleSystem();\n\n// Set the shader code for the particle system\nparticleSystem.shader = /* glsl */`\n    localPos *= particle_vertexData2.y * emitterScale;\n    localPos += particlePos;\n\n    gl_Position = matrix_viewProjection * vec4(localPos, 1.0);\n`;\n\n// Add particles to the system\nfor (var i = 0; i < 100; i++) {\n    particleSystem.addParticle();\n}\n\n// Render the particle system\nparticleSystem.update();\nparticleSystem.render();\n```\n\nIn this example, the shader code is set for the particle system, and particles are added to the system. The `update()` method is called to update the position of each particle, and the `render()` method is called to render the particles on the screen using the shader code.\n## Questions: \n 1. What is the purpose of the `particle_vertexData2.y` variable in the first line of code?\n- The `particle_vertexData2.y` variable is being used to scale the `localPos` vector.\n\n2. What is the significance of the `gl_Position` variable in the last line of code?\n- The `gl_Position` variable is used to set the final position of the vertex in clip space, which is necessary for rendering the particle.\n\n3. What is the role of the `matrix_viewProjection` variable in the last line of code?\n- The `matrix_viewProjection` variable is a matrix that transforms the vertex from world space to clip space, which is necessary for rendering the particle in the correct position on the screen.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_cpu_end.md"}}],["681",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_customFace.js)\n\nThis code is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to rotate a quad (a 2D shape with four sides) around the Z-axis by a given angle and then apply a custom transformation to its position. \n\nThe first line of code uses the `rotate` function to rotate the quad around the Z-axis by the `inAngle` value. The `rotate` function takes in the quad (`quadXY`) and the rotation matrix (`rotMatrix`) as parameters and returns the rotated quad. The `inAngle` value is a uniform variable that can be set externally to control the rotation angle.\n\nThe second line of code applies a custom transformation to the position of the quad. The `customFace` function takes in the `particlePos` (the position of the particle), and the rotated quad (`quadXY`) as parameters and returns a `vec3` (a 3D vector). The `vec3` represents the new position of the quad after the custom transformation has been applied. \n\nThis code can be used in the PlayCanvas engine project to create particle effects that require rotation and custom transformations. For example, if a particle effect needs to simulate a spinning object, this code can be used to rotate the quad and then apply a custom transformation to simulate the object's movement. \n\nHere is an example of how this code can be used in a PlayCanvas script:\n\n```\nvar shader = new pc.Shader(device, {\n    attributes: {\n        aPosition: pc.SEMANTIC_POSITION\n    },\n    vshader: /*glsl*/`\n        attribute vec3 aPosition;\n        uniform mat4 matrix_model;\n        uniform mat4 matrix_viewProjection;\n        varying vec2 vUv;\n        void main(void)\n        {\n            gl_Position = matrix_viewProjection * matrix_model * vec4(aPosition, 1.0);\n            vUv = aPosition.xy * 0.5 + 0.5;\n        }\n    `,\n    fshader: /*glsl*/`\n        uniform float inAngle;\n        uniform mat3 rotMatrix;\n        uniform vec3 particlePos;\n        varying vec2 vUv;\n        void main(void)\n        {\n            vec2 quadXY = vUv * 2.0 - 1.0;\n            quadXY = rotate(quadXY, inAngle, rotMatrix);\n            vec3 localPos = customFace(particlePos, quadXY);\n            gl_FragColor = vec4(localPos, 1.0);\n        }\n    `\n});\n```\n\nIn this example, the `shader` object is created with the GLSL code that includes the `rotate` and `customFace` functions. The `inAngle`, `rotMatrix`, and `particlePos` uniform variables are set externally to control the rotation angle, rotation matrix, and particle position, respectively. The `gl_FragColor` variable is set to the `localPos` value returned by the `customFace` function, which represents the new position of the quad after the rotation and custom transformation have been applied.\n## Questions: \n 1. What is the purpose of the `rotate` function being called on `quadXY`?\n   - The `rotate` function is being used to rotate `quadXY` by `inAngle` degrees using `rotMatrix`.\n2. What is the `customFace` function and how is it being used?\n   - The `customFace` function is being used to calculate the position of a particle based on its `particlePos` and `quadXY` values.\n3. What type of data is being exported by this file?\n   - This file is exporting a GLSL shader code as a default export.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_customFace.md"}}],["682",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_end.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) used in the PlayCanvas engine project. The purpose of this code is to calculate the position of particles in a particle system. \n\nThe code takes in two variables, `localPos` and `particlePos`, which represent the local position of the particle and the position of the particle system in the world, respectively. It then multiplies `localPos` by the scale of the particle system and the emitter scale, and adds `particlePos` to it. This calculation gives the final position of the particle in the world.\n\nThe code then checks if the `SCREEN_SPACE` flag is defined. If it is, the `gl_Position` variable is set to a 2D vector with the x and y coordinates of `localPos` and a z coordinate of 0. This means that the particle will be rendered in screen space, which is useful for 2D games or UI elements.\n\nIf the `SCREEN_SPACE` flag is not defined, the `gl_Position` variable is set to the result of multiplying `localPos` by the `matrix_viewProjection` matrix. This matrix is used to transform the particle's position from local space to world space. The resulting position is then used to render the particle in 3D space.\n\nThis code is used in the PlayCanvas engine to render particle systems in both 2D and 3D games. It is a crucial part of the engine's rendering pipeline and is used to calculate the position of each particle in the system. \n\nHere is an example of how this code might be used in a particle system:\n\n```javascript\n// Create a new particle system\nconst particleSystem = new pc.ParticleSystem();\n\n// Set the shader code for the particle system\nparticleSystem.shader = `\n    localPos *= scale * emitterScale;\n    localPos += particlePos;\n\n    #ifdef SCREEN_SPACE\n    gl_Position = vec4(localPos.x, localPos.y, 0.0, 1.0);\n    #else\n    gl_Position = matrix_viewProjection * vec4(localPos.xyz, 1.0);\n    #endif\n`;\n\n// Set the position of the particle system in the world\nparticleSystem.position.set(0, 0, 0);\n\n// Add particles to the system\nfor (let i = 0; i < 100; i++) {\n    particleSystem.addParticle();\n}\n\n// Render the particle system\nparticleSystem.render();\n``` \n\nIn this example, we create a new particle system and set its shader code to the code shown above. We then set the position of the particle system in the world and add 100 particles to the system. Finally, we render the particle system, which will use the shader code to calculate the position of each particle and render them on the screen.\n## Questions: \n 1. What is the purpose of the `localPos` variable and how is it being modified?\n    \n    The `localPos` variable is being scaled by `scale` and `emitterScale`, and then added to `particlePos`. Its purpose is likely to determine the position of a particle in local space.\n\n2. What is the significance of the `#ifdef SCREEN_SPACE` preprocessor directive?\n    \n    The `#ifdef SCREEN_SPACE` directive indicates that the following code will only be compiled if the `SCREEN_SPACE` macro is defined. This suggests that the code may be conditional based on some runtime configuration or build setting.\n\n3. What is the role of the `matrix_viewProjection` variable and how is it being used?\n    \n    The `matrix_viewProjection` variable is being used to transform `localPos` into clip space coordinates. It is likely a matrix that combines the view and projection matrices to transform from world space to clip space.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_end.md"}}],["683",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_init.js)\n\nThis code is a GLSL shader that is used in the PlayCanvas engine for particle effects. It defines a set of attributes and uniforms that are used to calculate the position, velocity, angle, and lifespan of individual particles in a particle system.\n\nThe `particle_vertexData` attribute is a vec4 that contains the position of the particle in XYZ coordinates and a unique ID plus a random factor in the W component. The `particle_uv` attribute is only used if the particle system is using a mesh to render the particles, and it contains the UV coordinates of the mesh.\n\nThe `matrix_viewProjection`, `matrix_model`, `matrix_normal`, and `matrix_viewInverse` uniforms are matrices that are used to transform the particle positions and orientations into world space. The `matrix_view` uniform is only used if the particle system is using a custom camera matrix.\n\nThe `numParticles`, `numParticlesPot`, `graphSampleSize`, `graphNumSamples`, `stretch`, `wrapBounds`, `emitterScale`, `emitterPos`, `faceTangent`, and `faceBinorm` uniforms are all used to control the behavior of the particle system. They determine the number of particles, the size and shape of the emitter, the direction and orientation of the particles, and other properties.\n\nThe `rate`, `rateDiv`, `lifetime`, `deltaRandomnessStatic`, `scaleDivMult`, `alphaDivMult`, `seed`, and `delta` uniforms are used to control the appearance of the particles. They determine the rate at which particles are emitted, the lifespan of the particles, and how their size and opacity change over time.\n\nThe `particleTexOUT`, `particleTexIN`, `internalTex0`, `internalTex1`, and `internalTex2` uniforms are textures that are used to render the particles. They contain information about the color, opacity, and other properties of the particles.\n\nThe `texCoordsAlphaLife` varying is used to pass information about the particle's texture coordinates, opacity, and lifespan to the fragment shader, which is responsible for actually rendering the particles.\n\nOverall, this shader is a critical component of the PlayCanvas engine's particle system. It defines the behavior and appearance of individual particles, and it is used in conjunction with other shaders and rendering techniques to create complex and dynamic particle effects.\n## Questions: \n 1. What is the purpose of this code and how is it used within the PlayCanvas engine?\n   - This code defines a GLSL shader for rendering particles in the PlayCanvas engine. It is used to calculate the position, velocity, angle, and lifetime of each particle, as well as to sample textures and apply transformations.\n2. What are some of the key variables and parameters being used in this code?\n   - Some of the key variables and parameters include: particle_vertexData (position and ID of each particle), matrix_viewProjection (combined view and projection matrix), numParticles (total number of particles), lifetime (duration of each particle), particleTexOUT and particleTexIN (textures used for rendering), and wrapBounds (bounds for wrapping particles around the scene).\n3. Are there any dependencies or requirements for using this code in a PlayCanvas project?\n   - Yes, this code assumes the use of certain uniforms and attributes that are defined elsewhere in the PlayCanvas engine. It also requires the use of GLSL shaders and a basic understanding of particle rendering techniques.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_init.md"}}],["684",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_localShift.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) that is used in the PlayCanvas engine project. The purpose of this code is to transform the position of a particle in 3D space using a model matrix. \n\nThe `particlePos` variable represents the position of the particle in 3D space. The `matrix_model` variable represents the model matrix that is used to transform the position of the particle. The `vec4` function is used to create a 4D vector from the `particlePos` variable, with a value of 1.0 for the fourth component. This is necessary to apply the transformation matrix to the position vector.\n\nThe `matrix_model` is then multiplied by the 4D vector using the `*` operator. This results in a new 4D vector that represents the transformed position of the particle. The `.xyz` property is then used to extract the first three components of the vector, which represent the x, y, and z coordinates of the transformed position. \n\nThis code is typically used in a particle system to transform the position of each particle based on the position, rotation, and scale of the particle's parent object. The transformed position is then used to render the particle in the correct location in 3D space.\n\nHere is an example of how this code might be used in a particle system:\n\n```glsl\nuniform mat4 matrix_model;\n\nattribute vec3 particlePos;\n\nvoid main() {\n  // Transform the position of the particle using the model matrix\n  vec3 transformedPos = (matrix_model * vec4(particlePos, 1.0)).xyz;\n\n  // Set the position of the particle\n  gl_Position = vec4(transformedPos, 1.0);\n}\n```\n\nIn this example, the `matrix_model` uniform is passed in from the parent object of the particle system. The `particlePos` attribute represents the position of each particle relative to the parent object. The `transformedPos` variable is then used to set the position of each particle in the shader.\n## Questions: \n 1. What is the purpose of this code?\n   - This code is likely a GLSL shader code that transforms the position of a particle using a model matrix.\n\n2. What is the data type of `particlePos` and `matrix_model`?\n   - Without additional context, it is unclear what data type `particlePos` and `matrix_model` are. They could be vectors, matrices, or arrays.\n\n3. What is the expected output of this code?\n   - The expected output of this code is likely a transformed position vector in 3D space. However, without additional context, it is unclear what the specific use case or application of this code is.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_localShift.md"}}],["685",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_mesh.js)\n\nThis code is written in GLSL, which is a shading language used for graphics programming. It exports a function that takes in two parameters: `meshLocalPos` and `inAngle`. \n\nThe first line of the function creates a new variable `localPos` and assigns it the value of `meshLocalPos`. This variable represents the position of a mesh in 3D space. \n\nThe next two lines use the `rotate` function to rotate the `x` and `y` components of `localPos` by `inAngle` degrees around the `z` axis. The `rotate` function takes in three parameters: the vector to be rotated, the angle of rotation, and the rotation matrix. The `rotate` function is not defined in this code snippet, but it is likely defined elsewhere in the PlayCanvas engine.\n\nThe final line of the function calls the `billboard` function, passing in two parameters: `particlePos` and `quadXY`. The `billboard` function is also not defined in this code snippet, but it is likely used to create a billboard effect, which is a technique used in computer graphics to create the illusion of a 3D object being a 2D object that always faces the camera.\n\nOverall, this code snippet is likely used in the PlayCanvas engine to manipulate the position of a mesh and create a billboard effect. It demonstrates the use of GLSL functions to perform mathematical operations on vectors and matrices.\n## Questions: \n 1. What is the purpose of the `rotate` function being used in this code?\n   - The `rotate` function is being used to rotate the `localPos` vector's `xy` and `yz` components by the `inAngle` angle using the `rotMatrix` matrix.\n\n2. What is the `billboard` function being called with as arguments?\n   - The `billboard` function is being called with `particlePos` and `quadXY` as arguments.\n\n3. What is the expected output or result of this code?\n   - It is unclear what the expected output or result of this code is without additional context or information about the `billboard` function and the purpose of the `meshLocalPos` and `quadXY` variables.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_mesh.md"}}],["686",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_normal.js)\n\nThis code exports a GLSL shader code that calculates the normal vector of a given position in 3D space. The `normalize()` function is used to ensure that the resulting vector has a length of 1, which is a requirement for normal vectors in many graphics applications.\n\nThe `localPos` variable represents the position of the vertex in local space, while `matrix_viewInverse[2].xyz` represents the third column of the inverse view matrix. This column corresponds to the direction that the camera is facing in world space. By adding these two vectors together, we obtain a vector that points from the vertex towards the camera.\n\nThe resulting vector is then normalized to obtain the normal vector. This normal vector can be used in lighting calculations to determine how much light is reflected off the surface at the given position.\n\nThis code is likely used in the larger PlayCanvas engine project to generate shaders for 3D models and other graphical objects. By calculating the normal vector for each vertex in a model, the engine can accurately simulate lighting and shading effects, resulting in more realistic and visually appealing graphics.\n\nHere is an example of how this code might be used in a shader:\n\n```\nattribute vec3 aPosition;\nuniform mat4 matrix_viewInverse;\n\nvoid main() {\n  vec3 localPos = aPosition;\n  vec3 normal = normalize(localPos + matrix_viewInverse[2].xyz);\n  // use the normal vector in lighting calculations\n  // ...\n}\n```\n## Questions: \n 1. What is the purpose of this code?\n   This code appears to be a GLSL shader code that calculates the normal vector of a 3D object based on its local position and the inverse view matrix.\n\n2. What is the data type of the variable \"Normal\"?\n   The data type of the variable \"Normal\" is not explicitly defined in this code, but it is likely a 3D vector since it represents a normal vector.\n\n3. What is the significance of the \"matrix_viewInverse\" variable?\n   The \"matrix_viewInverse\" variable is likely a 4x4 matrix that represents the inverse of the view matrix. It is used to transform the local position of the object into world space before calculating the normal vector.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_normal.md"}}],["687",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_pointAlong.js)\n\nThis code is a GLSL shader code that calculates the angle of a given velocity vector. The `atan` function is used to calculate the angle between the x and y components of the velocity vector. The resulting angle is stored in the `inAngle` variable.\n\nThis code is likely used in the PlayCanvas engine to calculate the angle of movement for various game objects. For example, if a game object is moving in a certain direction, this code can be used to determine the angle of movement and adjust the object's behavior accordingly. \n\nHere is an example of how this code might be used in a PlayCanvas game:\n\n```javascript\n// create a new velocity vector\nvar velocityV = new pc.Vec2(1, 1);\n\n// calculate the angle of movement using the shader code\nvar inAngle = shaderCode(velocityV);\n\n// adjust the behavior of the game object based on the angle of movement\nif (inAngle > 0 && inAngle < 90) {\n    // object is moving up and to the right\n} else if (inAngle > 90 && inAngle < 180) {\n    // object is moving up and to the left\n} else if (inAngle > 180 && inAngle < 270) {\n    // object is moving down and to the left\n} else {\n    // object is moving down and to the right\n}\n```\n\nOverall, this code is a small but important part of the PlayCanvas engine that helps to determine the movement behavior of game objects.\n## Questions: \n 1. **What does the `/* glsl */` comment mean?**  \nThe `/* glsl */` comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a high-level language used to write shaders for graphics processing units (GPUs).\n\n2. **What is the purpose of the `atan` function and how is it being used here?**  \nThe `atan` function is used to calculate the arctangent of the ratio of the `velocityV.x` and `velocityV.y` values. This is being assigned to the `inAngle` variable, which is likely being used to determine the direction of movement for a game object.\n\n3. **What is the `TODO` comment referring to and what is the suggested solution?**  \nThe `TODO` comment suggests that there is a more efficient way to create a rotation matrix from vectors than using the `atan` function. The comment suggests that a new function should be created to handle this task.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_pointAlong.md"}}],["688",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_soft.js)\n\nThis code exports a GLSL shader code as a default value. The purpose of this code is to calculate the linear depth of a given position in a 3D scene. The `getLinearDepth` function takes in a `localPos` parameter, which is the position of the current pixel being rendered in local space. The function then calculates the linear depth of the pixel by using the formula `depth = -viewMatrix * modelMatrix * vec4(localPos, 1.0)`. \n\nThe `vDepth` variable is then assigned the value returned by the `getLinearDepth` function. This variable is likely used in other parts of the shader code to perform depth-based operations, such as fog or shadow calculations.\n\nHere is an example of how this code may be used in a larger project:\n\n```glsl\n// vertex shader code\nattribute vec3 aPosition;\nuniform mat4 modelMatrix;\nuniform mat4 viewMatrix;\nuniform mat4 projectionMatrix;\n\nvarying float vDepth;\n\nvoid main() {\n    vec4 worldPos = modelMatrix * vec4(aPosition, 1.0);\n    vec4 viewPos = viewMatrix * worldPos;\n    gl_Position = projectionMatrix * viewPos;\n    vDepth = getLinearDepth(worldPos.xyz);\n}\n```\n\nIn this example, the `getLinearDepth` function is called in the vertex shader to calculate the linear depth of each vertex in the scene. The resulting `vDepth` value is then passed to the fragment shader, where it can be used to perform depth-based operations. Overall, this code plays an important role in rendering 3D scenes with accurate depth information.\n## Questions: \n 1. What is the purpose of the `getLinearDepth` function?\n   - The `getLinearDepth` function is used to calculate the linear depth of a given position in the scene.\n2. What is the data type of `vDepth`?\n   - The data type of `vDepth` is not specified in this code snippet, but it is likely a variable used to store the calculated linear depth value.\n3. What is the context in which this code is used?\n   - Without additional context, it is unclear where and how this code is being used within the PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_soft.md"}}],["689",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_stretch.js)\n\nThis code is written in GLSL and is used for particle animation in the PlayCanvas engine. The purpose of this code is to calculate the new position of a particle based on its previous position, velocity, and stretch factor. \n\nThe code starts by calculating the movement direction of the particle using the input velocity and stretch factor. It then calculates the previous position of the particle by subtracting the movement direction from the current position and adding the position moved by the particle. \n\nNext, the code calculates the vector from the center of the particle to its vertex in screen space. This is done by multiplying the local position of the particle by the view matrix and taking the x and y components of the resulting vector. The resulting vector is then normalized. \n\nFinally, the code calculates the interpolation factor between the previous position and the current position of the particle. This is done by taking the dot product of the negative velocity vector and the vector from the center of the particle to its vertex. The resulting value is then multiplied by 0.5 and added to 0.5 to get a value between 0 and 1. This value is then used to interpolate between the previous position and the current position of the particle using the mix function. \n\nThis code is used in the larger PlayCanvas engine project to animate particles in various ways. For example, it could be used to simulate the movement of smoke particles in a game or the motion of particles in a particle system. \n\nHere is an example of how this code could be used in a particle system:\n\n```glsl\nuniform float stretch;\nuniform vec3 inVel;\nuniform vec3 particlePos;\nuniform vec3 particlePosMoved;\nuniform mat4 matrix_view;\nuniform vec3 velocityV;\nuniform vec3 localPos;\n\nvec3 moveDir = inVel * stretch;\nvec3 posPrev = particlePos - moveDir;\nposPrev += particlePosMoved;\n\nvec2 centerToVertexV = normalize((mat3(matrix_view) * localPos).xy);\n\nfloat interpolation = dot(-velocityV, centerToVertexV) * 0.5 + 0.5;\n\nparticlePos = mix(particlePos, posPrev, interpolation);\n```\n\nIn this example, the uniforms are passed in to the shader from the CPU. The resulting particle position is then used to render the particle in the game.\n## Questions: \n 1. What is the purpose of this code?\n   - This code is likely part of a particle system in the PlayCanvas engine, and it appears to be calculating the new position of a particle based on its previous position, velocity, and stretch.\n\n2. What do the variables \"inVel\" and \"stretch\" represent?\n   - Without additional context, it's unclear what these variables represent. It's possible that \"inVel\" refers to the input velocity of the particle, while \"stretch\" could be a scalar value that affects how much the particle stretches as it moves.\n\n3. What is the significance of the \"interpolation\" variable?\n   - The \"interpolation\" variable appears to be used to blend between the particle's current position and its previous position, based on the dot product between its velocity and the vector from the camera to the particle's position. However, it's unclear why this blending is necessary or how it affects the particle's behavior.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_stretch.md"}}],["690",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/particle/vert/particle_wrap.js)\n\nThe code above is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to modify the position of particles in a 3D space. \n\nThe code starts by declaring a variable called `origParticlePos` which is assigned the value of `particlePos`. `particlePos` is a vector that represents the position of a particle in 3D space. \n\nNext, the code subtracts the translation component of the model matrix (`matrix_model[3].xyz`) from `particlePos`. This is done to ensure that the particle is positioned relative to the model's origin. \n\nThe `mod()` function is then used to wrap the particle position within the bounds of the `wrapBounds` vector. This is done to ensure that the particle stays within a specific area of the 3D space. The resulting value is then shifted by half of `wrapBounds` and added back to the translation component of the model matrix to ensure that the particle is positioned correctly relative to the model's origin. \n\nFinally, the code calculates the difference between the original particle position (`origParticlePos`) and the modified particle position (`particlePos`). This value is assigned to `particlePosMoved` and can be used to calculate the movement of the particle in the 3D space. \n\nThis code can be used in the PlayCanvas engine project to modify the position of particles in a 3D space. It can be used in conjunction with other GLSL shader codes to create complex particle effects such as explosions, smoke, and fire. \n\nExample usage of this code in a PlayCanvas project:\n\n```javascript\n// Create a new particle system\nconst particleSystem = new pc.ParticleSystem();\n\n// Set the GLSL shader code for the particle system\nparticleSystem.shader = `\n    // GLSL shader code here\n`;\n\n// Add particles to the particle system\nparticleSystem.addParticles(100);\n\n// Update the particle positions\nparticleSystem.updateParticles();\n```\n## Questions: \n 1. What is the purpose of the `matrix_model` variable?\n   - The `matrix_model` variable is used to transform the `particlePos` vector.\n\n2. What is the significance of the `wrapBounds` variable?\n   - The `wrapBounds` variable is used to wrap the `particlePos` vector around a specified boundary.\n\n3. What is the expected output of this code?\n   - The code modifies the `particlePos` vector and calculates the difference between the modified and original positions, which is stored in the `particlePosMoved` variable. The expected output would be the updated `particlePos` and `particlePosMoved` vectors.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/particle/vert/particle_wrap.md"}}],["691",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/skybox/frag/skyboxEnv.js)\n\nThis code is a GLSL shader that is used in the PlayCanvas engine to render environment maps. The purpose of this shader is to take a texture atlas of environment maps and use it to render a specific environment map based on the direction of the camera. \n\nThe shader takes in a few uniform variables, including a texture atlas of environment maps, a mip level, and a view direction. The view direction is used to calculate the UV coordinates of the environment map that should be rendered. The shader then decodes the texture at the specified mip level and applies a series of color correction and tone mapping operations to the resulting linear color values. Finally, the resulting color is output to the screen.\n\nThis shader is likely used in conjunction with other shaders and rendering techniques to create a realistic and immersive environment for the user. For example, it may be used in a skybox or cubemap to create the illusion of a vast and detailed environment surrounding the user. \n\nHere is an example of how this shader might be used in a PlayCanvas project:\n\n```javascript\n// create a new material using the environment map shader\nconst envMapMaterial = new pc.StandardMaterial();\nenvMapMaterial.chunks.environment = /* glsl */`\n    // insert the environment map shader code here\n`;\n\n// load the environment map texture atlas\nconst envMapTexture = new pc.Texture();\nenvMapTexture.loadFromUrl('path/to/envMapAtlas.png', () => {\n    // set the environment map texture and mip level uniform variables\n    envMapMaterial.setParameter('texture_envAtlas', envMapTexture);\n    envMapMaterial.setParameter('mipLevel', 0);\n});\n\n// create a new skybox entity and set its material to the environment map material\nconst skybox = new pc.Entity();\nskybox.addComponent('model', {\n    type: 'box',\n    material: envMapMaterial,\n    castShadows: false,\n    receiveShadows: false\n});\n``` \n\nIn this example, a new material is created using the environment map shader code. The environment map texture atlas is loaded and set as a uniform variable in the material. Finally, a new skybox entity is created and its material is set to the environment map material. This creates a skybox that uses the environment map shader to render a realistic and immersive environment for the user.\n## Questions: \n 1. What is the purpose of the `texture_envAtlas` uniform and how is it used in the code?\n   - The `texture_envAtlas` uniform is a sampler2D used to sample an environment map texture. It is used to calculate the `linear` variable which is then processed and output as the final color.\n   \n2. What is the significance of the `toSphericalUv` function and how does it affect the output?\n   - The `toSphericalUv` function takes a normalized direction vector and converts it to spherical coordinates in UV space. This is used to sample the environment map texture and affects the final color output.\n   \n3. What is the purpose of the `gammaCorrectOutput` and `toneMap` functions and how do they affect the final color output?\n   - The `gammaCorrectOutput` function applies gamma correction to the color output to improve color accuracy. The `toneMap` function applies tone mapping to the color output to improve contrast and brightness. Both functions are used to process the `linear` variable before it is output as the final color.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/skybox/frag/skyboxEnv.md"}}],["692",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/skybox/frag/skyboxHDR.js)\n\nThis code is a GLSL shader that is used to render a skybox in the PlayCanvas engine. The purpose of this shader is to take a texture cube map and map it onto the inside of a cube that surrounds the scene, creating the illusion of a sky. \n\nThe `varying vec3 vViewDir` variable is used to store the direction of the camera view. The `uniform samplerCube texture_cubeMap` variable is used to store the texture cube map that will be used to render the skybox. \n\nThe `void main(void)` function is the main entry point of the shader. The `vec3 dir=vViewDir` variable is used to store the direction of the camera view. The `dir.x *= -1.0` line is used to flip the direction of the camera view along the x-axis, which is necessary to correctly map the texture onto the skybox. \n\nThe `vec3 linear = $DECODE(textureCube(texture_cubeMap, fixSeamsStatic(dir, $FIXCONST)))` line is used to sample the texture cube map and decode the color values. The `fixSeamsStatic` function is used to fix any seams that may appear in the texture when it is mapped onto the skybox. The `$FIXCONST` variable is a constant that is used to control the amount of seam fixing that is applied. \n\nThe `gl_FragColor = vec4(gammaCorrectOutput(toneMap(processEnvironment(linear))), 1.0)` line is used to set the final color of the fragment. The `processEnvironment` function is used to apply any environment mapping that may be necessary. The `toneMap` function is used to adjust the brightness and contrast of the image. The `gammaCorrectOutput` function is used to apply gamma correction to the final color. \n\nOverall, this shader is an important component of the PlayCanvas engine, as it is used to render the skybox in 3D scenes. Developers can use this shader by specifying a texture cube map and applying it to a cube that surrounds the scene. They can also adjust the constants used in the shader to control the appearance of the skybox. \n\nExample usage:\n\n```javascript\n// Create a new skybox material\nvar skyboxMaterial = new pc.StandardMaterial();\n\n// Set the shader to the skybox shader\nskyboxMaterial.shader = pc.ShaderChunks.skybox;\n\n// Set the texture cube map to the skybox texture\nskyboxMaterial.setParameter('texture_cubeMap', skyboxTexture);\n\n// Create a new skybox entity\nvar skyboxEntity = new pc.Entity();\n\n// Create a new cube mesh\nvar cubeMesh = pc.createBox(graphicsDevice);\n\n// Set the mesh of the skybox entity to the cube mesh\nskyboxEntity.addComponent('model', {\n    type: 'box',\n    material: skyboxMaterial\n});\n\n// Add the skybox entity to the scene\napp.root.addChild(skyboxEntity);\n```\n## Questions: \n 1. What is the purpose of the `vViewDir` varying variable?\n   - The `vViewDir` varying variable is used to store the direction vector from the camera to the current fragment.\n\n2. What is the significance of the `texture_cubeMap` uniform variable?\n   - The `texture_cubeMap` uniform variable is used to store a samplerCube, which is a texture that contains six 2D images that represent the faces of a cube. This is used for environment mapping.\n\n3. What do the functions `fixSeamsStatic`, `$DECODE`, `toneMap`, `processEnvironment`, and `gammaCorrectOutput` do?\n   - `fixSeamsStatic` is a function that fixes seams in the textureCube caused by texture filtering.\n   - `$DECODE` is a function that decodes a linear color value from a textureCube.\n   - `toneMap` is a function that maps high dynamic range (HDR) colors to low dynamic range (LDR) colors.\n   - `processEnvironment` is a function that processes the environment map to remove any artifacts.\n   - `gammaCorrectOutput` is a function that applies gamma correction to the output color.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/skybox/frag/skyboxHDR.md"}}],["693",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/skybox/vert/skybox.js)\n\nThe code provided is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to render a skybox in a 3D scene. \n\nThe code starts by defining an attribute variable `aPosition` which represents the position of the vertices of the skybox. The `#ifndef` directive is used to check if the `VIEWMATRIX` variable has been defined before. If it has not been defined, it is defined as a uniform variable `matrix_view` which represents the view matrix of the camera. \n\nThe `matrix_projectionSkybox` variable is a uniform variable that represents the projection matrix used to render the skybox. The `cubeMapRotationMatrix` variable is a uniform variable that represents the rotation matrix used to rotate the skybox. \n\nThe `varying` variable `vViewDir` is used to pass the view direction of the camera to the fragment shader. \n\nThe `main` function is the entry point of the shader. It starts by creating a `view` matrix using the `matrix_view` uniform variable. The `view` matrix is then modified to set the translation component to zero. This is done to ensure that the skybox is always centered at the camera position. \n\nThe `gl_Position` variable is set to the product of the `matrix_projectionSkybox`, `view`, and `aPosition` vectors. This is the final position of the vertex in clip space. \n\nThe `gl_Position.z` value is then set to `gl_Position.w - 0.00001`. This is done to ensure that the skybox is always rendered at the far Z plane, regardless of the clip planes on the camera. \n\nFinally, the `vViewDir` variable is set to the product of `aPosition` and `cubeMapRotationMatrix`. This is used to pass the view direction of the camera to the fragment shader. \n\nOverall, this code is an essential part of the PlayCanvas engine project as it is used to render the skybox in a 3D scene. It is used in conjunction with other shaders and rendering techniques to create a realistic and immersive environment for the user.\n## Questions: \n 1. What is the purpose of the `aPosition` attribute and how is it used in this code?\n   - The `aPosition` attribute is a vec3 that likely represents the position of a vertex in 3D space. It is used to calculate the final position of the vertex in the skybox by multiplying it with the view and projection matrices.\n\n2. What is the `cubeMapRotationMatrix` uniform and how does it affect the output?\n   - The `cubeMapRotationMatrix` uniform is a mat3 that is used to rotate the `aPosition` attribute before it is multiplied with the view matrix. This allows for the skybox to be rotated in 3D space.\n\n3. Why is a fudge factor subtracted from `gl_Position.z`?\n   - The fudge factor is subtracted to ensure that the skybox is always rendered at the far Z plane, regardless of the clip planes on the camera. This helps to prevent floating point errors that could push pixels beyond the far Z plane.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/skybox/vert/skybox.md"}}],["694",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/alphaTest.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) that defines a function called `alphaTest`. The purpose of this function is to perform alpha testing on a given alpha value. Alpha testing is a technique used in computer graphics to discard fragments (pixels) that have an alpha value below a certain threshold. This is useful for rendering transparent objects such as glass or foliage, where the parts that are supposed to be transparent should not be rendered at all.\n\nThe `alphaTest` function takes a single parameter `a`, which is the alpha value of the fragment being processed. It compares this value with a uniform variable called `alpha_ref`, which is set externally by the application using this shader. If `a` is less than `alpha_ref`, the function discards the fragment using the `discard` keyword. Otherwise, the function returns normally and the fragment is processed further.\n\nThis shader code can be used in the PlayCanvas engine to render transparent objects with alpha testing. For example, a material that uses this shader could be applied to a mesh representing a tree with leaves. The `alpha_ref` uniform variable could be set to a value that corresponds to the desired level of transparency for the leaves. During rendering, the shader would discard all fragments with an alpha value below this threshold, resulting in a more realistic and efficient rendering of the tree.\n\nHere is an example of how this shader code could be used in a PlayCanvas script:\n\n```javascript\nvar material = new pc.StandardMaterial();\nmaterial.chunks.alphaTest = /* glsl */`\n    uniform float alpha_ref;\n\n    void alphaTest(float a) {\n        if (a < alpha_ref) discard;\n    }\n`;\nmaterial.setParameter('alpha_ref', 0.5); // set alpha threshold to 0.5\n```\n\nIn this example, a new `StandardMaterial` is created and the `alphaTest` function is assigned to the `chunks.alphaTest` property of the material. The `setParameter` method is then used to set the value of `alpha_ref` to 0.5. This material can then be applied to a mesh in the scene to achieve the desired alpha testing effect.\n## Questions: \n 1. What is the purpose of this code?\n   This code defines a GLSL function called `alphaTest` that discards fragments with an alpha value below a certain threshold specified by the `alpha_ref` uniform.\n\n2. How is this code used within the PlayCanvas engine?\n   It is likely that this code is used as part of the rendering pipeline in PlayCanvas to perform alpha testing on certain materials or objects.\n\n3. Are there any potential performance implications of using this code?\n   Depending on how frequently `alphaTest` is called and how many fragments are discarded, there could be performance implications such as increased GPU load or reduced frame rates. It would be important to test and optimize the usage of this code in the context of the larger PlayCanvas engine.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/alphaTest.md"}}],["695",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/ao.js)\n\nThe code above is a GLSL shader function that calculates ambient occlusion (AO) for a 3D model. The function is exported as the default export of the file. \n\nThe purpose of this code is to calculate the amount of ambient light that is occluded by nearby geometry, which can be used to create more realistic lighting in a 3D scene. The function takes no parameters and returns no value, but instead modifies a variable called `dAo` which is likely used elsewhere in the shader. \n\nThe first line of the function sets `dAo` to 1.0, which is the default value if no occlusion is detected. The function then checks for the presence of two optional texture maps that can be used to modify the occlusion value. If a texture called `MAPTEXTURE` is present, the function multiplies `dAo` by the value of the texture at the current UV coordinate, with an additional bias value applied. This allows for more fine-grained control over the occlusion value based on the texture. If a texture called `MAPVERTEX` is present, the function multiplies `dAo` by the saturation value of the vertex color at the current position. This allows for occlusion to be affected by the color of the model at each point, which can be useful for models with complex textures or materials. \n\nOverall, this function is a small but important part of a larger shader program that is used to render 3D models in the PlayCanvas engine. It is likely used in conjunction with other shader functions to create a realistic lighting model for the scene. Here is an example of how this function might be used in a larger shader program:\n\n```\n// Vertex shader\nattribute vec3 aPosition;\nattribute vec2 aUV;\nattribute vec4 aVertexColor;\n\nvarying vec2 vUV;\nvarying vec4 vVertexColor;\n\nvoid main() {\n    gl_Position = vec4(aPosition, 1.0);\n    vUV = aUV;\n    vVertexColor = aVertexColor;\n}\n\n// Fragment shader\nuniform sampler2D uSampler;\nuniform float uTextureBias;\n\nvarying vec2 vUV;\nvarying vec4 vVertexColor;\n\nvoid main() {\n    float dAo;\n    getAO(); // Call the ambient occlusion function\n    gl_FragColor = texture2D(uSampler, vUV) * dAo * vVertexColor;\n}\n``` \n\nIn this example, the vertex shader passes along the UV coordinates and vertex color for each vertex to the fragment shader. The fragment shader then calls the `getAO()` function to calculate the ambient occlusion value for the current pixel, and multiplies it with the texture color and vertex color to create the final output color.\n## Questions: \n 1. What is the purpose of this code and where is it used in the PlayCanvas engine?\n   - This code defines a function called `getAO` that appears to calculate ambient occlusion. It is likely used in the rendering pipeline of the PlayCanvas engine.\n   \n2. What are the variables `$SAMPLER`, `$UV`, `textureBias`, and `vVertexColor` and how are they defined?\n   - These variables are likely defined elsewhere in the PlayCanvas engine codebase and are used as inputs to the `getAO` function. The developer may need to consult other parts of the code to understand their definitions and usage.\n   \n3. What is the purpose of the `#ifdef` statements and how do they affect the behavior of the `getAO` function?\n   - The `#ifdef` statements are preprocessor directives that conditionally compile parts of the code based on whether certain macros are defined. In this case, the `MAPTEXTURE` and `MAPVERTEX` macros are used to selectively include or exclude certain calculations from the `getAO` function. The developer may need to understand the context in which this code is used to fully understand the purpose of these directives.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/ao.md"}}],["696",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/clearCoat.js)\n\nThe code above is a GLSL shader code that defines a function called `getClearCoat()`. This function is responsible for calculating the clear coat effect of a material. The clear coat effect is a type of reflection that simulates the appearance of a glossy surface on top of a material. \n\nThe function starts by setting the `ccSpecularity` variable to 1.0. This variable represents the specular reflection of the material, which is the reflection of light on a surface. \n\nThe function then checks if the `MAPFLOAT` macro is defined. If it is, it multiplies the `ccSpecularity` variable by the `material_clearCoat` uniform. This uniform represents the amount of clear coat effect that should be applied to the material. \n\nNext, the function checks if the `MAPTEXTURE` macro is defined. If it is, it multiplies the `ccSpecularity` variable by the value of a texture sample. The texture sample is obtained by calling the `texture2DBias()` function with three arguments: the sampler, the UV coordinates, and the texture bias. The `SAMPLER` and `UV` variables are predefined by the shader system, and the `textureBias` variable is a user-defined constant that controls the level of detail of the texture sample. The `$CH` variable represents the color channel of the texture sample that should be used. \n\nFinally, the function checks if the `MAPVERTEX` macro is defined. If it is, it multiplies the `ccSpecularity` variable by the saturation of the vertex color. The vertex color is obtained from the `vVertexColor` variable, which is a predefined variable that contains the color of the current vertex. The `$VC` variable represents the color channel of the vertex color that should be used. \n\nOverall, this function is an important part of the PlayCanvas engine's material system. It allows developers to create materials with a clear coat effect by defining the `material_clearCoat` uniform and/or a clear coat texture. The function can be called from other shader functions to calculate the specular reflection of the material with the clear coat effect applied. \n\nExample usage:\n\n```glsl\n// Define a material with a clear coat effect\nuniform float material_clearCoat;\nuniform sampler2D clearCoatTexture;\n\nvoid main() {\n  getClearCoat();\n  vec4 color = texture2D(u_diffuseMap, v_uv0);\n  gl_FragColor = vec4(color.rgb * ccSpecularity, color.a);\n}\n```\n## Questions: \n 1. What is the purpose of the `getClearCoat()` function?\n   - The `getClearCoat()` function is used to calculate the clear coat specularity of a material.\n2. What is the `ccSpecularity` variable and where is it defined?\n   - The `ccSpecularity` variable is used to store the calculated clear coat specularity value and its definition is not shown in this code snippet.\n3. What are the `MAPFLOAT`, `MAPTEXTURE`, and `MAPVERTEX` preprocessor directives used for?\n   - The `MAPFLOAT` directive is used to check if a uniform float variable called `material_clearCoat` is defined. The `MAPTEXTURE` directive is used to check if a texture sampler and UV coordinates are defined. The `MAPVERTEX` directive is used to check if a vertex color is defined.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/clearCoat.md"}}],["697",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/clearCoatGloss.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) that is used in the PlayCanvas engine project. The purpose of this code is to calculate the clear coat glossiness of a material. \n\nThe code defines a function called `getClearCoatGlossiness()` that takes no arguments. Within the function, a variable called `ccGlossiness` is initialized to 1.0. This variable represents the clear coat glossiness of the material. \n\nThe code then checks for the presence of certain preprocessor directives using `#ifdef` statements. If the `MAPFLOAT` directive is present, the code multiplies `ccGlossiness` by a uniform variable called `material_clearCoatGloss`. This means that if the material has a clear coat glossiness map, the value of `material_clearCoatGloss` will be multiplied by `ccGlossiness`.\n\nIf the `MAPTEXTURE` directive is present, the code multiplies `ccGlossiness` by the value of a texture sample at a specific UV coordinate. The texture sample is biased using a variable called `textureBias`. This means that if the material has a clear coat glossiness texture, the value of the texture at the specified UV coordinate will be multiplied by `ccGlossiness`.\n\nIf the `MAPVERTEX` directive is present, the code multiplies `ccGlossiness` by the saturation of a vertex color attribute called `vVertexColor.$VC`. This means that if the material has a clear coat glossiness vertex color, the saturation of the vertex color will be multiplied by `ccGlossiness`.\n\nIf the `MAPINVERT` directive is present, the code subtracts `ccGlossiness` from 1.0. This means that if the material has an inverted clear coat glossiness map, the value of `ccGlossiness` will be subtracted from 1.0.\n\nFinally, a small value of 0.0000001 is added to `ccGlossiness`. This is to avoid a divide-by-zero error in the shader code.\n\nOverall, this code is an important part of the PlayCanvas engine project as it calculates the clear coat glossiness of a material. It can be used in conjunction with other shader code to create realistic materials in 3D scenes. An example of how this code might be used in a larger project is shown below:\n\n```glsl\nuniform sampler2D clearCoatGlossinessMap;\nvarying vec2 vUv;\n\nvoid main() {\n    float ccGlossiness = 1.0;\n    #ifdef MAPTEXTURE\n    ccGlossiness *= texture2D(clearCoatGlossinessMap, vUv).r;\n    #endif\n\n    #include <getClearCoatGlossiness>\n\n    // Use ccGlossiness to calculate final material color\n    // ...\n}\n```\n\nIn this example, the `clearCoatGlossinessMap` uniform is used to sample a clear coat glossiness texture. The `getClearCoatGlossiness()` function is then called to calculate the final clear coat glossiness value. This value can then be used to calculate the final material color.\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n   - This code defines a function called `getClearCoatGlossiness` that calculates the clear coat glossiness of a material. It is likely used in the rendering pipeline of the PlayCanvas engine to determine how light interacts with materials in a scene.\n\n2. What are the different variables and parameters used in this code?\n   - The code uses several preprocessor directives (`#ifdef`) to conditionally compile different parts of the function based on whether certain variables are defined. These variables include `MAPFLOAT`, `MAPTEXTURE`, `MAPVERTEX`, and `MAPINVERT`. The function also uses a uniform variable called `material_clearCoatGloss`, a texture sampler called `$SAMPLER`, a texture coordinate called `$UV`, a texture bias value called `textureBias`, and a vertex color value called `vVertexColor.$VC`.\n\n3. Are there any potential issues or limitations with this code?\n   - One potential issue with this code is that it assumes certain variables are defined and available for use, which may not always be the case. Additionally, the function adds a small value (`0.0000001`) to the calculated glossiness, which may be unnecessary or cause unintended artifacts in certain situations.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/clearCoatGloss.md"}}],["698",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/clearCoatNormal.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) and it is used in the PlayCanvas engine project. The purpose of this code is to generate a clear coat normal map for a material. \n\nThe code starts by checking if the MAPTEXTURE flag is defined. If it is, then the code proceeds to calculate the clear coat normal map. The uniform variable material_clearCoatBumpiness is used to control the intensity of the bumpiness of the clear coat. \n\nThe function getClearCoatNormal() is responsible for generating the clear coat normal map. It starts by unpacking the normal map from the texture using the texture2DBias() function. The $SAMPLER and $UV variables are placeholders that will be replaced with the actual sampler and UV coordinates when the shader is compiled. The textureBias variable is used to adjust the level of detail of the texture. \n\nThe normal map is then mixed with a default normal vector (0, 0, 1) using the material_clearCoatBumpiness variable as the mixing factor. This creates the bumpiness effect on the clear coat. \n\nFinally, the ccNormalW variable is set to the normalized dot product of the tangent, bitangent, and normal vectors (dTBN) multiplied by the modified normal map. If the MAPTEXTURE flag is not defined, then the ccNormalW variable is set to the vertex normal vector (dVertexNormalW).\n\nThis code is used in the PlayCanvas engine to generate realistic clear coat materials for 3D models. It can be used in combination with other shaders and materials to create complex and visually appealing scenes. Here is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\nvar material = new pc.StandardMaterial();\nmaterial.clearCoat = 0.5;\nmaterial.clearCoatBumpiness = 0.2;\nmaterial.diffuseMap = diffuseTexture;\nmaterial.normalMap = normalTexture;\nmaterial.shader = shaderCode;\n```\n\nIn this example, a new StandardMaterial is created and the clear coat and clear coat bumpiness properties are set. The diffuse and normal textures are also set, and the shaderCode variable contains the GLSL code shown above. This material can then be applied to a 3D model in the PlayCanvas scene to create a clear coat effect.\n## Questions: \n 1. What is the purpose of the `getClearCoatNormal()` function?\n- The `getClearCoatNormal()` function is used to calculate the normal vector for a clear coat material.\n\n2. What is the `MAPTEXTURE` preprocessor directive used for?\n- The `MAPTEXTURE` preprocessor directive is used to conditionally compile code based on whether a texture map is being used or not.\n\n3. What is the purpose of the `material_clearCoatBumpiness` uniform variable?\n- The `material_clearCoatBumpiness` uniform variable is used to control the amount of bumpiness applied to the clear coat material.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/clearCoatNormal.md"}}],["699",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/detailModes.js)\n\nThe code provided is a set of functions that perform color blending operations. These functions are intended to be used in the context of a graphics engine, specifically the PlayCanvas engine. \n\nThe first two functions, `detailMode_mul` and `detailMode_add`, perform simple multiplication and addition operations on two input colors, respectively. These operations are commonly used in graphics to adjust the brightness or contrast of an image.\n\nThe next two functions, `detailMode_screen` and `detailMode_overlay`, implement more complex blending modes. `detailMode_screen` implements the \"screen\" blending mode, which is commonly used to brighten an image. `detailMode_overlay` implements the \"overlay\" blending mode, which is used to increase contrast and add a sense of depth to an image.\n\nThe final two functions, `detailMode_min` and `detailMode_max`, return the minimum and maximum values of the input colors, respectively. These functions are useful for implementing effects such as thresholding or edge detection.\n\nOverall, these functions provide a set of tools for blending colors in a variety of ways, allowing developers to create a wide range of visual effects. For example, a developer could use the `detailMode_screen` function to create a bright, glowing effect on a particular object in a scene, or use the `detailMode_overlay` function to add depth and contrast to a background image. \n\nHere is an example of how one of these functions could be used in a larger project:\n\n```javascript\n// create two colors\nconst color1 = new pc.Color(1, 0, 0);\nconst color2 = new pc.Color(0, 1, 0);\n\n// blend the colors using the detailMode_screen function\nconst blendedColor = detailMode_screen(color1, color2);\n\n// set the color of a material to the blended color\nmaterial.diffuse = blendedColor;\n```\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code defines several functions for manipulating colors in a detail mode, including multiplication, addition, screen blending, overlay blending, minimum, and maximum.\n\n2. What is the input and output of each function?\n    \n    Each function takes two vec3 color values as input and returns a vec3 color value as output.\n\n3. Where can I find more information about the blend modes used in the `detailMode_screen` and `detailMode_overlay` functions?\n    \n    The `detailMode_screen` function uses the blend mode described on the Wikipedia page for \"Screen\" blend modes, while the `detailMode_overlay` function uses the blend mode described on the Wikipedia page for \"Overlay\" blend modes. More information about these blend modes can be found on those pages.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/detailModes.md"}}],["700",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/diffuse.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) and is used in the PlayCanvas engine project. The purpose of this code is to calculate the albedo (the diffuse color of a surface) of a material based on various input maps and vertex colors.\n\nThe code starts by defining a uniform variable `material_diffuse` which is used to store the base color of the material. If the `MAPCOLOR` flag is defined, the base color is multiplied with the `material_diffuse.rgb` value to get the final albedo.\n\nNext, the code checks for the `MAPTEXTURE` flag. If it is defined, the code decodes the texture value at the given UV coordinates and applies a bias to it. The resulting color is then passed to the `addAlbedoDetail` function which applies additional detail to the albedo. The final albedo is then multiplied with the result of this function.\n\nFinally, the code checks for the `MAPVERTEX` flag. If it is defined, the code applies gamma correction to the vertex color and saturates it before multiplying it with the final albedo.\n\nThis code is used in the PlayCanvas engine to calculate the albedo of a material in a 3D scene. It takes into account various input maps and vertex colors to produce a final color that is used to render the material. Here is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// Create a new material\nvar material = new pc.StandardMaterial();\n\n// Set the base color of the material\nmaterial.diffuse.set(1, 0, 0);\n\n// Set the texture map of the material\nmaterial.diffuseMap = texture;\n\n// Set the vertex color of the material\nmaterial.vertexColor = true;\n\n// Set the shader code of the material\nmaterial.shaderDefinition = {\n    attributes: {\n        aPosition: pc.SEMANTIC_POSITION,\n        aNormal: pc.SEMANTIC_NORMAL,\n        aUv0: pc.SEMANTIC_TEXCOORD0,\n        aVertexColor: pc.SEMANTIC_COLOR\n    },\n    vshader: /* glsl */`\n        attribute vec3 aPosition;\n        attribute vec3 aNormal;\n        attribute vec2 aUv0;\n        attribute vec4 aVertexColor;\n\n        uniform mat4 matrix_model;\n        uniform mat4 matrix_viewProjection;\n\n        varying vec3 vNormal;\n        varying vec2 vUv0;\n        varying vec4 vVertexColor;\n\n        void main() {\n            gl_Position = matrix_viewProjection * matrix_model * vec4(aPosition, 1.0);\n            vNormal = aNormal;\n            vUv0 = aUv0;\n            vVertexColor = aVertexColor;\n        }\n    `,\n    fshader: /* glsl */`\n        uniform sampler2D texture_diffuseMap;\n        uniform vec3 material_diffuse;\n\n        #define MAPCOLOR\n        #define MAPTEXTURE\n        #define MAPVERTEX\n\n        void getAlbedo() {\n            dAlbedo = vec3(1.0);\n\n            #ifdef MAPCOLOR\n                dAlbedo *= material_diffuse.rgb;\n            #endif\n\n            #ifdef MAPTEXTURE\n                vec3 albedoBase = $DECODE(texture2DBias($SAMPLER, $UV, textureBias)).$CH;\n                dAlbedo *= addAlbedoDetail(albedoBase);\n            #endif\n\n            #ifdef MAPVERTEX\n                dAlbedo *= gammaCorrectInput(saturate(vVertexColor.$VC));\n            #endif\n        }\n\n        void main() {\n            getAlbedo();\n            gl_FragColor = vec4(dAlbedo, 1.0);\n        }\n    `\n};\n``` \n\nIn this example, a new material is created and various properties are set on it, including the base color, texture map, and vertex color. The shader code of the material is also set to the code shown above. When this material is applied to a 3D object in a scene, the shader code is used to calculate the albedo of the material based on the input maps and vertex colors. The resulting color is then used to render the material.\n## Questions: \n 1. What is the purpose of the `getAlbedo()` function?\n   - The `getAlbedo()` function is used to calculate the albedo (diffuse color) of a material.\n2. What is the significance of the `#ifdef` preprocessor directives?\n   - The `#ifdef` preprocessor directives are used to conditionally compile parts of the code based on whether certain macros are defined or not. In this code, certain parts of the code are only compiled if certain macros like `MAPCOLOR`, `MAPTEXTURE`, or `MAPVERTEX` are defined.\n3. What are the variables `$DECODE`, `$SAMPLER`, `$UV`, `textureBias`, `$CH`, and `gammaCorrectInput()`?\n   - Without more context, it is difficult to determine the exact purpose of these variables. However, they appear to be either predefined functions or variables that are used in the calculation of the albedo.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/diffuse.md"}}],["701",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/diffuseDetailMap.js)\n\nThe code provided is a shader function that takes in a vec3 color value representing the albedo of a material and returns a modified version of that color. The purpose of this function is to add detail to the albedo color based on a texture map, if one is available.\n\nThe function first checks if a texture map is available by using the preprocessor directive `#ifdef MAPTEXTURE`. If a texture map is available, the function retrieves the color value of the texture at the current UV coordinates using the `texture2DBias` function. The `textureBias` parameter is used to adjust the level of detail in the texture, and the `$CH` parameter specifies which color channel to use (e.g. red, green, or blue). The resulting color value is stored in the `albedoDetail` variable.\n\nThe function then calls another function, `detailMode_$DETAILMODE`, passing in the original albedo color and the albedo detail color as parameters. The `$DETAILMODE` parameter specifies which detail mode to use, and the function itself is not provided in this code snippet. It is likely that this function applies some sort of blending or filtering operation to combine the two colors in a visually pleasing way.\n\nIf no texture map is available, the function simply returns the original albedo color.\n\nThis shader function can be used in the larger PlayCanvas engine project to add detail to materials in a visually appealing way. By using texture maps to add small variations in color and shading, materials can look more realistic and less flat. The `detailMode_$DETAILMODE` function can be customized to achieve different effects, allowing for a wide range of material styles to be created.\n## Questions: \n 1. What is the purpose of the `addAlbedoDetail` function?\n- The `addAlbedoDetail` function takes in a `vec3` parameter called `albedo` and returns either the original `albedo` value or a modified value based on the presence of a texture map.\n\n2. What is the significance of the `#ifdef MAPTEXTURE` preprocessor directive?\n- The `#ifdef MAPTEXTURE` directive checks if the `MAPTEXTURE` macro has been defined. If it has, then the code inside the `#ifdef` block will be compiled, otherwise it will be skipped.\n\n3. What is the purpose of the `detailMode_$DETAILMODE` function call?\n- The `detailMode_$DETAILMODE` function takes in two `vec3` parameters and returns a modified `vec3` value based on the value of the `$DETAILMODE` macro. The specific implementation of this function is not shown in the code provided.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/diffuseDetailMap.md"}}],["702",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/emissive.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) that is used to calculate the emission of a material. The purpose of this code is to determine how much light a material emits, which is important for creating realistic lighting in a 3D scene. \n\nThe code starts by defining two uniforms: `material_emissive` and `material_emissiveIntensity`. These uniforms are used to specify the color and intensity of the material's emission. The `#ifdef` statements are used to conditionally compile the code based on whether certain features are enabled or not. \n\nThe `getEmission()` function is the main function that calculates the emission of the material. It starts by setting the initial emission value to `vec3(1.0)`, which means that the material emits white light. \n\nThe `#ifdef MAPFLOAT` block multiplies the emission value by the `material_emissiveIntensity` uniform, which is a float value that specifies the intensity of the emission. \n\nThe `#ifdef MAPCOLOR` block multiplies the emission value by the `material_emissive` uniform, which is a vec3 value that specifies the color of the emission. \n\nThe `#ifdef MAPTEXTURE` block multiplies the emission value by a texture value that is sampled using the `texture2DBias()` function. This function takes three arguments: the sampler, the UV coordinates, and the texture bias. The texture bias is used to adjust the texture sampling to avoid texture seams. The `$DECODE` macro is used to decode the texture value into a vec4 value, which is then accessed using the `$CH` macro to get the desired channel (red, green, or blue). \n\nThe `#ifdef MAPVERTEX` block multiplies the emission value by the vertex color of the mesh. This is done using the `vVertexColor` variable, which is a varying variable that is interpolated across the mesh. The `saturate()` function is used to clamp the vertex color to the range [0, 1], and the `gammaCorrectInput()` function is used to apply gamma correction to the color. \n\nOverall, this code is an important part of the PlayCanvas engine's rendering pipeline, as it determines how much light a material emits. It can be used to create a wide range of effects, from glowing objects to emissive surfaces. Here is an example of how this code might be used in a larger project:\n\n```glsl\n// vertex shader\nattribute vec3 aPosition;\nattribute vec2 aTexCoord;\nattribute vec3 aVertexColor;\n\nuniform mat4 uModelMatrix;\nuniform mat4 uViewMatrix;\nuniform mat4 uProjectionMatrix;\n\nvarying vec2 vTexCoord;\nvarying vec3 vVertexColor;\n\nvoid main() {\n    gl_Position = uProjectionMatrix * uViewMatrix * uModelMatrix * vec4(aPosition, 1.0);\n    vTexCoord = aTexCoord;\n    vVertexColor = aVertexColor;\n}\n\n// fragment shader\nuniform sampler2D uTexture;\n\n#ifdef MAPCOLOR\nuniform vec3 material_emissive;\n#endif\n\n#ifdef MAPFLOAT\nuniform float material_emissiveIntensity;\n#endif\n\nvarying vec2 vTexCoord;\nvarying vec3 vVertexColor;\n\nvoid getEmission() {\n    // code from previous example\n}\n\nvoid main() {\n    vec4 texColor = texture2D(uTexture, vTexCoord);\n    vec3 diffuse = texColor.rgb;\n    vec3 emission = vec3(0.0);\n\n    getEmission();\n\n    gl_FragColor = vec4(diffuse + emission, texColor.a);\n}\n```\n\nIn this example, the vertex shader passes the vertex color to the fragment shader using a varying variable. The fragment shader samples a texture and calculates the diffuse color using the texture color. The `getEmission()` function is called to calculate the emission color, which is added to the diffuse color to get the final color of the pixel.\n## Questions: \n 1. What is the purpose of this code?\n   This code defines a function called `getEmission()` that calculates the emissive color of a material based on various inputs such as color, texture, and vertex color.\n\n2. What are the `MAPFLOAT`, `MAPCOLOR`, `MAPTEXTURE`, and `MAPVERTEX` preprocessor directives used for?\n   These directives are used to conditionally compile different parts of the code based on whether certain material properties are present. For example, `MAPFLOAT` is used to compile code that handles a float value for emissive intensity.\n\n3. What is the meaning of the `/* glsl */` comment at the beginning of the code?\n   This comment indicates that the code is written in GLSL (OpenGL Shading Language), which is a high-level language used to write shaders for graphics processing units (GPUs). The comment is used by tools that parse GLSL code to identify the language and apply appropriate syntax highlighting and other features.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/emissive.md"}}],["703",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/gloss.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) used in the PlayCanvas engine project. The purpose of this code is to calculate the glossiness of a material. \n\nThe code starts by defining a uniform variable called `material_gloss` which is used to store the glossiness value of the material. The `getGlossiness()` function is then defined which calculates the final glossiness value of the material. \n\nThe function starts by initializing the `dGlossiness` variable to 1.0. Then, it checks for the presence of various preprocessor directives such as `MAPFLOAT`, `MAPTEXTURE`, `MAPVERTEX`, and `MAPINVERT`. \n\nIf `MAPFLOAT` is defined, the `material_gloss` value is multiplied with `dGlossiness`. This means that if the material has a glossiness map, the value from the map will be multiplied with the `material_gloss` value to get the final glossiness value. \n\nIf `MAPTEXTURE` is defined, the function multiplies `dGlossiness` with the value of the glossiness map at the specified texture coordinates. The texture coordinates are obtained from the `$UV` variable and the texture bias is obtained from the `textureBias` variable. The channel to sample from the texture is specified by the `$CH` variable. \n\nIf `MAPVERTEX` is defined, the function multiplies `dGlossiness` with the vertex color of the material. The vertex color is obtained from the `vVertexColor` variable and the channel to sample from is specified by the `$VC` variable. \n\nIf `MAPINVERT` is defined, the function inverts the `dGlossiness` value by subtracting it from 1.0. \n\nFinally, the function adds a small value of `0.0000001` to the `dGlossiness` value to avoid division by zero errors. \n\nThis code is used in the PlayCanvas engine to calculate the glossiness of a material. It is part of the shader code that is executed on the GPU to render 3D objects in the scene. The final glossiness value calculated by this code is used to determine how shiny or reflective the material appears in the scene. \n\nExample usage of this code in a PlayCanvas project:\n\n```javascript\n// Create a new material\nvar material = new pc.StandardMaterial();\n\n// Set the glossiness value of the material\nmaterial.glossiness = 0.8;\n\n// Set the glossiness map of the material\nmaterial.glossinessMap = glossinessTexture;\n\n// Set the vertex color of the material\nmaterial.vertexColor = new pc.Color(1, 1, 1, 1);\n\n// Set the shader code for the material\nmaterial.chunks.glossinessPS = /* glsl */`\n    #ifdef MAPFLOAT\n    uniform float material_gloss;\n    #endif\n\n    void getGlossiness() {\n        dGlossiness = 1.0;\n\n        #ifdef MAPFLOAT\n        dGlossiness *= material_gloss;\n        #endif\n\n        #ifdef MAPTEXTURE\n        dGlossiness *= texture2DBias($SAMPLER, $UV, textureBias).$CH;\n        #endif\n\n        #ifdef MAPVERTEX\n        dGlossiness *= saturate(vVertexColor.$VC);\n        #endif\n\n        #ifdef MAPINVERT\n        dGlossiness = 1.0 - dGlossiness;\n        #endif\n\n        dGlossiness += 0.0000001;\n    }\n`;\n\n// Assign the material to a mesh instance\nmeshInstance.material = material;\n```\n## Questions: \n 1. What is the purpose of this code?\n   This code defines a function called `getGlossiness` that calculates the glossiness of a material based on various inputs.\n\n2. What is the significance of the `#ifdef` statements?\n   The `#ifdef` statements are preprocessor directives that check if certain macros are defined. If they are defined, the corresponding code block is included in the final compiled code.\n\n3. What are the inputs to the `getGlossiness` function?\n   The inputs to the `getGlossiness` function are not explicitly defined in this code snippet, but it appears to rely on various material properties such as `material_gloss`, a texture sampler `$SAMPLER`, a UV coordinate `$UV`, a texture bias `textureBias`, and a vertex color `$VC`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/gloss.md"}}],["704",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/iridescence.js)\n\nThe code above is a GLSL shader code that defines a function called `getIridescence()`. This function calculates the iridescence value of a material. Iridescence is a phenomenon where the color of an object appears to change when viewed from different angles. This effect is caused by the interference of light waves.\n\nThe function starts by initializing the iridescence value to 1.0. Then, it checks if the `MAPFLOAT` macro is defined. If it is defined, it multiplies the iridescence value by a uniform variable called `material_iridescence`. This variable is expected to be a float value between 0 and 1 that controls the strength of the iridescence effect.\n\nNext, the function checks if the `MAPTEXTURE` macro is defined. If it is defined, it multiplies the iridescence value by a texture sample. The texture sample is obtained by calling the `texture2DBias()` function with three arguments: a sampler object, a UV coordinate, and a bias value. The sampler object is represented by the `$SAMPLER` variable, which is expected to be set by the calling code. The UV coordinate is represented by the `$UV` variable, which is also expected to be set by the calling code. The bias value is represented by the `textureBias` variable, which is expected to be a vec2 value that controls the sampling position of the texture. The `$CH` variable represents the channel of the texture sample that is used to calculate the iridescence value.\n\nFinally, the function sets the `dIridescence` variable to the calculated iridescence value. The `dIridescence` variable is expected to be a varying variable that is used to pass the iridescence value to other parts of the shader code.\n\nThis code is part of the PlayCanvas engine project and can be used to implement iridescence effects in 3D graphics applications. The code can be included in a shader program and called from other parts of the program to calculate the iridescence value of a material. The `material_iridescence` uniform variable can be set by the calling code to control the strength of the iridescence effect. The texture sample can be used to add more variation to the iridescence effect.\n## Questions: \n 1. What is the purpose of the `getIridescence()` function?\n   - The `getIridescence()` function is used to calculate the iridescence value for a material.\n2. What is the significance of the `MAPFLOAT` and `MAPTEXTURE` preprocessor directives?\n   - The `MAPFLOAT` and `MAPTEXTURE` preprocessor directives are used to conditionally compile code based on whether the material has a float or texture map for iridescence.\n3. What is the purpose of the `dIridescence` variable?\n   - The `dIridescence` variable is used to store the calculated iridescence value for use in other parts of the code.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/iridescence.md"}}],["705",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/iridescenceThickness.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) that is used in the PlayCanvas engine project. The purpose of this code is to calculate the iridescence thickness of a material. Iridescence is a phenomenon where the color of a material changes when viewed from different angles. The thickness of the material determines the amount of iridescence that is visible.\n\nThe code starts by defining a uniform variable called `material_iridescenceThicknessMax`. Uniform variables are used to pass data from the CPU to the GPU. In this case, the variable is a float that represents the maximum iridescence thickness of the material.\n\nNext, there is an `#ifdef` preprocessor directive that checks if the `MAPTEXTURE` macro is defined. If it is defined, then another uniform variable called `material_iridescenceThicknessMin` is defined. This variable represents the minimum iridescence thickness of the material. The `#ifdef` directive is used to conditionally compile the code based on whether or not the `MAPTEXTURE` macro is defined.\n\nThe `getIridescenceThickness()` function is then defined. This function calculates the iridescence thickness of the material based on the values of the `material_iridescenceThicknessMax` and `material_iridescenceThicknessMin` variables. If the `MAPTEXTURE` macro is defined, then the function uses the `texture2DBias()` function to sample a texture and get the value of a specific channel (`$CH`) at a specific UV coordinate (`$UV`). The `mix()` function is then used to interpolate between the minimum and maximum iridescence thickness based on the sampled value. If the `MAPTEXTURE` macro is not defined, then the function simply uses the maximum iridescence thickness.\n\nFinally, the function sets the value of a variable called `dIridescenceThickness` to the calculated iridescence thickness. It is not clear from this code where `dIridescenceThickness` is defined or how it is used in the larger project.\n\nOverall, this code is an important part of the PlayCanvas engine's shader system. It allows materials to have iridescence and provides a way to control the amount of iridescence based on the thickness of the material. This code can be used in various parts of the engine, such as in the rendering pipeline or in the material editor.\n## Questions: \n 1. What is the purpose of the `material_iridescenceThicknessMax` uniform variable?\n- `material_iridescenceThicknessMax` is a uniform variable used to set the maximum value for the iridescence thickness.\n\n2. What is the purpose of the `getIridescenceThickness()` function?\n- The `getIridescenceThickness()` function is used to calculate the iridescence thickness based on the `material_iridescenceThicknessMax` and `material_iridescenceThicknessMin` uniform variables and a texture blend value.\n\n3. What is the purpose of the `MAPTEXTURE` preprocessor directive?\n- The `MAPTEXTURE` preprocessor directive is used to conditionally compile code based on whether a texture is being used or not. If a texture is being used, the code inside the `#ifdef MAPTEXTURE` block will be compiled, otherwise the code inside the `#else` block will be compiled.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/iridescenceThickness.md"}}],["706",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/lightmapDir.js)\n\nThis code is a shader code written in GLSL (OpenGL Shading Language) and is used in the PlayCanvas engine project. The purpose of this code is to get the light map and direction light map from the textures and use them to calculate the diffuse light for a 3D object.\n\nThe code starts with two uniform variables, `texture_lightMap` and `texture_dirLightMap`, which are 2D textures used to store the light maps. The `getLightMap()` function is then defined, which is responsible for decoding the light maps and calculating the diffuse light.\n\nThe first line of the function decodes the light map by using the `texture2DBias()` function to sample the `texture_lightMap` texture at the current UV coordinates (`$UV`) with a bias value (`textureBias`). The result is then passed through the `$CH` function to extract the color channels. The decoded light map is stored in the `dLightmap` variable.\n\nThe second line of the function calculates the direction light map by sampling the `texture_dirLightMap` texture at the current UV coordinates with a bias value. The result is then multiplied by 2 and subtracted by 1 to map the values from [0,1] to [-1,1]. The dot product of the direction vector and itself is then calculated, and if it is greater than 0.001, the direction vector is normalized and stored in the `dLightmapDir` variable. Otherwise, a zero vector is stored.\n\nThis code can be used in the larger PlayCanvas engine project to calculate the diffuse light for a 3D object. The `getLightMap()` function can be called in the vertex shader to calculate the light map and direction light map for each vertex, which can then be used in the fragment shader to calculate the final diffuse light. An example of how this code can be used in a shader is shown below:\n\n```glsl\nuniform mat4 modelMatrix;\nuniform mat4 viewMatrix;\nuniform mat4 projectionMatrix;\n\nattribute vec3 vertexPosition;\nattribute vec2 vertexUV;\n\nvarying vec2 vUV;\nvarying vec3 vPosition;\nvarying vec3 vNormal;\n\nvoid main() {\n    vUV = vertexUV;\n    vPosition = (modelMatrix * vec4(vertexPosition, 1.0)).xyz;\n    vNormal = normalize(cross(dFdx(vPosition), dFdy(vPosition)));\n\n    getLightMap();\n\n    gl_Position = projectionMatrix * viewMatrix * modelMatrix * vec4(vertexPosition, 1.0);\n}\n```\n\nIn this example, the `getLightMap()` function is called in the vertex shader to calculate the light maps for each vertex. The `vUV`, `vPosition`, and `vNormal` variables are then passed to the fragment shader to calculate the final diffuse light.\n## Questions: \n 1. What is the purpose of the `getLightMap()` function?\n    - The `getLightMap()` function is used to retrieve and process light map data from two different textures.\n\n2. What do the `texture_lightMap` and `texture_dirLightMap` uniforms represent?\n    - `texture_lightMap` and `texture_dirLightMap` are sampler2D uniforms that represent two different textures used for light mapping.\n\n3. What is the purpose of the `texture2DBias()` function and how is it used in this code?\n    - The `texture2DBias()` function is used to sample a texture with a bias value, which can be used to adjust the texture sampling position. In this code, it is used to sample the `texture_lightMap` and `texture_dirLightMap` textures with a bias value of `textureBias`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/lightmapDir.md"}}],["707",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/lightmapSingle.js)\n\nThe code above is a shader function written in GLSL that is used to calculate the light map for a 3D object in the PlayCanvas engine. The purpose of this function is to determine how much light should be applied to each pixel of the object's surface based on the lighting conditions in the scene.\n\nThe function starts by initializing a variable called \"dLightmap\" to a vector with a value of 1.0. This vector will be used to store the final light map value for each pixel.\n\nThe function then checks for the presence of two preprocessor directives: MAPTEXTURE and MAPVERTEX. These directives are used to determine which method should be used to calculate the light map.\n\nIf the MAPTEXTURE directive is present, the function multiplies the dLightmap vector by the result of a texture lookup. The texture is accessed using the texture2DBias function, which takes three parameters: a sampler, a UV coordinate, and a bias value. The sampler and UV coordinate are passed in as variables ($SAMPLER and $UV, respectively), while the bias value is passed in as a constant called textureBias. The result of the texture lookup is then accessed using the $CH variable, which represents the color channel of the texture.\n\nIf the MAPVERTEX directive is present, the function multiplies the dLightmap vector by the result of a vertex color lookup. The vertex color is accessed using the vVertexColor variable, which is a varying variable that is passed from the vertex shader to the fragment shader. The result of the vertex color lookup is then passed through the saturate function, which clamps the value between 0 and 1.\n\nOverall, this function is an important part of the PlayCanvas engine's rendering pipeline, as it helps to determine how light is applied to 3D objects in a scene. It can be used in conjunction with other shader functions to create complex lighting effects, such as shadows and reflections. Here is an example of how this function might be used in a shader:\n\n```\nuniform sampler2D diffuseMap;\nvarying vec2 vUv;\nvarying vec3 vVertexColor;\n\nvoid main() {\n    vec3 lightMap;\n    getLightMap(diffuseMap, vUv, vVertexColor, lightMap);\n    gl_FragColor = vec4(lightMap, 1.0);\n}\n```\n\nIn this example, the getLightMap function is called with the diffuseMap, vUv, and vVertexColor variables, and the resulting light map is stored in a variable called lightMap. This light map is then used to set the color of the fragment in the output buffer.\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n   - This code defines a function called `getLightMap()` that is likely used to calculate lighting information for a 3D scene in the PlayCanvas engine.\n2. What do the `#ifdef` statements do and how do they affect the behavior of the function?\n   - The `#ifdef` statements are preprocessor directives that conditionally include or exclude certain code based on whether certain macros are defined. In this case, the `MAPTEXTURE` and `MAPVERTEX` macros control whether the function applies a texture or vertex color to the lightmap calculation.\n3. What do the variables `$DECODE`, `$SAMPLER`, `$UV`, `textureBias`, `$CH`, `saturate`, `vVertexColor`, and `$VC` represent and where are they defined?\n   - Without more context, it is difficult to determine the exact meaning and source of these variables. They may be defined elsewhere in the PlayCanvas engine or in other files within the same project. A smart developer would likely consult the PlayCanvas documentation or seek clarification from other members of the development team to understand their purpose.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/lightmapSingle.md"}}],["708",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/litShaderArgs.js)\n\nThe code above defines several structs that are used as arguments for a lit shader in the PlayCanvas engine. The lit shader is responsible for rendering the appearance of a 3D object in a scene, taking into account various lighting and material properties. The structs defined in this code provide additional arguments that can be used to extend the functionality of the lit shader.\n\nThe `IridescenceArgs` struct contains two properties: `intensity` and `thickness`. These properties are used to control the intensity and thickness of an iridescent microfilm layer that can be applied to a material. The `intensity` property is a float value between 0 and 1 that determines the strength of the iridescence effect. The `thickness` property is a float value in nanometers between 0 and 1000 that determines the thickness of the microfilm layer.\n\nThe `ClearcoatArgs` struct contains three properties: `specularity`, `gloss`, and `worldNormal`. These properties are used to control the appearance of a clearcoat layer that can be applied to a material. The `specularity` property is a float value between 0 and 1 that determines the intensity of the clearcoat layer. The `gloss` property is a float value between 0 and 1 that determines the glossiness of the clearcoat layer. The `worldNormal` property is a vec3 value that represents the normal direction of the clearcoat layer.\n\nThe `SheenArgs` struct contains two properties: `gloss` and `specularity`. These properties are used to control the appearance of a sheen layer that can be applied to a material. The `gloss` property is a float value between 0 and 1 that determines the glossiness of the sheen layer. The `specularity` property is a vec3 value that represents the color of the f0 specularity factor for the sheen layer.\n\nThe `LitShaderArguments` struct contains a large number of properties that are used to control the appearance of a material. These properties include `opacity`, `worldNormal`, `albedo`, `transmission`, `thickness`, `specularity`, `gloss`, `metalness`, `specularityFactor`, `ao`, `emission`, `lightmap`, `lightmapDir`, `iridescence`, `clearcoat`, and `sheen`. These properties are used to control various aspects of the material's appearance, such as its transparency, surface color, reflectivity, and more.\n\nOverall, this code provides a way to extend the functionality of the lit shader in the PlayCanvas engine by allowing developers to control additional material properties such as iridescence, clearcoat, and sheen. These properties can be used to create more realistic and visually interesting materials in a 3D scene. An example of how these properties might be used in a material definition is shown below:\n\n```\nmaterial = new pc.StandardMaterial();\nmaterial.opacity = 0.5;\nmaterial.albedo = new pc.Color(1, 0, 0);\nmaterial.specularity = new pc.Color(1, 1, 1);\nmaterial.gloss = 0.8;\nmaterial.metalness = 0.2;\nmaterial.iridescence.intensity = 0.5;\nmaterial.iridescence.thickness = 500;\nmaterial.clearcoat.specularity = 0.5;\nmaterial.clearcoat.gloss = 0.2;\nmaterial.clearcoat.worldNormal = new pc.Vec3(0, 1, 0);\nmaterial.sheen.gloss = 0.5;\nmaterial.sheen.specularity = new pc.Color(0.5, 0.5, 0.5);\n```\n## Questions: \n 1. What is the purpose of this code?\n- This code defines several structs and their properties that are used as arguments for a lit shader.\n\n2. What is the range of values for the \"thickness\" property in the \"IridescenceArgs\" struct?\n- The range of values for the \"thickness\" property is [0..1000].\n\n3. What is the purpose of the \"ClearcoatArgs\" struct and its properties?\n- The \"ClearcoatArgs\" struct and its properties define the intensity and glossiness of a clearcoat layer, as well as the normal used for the layer.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/litShaderArgs.md"}}],["709",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/metalness.js)\n\nThe code above is a GLSL shader code that is used to calculate the metalness of a material. The purpose of this code is to provide a way to calculate the metalness of a material based on different inputs such as a float value, a texture, or a vertex color. This code is a part of the PlayCanvas engine project and can be used in any 3D application that uses the PlayCanvas engine.\n\nThe code starts by defining a uniform variable called \"material_metalness\" which is used to store the metalness value of the material. This value is then multiplied by the metalness value calculated from the texture, vertex color, or float value. The final metalness value is then stored in the variable \"dMetalness\".\n\nThe code uses preprocessor directives to check if certain inputs are available. If the \"MAPFLOAT\" directive is defined, the code multiplies the metalness value by the \"material_metalness\" uniform variable. If the \"MAPTEXTURE\" directive is defined, the code multiplies the metalness value by the metalness value calculated from the texture. The texture is accessed using the \"texture2DBias\" function which takes in a sampler, UV coordinates, and a bias value. The bias value is used to adjust the texture sampling to avoid aliasing artifacts. The \"$SAMPLER\" and \"$UV\" variables are placeholders that are replaced with actual values at runtime. The \"$CH\" variable is used to access a specific channel of the texture. If the \"MAPVERTEX\" directive is defined, the code multiplies the metalness value by the vertex color value. The vertex color value is accessed using the \"vVertexColor\" variable which is a varying variable that is interpolated across the vertices of the mesh.\n\nHere is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// Create a new material\nvar material = new pc.StandardMaterial();\n\n// Set the metalness value of the material\nmaterial.metalness = 0.5;\n\n// Set the metalness texture of the material\nmaterial.metalnessMap = new pc.Texture(\"metalness.png\");\n\n// Set the vertex color of the material\nmaterial.vertexColor = true;\n\n// Set the shader of the material to the metalness shader\nmaterial.shader = pc.shaderChunks.metalness;\n\n// Assign the material to a mesh instance\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n``` \n\nIn this example, a new material is created and its metalness value is set to 0.5. A metalness texture is also set for the material and the vertex color is enabled. The shader of the material is set to the metalness shader which uses the code above to calculate the metalness value. Finally, the material is assigned to a mesh instance which is then added to the scene.\n## Questions: \n 1. What is the purpose of this code?\n   This code defines a function called `getMetalness()` that calculates the metalness value of a material based on various inputs.\n\n2. What is the significance of the `#ifdef` statements?\n   The `#ifdef` statements are preprocessor directives that check if certain macros are defined. If they are defined, the corresponding code block is included in the final compiled code.\n\n3. What are the possible inputs that can affect the metalness value?\n   The metalness value can be affected by a uniform float value called `material_metalness`, a texture map called `MAPTEXTURE`, and a vertex color value called `MAPVERTEX`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/metalness.md"}}],["710",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/normalDetailMap.js)\n\nThe code above is a GLSL shader code that is used in the PlayCanvas engine project. The purpose of this code is to add normal detail to a 3D model's surface. \n\nThe `addNormalDetail` function takes in a `normalMap` parameter, which is a texture that contains the surface normal information of the 3D model. If the `MAPTEXTURE` flag is defined, the function will also use a `normalDetailMap` texture to add additional detail to the surface normal. The `material_normalDetailMapBumpiness` uniform controls the strength of the effect. \n\nThe `blendNormals` function is used to blend the `normalMap` and `normalDetailMap` together. It uses the technique described in the paper \"Blending in Detail\" by John Hable to achieve a more detailed and natural-looking surface normal. \n\nThis code can be used in the larger PlayCanvas engine project to enhance the visual quality of 3D models. By adding normal detail to the surface, the models will appear more realistic and detailed. This can be especially useful in games or other interactive applications where visual fidelity is important. \n\nHere is an example of how this code might be used in a PlayCanvas project:\n\n```javascript\n// create a material for the 3D model\nvar material = new pc.StandardMaterial();\n\n// set the normal map texture\nmaterial.normalMap = normalMapTexture;\n\n// enable normal detail mapping\nmaterial.chunks.normalMapPS = \"#define MAPTEXTURE\\n\" + material.chunks.normalMapPS;\n\n// set the normal detail map texture\nmaterial.setParameter('texture_bias', 0.1);\nmaterial.setParameter('material_normalDetailMapBumpiness', 0.5);\nmaterial.setParameter('texture_mapTexture', normalDetailMapTexture);\n\n// assign the material to the 3D model\nmodel.meshInstances[0].material = material;\n```\n\nIn this example, we create a `StandardMaterial` for a 3D model and set its `normalMap` texture. We then enable normal detail mapping by adding the `MAPTEXTURE` flag to the material's `normalMapPS` chunk. We set the `texture_bias` parameter to control the bias of the `normalDetailMap` texture, and the `material_normalDetailMapBumpiness` parameter to control the strength of the effect. Finally, we assign the material to the 3D model's mesh instance.\n## Questions: \n 1. What is the purpose of the `blendNormals` function?\n- The `blendNormals` function is used to blend two normal vectors together, as described in the referenced blog post.\n\n2. What is the `addNormalDetail` function doing?\n- The `addNormalDetail` function is adding detail to a normal map, using a detail map and a bumpiness value. If a detail map is not provided, it simply returns the original normal map.\n\n3. What is the `MAPTEXTURE` preprocessor directive used for?\n- The `MAPTEXTURE` preprocessor directive is used to conditionally compile parts of the code based on whether a texture map is provided or not. In this case, it is used to determine whether to add detail to the normal map or not.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/normalDetailMap.md"}}],["711",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/normalMap.js)\n\nThe code provided is a GLSL shader code that is used to generate normal maps for 3D models. Normal maps are used to add detail to a 3D model without increasing the polygon count. The purpose of this code is to calculate the normal vector of a point on a 3D model's surface. The normal vector is a vector that is perpendicular to the surface at that point. This information is used to calculate how light interacts with the surface, which is important for creating realistic lighting effects.\n\nThe code starts by checking if the MAPTEXTURE flag is defined. If it is defined, it means that a normal map texture is being used. The code then declares a uniform variable called material_bumpiness, which is used to control the amount of bumpiness in the surface. If MAPTEXTURE is not defined, the code uses the vertex normal vector instead.\n\nThe getNormal() function is then defined. This function calculates the normal vector for a point on the surface. If MAPTEXTURE is defined, the code unpacks the normal map texture using the texture2DBias() function. This function takes three arguments: a sampler, a UV coordinate, and a texture bias. The sampler is a reference to the texture that is being used as the normal map. The UV coordinate is the position on the surface where the normal vector is being calculated. The texture bias is used to adjust the texture sampling to avoid texture seams. The unpackNormal() function is then used to convert the texture data into a normal vector.\n\nThe normal vector is then adjusted based on the material_bumpiness value. This is done using the mix() function, which takes three arguments: a starting value, an ending value, and a blend factor. The blend factor is a value between 0 and 1 that determines how much of the ending value is used. In this case, the starting value is (0, 0, 1), which represents a flat surface. The ending value is the normal vector from the texture. The blend factor is material_bumpiness, which controls how much of the normal vector is used. The resulting vector is then multiplied by the dTBN matrix, which transforms the vector from tangent space to world space.\n\nIf MAPTEXTURE is not defined, the code simply uses the vertex normal vector, which is already in world space. The resulting normal vector is then normalized using the normalize() function and stored in the dNormalW variable.\n\nOverall, this code is an important part of the PlayCanvas engine's rendering pipeline. It is used to generate normal maps for 3D models, which are essential for creating realistic lighting effects. The material_bumpiness value can be adjusted to control the amount of detail in the surface. This code is likely used in conjunction with other shaders and rendering techniques to create a complete 3D scene.\n## Questions: \n 1. What is the purpose of the `MAPTEXTURE` preprocessor directive?\n- The `MAPTEXTURE` preprocessor directive is used to conditionally compile code based on whether a texture map is present or not.\n\n2. What is the `material_bumpiness` uniform used for?\n- The `material_bumpiness` uniform is used to control the amount of bumpiness applied to the normal map.\n\n3. What is the `addNormalDetail` function used for?\n- The `addNormalDetail` function is used to add additional detail to the normal map.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/normalMap.md"}}],["712",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/normalXY.js)\n\nThe code above is a GLSL function called `unpackNormal` that takes in a `vec4` parameter called `nmap` and returns a `vec3` value. The purpose of this function is to unpack a normal map stored in a `vec4` format and convert it into a `vec3` format that can be used for lighting calculations.\n\nThe `vec4` parameter `nmap` is assumed to contain the normal map data in the following format: `nmap.xyz` contains the tangent space normal vector, and `nmap.w` contains the sign of the bitangent direction. The function first extracts the `xy` components of the normal vector and scales them from the range `[0, 1]` to the range `[-1, 1]` using the formula `nmap.wy * 2.0 - 1.0`. The `z` component of the normal vector is then calculated using the Pythagorean theorem: `sqrt(1.0 - saturate(dot(normal.xy, normal.xy)))`. The `saturate` function is used to ensure that the dot product of `normal.xy` with itself is clamped to the range `[0, 1]` before taking the square root.\n\nThe resulting `vec3` value represents the normal vector in world space coordinates, and can be used for lighting calculations such as diffuse and specular shading. This function is likely used in the larger PlayCanvas engine project to implement realistic lighting effects in 3D scenes. Here is an example usage of the `unpackNormal` function:\n\n```glsl\nuniform sampler2D normalMap;\nvarying vec2 uv;\n\nvoid main() {\n    vec4 nmap = texture2D(normalMap, uv);\n    vec3 normal = unpackNormal(nmap);\n    // Use normal vector for lighting calculations\n    // ...\n}\n```\n## Questions: \n 1. What does this code do?\n   This code defines a function called `unpackNormal` that takes in a vec4 parameter `nmap` and returns a vec3 normal vector.\n\n2. What is the purpose of the `sqrt` and `saturate` functions in this code?\n   The `sqrt` function is used to calculate the z component of the normal vector, while the `saturate` function is used to clamp the dot product of the x and y components of the normal vector to a range of 0 to 1.\n\n3. What is the expected input format for the `nmap` parameter?\n   The `nmap` parameter is expected to be a vec4 vector, where the x, y, and z components represent the normal vector and the w component represents the sign of the z component.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/normalXY.md"}}],["713",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/normalXYZ.js)\n\nThe code above is a function called `unpackNormal` that takes in a `vec4` parameter called `nmap`. The purpose of this function is to unpack a normal map stored in a `vec4` format and return it as a `vec3` format. \n\nThe normal map is a texture that stores the surface normals of a 3D model. It is used to create the illusion of surface details and improve the lighting of the model. The normal map is usually stored in a `vec4` format, where the first three components represent the x, y, and z coordinates of the normal vector, and the fourth component is unused. \n\nThe `unpackNormal` function first extracts the x, y, and z components of the normal vector by accessing the `xyz` property of the `nmap` parameter. It then scales the components by 2.0 and subtracts 1.0 from each component. This is done to convert the range of the components from [0, 1] to [-1, 1]. The resulting `vec3` is then returned.\n\nThis function can be used in the larger PlayCanvas engine project to unpack normal maps for use in lighting calculations. For example, if a 3D model has a normal map stored in a `vec4` format, the `unpackNormal` function can be called to extract the normal vector and use it in lighting calculations. \n\nHere is an example usage of the `unpackNormal` function:\n\n```javascript\n// assume nmap is a vec4 normal map texture\nvar normal = unpackNormal(nmap);\n// use normal vector in lighting calculations\n```\n\nOverall, the `unpackNormal` function is a useful utility function for working with normal maps in the PlayCanvas engine project.\n## Questions: \n 1. What does this code do?\n   This code exports a function called `unpackNormal` that takes in a vec4 and returns a vec3.\n\n2. What is the purpose of the `/* glsl */` comment?\n   This comment indicates that the code is written in GLSL (OpenGL Shading Language) syntax.\n\n3. How is the `unpackNormal` function used in the PlayCanvas engine?\n   It is unclear from this code snippet alone how the `unpackNormal` function is used in the PlayCanvas engine. Further context is needed.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/normalXYZ.md"}}],["714",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/opacity.js)\n\nThe code above is a GLSL shader code that defines a function called `getOpacity()`. This function calculates the opacity of a material based on several factors. The purpose of this code is to provide a way to calculate the opacity of a material in a 3D scene.\n\nThe function starts by setting the `dAlpha` variable to 1.0. This variable represents the opacity of the material and will be multiplied by different factors to calculate the final opacity value.\n\nThe first factor that affects the opacity is the `material_opacity` uniform. If the `MAPFLOAT` preprocessor macro is defined, the `dAlpha` variable is multiplied by the value of `material_opacity`. This allows the material to have a custom opacity value that can be set by the user.\n\nThe second factor that affects the opacity is a texture. If the `MAPTEXTURE` preprocessor macro is defined, the `dAlpha` variable is multiplied by the value of a texture sample. The texture is sampled using the `$SAMPLER` uniform and the `$UV` variable, which represents the texture coordinates. The `textureBias` variable is used to adjust the texture sampling, and `$CH` is used to select a specific channel of the texture. This allows the material to have a transparent or translucent texture that affects its opacity.\n\nThe third factor that affects the opacity is a vertex color. If the `MAPVERTEX` preprocessor macro is defined, the `dAlpha` variable is multiplied by the value of the `vVertexColor.$VC` variable. This variable represents the vertex color of the material and is clamped between 0 and 1. This allows the material to have a vertex color that affects its opacity.\n\nOverall, this code provides a way to calculate the opacity of a material based on different factors. It can be used in the larger PlayCanvas engine project to create materials with custom opacity values, transparent or translucent textures, and vertex colors that affect their opacity. Here is an example of how this code can be used in a PlayCanvas material:\n\n```\nvar material = new pc.StandardMaterial();\nmaterial.chunks.opacityPS = /* glsl */`\n    void getOpacity() {\n        dAlpha = 0.5;\n        dAlpha *= material_opacity;\n        dAlpha *= texture2DBias($diffuseMap, $uv, textureBias).r;\n        dAlpha *= clamp(vVertexColor.rgb, 0.0, 1.0);\n    }\n`;\nmaterial.opacity = 0.5;\nmaterial.diffuseMap = diffuseTexture;\nmaterial.update();\n```\n## Questions: \n 1. What is the purpose of the `getOpacity()` function?\n   - The `getOpacity()` function is used to calculate the opacity of a material.\n2. What is the `MAPFLOAT` variable used for?\n   - The `MAPFLOAT` variable is used to check if the material has a uniform float value for opacity.\n3. What is the purpose of the `clamp()` function in the `MAPVERTEX` block?\n   - The `clamp()` function is used to ensure that the vertex color value is within the range of 0 to 1, which is necessary for calculating the opacity.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/opacity.md"}}],["715",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/parallax.js)\n\nThe code above is a GLSL shader code that defines a function called `getParallax()`. This function is used to calculate the parallax effect for a material in the PlayCanvas engine. The parallax effect is a technique used to create the illusion of depth on a 2D surface by simulating the displacement of pixels based on their perceived depth.\n\nThe function takes a uniform variable called `material_heightMapFactor` which is used to scale the height map values. The height map is a texture that stores the height information of the material. The `parallaxScale` variable is calculated by multiplying the `material_heightMapFactor` with the height value obtained from the height map texture.\n\nThe height value is obtained by sampling the height map texture using the `texture2DBias()` function. The `textureBias` parameter is used to adjust the sampling position to avoid texture aliasing. The `$SAMPLER` and `$UV` parameters are placeholders for the actual texture sampler and UV coordinates respectively. The `$CH` parameter is a placeholder for the channel of the texture to sample.\n\nThe sampled height value is then multiplied by the `parallaxScale` and subtracted by `parallaxScale*0.5` to center the displacement. The `viewDirT` variable is calculated by transforming the view direction vector `dViewDirW` into tangent space using the `dTBN` matrix. The `z` component of `viewDirT` is then increased by `0.42` to adjust the parallax effect.\n\nFinally, the `dUvOffset` variable is calculated by multiplying the height value with the ratio of `viewDirT.xy` to `viewDirT.z`. This offset is then used to displace the texture coordinates in the vertex shader to create the parallax effect.\n\nIn the larger project, this code is used as a part of the material system in the PlayCanvas engine to create realistic 3D graphics. The `getParallax()` function is called by the material shader to calculate the parallax effect for the material. The resulting offset is then used to displace the texture coordinates in the vertex shader to create the illusion of depth. This technique is commonly used in games and other real-time 3D applications to create more realistic and immersive environments.\n## Questions: \n 1. What is the purpose of the `material_heightMapFactor` uniform variable?\n- The `material_heightMapFactor` uniform variable is used to set the scale of the parallax effect.\n\n2. What is the input for the `getParallax()` function?\n- The `getParallax()` function takes no input parameters.\n\n3. What is the output of the `getParallax()` function?\n- The `getParallax()` function sets the value of the `dUvOffset` variable, which is used to offset the texture coordinates in order to create the parallax effect.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/parallax.md"}}],["716",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/sheen.js)\n\nThe code above is a GLSL shader code that defines a function called `getSheen()`. This function is responsible for calculating the sheen color of a material. The sheen color is a subtle highlight that appears on the surface of a material when it is viewed from certain angles. \n\nThe function starts by initializing a `vec3` variable called `sheenColor` with the value `(1, 1, 1)`. This means that the default sheen color is white. \n\nThe function then checks if the `MAPCOLOR` flag is defined. If it is, the function multiplies the `sheenColor` variable by the `material_sheen` uniform. This means that if the material has a sheen color defined, it will be multiplied with the default sheen color. \n\nNext, the function checks if the `MAPTEXTURE` flag is defined. If it is, the function multiplies the `sheenColor` variable by the result of a texture lookup. The texture lookup is performed using the `texture2DBias()` function, which takes three arguments: the texture sampler, the texture coordinates, and a bias value. The bias value is used to adjust the level of detail of the texture lookup. The `$DECODE` macro is used to extract the appropriate channel from the texture lookup result. \n\nFinally, the function checks if the `MAPVERTEX` flag is defined. If it is, the function multiplies the `sheenColor` variable by the vertex color of the mesh. The `vVertexColor` variable is a varying that is interpolated across the surface of the mesh. The `saturate()` function is used to clamp the vertex color to the range [0, 1]. \n\nThe resulting `sheenColor` variable is then assigned to the `sSpecularity` variable, which is used to calculate the specular highlight of the material. \n\nThis code is part of the PlayCanvas engine's shader system, which is used to render 3D graphics in real-time. The `getSheen()` function is likely used in conjunction with other shader functions to calculate the final color of a material. The `MAPCOLOR`, `MAPTEXTURE`, and `MAPVERTEX` flags are likely defined elsewhere in the shader code, and are used to control which parts of the `getSheen()` function are executed. \n\nHere is an example of how this code might be used in a PlayCanvas project:\n\n```javascript\n// Create a new material\nvar material = new pc.StandardMaterial();\n\n// Set the sheen color of the material\nmaterial.sheen = new pc.Color(1, 0, 0);\n\n// Set the texture of the material\nmaterial.diffuseMap = texture;\n\n// Set the vertex color of the mesh\nmesh.vertexColors = [new pc.Color(1, 1, 1), new pc.Color(0, 0, 0), ...];\n\n// Set the shader of the material\nmaterial.shader = shader;\n\n// Render the mesh with the material\nmeshInstance.material = material;\n``` \n\nIn this example, the `sheen` property of the material is set to red, which means that the `material_sheen` uniform in the shader will be set to `(1, 0, 0)`. The `diffuseMap` property of the material is set to a texture, which means that the `MAPTEXTURE` flag in the shader will be defined. The `vertexColors` property of the mesh is set to an array of colors, which means that the `MAPVERTEX` flag in the shader will be defined. Finally, the `shader` property of the material is set to the shader that contains the `getSheen()` function. When the mesh is rendered with this material, the `getSheen()` function will be executed to calculate the final color of the material.\n## Questions: \n 1. What is the purpose of the `getSheen()` function?\n   - The `getSheen()` function is used to calculate the sheen color of a material based on various inputs such as a color map, texture map, and vertex color.\n\n2. What is the significance of the `#ifdef` preprocessor directives?\n   - The `#ifdef` preprocessor directives are used to conditionally compile certain parts of the code based on whether certain macros are defined or not. In this code, it is used to check if certain maps or textures are available before using them in the calculation of the sheen color.\n\n3. What is the meaning of the `/* glsl */` comment at the beginning of the code?\n   - The `/* glsl */` comment is used to indicate that the code is written in GLSL (OpenGL Shading Language), which is a high-level shading language used for programming shaders in graphics processing units (GPUs). This comment is used by some tools to provide syntax highlighting and other features specific to GLSL.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/sheen.md"}}],["717",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/sheenGloss.js)\n\nThe code provided is a GLSL shader code that is used to calculate the sheen glossiness of a material. The sheen glossiness is a property of a material that determines how shiny or glossy the material appears. This code is a part of the PlayCanvas engine project and is used to render 3D graphics in real-time.\n\nThe code exports a function called `getSheenGlossiness()` that calculates the sheen glossiness of a material. The function takes into account several factors that affect the sheen glossiness of a material. These factors include a material property called `material_sheenGloss`, a texture map, a vertex color map, and an invert map.\n\nThe `material_sheenGloss` property is a float value that determines the overall sheen glossiness of the material. If this property is defined, the function multiplies the current sheen glossiness value by this property.\n\nThe texture map is used to add detail to the sheen glossiness of the material. If a texture map is defined, the function multiplies the current sheen glossiness value by the value of the texture map at the current UV coordinates.\n\nThe vertex color map is used to add color variation to the sheen glossiness of the material. If a vertex color map is defined, the function multiplies the current sheen glossiness value by the saturation value of the vertex color at the current vertex.\n\nThe invert map is used to invert the sheen glossiness of the material. If an invert map is defined, the function subtracts the current sheen glossiness value from 1.\n\nFinally, the function adds a small value of 0.0000001 to the sheen glossiness value to avoid division by zero errors.\n\nThis code is used in the PlayCanvas engine project to render 3D graphics in real-time. It is a part of the shader code that is used to calculate the appearance of materials in the scene. The sheen glossiness value calculated by this function is used to determine how shiny or glossy a material appears in the scene.\n## Questions: \n 1. What is the purpose of this code?\n   - This code defines a function called `getSheenGlossiness()` that calculates a value for sheen glossiness based on various inputs.\n\n2. What are the inputs to the `getSheenGlossiness()` function?\n   - The inputs to the function are `material_sheenGloss` (a uniform float), `$SAMPLER` (a texture sampler), `$UV` (a texture coordinate), `textureBias` (a bias value for the texture), and `vVertexColor.$VC` (a vertex color value).\n\n3. What is the output of the `getSheenGlossiness()` function?\n   - The output of the function is the calculated value for sheen glossiness, which is stored in the variable `sGlossiness`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/sheenGloss.md"}}],["718",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/specular.js)\n\nThe code above is a shader code written in GLSL (OpenGL Shading Language) that is used in the PlayCanvas engine project. The purpose of this code is to calculate the specularity of a material. \n\nThe code starts by defining a uniform variable called `material_specular` which is a vec3 (a vector with three components) that represents the specular color of the material. This variable is only defined if the `MAPCOLOR` flag is defined. \n\nThe `getSpecularity()` function is then defined, which calculates the specularity of the material. It starts by defining a `specularColor` variable which is initialized to white (1,1,1). \n\nThe code then checks if the `MAPCOLOR` flag is defined. If it is, the `specularColor` variable is multiplied by the `material_specular` uniform variable. This means that the specular color of the material is multiplied by the `material_specular` value. \n\nThe code then checks if the `MAPTEXTURE` flag is defined. If it is, the `specularColor` variable is multiplied by a value that is calculated using the `texture2DBias()` function. This function takes three arguments: a sampler (`$SAMPLER`), a UV coordinate (`$UV`), and a texture bias (`textureBias`). The `$DECODE` function is used to decode the texture value and the `$CH` property is used to access the blue channel of the decoded value. This means that the specular color of the material is multiplied by the blue channel of the texture value. \n\nThe code then checks if the `MAPVERTEX` flag is defined. If it is, the `specularColor` variable is multiplied by the saturation of the vertex color (`vVertexColor.$VC`). This means that the specular color of the material is multiplied by the saturation of the vertex color. \n\nFinally, the `dSpecularity` variable is set to the `specularColor` value. This means that the specularity of the material is set to the final value of the `specularColor` variable. \n\nThis code is used in the PlayCanvas engine project to calculate the specularity of a material. It is part of a larger system of shaders and materials that are used to render 3D graphics in real-time. Here is an example of how this code might be used in a material definition:\n\n```\nvar material = new pc.StandardMaterial();\nmaterial.chunks.specularPS = /* glsl */`\n    #define MAPCOLOR\n    #define MAPTEXTURE\n    #define MAPVERTEX\n    uniform vec3 material_specular;\n    varying vec3 vVertexColor;\n    uniform sampler2D texture_diffuseMap;\n    uniform float textureBias;\n    void getSpecularity() {\n        vec3 specularColor = vec3(1,1,1);\n        specularColor *= material_specular;\n        specularColor *= $DECODE(texture2DBias(texture_diffuseMap, $UV, textureBias)).b;\n        specularColor *= saturate(vVertexColor.g);\n        dSpecularity = specularColor;\n    }\n`;\n```\n\nIn this example, the `chunks.specularPS` property of the material is set to the GLSL code above. This means that the material will use this code to calculate its specularity. The material also defines a `material_specular` uniform variable, a `texture_diffuseMap` sampler, and a `vVertexColor` varying variable. These variables are used in the GLSL code to calculate the specularity of the material.\n## Questions: \n 1. What is the purpose of the `getSpecularity()` function?\n- The `getSpecularity()` function is used to calculate the specular color of a material.\n\n2. What is the significance of the `#ifdef` statements in the code?\n- The `#ifdef` statements are used to conditionally compile certain parts of the code based on whether certain preprocessor macros are defined or not.\n\n3. What is the meaning of the variables `$DECODE`, `$SAMPLER`, `$UV`, `$CH`, and `vVertexColor.$VC`?\n- These variables are likely placeholders for values that will be replaced with actual values during runtime. Without more context, it is difficult to determine their exact meaning.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/specular.md"}}],["719",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/specularityFactor.js)\n\nThe code provided is a GLSL shader code that calculates the specularity factor of a material. The specularity factor is a value that determines how shiny or reflective a material appears. This code is a part of the PlayCanvas engine project and is used to render 3D graphics.\n\nThe code starts with an `#ifdef` preprocessor directive that checks if the `MAPFLOAT` macro is defined. If it is defined, the code declares a uniform variable `material_specularityFactor` of type `float`. Uniform variables are variables that are shared between the CPU and GPU and can be set from the CPU. In this case, `material_specularityFactor` is used to set the specularity factor of the material.\n\nThe `getSpecularityFactor()` function calculates the specularity factor of the material. It starts by initializing a local variable `specularityFactor` to 1.0. This variable is then multiplied by different factors based on the macros defined.\n\nIf the `MAPFLOAT` macro is defined, `specularityFactor` is multiplied by `material_specularityFactor`. This means that the specularity factor of the material can be set from the CPU by setting the value of `material_specularityFactor`.\n\nIf the `MAPTEXTURE` macro is defined, `specularityFactor` is multiplied by the value of a texture sample. The texture sample is obtained using the `texture2DBias()` function, which takes three arguments: the sampler, the UV coordinates, and the texture bias. The sampler is a reference to the texture that is being sampled, the UV coordinates are the coordinates of the pixel being shaded, and the texture bias is an offset that is added to the UV coordinates to sample a different part of the texture. The `$SAMPLER`, `$UV`, and `$CH` variables are placeholders that are replaced with actual values during the shader compilation process.\n\nIf the `MAPVERTEX` macro is defined, `specularityFactor` is multiplied by the saturation of the vertex color. The vertex color is a color value that is interpolated across the surface of the material. The `saturate()` function clamps the value between 0 and 1, ensuring that the specularity factor is not negative.\n\nFinally, the function sets the value of `dSpecularityFactor` to `specularityFactor`. `dSpecularityFactor` is an output variable that is used to pass the specularity factor to the next stage of the rendering pipeline.\n\nIn summary, this code calculates the specularity factor of a material based on different factors such as a uniform value, a texture sample, and a vertex color. It is a part of the PlayCanvas engine project and is used to render 3D graphics. Here is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\n// Set the specularity factor of the material\nmaterial.setParameter('material_specularityFactor', 0.5);\n\n// Set the texture that will be used to calculate the specularity factor\nmaterial.setParameter('MAPTEXTURE', texture);\n\n// Set the vertex color of the material\nmaterial.vertexColors = true;\n\n// Render the mesh using the material\nmeshInstance.material = material;\n```\n## Questions: \n 1. What is the purpose of the `getSpecularityFactor()` function?\n   - The function calculates a specularity factor based on various inputs such as a material specularity factor, a texture map, and vertex color.\n2. What is the `MAPFLOAT` preprocessor directive used for?\n   - It is used to conditionally compile code that depends on the existence of a `material_specularityFactor` uniform variable.\n3. What is the purpose of the `texture2DBias()` function and its arguments?\n   - The function samples a texture map using the provided sampler, UV coordinates, and texture bias values, and returns the color value at the specified channel (`$CH`).","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/specularityFactor.md"}}],["720",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/textureSample.js)\n\nThe code provided is a set of GLSL shader functions that perform texture sampling and color space conversion. These functions are designed to be used in conjunction with the PlayCanvas engine, which is a WebGL-based game engine that provides a high-level API for building 3D games and applications.\n\nThe first two functions, `texture2DSRGB`, perform texture sampling and gamma correction for sRGB textures. The `sampler2D` parameter specifies the texture to sample, and the `vec2` parameter `uv` specifies the texture coordinates to sample at. The first function returns a `vec4` color value, while the second function takes an additional `float` parameter `bias` that can be used to adjust the texture sampling level. Both functions use the `gammaCorrectInput` function to perform gamma correction on the sampled color value.\n\nThe next two functions, `texture2DRGBM`, perform texture sampling and decoding for RGBM textures. The `sampler2D` and `vec2` parameters are the same as before, and the functions return a `vec3` color value. The second function takes an additional `float` parameter `bias` that can be used to adjust the texture sampling level. These functions use the `decodeRGBM` function to decode the sampled color value from the RGBM format.\n\nThe final two functions, `texture2DRGBE`, perform texture sampling and decoding for RGBE textures. The `sampler2D` and `vec2` parameters are the same as before, and the functions return a `vec3` color value. The second function takes an additional `float` parameter `bias` that can be used to adjust the texture sampling level. These functions also use the `decodeRGBM` function to decode the sampled color value from the RGBE format.\n\nOverall, these functions provide a set of utility functions for working with different types of textures in the PlayCanvas engine. They allow developers to easily sample and convert textures between different color spaces, which is an important part of rendering high-quality 3D graphics.\n## Questions: \n 1. What is the purpose of this code?\n    \n    This code provides functions for sampling textures in different color spaces and applying gamma correction and bias.\n\n2. What is the difference between the `texture2DSRGB` and `texture2DRGBM` functions?\n    \n    The `texture2DSRGB` functions apply gamma correction to the texture samples, while the `texture2DRGBM` functions decode the RGBM format used for compressed textures.\n\n3. What is the `texture2DRGBE` function and how does it differ from the other functions?\n    \n    The `texture2DRGBE` function decodes the RGBE format used for high dynamic range textures, and it is similar to the `texture2DRGBM` function but with a different decoding algorithm.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/textureSample.md"}}],["721",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/thickness.js)\n\nThe code above is a GLSL shader code that is used to calculate the thickness of a material. It is a part of the PlayCanvas engine project and is used to render 3D graphics in web applications. \n\nThe purpose of this code is to calculate the thickness of a material based on various factors such as the material thickness, texture, and vertex color. The thickness is calculated by multiplying the default thickness value of 1.0 with the values obtained from the different factors. \n\nThe code starts by checking if the MAPFLOAT flag is defined. If it is defined, the value of the material_thickness uniform is multiplied with the default thickness value. This means that if the material has a thickness value defined, it will be used to calculate the final thickness of the material. \n\nNext, the code checks if the MAPTEXTURE flag is defined. If it is defined, the texture2DBias function is called with the SAMPLER, UV, and textureBias parameters. This function returns a vec4 value that contains the texture color values. The $CH parameter is used to access the color channel of the texture. This color value is then multiplied with the default thickness value. This means that if the material has a texture defined, it will be used to calculate the final thickness of the material. \n\nFinally, the code checks if the MAPVERTEX flag is defined. If it is defined, the saturate function is called with the vVertexColor.$VC parameter. This function returns a value between 0 and 1 that is used to clamp the vertex color value. This clamped value is then multiplied with the default thickness value. This means that if the material has a vertex color defined, it will be used to calculate the final thickness of the material. \n\nOverall, this code is an important part of the PlayCanvas engine project as it is used to calculate the thickness of materials in 3D graphics rendering. It allows for the creation of more realistic and detailed graphics in web applications. \n\nExample usage:\n\n```glsl\nuniform float material_thickness;\nuniform sampler2D texture;\n\nvoid main() {\n  vec4 texColor = texture2D(texture, vUv);\n  getThickness();\n  gl_FragColor = vec4(texColor.rgb, dThickness);\n}\n```\n\nIn the example above, the getThickness() function is called to calculate the final thickness of the material. The texture color value is obtained using the texture2D function and is stored in the texColor variable. The final color value is then calculated by combining the texture color value with the thickness value obtained from the getThickness() function.\n## Questions: \n 1. **What is the purpose of the `getThickness()` function?**\n    \n    The `getThickness()` function is used to calculate the thickness of a material based on various factors such as a uniform float value, a texture, and vertex color.\n\n2. **What is the significance of the `MAPFLOAT` preprocessor directive?**\n    \n    The `MAPFLOAT` preprocessor directive is used to check if a uniform float value called `material_thickness` is defined. If it is defined, the value is used to calculate the thickness of the material.\n\n3. **What is the purpose of the `saturate()` function in the `MAPVERTEX` block?**\n    \n    The `saturate()` function is used to clamp the value of `vVertexColor.$VC` between 0 and 1. This ensures that the thickness calculation is not affected by any negative or out-of-range values in the vertex color.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/thickness.md"}}],["722",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/chunks/standard/frag/transmission.js)\n\nThe code above is a GLSL shader code that is used to calculate the refraction of a material. It is a part of the PlayCanvas engine project and is used to render 3D graphics in real-time. \n\nThe purpose of this code is to calculate the refraction of a material based on various inputs such as a float value, a texture, and a vertex color. The refraction value is then stored in a variable called dTransmission. \n\nThe code starts by defining a uniform float variable called material_refraction. This variable is used to store the refraction value of the material. If the MAPFLOAT flag is defined, the value of material_refraction is used to calculate the refraction. \n\nNext, the code checks if the MAPTEXTURE flag is defined. If it is, the code multiplies the refraction value by the texture2DBias function. This function takes three parameters: a sampler, a UV coordinate, and a texture bias. The texture2DBias function returns a vec4 value that contains the texture color values. The $SAMPLER, $UV, and $CH variables are replaced with actual values during runtime. \n\nFinally, the code checks if the MAPVERTEX flag is defined. If it is, the code multiplies the refraction value by the saturate function. This function takes a vec4 value as input and returns a vec4 value with each component clamped between 0 and 1. The vVertexColor.$VC variable is replaced with an actual value during runtime. \n\nOverall, this code is used to calculate the refraction of a material based on various inputs. It is a part of the PlayCanvas engine project and is used to render 3D graphics in real-time. Here is an example of how this code can be used in a PlayCanvas project:\n\n```javascript\nvar material = new pc.StandardMaterial();\nmaterial.refraction = 0.5;\nmaterial.diffuseMap = diffuseMap;\nmaterial.vertexColors = true;\nmaterial.update();\n\nvar meshInstance = new pc.MeshInstance(node, mesh, material);\n``` \n\nIn this example, a new StandardMaterial is created with a refraction value of 0.5. A diffuseMap texture is assigned to the material and vertex colors are enabled. The material is then updated. Finally, a new MeshInstance is created with the material and added to a node in the scene.\n## Questions: \n 1. What is the purpose of this code?\n   - This code defines a function called `getRefraction()` that calculates the refraction value based on various inputs.\n\n2. What is the significance of the `#ifdef` statements?\n   - The `#ifdef` statements are preprocessor directives that check if certain macros are defined. If they are defined, the corresponding code block is included in the final compiled code.\n\n3. What are the variables `material_refraction`, `textureBias`, `$SAMPLER`, `$UV`, `vVertexColor`, and `$VC`?\n   - `material_refraction` is a uniform float variable that may be defined elsewhere in the code.\n   - `textureBias`, `$SAMPLER`, and `$UV` are likely related to texture mapping.\n   - `vVertexColor` is likely a vertex attribute that stores per-vertex color information.\n   - `$VC` is likely a component of `vVertexColor`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/chunks/standard/frag/transmission.md"}}],["723",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/get-program-library.js)\n\nThe code above is a module that exports two functions: `getProgramLibrary` and `setProgramLibrary`. These functions are used to manage a cache of program libraries for different graphics devices in the PlayCanvas engine.\n\nThe `getProgramLibrary` function takes a `GraphicsDevice` object as its parameter and returns the corresponding `ProgramLibrary` object from the cache. If the library is not found in the cache, an assertion error is thrown. \n\nHere is an example of how `getProgramLibrary` can be used:\n\n```javascript\nimport { getProgramLibrary } from 'playcanvas-engine';\n\nconst device = new GraphicsDevice();\nconst programLibrary = getProgramLibrary(device);\n```\n\nThe `setProgramLibrary` function takes a `GraphicsDevice` object and a `ProgramLibrary` object as its parameters. It assigns the `ProgramLibrary` object to the cache for the corresponding `GraphicsDevice`. If the library is not provided, an assertion error is thrown.\n\nHere is an example of how `setProgramLibrary` can be used:\n\n```javascript\nimport { setProgramLibrary } from 'playcanvas-engine';\n\nconst device = new GraphicsDevice();\nconst programLibrary = new ProgramLibrary();\nsetProgramLibrary(device, programLibrary);\n```\n\nOverall, this module provides a way to manage program libraries for different graphics devices in the PlayCanvas engine. By caching program libraries, the engine can avoid the overhead of compiling shaders every time they are used.\n## Questions: \n 1. What is the purpose of the `program-library.js` file that is being imported?\n   - The `program-library.js` file contains a class called `ProgramLibrary` that is being used to create instances of program libraries.\n2. What is the `DeviceCache` class and how is it being used in this code?\n   - The `DeviceCache` class is a cache that stores device-specific data. In this code, it is being used to store instances of program libraries for each device.\n3. Why are there `Debug.assert` statements in the code and what do they do?\n   - The `Debug.assert` statements are used to check if a condition is true and throw an error if it is not. In this code, they are being used to ensure that the program library being accessed or assigned is not null.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/get-program-library.md"}}],["724",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/program-library.js)\n\nThe `ProgramLibrary` class is responsible for creating and caching shaders used in the PlayCanvas engine. It has two levels of cache: the first level generates the shader based on the provided options, while the second level processes this generated shader using processing options, which in most cases modifies it to support uniform buffers. \n\nThe class has two caches: `processedCache` and `definitionsCache`. `processedCache` is a map of shaders processed using processing options, while `definitionsCache` is a map of shader definitions before processing. \n\nThe `ProgramLibrary` class has several methods for generating and caching shaders. The `generateShaderDefinition` method generates a shader definition for a given generator, name, key, and options. The `getProgram` method gets a shader program for a given name, options, and processing options. It generates a key for shader source code generation, a key for its further processing to work with uniform buffers, and a final key to get the processed shader from the cache. If the final processed shader is not found in the cache, it generates a new shader definition and adds it to the processed cache. \n\nThe `storeNewProgram` method stores a new program in the unique non-cached programs collection to dump and update game shaders cache. The `dumpPrograms` method generates a script for precompiling shaders. The `clearCache` method clears the cache of processed shaders. The `removeFromCache` method removes a shader from the cache. \n\nThe `ProgramLibrary` class is used in the PlayCanvas engine to create and cache shaders. It is used in conjunction with other classes and modules in the engine to render graphics. For example, the `Shader` class is used to create shaders, while the `StandardMaterialOptions` class is used to set options for standard materials. \n\nOverall, the `ProgramLibrary` class is an important part of the PlayCanvas engine, as it is responsible for generating and caching shaders used in rendering graphics.\n## Questions: \n 1. What is the purpose of the `ProgramLibrary` class?\n- The `ProgramLibrary` class is responsible for creating and caching required shaders for the PlayCanvas engine.\n2. What are the two level caches used by the `ProgramLibrary` class?\n- The first level cache generates the shader based on the provided options, while the second level processes this generated shader using processing options, in most cases modifying it to support uniform buffers.\n3. What is the purpose of the `dumpPrograms` method?\n- The `dumpPrograms` method is used to build a shader options script by dumping the collection of unique non-cached programs and updating the game shaders cache.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/program-library.md"}}],["725",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/programs/basic.js)\n\nThe code defines a basic shader for rendering 3D objects in the PlayCanvas engine. The shader is generated based on a set of options passed to it, such as whether to use fog, alpha testing, vertex colors, diffuse maps, skinning, instancing, and morphing. The shader is used to render objects in different passes, such as depth pass and forward pass.\n\nThe `generateKey` function generates a unique key for the shader based on the options passed to it. This key is used to cache the shader so that it doesn't need to be regenerated every time it is used.\n\nThe `createShaderDefinition` function generates the shader code based on the options passed to it. It first generates the attributes for the shader based on the options for skinning, vertex colors, and diffuse maps. It then generates the vertex shader code, which includes the declarations for the shader inputs and outputs, as well as the shader body that transforms the vertices and outputs the position, color, and texture coordinates. The fragment shader code is then generated, which reads the color and texture data from the vertex shader and outputs the final color of the pixel. The fragment shader also includes code for fog and alpha testing.\n\nThe `basic` object exports the `generateKey` and `createShaderDefinition` functions for use in other parts of the PlayCanvas engine.\n\nExample usage:\n\n```javascript\nconst options = {\n  fog: true,\n  alphaTest: false,\n  vertexColors: true,\n  diffuseMap: true,\n  skin: false,\n  screenSpace: false,\n  useInstancing: false,\n  useMorphPosition: false,\n  useMorphNormal: false,\n  useMorphTextureBased: false,\n  pass: SHADER_DEPTH\n};\n\nconst shader = basic.createShaderDefinition(device, options);\n```\n## Questions: \n 1. What is the purpose of the `generateKey` function?\n- The `generateKey` function generates a unique key based on the options passed to it, which is used to identify a specific shader definition.\n\n2. What are the different attributes that can be generated in the `createShaderDefinition` function?\n- The different attributes that can be generated are `vertex_position`, `vertex_boneWeights`, `vertex_boneIndices`, `vertex_color`, and `vertex_texCoord0`.\n\n3. What is the purpose of the `SHADER_DEPTH` constant?\n- The `SHADER_DEPTH` constant is used to identify a specific pass in the shader, which is responsible for rendering the depth of the scene.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/programs/basic.md"}}],["726",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/programs/common.js)\n\nThe code provided is a module that exports several functions used for generating shader code for various graphics effects. The module imports constants from another file and shader code chunks from a separate directory. \n\nThe `gammaCode` function takes a gamma value as input and returns the corresponding shader code for gamma correction. The function checks the input value against predefined constants for different gamma values and returns the appropriate shader code chunk. If no matching constant is found, the function returns the default gamma correction shader code.\n\nThe `tonemapCode` function takes a tonemapping value as input and returns the corresponding shader code for tonemapping. The function checks the input value against predefined constants for different tonemapping values and returns the appropriate shader code chunk. If no matching constant is found, the function returns the default tonemapping shader code.\n\nThe `fogCode` function takes a fog type as input and returns the corresponding shader code for fog. The function checks the input value against predefined constants for different fog types and returns the appropriate shader code chunk. If no matching constant is found, the function returns the default fog shader code.\n\nThe `skinCode` function takes a device and shader chunks as input and returns the corresponding shader code for skinning. The function checks if the device supports bone textures and returns the appropriate shader code chunk. If bone textures are not supported, the function returns the default skinning shader code.\n\nThe `begin` and `end` functions return the beginning and ending lines of a shader program, respectively.\n\nThese functions are used to generate shader code for various graphics effects in the PlayCanvas engine. For example, the `gammaCode` function may be used to generate shader code for gamma correction in a post-processing effect, while the `skinCode` function may be used to generate shader code for skinning in a mesh rendering shader. The `begin` and `end` functions may be used to wrap the generated shader code in a complete shader program.\n## Questions: \n 1. What is the purpose of the `constants.js` file being imported?\n    \n    The `constants.js` file is being imported to access the constants `GAMMA_SRGB`, `GAMMA_SRGBFAST`, `GAMMA_SRGBHDR`, `TONEMAP_ACES`, `TONEMAP_ACES2`, `TONEMAP_FILMIC`, `TONEMAP_HEJL`, and `TONEMAP_LINEAR` which are used in the `gammaCode`, `tonemapCode`, and `fogCode` functions.\n\n2. What is the purpose of the `shaderChunks` being imported?\n    \n    The `shaderChunks` are being imported to access the shader code chunks that are used in the `gammaCode`, `tonemapCode`, `fogCode`, and `skinCode` functions.\n\n3. What is the purpose of the `skinCode` function?\n    \n    The `skinCode` function is used to generate the shader code for skinning meshes. It checks if the device supports bone textures and returns the appropriate shader code. If bone textures are not supported, it returns the shader code with a bone limit defined by the device.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/programs/common.md"}}],["727",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/programs/particle.js)\n\nThe code defines a module called `particle` that contains a set of functions for creating and generating shader definitions for particle effects. The `createShaderDefinition` function takes in a device object and an options object, and returns a shader definition object that can be used to create a shader program for rendering particles.\n\nThe `createShaderDefinition` function first defines the vertex shader code by concatenating various shader chunks based on the options passed in. The function checks for options such as `mesh`, `localSpace`, `screenSpace`, `animTex`, `wrap`, `alignToMotion`, `normal`, and `stretch`, and adds the appropriate shader chunks to the vertex shader code. The function also checks if the `useCpu` option is set, and adds the appropriate shader chunks for CPU-based particle simulation.\n\nThe function then defines the fragment shader code by concatenating various shader chunks based on the options passed in. The function checks for options such as `gamma`, `toneMap`, `fog`, `normal`, `halflambert`, and `blend`, and adds the appropriate shader chunks to the fragment shader code.\n\nFinally, the function returns a shader definition object that contains the vertex and fragment shader code, which can be used to create a shader program for rendering particles.\n\nThe `generateKey` function generates a unique key for a set of particle options, which can be used to cache the shader definition object for faster shader creation.\n\nThe `_animTex` function generates the vertex shader code for animating particle textures based on the `animTexLoop` option.\n\nOverall, this module provides a flexible and customizable way to create and generate shader definitions for particle effects in the PlayCanvas engine. Developers can use this module to create custom particle effects with various options and settings.\n## Questions: \n 1. What is the purpose of the `generateKey` function?\n- The `generateKey` function generates a unique key for a particle based on its options.\n\n2. What is the purpose of the `createShaderDefinition` function?\n- The `createShaderDefinition` function creates a shader definition for rendering particles based on their options.\n\n3. What are the different blend modes supported by this code?\n- The different blend modes supported by this code are `BLEND_NORMAL`, `BLEND_ADDITIVE`, and `BLEND_MULTIPLICATIVE`.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/programs/particle.md"}}],["728",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/programs/skybox.js)\n\nThe code defines an object called `skybox` that contains two methods: `generateKey` and `createShaderDefinition`. The purpose of this object is to generate a shader definition for a skybox, which is a cube that surrounds a 3D scene and is used to simulate a background environment. \n\nThe `generateKey` method takes an `options` object as input and returns a string that represents a unique key for the skybox. The key is generated based on the options passed in, which include the type of skybox (cubemap or spherical), the encoding format, whether to use intensity, gamma correction, tone mapping, and whether to fix seams in the cubemap. If the type is cubemap, the key also includes the mip level. This key is used to cache the shader definition for the skybox, so that it can be reused if the same options are used again.\n\nThe `createShaderDefinition` method takes a `device` object and an `options` object as input and returns a shader definition object that can be used to render the skybox. The shader definition includes vertex and fragment code, as well as attribute and uniform definitions. The vertex code is a simple pass-through shader that sets the position of each vertex. The fragment code is more complex and depends on the options passed in. \n\nIf the skybox type is cubemap, the fragment code first fixes seams in the cubemap if the `fixSeams` option is set to true. It then multiplies the environment map by a constant or by the intensity of the environment map, depending on the `useIntensity` option. The resulting color is then decoded from the specified encoding format and gamma-corrected and tone-mapped based on the `gamma` and `toneMapping` options. Finally, the fragment code samples the cubemap based on the direction of the incoming ray and returns the resulting color. \n\nIf the skybox type is spherical, the fragment code is simpler and does not include cubemap-specific code. Instead, it multiplies the environment map by a constant or by the intensity of the environment map, decodes the resulting color, and applies gamma correction and tone mapping. It then samples an environment map atlas based on the direction of the incoming ray and returns the resulting color. \n\nOverall, this code provides a way to generate a shader definition for a skybox based on various options, which can be used to render a background environment in a 3D scene.\n## Questions: \n 1. What is the purpose of the `skybox` object?\n- The `skybox` object contains two methods: `generateKey` and `createShaderDefinition` that are used to generate a key and create a shader definition for a skybox.\n\n2. What are the possible values for the `options.type` parameter in the `createShaderDefinition` method?\n- The possible values for the `options.type` parameter are `'cubemap'` and anything else.\n\n3. What is the purpose of the `ShaderUtils.createDefinition` method in the `createShaderDefinition` method?\n- The `ShaderUtils.createDefinition` method is used to create a shader definition for the skybox using the vertex and fragment code generated by the `createShaderDefinition` method.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/programs/skybox.md"}}],["729",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-lib/utils.js)\n\nThe code defines three functions related to creating and processing shaders in the PlayCanvas engine. The `createShader` function takes a graphics device, vertex shader name, fragment shader name, and an optional boolean flag for using transform feedback. It returns a new `Shader` object created from the specified shader chunks. The `createShaderFromCode` function takes a graphics device, vertex shader code, fragment shader code, a unique name for the shader, an optional object mapping vertex shader attribute names to semantics, and an optional boolean flag for using transform feedback. It returns a new `Shader` object created from the supplied shader code. This function also caches the shader in the `ProgramLibrary` to avoid creating duplicate shaders. The `processShader` function takes a `Shader` object and shader processing options, and returns a processed `Shader` object. This function utilizes the `ProgramLibrary` cache to avoid processing the same shader multiple times.\n\nThese functions are useful for creating and processing shaders in the PlayCanvas engine. They allow developers to create shaders from named chunks or from custom code, and to process shaders with different options. The `Shader` object is a key component of the engine's rendering pipeline, and is used to define how 3D models are rendered on the screen. By providing these functions, the PlayCanvas engine makes it easier for developers to create and customize shaders for their projects.\n\nExample usage of `createShader`:\n\n```\nconst device = app.graphicsDevice;\nconst vsName = 'basic';\nconst fsName = 'basic';\nconst useTransformFeedback = false;\nconst shader = createShader(device, vsName, fsName, useTransformFeedback);\n```\n\nExample usage of `createShaderFromCode`:\n\n```\nconst device = app.graphicsDevice;\nconst vsCode = `\n    attribute vec3 vertex_position;\n    uniform mat4 matrix_model;\n    uniform mat4 matrix_viewProjection;\n    void main() {\n        gl_Position = matrix_viewProjection * matrix_model * vec4(vertex_position, 1.0);\n    }\n`;\nconst fsCode = `\n    precision mediump float;\n    void main() {\n        gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0);\n    }\n`;\nconst uniqueName = 'custom-shader';\nconst attributes = {\n    vertex_position: pc.SEMANTIC_POSITION\n};\nconst useTransformFeedback = false;\nconst shader = createShaderFromCode(device, vsCode, fsCode, uniqueName, attributes, useTransformFeedback);\n```\n\nExample usage of `processShader`:\n\n```\nconst shader = ...; // get a Shader object\nconst processingOptions = {\n    skin: true,\n    instancing: true\n};\nconst processedShader = processShader(shader, processingOptions);\n```\n## Questions: \n 1. What is the purpose of the `createShader` function?\n   \n   The `createShader` function creates a new shader from named shader chunks, with the option to use transform feedback.\n\n2. What is the purpose of the `createShaderFromCode` function?\n   \n   The `createShaderFromCode` function creates a new shader from supplied source code, with the option to specify vertex shader attributes and use transform feedback.\n\n3. What is the purpose of the `processShader` function?\n   \n   The `processShader` function processes a shader using shader processing options, utilizing cache of the ProgramLibrary.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-lib/utils.md"}}],["730",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/shader-pass.js)\n\nThe code defines a utility class called `ShaderPass` that is responsible for math operations on shader pass constants. The class contains several static methods that are used to determine the type of a shader pass, whether it is a forward or shadow pass, and to convert between light and shadow types and shader passes. \n\nThe `getType` method takes a shader pass constant as an argument and returns the corresponding shader type constant. The `isForward` and `isShadow` methods take a shader pass constant as an argument and return a boolean indicating whether the pass is a forward or shadow pass, respectively. The `toLightType` and `toShadowType` methods take a shadow pass constant as an argument and return the corresponding light or shadow type constant. The `getShadow` method takes a light type and shadow type constant as arguments and returns the corresponding shadow pass constant. Finally, the `getPassShaderDefine` method takes a shader pass constant as an argument and returns a string containing the corresponding define code line.\n\nThese methods are used in the larger PlayCanvas engine project to facilitate the creation and management of shaders. They allow developers to easily determine the type of a shader pass and to convert between different types of passes. For example, the `getShadow` method can be used to create a shadow pass for a specific light and shadow type, while the `getPassShaderDefine` method can be used to generate the define code line for a specific shader pass. \n\nOverall, the `ShaderPass` class provides a useful set of utility methods for working with shader passes in the PlayCanvas engine project.\n## Questions: \n 1. What is the purpose of the `ShaderPass` class?\n- The `ShaderPass` class is a pure static utility class responsible for math operations on the shader pass constants.\n\n2. What are the different types of shader passes defined in the `constants.js` file?\n- The different types of shader passes defined in the `constants.js` file are `SHADER_FORWARD`, `SHADER_FORWARDHDR`, `SHADER_DEPTH`, `SHADER_PICK`, and `SHADER_SHADOW`.\n\n3. What is the purpose of the `getShadow` method in the `ShaderPass` class?\n- The `getShadow` method in the `ShaderPass` class returns a shader pass for a specified light and shadow type.","metadata":{"source":".autodoc/docs/markdown/src/scene/shader-pass.md"}}],["731",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/skin-instance-cache.js)\n\nThe code defines a static class called `SkinInstanceCache` that implements a cache of `SkinInstance` objects used by a render component. The purpose of the cache is to allow sharing of `SkinInstance` objects between multiple render components that use the same `glb` file. \n\nThe `SkinInstanceCache` class contains several static methods for managing the cache. The `createCachedSkinInstance` method takes a `skin`, a `rootBone`, and an `entity` as arguments, and returns a cached `SkinInstance` object for the given `skin` and `rootBone`. If a cached `SkinInstance` object does not exist for the given `skin` and `rootBone`, a new `SkinInstance` object is created, resolved with the `rootBone` and `entity`, and added to the cache. \n\nThe `getCachedSkinInstance` method takes a `skin` and a `rootBone` as arguments, and returns a cached `SkinInstance` object for the given `skin` and `rootBone`. If a cached `SkinInstance` object exists for the given `skin` and `rootBone`, its reference count is increased and the `SkinInstance` object is returned. \n\nThe `addCachedSkinInstance` method takes a `skin`, a `rootBone`, and a `skinInstance` as arguments, and adds the `skinInstance` object to the cache for the given `skin` and `rootBone`. If a cached `SkinInstance` object does not exist for the given `skin` and `rootBone`, a new `SkinInstanceCachedObject` is created with the `skin` and `skinInstance`, added to the cache, and its reference count is increased. \n\nThe `removeCachedSkinInstance` method takes a `skinInstance` as an argument, and removes the `skinInstance` object from the cache. If the reference count of the `SkinInstanceCachedObject` associated with the `skinInstance` object reaches 0, the `SkinInstance` object is destroyed and the `SkinInstanceCachedObject` is removed from the cache. \n\nThe `SkinInstanceCache` class also contains a private static property `_skinInstanceCache`, which is a `Map` that maps a `rootBone` to an array of `SkinInstanceCachedObject` objects. \n\nThe code also defines a `SkinInstanceCachedObject` class, which is used as an entry in the ref-counted skin instance cache. The `SkinInstanceCachedObject` class extends the `RefCountedObject` class and has a `skin` and a `skinInstance` property. \n\nOverall, the `SkinInstanceCache` class provides a way to cache and share `SkinInstance` objects between multiple render components that use the same `glb` file, which can improve performance and reduce memory usage.\n## Questions: \n 1. What is the purpose of the `SkinInstanceCache` class?\n- The `SkinInstanceCache` class is a static class that implements a cache of skin instances used by the render component.\n\n2. What is the significance of the `SkinInstanceCachedObject` class?\n- The `SkinInstanceCachedObject` class is used as an entry in the ref-counted skin instance cache.\n\n3. What is the purpose of the `logCachedSkinInstances` function?\n- The `logCachedSkinInstances` function is a debugging function that logs out the state of the skin instances cache. It is only executed if the `_DEBUG` flag is set.","metadata":{"source":".autodoc/docs/markdown/src/scene/skin-instance-cache.md"}}],["732",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/skin-instance.js)\n\nThe code defines a class called `SkinInstance` that is responsible for generating a matrix palette used to skin vertices from object space to world space. The class has a constructor that takes a `Skin` object as an argument, which provides the inverse bind pose matrices to generate the final matrix palette. The class has a property called `bones`, which is an array of nodes representing each bone in the skin instance. \n\nThe `init` method initializes the `matrixPalette` property of the class. If the device supports bone textures, it creates a texture with a size that can fit all the bones, and the `matrixPalette` is set to the locked texture. Otherwise, the `matrixPalette` is set to a new `Float32Array`. The `destroy` method destroys the bone texture if it exists.\n\nThe `resolve` method resolves bone IDs to actual graph nodes of the hierarchy. It takes a `rootBone` and an `entity` as arguments. The `rootBone` is set as the root of the hierarchy, and the `bones` property is populated with the nodes representing each bone in the skin instance.\n\nThe `updateMatrices` method updates the matrices of the bones in the skin instance. It takes a `rootNode` and a `skinUpdateIndex` as arguments. If the `_updateBeforeCull` property is true, it calls the `_updateMatrices` method to update the matrices.\n\nThe `_updateMatrices` method updates the matrices of the bones in the skin instance. It takes a `rootNode` and a `skinUpdateIndex` as arguments. It copies the inverse of the `rootNode`'s world transform to `_invMatrix`. It then iterates over the bones in reverse order and multiplies the inverse of the `rootNode`'s world transform with the bone's world transform to get the matrix in `rootNode` space. It then multiplies the matrix with the inverse bind pose matrix to get the matrix in bind space.\n\nThe `updateMatrixPalette` method updates the matrix palette of the skin instance. It takes a `rootNode` and a `skinUpdateIndex` as arguments. It calls the `_updateMatrices` method to update the matrices. It then copies the matrices to the `matrixPalette` property in a format that can be sent to the vertex shader. Finally, it calls the `uploadBones` method to upload the bone texture to the device.\n\nOverall, the `SkinInstance` class is an important part of the PlayCanvas engine that is used to generate a matrix palette for skinning vertices. It provides methods to initialize the matrix palette, resolve bone IDs to actual graph nodes of the hierarchy, update the matrices of the bones in the skin instance, and update the matrix palette.\n## Questions: \n 1. What is the purpose of the `SkinInstance` class?\n- The `SkinInstance` class is responsible for generating the matrix palette used to skin vertices from object space to world space.\n\n2. What is the `boneTexture` property used for?\n- The `boneTexture` property is used to store the matrix palette as a texture if the device supports bone textures.\n\n3. What is the purpose of the `updateBeforeCull` property?\n- The `updateBeforeCull` property is used to determine whether the matrices need to be updated before frustum culling.","metadata":{"source":".autodoc/docs/markdown/src/scene/skin-instance.md"}}],["733",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/skin.js)\n\nThe code defines a class called `Skin` which represents the data about the bones in a hierarchy that drive a skinned mesh animation. In particular, the `Skin` class stores the bone name and inverse bind matrix for each bone. The inverse bind matrices are important in the mathematics of vertex skinning.\n\nThe `Skin` class has a constructor that takes three parameters: `graphicsDevice`, `ibp`, and `boneNames`. The `graphicsDevice` parameter is an instance of the `GraphicsDevice` class which is used to manage the skin. The `ibp` parameter is an array of inverse bind matrices, and the `boneNames` parameter is an array of bone names for the bones referenced by this skin.\n\nThe `Skin` class is designed to be used in the larger project as a way to store and manage the data about the bones in a hierarchy that drive a skinned mesh animation. This data can then be used to perform vertex skinning calculations to animate the mesh.\n\nHere is an example of how the `Skin` class might be used in the larger project:\n\n```javascript\nimport { Skin } from 'playcanvas-engine';\n\n// create an instance of the GraphicsDevice class\nconst graphicsDevice = new GraphicsDevice();\n\n// create an array of inverse bind matrices\nconst ibp = [\n  new Mat4(),\n  new Mat4(),\n  new Mat4()\n];\n\n// create an array of bone names\nconst boneNames = ['bone1', 'bone2', 'bone3'];\n\n// create a new instance of the Skin class\nconst skin = new Skin(graphicsDevice, ibp, boneNames);\n```\n\nIn this example, we create an instance of the `GraphicsDevice` class, an array of inverse bind matrices, and an array of bone names. We then create a new instance of the `Skin` class, passing in the `graphicsDevice`, `ibp`, and `boneNames` parameters. This creates a new `Skin` object that can be used to manage the data about the bones in a hierarchy that drive a skinned mesh animation.\n## Questions: \n 1. What is the purpose of the Skin class?\n- The Skin class contains data about the bones in a hierarchy that drive a skinned mesh animation, specifically the bone name and inverse bind matrix for each bone.\n\n2. What parameters are required to create a new instance of the Skin class?\n- To create a new instance of the Skin class, a graphics device, an array of inverse bind matrices, and an array of bone names for the bones referenced by the skin are required.\n\n3. What is the significance of the inverse bind matrices in vertex skinning?\n- Inverse bind matrices are instrumental in the mathematics of vertex skinning, as they are used to transform vertices from model space to bone space during animation.","metadata":{"source":".autodoc/docs/markdown/src/scene/skin.md"}}],["734",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/sky.js)\n\nThe code defines a class called `Sky` which represents a visual representation of the sky in a 3D scene. The class is not intended to be used directly by developers, hence the `@ignore` tag in the JSDoc comment. \n\nThe `Sky` class has a constructor that takes three parameters: a `GraphicsDevice` object, a `Scene` object, and a `Texture` object. The constructor creates a `Material` object and sets its properties based on the `Texture` object passed in. If the texture is a cubemap, the material's `texture_cubeMap` parameter is set to the texture. Otherwise, the material's `texture_envAtlas` parameter is set to the texture and the `mipLevel` parameter is set to the scene's `_skyboxMip` property. The material's `cull` property is set to `CULLFACE_FRONT` and its `depthWrite` property is set to `false`.\n\nThe constructor also creates a `MeshInstance` object using a box mesh created by the `createBox` function. The `MeshInstance` object is initialized with the `Material` object created earlier and a new `GraphNode` object. The `MeshInstance` object's `cull` property is set to `false` and its `_noDepthDrawGl1` property is set to `true`. Finally, the `MeshInstance` object is added to the `skyLayer` of the `Scene` object passed in.\n\nThe `Sky` class also has a `destroy` method that removes the `MeshInstance` object from the `skyLayer` and destroys the `MeshInstance` object.\n\nOverall, the `Sky` class is used to create a skybox in a 3D scene. It creates a `MeshInstance` object with a box mesh and a material that uses a texture to represent the sky. The `MeshInstance` object is added to the `skyLayer` of the `Scene` object passed in. The `Sky` class is not intended to be used directly by developers, but rather as a helper class for the `Scene` class.\n## Questions: \n 1. What is the purpose of this code and how does it fit into the PlayCanvas engine? \n\nThis code defines a class called Sky that represents the visual representation of the sky in a scene. It is used to create a mesh instance that is added to the sky layer of a scene. \n\n2. What is the significance of the material.getShaderVariant function and how does it work? \n\nThe material.getShaderVariant function is used to get the appropriate shader program for rendering the sky based on the texture and scene settings. It takes in various parameters and returns a shader program from the program library that matches the specified options. \n\n3. What is the difference between a cubemap and an envAtlas texture, and how does the code handle each type? \n\nA cubemap texture is a 6-sided texture used to represent the environment around an object, while an envAtlas texture is a texture that contains multiple views of the environment in a single image. The code checks whether the texture is a cubemap or an envAtlas and sets the appropriate parameters in the material accordingly.","metadata":{"source":".autodoc/docs/markdown/src/scene/sky.md"}}],["735",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/sprite.js)\n\nThe code defines a class called `Sprite` that represents a sprite image. A sprite is a 2D image that is used in video games and other interactive applications to represent characters, objects, and other elements. The `Sprite` class extends the `EventHandler` class, which allows it to emit events.\n\nThe `Sprite` class has several properties, including `pixelsPerUnit`, `renderMode`, `atlas`, and `frameKeys`. The `pixelsPerUnit` property specifies the number of pixels that map to one PlayCanvas unit. The `renderMode` property specifies the rendering mode of the sprite, which can be simple, sliced, or tiled. The `atlas` property specifies the texture atlas that contains the sprite image, and the `frameKeys` property specifies the keys of the frames in the sprite atlas that this sprite is using.\n\nThe `Sprite` class has several methods, including `_createMeshes()`, `_createSimpleMesh()`, and `_create9SliceMesh()`. The `_createMeshes()` method creates a mesh for each frame in the sprite. The `_createSimpleMesh()` method creates a simple mesh for a frame, and the `_create9SliceMesh()` method creates a 9-sliced mesh for a frame.\n\nThe `Sprite` class also has several event handlers, including `_onSetFrames()`, `_onFrameChanged()`, and `_onFrameRemoved()`. These event handlers are called when the frames in the sprite atlas are set, changed, or removed.\n\nOverall, the `Sprite` class is an important part of the PlayCanvas engine, as it allows developers to create and manipulate sprite images in their applications. Developers can use the `Sprite` class to specify the properties of a sprite, create meshes for the sprite, and handle events related to the sprite.\n## Questions: \n 1. What is the purpose of the `Sprite` class?\n    \n    The `Sprite` class contains references to one or more frames of a texture atlas and can be used to render a single frame or a sprite animation.\n\n2. What are the different rendering modes available for a sprite and how are they set?\n    \n    The different rendering modes available for a sprite are `SPRITE_RENDERMODE_SIMPLE`, `SPRITE_RENDERMODE_SLICED`, and `SPRITE_RENDERMODE_TILED`. They can be set using the `renderMode` property of the `Sprite` class.\n\n3. How are the meshes for each frame of the sprite created?\n    \n    The meshes for each frame of the sprite are created using either the `_createSimpleMesh` or `_create9SliceMesh` function, depending on the rendering mode. The mesh is created by passing in the positions, normals, uvs, and indices of the mesh to the `createMesh` function.","metadata":{"source":".autodoc/docs/markdown/src/scene/sprite.md"}}],["736",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/stencil-parameters.js)\n\nThe code defines a class called `StencilParameters` which holds settings for stencil testing. Stencil testing is a technique used in computer graphics to selectively render parts of a scene based on a stencil buffer. The `StencilParameters` class has several properties that can be set to configure the stencil test, including the stencil test function, reference value, and operation to perform if the test passes or fails.\n\nThe `StencilParameters` class has a constructor that takes an options object as a parameter. The options object can be used to set the various properties of the `StencilParameters` instance. If a property is not specified in the options object, a default value is used. For example, if the `func` property is not specified, the `FUNC_ALWAYS` constant is used as the default value.\n\nThe `StencilParameters` class also has a `clone` method that returns a new `StencilParameters` instance with the same property values as the original instance. This can be useful for creating a copy of a `StencilParameters` instance that can be modified without affecting the original instance.\n\nOverall, the `StencilParameters` class provides a way to configure the stencil test for rendering in the PlayCanvas engine. It can be used in conjunction with other classes and methods in the engine to create complex rendering effects. For example, it could be used to selectively render parts of a scene based on their depth or position, or to create complex masking effects. Here is an example of how the `StencilParameters` class could be used to configure the stencil test:\n\n```\nconst stencilParams = new StencilParameters({\n  func: GraphicsDevice.FUNC_EQUAL,\n  ref: 1,\n  readMask: 0xFF,\n  writeMask: 0xFF,\n  fail: GraphicsDevice.STENCILOP_ZERO,\n  zfail: GraphicsDevice.STENCILOP_ZERO,\n  zpass: GraphicsDevice.STENCILOP_REPLACE\n});\n\ngraphicsDevice.setStencilFunc(stencilParams.func, stencilParams.ref, stencilParams.readMask);\ngraphicsDevice.setStencilOperation(stencilParams.fail, stencilParams.zfail, stencilParams.zpass, stencilParams.writeMask);\n```\n## Questions: \n 1. What is the purpose of this code and how is it used in the PlayCanvas engine?\n- This code defines a class called `StencilParameters` that holds settings for stencil testing in the PlayCanvas engine's graphics device. It can be used to configure the stencil test function, reference value, operation for failed tests, and masks for reading and writing stencil values.\n\n2. What are the default values for the stencil parameters if no options are provided to the constructor?\n- If no options are provided to the constructor, the `func` parameter defaults to `FUNC_ALWAYS`, the `ref` parameter defaults to `0`, the `readMask` and `writeMask` parameters default to `0xFF`, and the `fail`, `zfail`, and `zpass` parameters all default to `STENCILOP_KEEP`.\n\n3. How can a developer create a copy of a `StencilParameters` object with the same settings?\n- A developer can call the `clone()` method on a `StencilParameters` object to create a new instance with the same settings. The cloned object will be a separate instance that can be modified independently of the original.","metadata":{"source":".autodoc/docs/markdown/src/scene/stencil-parameters.md"}}],["737",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/src/scene/texture-atlas.js)\n\n## TextureAtlas Class\n\nThe `TextureAtlas` class is a subclass of `EventHandler` that contains a number of frames from a texture. Each frame defines a region in a texture. The TextureAtlas is referenced by `Sprite`s. \n\n### Constructor\n\nThe constructor creates a new instance of the `TextureAtlas` class. It initializes two private properties `_texture` and `_frames` to `null`.\n\n### Properties\n\n#### texture\n\nThe `texture` property is a setter and getter that sets and gets the texture used by the atlas. It is of type `import('../platform/graphics/texture.js').Texture`.\n\n#### frames\n\nThe `frames` property is a setter and getter that sets and gets the frames which define portions of the texture atlas. It is of type `object`.\n\n### Methods\n\n#### setFrame(key, data)\n\nThe `setFrame` method sets a new frame in the texture atlas. It takes two parameters: `key` and `data`. `key` is the key of the frame and `data` is an object that contains the properties of the frame. The properties of the `data` object are:\n\n- `rect`: The u, v, width, height properties of the frame in pixels. It is of type `import('../core/math/vec4.js').Vec4`.\n- `pivot`: The pivot of the frame - values are between 0-1. It is of type `import('../core/math/vec2.js').Vec2`.\n- `border`: The border of the frame for 9-slicing. Values are ordered as follows: left, bottom, right, top border in pixels. It is of type `import('../core/math/vec4.js').Vec4`.\n\nIf the frame with the given `key` already exists, its properties are updated with the new values. Otherwise, a new frame is created with the given `key` and `data` properties.\n\n#### removeFrame(key)\n\nThe `removeFrame` method removes a frame from the texture atlas. It takes one parameter: `key`, which is the key of the frame to be removed.\n\n#### destroy()\n\nThe `destroy` method frees up the underlying texture owned by the atlas.\n\n### Example\n\n```javascript\nvar atlas = new pc.TextureAtlas();\natlas.frames = {\n    '0': {\n        // rect has u, v, width and height in pixels\n        rect: new pc.Vec4(0, 0, 256, 256),\n        // pivot has x, y values between 0-1 which define the point\n        // within the frame around which rotation and scale is calculated\n        pivot: new pc.Vec2(0.5, 0.5),\n        // border has left, bottom, right and top in pixels defining regions for 9-slicing\n        border: new pc.Vec4(5, 5, 5, 5)\n    },\n    '1': {\n        rect: new pc.Vec4(256, 0, 256, 256),\n        pivot: new pc.Vec2(0.5, 0.5),\n        border: new pc.Vec4(5, 5, 5, 5)\n    }\n};\n\natlas.setFrame('1', {\n    rect: new pc.Vec4(0, 0, 128, 128),\n    pivot: new pc.Vec2(0.5, 0.5),\n    border: new pc.Vec4(5, 5, 5, 5)\n});\n\natlas.removeFrame('1');\n\natlas.destroy();\n```\n\nIn the above example, a new instance of the `TextureAtlas` class is created. The `frames` property is set to an object that contains two frames with keys `0` and `1`. The `setFrame` method is called to update the properties of the frame with key `1`. The `removeFrame` method is called to remove the frame with key `1`. Finally, the `destroy` method is called to free up the underlying texture owned by the atlas.\n## Questions: \n 1. What is the purpose of the TextureAtlas class?\n    \n    The TextureAtlas class contains a number of frames from a texture, where each frame defines a region in a texture, and is referenced by Sprites.\n\n2. What are the properties of a frame in the texture atlas?\n    \n    A frame in the texture atlas has properties such as rect (u, v, width, height in pixels), pivot (x, y values between 0-1 which define the point within the frame around which rotation and scale is calculated), and border (left, bottom, right and top in pixels defining regions for 9-slicing).\n\n3. How can a developer set or remove a frame in the texture atlas?\n    \n    A developer can set a new frame in the texture atlas using the `setFrame` method, which takes a key and data object containing properties of the frame. A developer can remove a frame from the texture atlas using the `removeFrame` method, which takes a key of the frame to be removed.","metadata":{"source":".autodoc/docs/markdown/src/scene/texture-atlas.md"}}],["738",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/tsconfig.json)\n\nThis code is a configuration file for the TypeScript compiler. It specifies various options for the compiler, such as the base URL for resolving module imports, the target version of JavaScript to compile to, and the type roots to use for type checking.\n\nThe `baseUrl` option specifies the base URL for resolving module imports. This is useful for simplifying import statements, as it allows you to specify relative paths from a common base directory. For example, if the `baseUrl` is set to `\".\"`, you can import a module located in `src/foo/bar.js` like this: `import { Baz } from \"foo/bar\";`.\n\nThe `allowJs` option allows the compiler to compile JavaScript files as well as TypeScript files. This is useful if you have existing JavaScript code that you want to gradually migrate to TypeScript.\n\nThe `module` option specifies the module format to use for compiled code. In this case, it is set to `\"es6\"`, which means that the compiler will output ES6 modules. This is useful if you are targeting modern browsers or Node.js versions that support ES6 modules.\n\nThe `target` option specifies the version of JavaScript to compile to. In this case, it is set to `\"es6\"`, which means that the compiler will output ES6 syntax. This is useful if you are targeting modern browsers or Node.js versions that support ES6 syntax.\n\nThe `moduleResolution` option specifies how the compiler should resolve module imports. In this case, it is set to `\"node\"`, which means that the compiler will use Node.js-style module resolution. This is useful if you are using Node.js-style module imports in your code.\n\nThe `typeRoots` option specifies the directories to use for type checking. In this case, it includes two directories: `./node_modules/@webgpu/types` and `./node_modules/@types`. These directories contain TypeScript type definitions for various libraries and APIs, which can be used to provide better type checking and editor support.\n\nOverall, this configuration file is an important part of the PlayCanvas engine project, as it specifies how TypeScript code should be compiled and type checked. It allows developers to write modern, type-safe code that can be easily integrated into the larger project.\n## Questions: \n 1. **What is the purpose of this code?**\\\nThis code is a configuration file for the TypeScript compiler options used in the PlayCanvas engine project.\n\n2. **What is the significance of the \"allowJs\" option being set to true?**\\\nThe \"allowJs\" option being set to true allows the TypeScript compiler to also compile JavaScript files in addition to TypeScript files.\n\n3. **What is the purpose of the \"typeRoots\" option and what directories does it include?**\\\nThe \"typeRoots\" option specifies the directories where the TypeScript compiler should look for type definitions. In this case, it includes the \"@webgpu/types\" and \"@types\" directories located in the \"node_modules\" folder.","metadata":{"source":".autodoc/docs/markdown/tsconfig.md"}}],["739",{"pageContent":"[View code on GitHub](https://github.com/playcanvas/engine/types-undollar.mjs)\n\nThe code is a TypeScript preprocessor that modifies the PlayCanvas engine's TypeScript definition file. The purpose of this code is to remove type aliases from the definition file and replace them with their original type names. This is done to ensure that the definition file is compatible with the TypeScript compiler and can be used to generate accurate type information for the PlayCanvas engine.\n\nThe code first reads the contents of the `playcanvas.d.ts` file using the `fs` module. It then defines a regular expression that matches type aliases in the format `type <TYPE-ALIAS> = <ORIGINAL-TYPE>;`. The regular expression is used to remove all lines that match this pattern from the file.\n\nNext, the code replaces all occurrences of the type aliases with their original type names. It does this by iterating over all the type aliases that were removed in the previous step and replacing them with their original type names. This is done in reverse order of length to ensure that longer type names are replaced before shorter ones. This is necessary because shorter type names may be substrings of longer type names and replacing them first would result in incorrect replacements.\n\nFinally, the code exports all callback types in the definition file by adding the `export` keyword before their declaration. This is done to ensure that the callback types are included in the generated type information.\n\nOverall, this code is an important part of the PlayCanvas engine's build process. It ensures that the TypeScript definition file is compatible with the TypeScript compiler and can be used to generate accurate type information for the engine. Developers who use the PlayCanvas engine can benefit from this code by having access to accurate type information when developing their applications.\n## Questions: \n 1. What is the purpose of the `regexType` regular expression?\n    \n    The `regexType` regular expression is used to match code that defines type aliases in the PlayCanvas engine codebase.\n\n2. What is the significance of the `seenTypes` set?\n\n    The `seenTypes` set is used to keep track of all the type aliases that have been seen in the codebase, so that they can be replaced with their original type names later on.\n\n3. What is the purpose of the `debug` variable?\n\n    The `debug` variable is used to control whether or not debugging information is printed to the console during the execution of the code. If it is set to `true`, then debugging information will be printed; otherwise, it will not.","metadata":{"source":".autodoc/docs/markdown/types-undollar.md"}}]]