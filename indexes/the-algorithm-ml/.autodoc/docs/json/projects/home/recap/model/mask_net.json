{
  "fileName": "mask_net.py",
  "filePath": "projects/home/recap/model/mask_net.py",
  "url": "https://github.com/twitter/the-algorithm-ml/projects/home/recap/model/mask_net.py",
  "summary": "This code defines a neural network architecture called MaskNet, which is used for recommendation tasks. The architecture is based on the MaskBlock module, which is a feedforward neural network that takes two inputs: a \"net\" tensor and a \"mask_input\" tensor. The \"net\" tensor represents the input features, while the \"mask_input\" tensor is used to mask certain features during training. The MaskBlock module consists of three layers: a mask layer, a hidden layer, and a layer normalization layer. The mask layer reduces the dimensionality of the \"mask_input\" tensor and applies a non-linear activation function (ReLU) before mapping it back to the same dimensionality as the \"net\" tensor. The hidden layer then applies a linear transformation to the element-wise product of the \"net\" and \"mask_input\" tensors, and the output is passed through a layer normalization layer.\n\nThe MaskNet architecture consists of multiple MaskBlock modules stacked on top of each other. The number of MaskBlock modules and their configurations are specified in the MaskNetConfig object. The output of each MaskBlock module is concatenated along the feature dimension and passed through an MLP (multi-layer perceptron) if specified in the MaskNetConfig object. The output of the MLP is the final output of the MaskNet architecture.\n\nThe purpose of this code is to provide a flexible and modular neural network architecture for recommendation tasks. The MaskBlock module allows for the selective masking of input features, which can be useful for tasks where certain features are more important than others. The MaskNet architecture allows for the stacking of multiple MaskBlock modules, which can capture complex interactions between input features. The MLP layer provides additional flexibility for non-linear transformations of the concatenated output of the MaskBlock modules. Overall, this code provides a powerful tool for recommendation tasks that require a high degree of customization and flexibility. \n\nExample usage:\n\n```python\nfrom tml.projects.home.recap.model import config\nfrom masknet import MaskNet\n\n# Define MaskNet configuration\nmask_net_config = config.MaskNetConfig(\n    use_parallel=True,\n    mask_blocks=[\n        config.MaskBlockConfig(\n            input_layer_norm=True,\n            reduction_factor=0.5,\n            output_size=64\n        ),\n        config.MaskBlockConfig(\n            input_layer_norm=True,\n            reduction_factor=0.5,\n            output_size=32\n        )\n    ],\n    mlp=config.MlpConfig(\n        layer_sizes=[128, 64],\n        activations=[\"relu\", \"relu\"],\n        dropout=0.2\n    )\n)\n\n# Create MaskNet instance\nmask_net = MaskNet(mask_net_config, in_features=100)\n\n# Forward pass\ninputs = torch.randn(32, 100)\noutput = mask_net(inputs)\nprint(output[\"output\"].shape)  # torch.Size([32, 64])\n```",
  "questions": "1. What is the purpose of this code?\n- This code implements the MaskNet model for recommendation, as described in a paper by Wang et al.\n\n2. What are the inputs and outputs of the MaskNet model?\n- The input is a tensor of features, and the output is a dictionary containing the model's output tensor and a shared layer tensor.\n\n3. What is the purpose of the MaskBlock class?\n- The MaskBlock class is a building block of the MaskNet model, which applies a mask to the input tensor and passes it through a linear layer and a ReLU activation function before outputting the result."
}